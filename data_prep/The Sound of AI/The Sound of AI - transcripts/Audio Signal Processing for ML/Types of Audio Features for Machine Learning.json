{
    "jobName": "transcript-job-audio-assistant",
    "accountId": "337909742319",
    "status": "COMPLETED",
    "results": {
        "transcripts": [
            {
                "transcript": "Hi, everybody and welcome to a new exciting video and the audio processing for machine learning series. This time, we're gonna introduce a few audio features, but mainly we're gonna be focusing on strategies that we can use to categorize this different audio features. Before we get into that, we should ask a couple of very important questions. One, what are audio features? Two, why are they important to us? Well, audio features are descriptors of sound. And the basic idea here is that when we have different audio features, they will tell us a different or they will uh provide us information about different aspects of sound. And the catch here is that we can use these audio features for training our intelligent audio systems. In other words, we should identify a few of these audio features we're interested in and then pass it into onto our um machine learning system so that they will hopefully like learn patterns and then solve our task or problems. OK. So now let's review a few strategies that we can use to categorize this audio features. So here I've listed a five. Now, the basic idea here is that I want for these strategies like to be as general as possible. And by that, I mean that they would be able to deal with any type of sound really. But some of this category of strategies are specific to music type of signal. So for example, the third one here, so music as aspects and also like the first one here to a certain extent level of obstruction, but let's review this. So we have level of obstruction, temporal scope, music aspects, signal, domain and machine learning approach. For the remaining of the video, I'm gonna be delving diving into each of these strategies for categorization and we'll see what all of this like mean. OK. So let's get started with the first one. So your level of abstraction. Now this is uh one that mainly I should say uh uh kind of like covers like mu music signal more than general sound or audio. So, and we can divide this like into three levels. So low level audio features, mid-level audio features and high level audio features. Now, I've taken like this distinction and also like this picture here from this great book called Music Similarity and Retrieval and Introduction to audio and web based Strategies. So this is like a an incredible book in music information Retrieval. And I highly suggest you to check that out. But let's focus on this uh strategy like for classifying audio features. So uh what's low level features? So what are these, well, these are features that make sense for the machine, but don't make that much sense for us human beings. So basically, these are like kind of like statistical features that get extracted directly from audio and we human beings or I would say that people who are not trained in audio processing can't really understand what all of these things are. Right. Now, if we go up one level, we are at the mid level. So here we have features that start to make sense from a perceptual perspective, right? And so here we have things that are in features that are like involved with peach and beat uh attributes. So in this category, we have audio features like notes onset, which is basically like when a note like starts, we have fluctuation patterns and MF CCS. Now if we go up another level, we arrive at the high level features and these are very abstract features and these kind of like tend to map to uh musical constructs that uh we can understand and like perceive when we listen to some music. In this case, we can talk about uh instrumentation, key chords, melody, rhythm, tempo, right? So basically the idea is like the higher you go and the more abstract these audio features become OK. So now this is a way a strategy that we can use to classify all your features. But let's review another strategy and this is the temporal scope and this strategy applies to any type of sound music or non music. And so here we can uh kind of like divide the audio features into like three categories. So we have audio features that are focused that give us informa instantaneous information about the audio signal. And basically like this consider very short chunks of audio signal, which usually is around like 50 to 100 milliseconds or like 20 to 100 milliseconds like something like that. So it's like almost like instantaneous information here, it is important to uh like uh remember that the kind of like minimal resolution, temporal resolution that we human beings are capable of appreciating is around 10 milliseconds. So we can really go below that threshold because otherwise, I mean, it, it wouldn't make any sense for us on a perceptual level. OK. So then we have uh the segment level features. And so these are audio features that we can calculate on segments of audio, right? And here we're talking about seconds. So anywhere from 2 to 5, 1015, 20 seconds, even something like that. And now I can give you an example again in music because it makes a lot of sense. So these are features that give us information about uh for example, a bar or a musical phrase. And so it's something like that provides us information about something that makes sense from a musical standpoint, right? And it tends to yeah, just like aggregate something from instantaneous information from like a longer period. And finally, we have features that uh provide us information about the whole sound. So these are basically like aggregate features. So we can kind of like aggregate the results from like a lower temporal resolution features like instantaneous or segment level features. And then I don't know we can use some kind of average or more sophisticated ways of like aggregating results and then we would get like a number or like a feature vector, whatever that describes the whole sound, the whole signal. So we have only one descriptor for the whole thing, right? OK. So this is another strategy we can use to uh categorize um audio features. So what about the third one? Now, this is clearly focused on music only. And so here, if you remember like the level features and high level features, so there they started to like deal with some sort of like musical aspects, like for example, note onset kind of like relates to, to, to beat, to melody a little bit as well. And but something else uh like, for example, I don't know, like key could be related with both like harmony and pitch to a certain extent, right? So basically, like we can kind of like classify these audio features based on the musical aspects that they give us a glimpse on to. OK. And um yeah, so as I said, this is like a strategy categorization strategy that we can use mainly for music for obvious reasons, right? So what about like audio, like more in general? So do we have like also other types of like classifying uh or categorizing audio features for all sorts of audios? Yes. And this is like probably the most important kind of like strategy that you have for categorizing the different audio features. And uh this is based on like the signal domain that we are in. OK. So I know like this may sound a little bit weird but like with a few examples, it's gonna be super clear to like understand. OK. So we have certain audio features that are like in the time domain. So some of these are amplitude envelope root mean square energy or zero crossing rate. There are way more than this and we're gonna cover them in future videos. Uh But for now, uh I don't really want to get into the details of this different audio features. I just want to give you an idea why they are time domain features, right? OK. And this is because they are extracted from away from us. So from the basic raw audio. And so uh here like the time domain just like as a definition is captured in a waveform. And you should be familiar with this because like we covered this like extensively in previous videos, basically uh in a waveform what you have on the X axis is time and on the Y axis is amplitude. And so basically we can take a look at all the events which happen in a sound and the time domain audio features, extract information from this representation. So this is why we call them time domain audio features. OK. So now this is all good and well, but we have a problem and the problem is that uh sound is kind of characterized by frequency. The frequency is an extremely important descriptor of sound as we've seen in previous videos. But with the time domain representation representation, we don't have any of that, right. And so there are a bunch of other features which go under the name of like frequency domain features which actually focus on the frequency components of uh sound. So some of these features are band energy ratio spectral Centroid spectral flux spectral spread. I mean you name it you have a list which is almost endless. Once again, we're gonna cover this in uh future videos. But here I just want to give you like the high level idea, right? So here we're talking about the frequency domain, but what's the frequency domain? And how do we get to a frequency domain? Well, the basic idea here is that we take the row row audio. So like we take a signal from its time domain representation, we apply the fourier transform and we basically translate the signal from the time domain to the frequency domain. Now don't be scared like if you are not familiar with the um fourier transform because this is again something that will cover quite in detail in future videos. But basically when we apply the fourier transform to uh the time domain representation of signal, we get a spectrum which we can visualize like here. So as you can see here on the X axis, you have frequency and on the y axis you have magnitude. In other words, here we are taking a picture of the sound and analyzing which components, frequency components are present in that sound, right? And so basically here you can see like that there is like a a very like high peak here which should be, I don't know probably around 250 something uh Hertz, right? And basically like the idea here is that you have information about all the frequencies that make up a sound in this particular example here, like this spectrum is the uh kind of like the the I've obtained this by applying the fourier transform to this waveform here. So as you can see this two categories of uh audio features give us information, complementary information. So time domain audio features give us information about like stuff that has have has to do with time. Whereas frequency domain gives us information about stuff that has to do with uh frequency, right. The problem though is that we with this representations, we don't have um both of those things together. So information about time and frequency, but wouldn't it be wonderful if we had those type of audio features. Well, indeed, we have those we have time frequency domain uh features and we use the time frequency representation for those uh some of uh these um features are spectrograms, male spectrograms or constant que transform. So now let's focus just like on the first one, the spectrogram, which is the most famous one, right? So how do we obtain that? Well, we start once again from the um time to demand representation of signal and then we apply a short time fourier transform and we obtain in a spectrogram which is this guy down here. Now, once again, we're going to cover the short time fourier transform in future videos quite in detail. So I know I've made a lot of promises and I hope I'll deliver like throughout the series, but let's focus on the spectrogram for a second. Now, uh if I remember correctly, we we already, so you should have already seen spectrograms in previous videos. But let's review this once again. Basically the idea here is that in a spectrum, you have information both about time and frequency on the X axis indeed you have time whereas on the y axis you have frequencies and basically here what you see is the different frequency components at different points in time. And the amount of contribution that each frequency band has at this particular time is described through a color. In this case, the brighter the color the more the um contribution that's particular frequency band at that specific time. So, and as you can see here, we have like a lot of contribution from this uh frequency of 256 Hertz, which is A I I believe like middle C, OK. And this like uh computes correctly because if we go back to the spectrum, which is kind of like snapshot for the whole sound, we have a peak here which is most likely around 256 as well. OK. So now I hope like you have like a good understanding like of this different types of like audio features. So like the time domain audio features, the frequency domain ones and the time frequency uh features. Now uh one last way we can use to classify all your features is based on the machine learning approach that we use obviously. Like here, we can make a quite important distinction between traditional machine learning algorithms like support vector machines, logistic regression or linear regression and deep learning architectures. And we'll see why that's the case in a second. So for a traditional machine learning, what we usually do is we just look at all the possible audio features both in the audio domain and sorry, both in the time domain and in the frequency domain. And basically what we do is we hand pick the ones that we believe will work the best for the problem that we want to solve. For example, we could say, yeah, we uh we want to solve like audio classification. So we we want to pass to machine learn a support vector machine for example, some like audio files. And then we want to know whether like we have the sound of a car engine of an airplane or or a gunshot for example, right. So what we could do is like identify a few audio features that make sense. So say for example is this ridge amplitude envelope zero crossing rates and spectral flux, we would isolate them, we would extract them from the audio files and then we would feed them into the traditional machine learning algorithm so that we can train like our support vector machine for example. And then at inference time, we would just like feed the these three audio features for the audio that we want to analyze and hopefully we'll get a result which in this case appears to be car engine good. So how does this differ from deep learning? Well, we know that in deep learning and not just in the audio space, we tend to use unstructured data. For example, in image processing, we don't pass any specifically handcrafted like feature like image feature, we just pass the whole uh image as a as a pixel, right, as A as a as a kind of like collection of pixels. So in a sense, we do a similar thing here in um audio processing. So what we do is we pass uh a bunch of like unstructured audio representations. And by that, I mean, we could just pass the whole row audio in its basic like time a domain representation or we could pass spectrogram like audio features. So spectrograms males, spectrograms, constant Q transforms, sorry or we could pass even MF CCS but right now like uh I don't see like many papers uh coming out uh like with MFCC anymore, right? So basically, the idea is that we get like something like this, the whole whole spectrogram, we feed that to a deep neural network and then we get back our prediction. OK. So now let's uh review, I think it's important to like to review like the different types of like audio uh intelligent systems that we can build traditionally before the advent of machine learning, we used to use basic digital processing techniques. And so we would use like this audio DS P techniques and we would put them together and use whatever like audio features we wanted and use some kind of rules to uh for example, like classify sounds or to identify notes onsets or I don't know, like to uh recognize like the, the beat in a, in a music piece. OK. Then with the advent of the machine learning and data sets, we started to work with a traditional machine learning approaches. And here we've, we've seen it already, right? So we do some kind of like feature engineering, we start with the whole audio features in the time frequency domain and we just decide which ones of those like are worth experimenting with and using in our application. Then with the advent and the revolution of deep learning, all of this changed. And now we just pass in unstructured spectrograms or even raw audio. And the promise of deep learning is that the algorithms will be able to extract the features automatically by themselves. Without us, the human engineers doing like human hand ping of features and things like that. OK. That's it. So now you should have more or less like an idea of the different types of audio features we'll be dealing with. So uh if there's one thing that I hope you'll take away from this uh video is that's like the main categorization that we can use instead of like the signal domain. So we, and that's like what we'll be doing in future videos, we'll be reviewing uh time domain audio features, frequency domain audio features and time frequency uh features as well. OK. So what's up next then? So now you have a good picture like of the different ingredients that we are going to play around with. So namely the audio features, the next step is to understand how we extract them directly from audio. So in the next video, we're gonna look at the feature extraction pipeline both for time and frequency domain features. OK. So yeah, I hope you enjoyed this video. But before dashing off, I just want to remind you about the Sound of the Ice LA community, which is a community of people who are interested in all your processing A I music A I audio. So I really suggest you to join that if you haven't done so already. And I'll leave you the uh sign up link to the Slack community in the description below. OK? So that's it for today. I really hope you enjoyed this video and I'll see you next time. Cheers."
            }
        ],
        "audio_segments": [
            {
                "id": 0,
                "transcript": "Hi, everybody and welcome to a new exciting video and the audio processing for machine learning series. This time, we're gonna introduce a few audio features, but mainly we're gonna be focusing on strategies that we can use to categorize this different audio features.",
                "start_time": "0.33",
                "end_time": "17.51"
            },
            {
                "id": 1,
                "transcript": "Before we get into that, we should ask a couple of very important questions. One, what are audio features? Two, why are they important to us? Well, audio features are descriptors of sound. And the basic idea here is that when we have different audio features, they will tell us a different or they will uh provide us information about different aspects of sound.",
                "start_time": "17.709",
                "end_time": "43.939"
            },
            {
                "id": 2,
                "transcript": "And the catch here is that we can use these audio features for training our intelligent audio systems. In other words, we should identify a few of these audio features we're interested in and then pass it into onto our um machine learning system so that they will hopefully like learn patterns and then solve our task or problems. OK.",
                "start_time": "44.389",
                "end_time": "68.75"
            },
            {
                "id": 3,
                "transcript": "So now let's review a few strategies that we can use to categorize this audio features. So here I've listed a five. Now, the basic idea here is that I want for these strategies like to be as general as possible. And by that, I mean that they would be able to deal with any type of sound really. But some of this category of strategies are specific to music type of signal.",
                "start_time": "68.9",
                "end_time": "96.339"
            },
            {
                "id": 4,
                "transcript": "So for example, the third one here, so music as aspects and also like the first one here to a certain extent level of obstruction, but let's review this. So we have level of obstruction, temporal scope, music aspects, signal, domain and machine learning approach.",
                "start_time": "96.599",
                "end_time": "111.94"
            },
            {
                "id": 5,
                "transcript": "For the remaining of the video, I'm gonna be delving diving into each of these strategies for categorization and we'll see what all of this like mean. OK. So let's get started with the first one. So your level of abstraction. Now this is uh one that mainly I should say uh uh kind of like covers like mu music signal more than general sound or audio.",
                "start_time": "112.139",
                "end_time": "140.289"
            },
            {
                "id": 6,
                "transcript": "So, and we can divide this like into three levels. So low level audio features, mid-level audio features and high level audio features. Now, I've taken like this distinction and also like this picture here from this great book called Music Similarity and",
                "start_time": "140.44",
                "end_time": "157.85"
            },
            {
                "id": 7,
                "transcript": "Retrieval and Introduction to audio and web based Strategies. So this is like a an incredible book in music information Retrieval. And I highly suggest you to check that out. But let's focus on this uh strategy like for classifying audio features. So",
                "start_time": "157.86",
                "end_time": "175.279"
            },
            {
                "id": 8,
                "transcript": "uh what's low level features? So what are these, well, these are features that make sense for the machine, but don't make that much sense for us human beings. So basically, these are like kind of like statistical features that get extracted directly from audio and we human beings or I would say that people who are not trained in audio processing can't really understand what all of these things are. Right.",
                "start_time": "175.49",
                "end_time": "202.539"
            },
            {
                "id": 9,
                "transcript": "Now, if we go up one level, we are at the mid level. So here we have features that start to make sense from a perceptual perspective, right? And so here we have things that are in features that are like involved with peach and beat uh attributes. So in this category,",
                "start_time": "202.789",
                "end_time": "223.914"
            },
            {
                "id": 10,
                "transcript": "we have audio features like notes onset, which is basically like when a note like starts, we have fluctuation patterns and MF CCS. Now if we go up another level, we arrive at the high level features and these are very abstract features and these kind of like tend to map to uh",
                "start_time": "223.925",
                "end_time": "246.05"
            },
            {
                "id": 11,
                "transcript": "musical constructs that uh we can understand and like perceive when we listen to some music. In this case, we can talk about uh instrumentation, key chords, melody, rhythm, tempo, right? So basically the idea is like the higher you go and the more abstract these audio features become",
                "start_time": "246.27",
                "end_time": "272.839"
            },
            {
                "id": 12,
                "transcript": "OK. So now this is a way a strategy that we can use to classify all your features. But let's review another strategy and this is the temporal scope and this strategy applies to any type of sound music or non music. And so here we can uh kind of like divide the audio features into like three categories. So we have",
                "start_time": "273.19",
                "end_time": "298.549"
            },
            {
                "id": 13,
                "transcript": "audio features that are focused that give us informa instantaneous information about the audio signal. And basically like this consider very short chunks of audio signal, which usually is around like 50 to 100 milliseconds or like 20 to 100 milliseconds like something like that. So it's like almost like instantaneous information",
                "start_time": "298.92",
                "end_time": "323.579"
            },
            {
                "id": 14,
                "transcript": "here, it is important to uh like uh remember that the kind of like minimal resolution, temporal resolution that we human beings are capable of appreciating is around 10 milliseconds. So we can really go below that threshold because otherwise, I mean, it, it wouldn't make any sense for us on a perceptual level.",
                "start_time": "323.769",
                "end_time": "345.369"
            },
            {
                "id": 15,
                "transcript": "OK. So then we have uh the segment level features. And so these are audio features that we can calculate on segments of audio, right? And here we're talking about seconds. So anywhere from 2 to 5, 1015, 20 seconds, even something like that. And now I can give you an example",
                "start_time": "345.779",
                "end_time": "368.984"
            },
            {
                "id": 16,
                "transcript": "again in music because it makes a lot of sense. So these are features that give us information about uh for example, a bar or a musical phrase. And so it's something like that provides us information about something that makes sense from a musical standpoint, right? And it tends to yeah, just like aggregate something from instantaneous information from like a longer period.",
                "start_time": "368.994",
                "end_time": "392.209"
            },
            {
                "id": 17,
                "transcript": "And finally, we have features that uh provide us information about the whole sound. So these are basically like aggregate features. So we can kind of like aggregate the results from like a lower temporal resolution features like instantaneous or segment level features. And then I don't know we can use",
                "start_time": "392.859",
                "end_time": "412.059"
            },
            {
                "id": 18,
                "transcript": "some kind of average or more sophisticated ways of like aggregating results and then we would get like a number or like a feature vector, whatever that describes the whole sound, the whole signal. So we have only one descriptor for the whole thing, right? OK.",
                "start_time": "412.07",
                "end_time": "431.279"
            },
            {
                "id": 19,
                "transcript": "So this is another strategy we can use to uh categorize um audio features. So what about the third one? Now, this is clearly focused on music only. And so here, if you remember like the",
                "start_time": "431.5",
                "end_time": "447.399"
            },
            {
                "id": 20,
                "transcript": "level features and high level features, so there they started to like deal with some sort of like musical aspects, like for example, note onset kind of like relates to, to, to beat, to melody a little bit as well. And but something else uh like, for example, I don't know, like key could be related with both like harmony and pitch to a certain extent, right?",
                "start_time": "447.41",
                "end_time": "474.66"
            },
            {
                "id": 21,
                "transcript": "So basically, like we can kind of like classify these audio features based on the musical aspects that they give us a glimpse on to. OK. And um yeah, so as I said, this is like a strategy categorization strategy that we can use mainly for music for obvious reasons, right? So what about like audio, like more in general? So do we have like also other types of like classifying",
                "start_time": "474.859",
                "end_time": "504.73"
            },
            {
                "id": 22,
                "transcript": "uh or categorizing audio features for all sorts of audios? Yes. And this is like probably the most important kind of like strategy that you have for categorizing the different audio features. And uh this is based on like the signal domain that we are in. OK. So I know like this may sound a little bit weird but like with a few examples, it's gonna be super clear to like understand.",
                "start_time": "504.88",
                "end_time": "532.419"
            },
            {
                "id": 23,
                "transcript": "OK. So we have certain audio features that are like in the",
                "start_time": "532.57",
                "end_time": "537.659"
            },
            {
                "id": 24,
                "transcript": "time domain. So some of these are amplitude envelope root mean square energy or zero crossing rate. There are way more than this and we're gonna cover them in future videos. Uh But for now, uh I don't really want to get into the details of this different audio features. I just want to give you an idea why they are time domain features, right? OK. And this is because they are extracted from",
                "start_time": "537.809",
                "end_time": "567.63"
            },
            {
                "id": 25,
                "transcript": "away from us. So from the basic raw audio. And so",
                "start_time": "567.979",
                "end_time": "572.159"
            },
            {
                "id": 26,
                "transcript": "uh here like the time domain just like as a definition is captured in a waveform. And you should be familiar with this because like we covered this like extensively in previous videos, basically uh in a waveform what you have on the X axis is time and on the Y axis is amplitude. And so basically we can take a look at all the events which happen in a sound and the time domain audio features,",
                "start_time": "572.809",
                "end_time": "600.559"
            },
            {
                "id": 27,
                "transcript": "extract information from this representation. So this is why we call them time domain audio features. OK.",
                "start_time": "600.739",
                "end_time": "608.83"
            },
            {
                "id": 28,
                "transcript": "So now this is all good and well, but we have a problem and the problem is that uh sound is kind of characterized by frequency. The frequency is an extremely important descriptor of sound as we've seen in previous videos. But with the time domain representation representation, we don't have any of that, right. And so there are a bunch of other features which go",
                "start_time": "609.02",
                "end_time": "636.89"
            },
            {
                "id": 29,
                "transcript": "under the name of like frequency domain features which actually focus on the frequency components of uh sound. So some of these features are band energy ratio spectral Centroid spectral flux spectral spread. I mean you name it you have a list which is almost endless. Once again, we're gonna cover this in uh future videos. But here I just want to give you like the high level idea, right?",
                "start_time": "637.229",
                "end_time": "665.83"
            },
            {
                "id": 30,
                "transcript": "So here we're talking about the frequency domain, but what's the frequency domain? And how do we get to a frequency domain? Well, the basic idea here is that we take the row row audio. So like we take a signal from its time domain representation, we apply the fourier transform and we basically translate the signal from the time domain to the frequency domain.",
                "start_time": "666.039",
                "end_time": "692.659"
            },
            {
                "id": 31,
                "transcript": "Now don't be scared like if you are not familiar with the um fourier transform because this is again something that will cover quite in detail in future videos. But basically when we apply the fourier transform to uh the time domain representation of signal, we get a spectrum which we can visualize like here.",
                "start_time": "692.82",
                "end_time": "715.03"
            },
            {
                "id": 32,
                "transcript": "So as you can see here on the X axis, you have frequency and on the y axis you have magnitude. In other words, here we are taking a picture of the sound and analyzing which components, frequency",
                "start_time": "715.179",
                "end_time": "732.15"
            },
            {
                "id": 33,
                "transcript": "components are present in that sound, right? And so basically here you can see like that there is like a a very like high peak here which should be, I don't know probably around 250 something uh Hertz, right?",
                "start_time": "732.159",
                "end_time": "749.14"
            },
            {
                "id": 34,
                "transcript": "And basically like the idea here is that you have information about all the frequencies that make up a sound in this particular example here, like this spectrum is the uh kind of like the the I've obtained this by applying the fourier transform to this waveform here. So as you can see this two",
                "start_time": "749.39",
                "end_time": "774.64"
            },
            {
                "id": 35,
                "transcript": "categories of",
                "start_time": "775.21",
                "end_time": "777.13"
            },
            {
                "id": 36,
                "transcript": "uh audio features give us information, complementary information. So time domain audio features give us information about like stuff that has have has to do with time. Whereas frequency domain gives us information about stuff that has to do with uh frequency, right. The problem though is that we with this representations, we don't have um both of those things together. So information about time and frequency,",
                "start_time": "777.539",
                "end_time": "806.39"
            },
            {
                "id": 37,
                "transcript": "but wouldn't it be wonderful if we had those type of audio features. Well, indeed, we have those we have time frequency domain uh features and we use the time frequency representation for those uh some of",
                "start_time": "806.789",
                "end_time": "823.58"
            },
            {
                "id": 38,
                "transcript": "uh these um features are spectrograms, male spectrograms or constant que transform. So now let's focus just like on the first one, the spectrogram, which is the most famous one, right? So how do we obtain that? Well, we start once again from the um time to demand representation of signal and then we apply a short time fourier transform and we obtain",
                "start_time": "823.859",
                "end_time": "850.594"
            },
            {
                "id": 39,
                "transcript": "in a spectrogram which is this guy down here. Now, once again, we're going to cover the short time fourier transform in future videos quite in detail. So I know I've made a lot of promises and I hope I'll deliver like throughout the series, but let's focus on the spectrogram for a second. Now, uh if I remember correctly, we we already, so you should have already seen spectrograms in previous videos. But let's review this once again.",
                "start_time": "850.604",
                "end_time": "877.349"
            },
            {
                "id": 40,
                "transcript": "Basically the idea here is that in a spectrum, you have information both about time and frequency on the X axis indeed you have time whereas on the y axis you have frequencies and basically here what you see is the different frequency components at different points in time.",
                "start_time": "877.53",
                "end_time": "897.789"
            },
            {
                "id": 41,
                "transcript": "And the amount of contribution that each frequency band has at this particular time is described through a color. In this case, the brighter the color the more the um contribution that's particular frequency band at that specific time. So, and as you can see here, we have like a lot of contribution from this uh frequency of 256 Hertz, which is A I I believe like middle C,",
                "start_time": "897.919",
                "end_time": "927.609"
            },
            {
                "id": 42,
                "transcript": "OK. And this like uh computes correctly because if we go back to the spectrum, which is kind of like snapshot for the whole sound, we have a peak here which is most likely around 256 as well. OK.",
                "start_time": "928.809",
                "end_time": "947.82"
            },
            {
                "id": 43,
                "transcript": "So now I hope like you have like a good understanding like of this different types of like audio features. So like the time domain audio features, the frequency domain ones and the time frequency uh features.",
                "start_time": "947.989",
                "end_time": "962.789"
            },
            {
                "id": 44,
                "transcript": "Now uh one last way we can use to classify all your features is based on the machine learning approach that we use obviously. Like here, we can make a quite important distinction between traditional machine learning algorithms like support vector machines, logistic regression or linear regression and deep learning architectures. And we'll see why that's the case in a second. So for a traditional machine learning,",
                "start_time": "962.94",
                "end_time": "990.01"
            },
            {
                "id": 45,
                "transcript": "what we usually do is we just look at all the possible audio features both in the audio domain and sorry, both in the time domain and in the frequency domain. And basically what we do is we hand pick the ones that we believe will work the best for the problem that we want to solve. For example, we could say, yeah, we",
                "start_time": "990.309",
                "end_time": "1013.599"
            },
            {
                "id": 46,
                "transcript": "uh we want to solve like audio classification. So we we want to pass to machine learn a support vector machine for example, some like audio files. And then we want to know whether like we have the sound of a car engine of an airplane or or a gunshot for example, right. So what we could do is like identify a few",
                "start_time": "1013.739",
                "end_time": "1034.01"
            },
            {
                "id": 47,
                "transcript": "audio features that make sense. So say for example is this ridge amplitude envelope zero crossing rates and spectral flux, we would isolate them, we would extract them from the audio files and then we would feed them into the traditional machine learning algorithm so that we can train",
                "start_time": "1034.18",
                "end_time": "1050.854"
            },
            {
                "id": 48,
                "transcript": "like our support vector machine for example. And then at inference time, we would just like feed the these three audio features for the audio that we want to analyze and hopefully we'll get a result which in this case appears to be car engine",
                "start_time": "1050.864",
                "end_time": "1067.55"
            },
            {
                "id": 49,
                "transcript": "good. So how does this differ from deep learning? Well, we know that in deep learning and not just in the audio space, we tend to use unstructured data. For example, in image processing, we don't pass any specifically handcrafted like feature like image feature, we just pass the whole uh image as a as a pixel, right, as A as a as a",
                "start_time": "1067.91",
                "end_time": "1095.839"
            },
            {
                "id": 50,
                "transcript": "kind of like collection of pixels. So in a sense, we do a similar thing here in um audio processing. So what we do is we pass uh a bunch of like unstructured audio representations. And by that, I mean, we could just pass the whole row audio",
                "start_time": "1096.13",
                "end_time": "1117.26"
            },
            {
                "id": 51,
                "transcript": "in its basic like time a domain representation or we could pass spectrogram like audio features. So spectrograms males, spectrograms, constant Q transforms,",
                "start_time": "1117.479",
                "end_time": "1133.109"
            },
            {
                "id": 52,
                "transcript": "sorry or we could pass even MF CCS but right now like uh I don't see like many papers uh coming out uh like with MFCC anymore, right? So basically, the idea is that we get like something like this, the whole whole spectrogram, we feed that to a deep neural network and then we get back our prediction. OK. So now let's uh review,",
                "start_time": "1133.51",
                "end_time": "1162.239"
            },
            {
                "id": 53,
                "transcript": "I think it's important to like to review like the different types of like audio uh intelligent systems that we can build traditionally before the advent of machine learning, we used to use basic digital processing techniques. And so we would use like this audio DS P techniques and",
                "start_time": "1162.67",
                "end_time": "1182.819"
            },
            {
                "id": 54,
                "transcript": "we would put them together and use whatever like audio features we wanted and use some kind of rules to uh for example, like classify sounds or to identify notes onsets or I don't know, like to uh recognize like the, the beat in a, in a music piece. OK. Then with the advent of the machine",
                "start_time": "1183.119",
                "end_time": "1205.375"
            },
            {
                "id": 55,
                "transcript": "learning and data sets, we started to work with a traditional machine learning approaches. And here we've, we've seen it already, right? So we do some kind of like feature engineering, we start with the whole audio features in the time frequency domain and we just decide which ones of those like are worth experimenting with and using in our application.",
                "start_time": "1205.385",
                "end_time": "1227.65"
            },
            {
                "id": 56,
                "transcript": "Then with the advent and the revolution of deep learning, all of this changed. And now we just pass in unstructured spectrograms or even raw audio. And the promise of deep learning is that the algorithms will be able to extract",
                "start_time": "1228.209",
                "end_time": "1247.739"
            },
            {
                "id": 57,
                "transcript": "the features automatically by themselves. Without us, the human engineers doing like human hand ping of features and things like that. OK.",
                "start_time": "1248.15",
                "end_time": "1260.319"
            },
            {
                "id": 58,
                "transcript": "That's it. So now you should have more or less like an idea of the different types of audio features we'll be dealing with. So uh if there's one thing that I hope you'll take away from this uh video is that's like the main categorization that we can use instead of like the signal domain. So we, and that's like what we'll be doing in future videos, we'll be reviewing uh time domain audio features,",
                "start_time": "1260.739",
                "end_time": "1290.199"
            },
            {
                "id": 59,
                "transcript": "frequency domain audio features and time frequency uh features as well. OK. So what's up next then?",
                "start_time": "1290.64",
                "end_time": "1300.26"
            },
            {
                "id": 60,
                "transcript": "So now you have a good picture like of the different ingredients that we are going to play around with. So namely the audio features, the next step is to understand how we extract them directly from audio. So in the next video, we're gonna look at the feature",
                "start_time": "1300.54",
                "end_time": "1320.745"
            },
            {
                "id": 61,
                "transcript": "extraction pipeline both for time and frequency domain features. OK. So yeah, I hope you enjoyed this video. But before dashing off, I just want to remind you about the Sound of the Ice LA community, which is a community of people",
                "start_time": "1320.755",
                "end_time": "1340.962"
            },
            {
                "id": 62,
                "transcript": "who are interested in all your processing A I music A I audio. So I really suggest you to join that if you haven't done so already. And I'll leave you the uh sign up link to the Slack community in the description below. OK? So that's it for today. I really hope you enjoyed this video and I'll see you next time. Cheers.",
                "start_time": "1340.972",
                "end_time": "1361.189"
            }
        ]
    }
}