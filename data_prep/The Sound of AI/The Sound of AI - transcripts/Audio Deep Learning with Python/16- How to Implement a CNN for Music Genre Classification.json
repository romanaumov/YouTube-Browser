{
    "jobName": "transcript-job-audio-assistant",
    "accountId": "337909742319",
    "status": "COMPLETED",
    "results": {
        "transcripts": [
            {
                "transcript": "Hi, everybody and welcome to another video in the Deep Learning for audio with Python series. This time, we're gonna build a convolutional neural network for performing music genre classification. So basically, we're building on top of the previous video where we reviewed. Uh Well, we analyze what a CNN is and how it works as well as like the work that we've done on previous videos on uh music genre classification using a multi layer perception. Now, this is gonna be like a quite intense video. So just like take it and uh relax uh but this is gonna be fun for real. OK. So what I want to do first is just like provide all the different steps, high level steps that we need to go through to build uh like this um CNM. And so uh let me just like start by doing if name is equal to main, right? OK. And so here I'll just like jot down all the different steps. So let's start from the first one. So we want to create a train validation and test sets. Now we've already seen the train and test set. Uh We don't really know that much about validation set or cross validation. So, and we'll see that like in a second. But before let's go through like all the different steps here. OK? So before anything else, we build like this different like train validation and test sets, then we want to actually uh build uh the, build the CNN net, right? So once we've built the CNN network, we need to compile, compile the network and then train the CNN. And once we've trained it, we are gonna evaluate the uh CNN on the test set. And finally, this is something like that. Many of you guys have asked me uh in the previous video. So you'd like to show you how to do inference with the model uh that we've trained. So we're gonna uh make a prediction, right? So we're gonna learn how to make predictions, make prediction. Um I'd say like on a sample, right? OK. So we have a lot on our hands here. So let's get started from this uh first thing. So um what I want to um let you guys understand here is that so far we focused on only like a couple of like uh sets when we were doing like training on RDL and deep learning like models. So then we had like the train set that we use for training purposes and the test set that we use for evaluation. But the evaluation that we were doing uh was basically, so we just had like the test set. And so that's all good. Like if you're not gonna do like, I mean a lot of stuff, a lot of like hyper parameter tricking. So if you are changing like the number of epochs that we use, the number uh of like yeah, the batch size, the number of layers in the architecture or the number of neurons per layers. But if you are changing all of these things to get better results, you can just use like the test set because in a sense by doing that, you are um kind of like optimizing uh the the the results like of the model to uh the test data to this test set as well. So what you want to do with the test set is just like create like split like from the main data set and keep it there until you're done with training and hyper parameter tweaking and then just use it in the end so that uh the model you, we are sure that has never seen that data before, right? And so, and this is where the validation set comes in. So we can split like our data set into the training set, the validation and the test set. So we're gonna use the validation set for evaluating um like our model while we track all the hyper parameters and see how well like it does there. And so we optimize it also like on the validation set, but then we'll leave test that in the end so that we've never seen, the model has never seen the data before, right? OK. So how do we get that? Yeah, the splits. Well, we are gonna build a custom function for D do. So what we expect here is uh first of all A X train and now with, with XS, I'm referring like to the inputs, right? So input train and then we're gonna do an X uh validation and X uh test. And then we also wanna get the Y uh train, the Y validation and the Y test. Cool. So, and for why here I'm referring to the outputs or the targets. Cool. OK? So let's uh create like this uh this thing, right? So the, the, the, the function or let's just like use it. OK? So we are gonna use a function that's called prepare data sets. Now, this is a custom function. So we need to uh define it and here we're gonna pass in a couple, a couple of arguments. So one is uh is gonna be called the test size and uh we'll put this like to 0.25 and the other one is gonna be called the uh validation size and we'll put this uh to 0.2. Uh So what the test size uh like it tells us is basically how much of the training set we wanna use for um the, the test set, right? And for the validation size over here. We, we're basically saying like with that value that 20% of the training set that we've already separated from the test set is gonna be used for uh the validation set, right? OK. So now let's go build uh this function. So we'll define it over here. So we'll say, hey, give me uh a prepare uh data sets and we already say thats uh we want a test size and a validation size arguments. Cool. OK. So now what should we do here? So what we want to do here is first of all load in the data. So we'll do a, we want to uh load uh data as the first step. Then as a second step, what we wanna do is basically create the sh trainin test split. The first step here is create the train validation split and uh let's stop uh to this uh next steps for now, right. OK. So how do we load the data here? Uh This is straightforward and we already have a function here that we can use. And so I'm gonna use this one that, as we see here, returns X and Y for like all of our data uh fetching it from uh the JSON file that we used. And it's this guy over here uh where we extracted all the MF CCS, all the labels and we have a mapping with the indexes and the relative um labels here. And so we're gonna uh like read that file and extract the X and the Y. So the inputs and the outputs. So we'll do XY and we'll do a load data and I'm gonna pass this data path, that's a constant that I have over here. And this is my uh path. You may have something else to remember to uh just like input the right path there, right? OK. So now we have uh the data. So now what we wanna do is split these days into train and test uh sets. Uh OK. So in order to do that, we need a function that we've come to know uh quite a lot by now. And that comes from uh psychic learn. And so from psych learn model selection, uh we want to import train test split. This is a nice function that we can use to split a data into like a train and, and, and test split. OK. So here we expect X train, then we expect uh X test and then Y uh Y train and a Y test and we'll do a train test split and we'll pass in X and, and Y. And so this is gonna shuffle around the X and Y and then it's gonna split them and we, we should specify the proportion that we want to split this into. And so we have a test size argument here. Well, not surprisingly, we're gonna pass in our test size uh argument that, that the prepared data sets uh function accepts, right? And so this way we should have our X train, X test way train way test. Nice. Now, we should build the train validation split. Ok. So what we'll do is again, we'll do X train here, we'll do uh uh X validation here. And then here we again expect why train? And this time we'll have Y uh validation. Again, we want to reuse the train to splits function. But this time we're going to pass in uh X train and Y train, right? So we want to split the train set and split it into train and validation right, into two subsets. And again, like the, the test size, this time is gonna be our validation size. And so this is gonna be the percentage that's gonna be used for uh validation, right? And in this case, given we've given a 0.2 it's gonna be a 20% right? OK. So now we are, we could say like that we are basically done, right? So because we have uh X train X uh test X validation, we train, we test, we validation so we could return all of these guys and we would be done. But unfortunately, this is uh not the case. And the reason why this is not the case is but because uh tensorflow in this case is gonna uh for an X and N, sorry for a CNN, I don't know what an X and N is, but I mean, it could be like a new type of network, who knows? Right. OK. So for a CNN um tensorflow expects a 3d array uh for each right. And so far, uh this X strain here basically has samples where each sample is a two D array which should have this shape if I'm not wrong. So where 100 30 is the number of like time bins that we have and at each of these time bins, we're taking the 13 MFCC uh values right now, if you guys remember from my previous view, uh CNN and like image data where that we usually use with CNN S expect three dimensional um arrays. And that's because we have a third dimension which is the channel. And in this case, uh the channel is gonna be just a dimension uh just the mono dimensional, right? It's as if like all your data was uh gray scale images. So where you have like one value uh for each pixel that's determined by X and Y, right? OK. If it was like an R GB, we would have like three here, right? But uh we, we need to like add these extra dimension over here. So how do we do that? Well, this is like quite simple to do and uh so we'll do it on X train first. And so all we need to do here is take X train and given this is a nun pi array. So we're going to say that we, we're gonna put like three dots there and we're gonna put in a NP dot near axis, right? So with this three dots, we're basically saying, hey, give me what I have so far in terms of like the, the, the X train array and then give me an extra axis after that, right? And now X train is gonna be a four D array. So why is that? Well, because we have the number, it's not the boon, but it's the number of samples. And then we're gonna have 100 3013 and one, right? So here like the, the, the, the first dimension is just like the number of samples. So if we have 5000 samples, this is gonna be equal to 5000, right? OK. So X trainin is not the only um yeah, a ray that we need to uh add a a dimension on. So we also want to uh change uh X validation. And so we'll basically do the same here and same thing for X test. Cool. OK. So here we have XTA good. So now we should have all we need for like our training process and validation and testing, right? OK. So now we can return all of this test, split, all of this data set splits. So we'll take these guys here. So we have already written them. So I'm not gonna rewrite them uh cool. So this should be done. So if this works correctly, this function works correctly. Now, we should be able to have all of our train validation and test sets down here, which is great. Now on to the next step building the CNN. So here we need to build the, the architecture itself, the network. So what we, we're gonna use again, another like custom function for doing that. I'm not gonna write like all the instructions here like in the uh yeah here like in the main. And the main reason for that is because like, it's a lot of like uh stuff that we're gonna write. And so I don't want to like have a lot of mass. So I prefer just like to modular everything. By the way, this is a good, a very good advice if you have like a lot of instructions that go well, like together, even if you're doing just like simple scripting, just try like to, to use like either functional programming or like object oriented programming. So that, I mean, you don't end up like with a lot of like instructions that are difficult to understand. OK. So we'll do so we expect like the model itself. And so we'll create a uh function here that we'll call it uh built model. And now this function is gonna accept an argument uh that's called uh the input shape, right? So now we, we'll see what this is like in a second. But before that let's start building this function. So yeah, let me slide this down. OK. So yeah, we don't want all of that for sure. But we want uh to define BUILD model, we have the argument that's uh input shape, right? OK. So now we need to do like a bunch of things. So we want to, first of all uh create a model then this model uh is gonna be a CNN with three convolutional layers followed by max pooling uh layers. So we are gonna write first con uh layer, then we are gonna have a second con uh layer and a third com layer. Then we're gonna flatten uh the output of the convolutional layers and fit uh into dense layer. So we'll feed that into dense layer. And uh finally, we'll have an output layer that uses soft max cool. OK. So let's let's uh build all of this. But before we can do that, we need to import keras, right. OK. So we'll do a import and we should say tensorflow dot uh carers and we'll import this as carers, right? So, yeah, I'm really busy. I don't want to write too many things. OK. So we need to build the model uh initially. So we'll do a model uh that's equal to uh we should say uh KIS and uh sequential. So this is a sequential model, right? And now we want to build uh the first uh convolutional layer. So how do we do that. Well, we're gonna take the model and there's a great method that we can use on the, on the model. And it's called a here. And so here we can add a layer to a model that's basically like the, the, the idea like the semantics of model dot A. OK. So what do we wanna uh add here? So we want to add a layer and specifically uh we want to add a con to D layer. So it's convolutional uh layer. Now, we need to pass quite uh a lot of values like to, to this uh layer. So first of all, we should decide which uh not which, but how many kernels, how many filters we want in these convolutional layers. And so we'll say we want 32 filters, then we should decide uh the grid size of the kernel. And this is gonna be a quite customary three by three. And then what we wanna uh specify here is the uh type of activation that we wanna use. So the activation function and we're gonna be using R so rectified linear unit. Now, if you're not really familiar with all of these like weird terms that I'm using so uh like kernel uh great size convolution. So you can check out uh my previous video that introduced like the theory behind uh CNN si think it's gonna show up any time like over here. And so you can click that and check that out and then come back here because this is the fun stuff. OK. So now we've specified the number of kernels, the uh the size of the grid, uh the size of the kernel and the activation function that we want to use. And since we are like the first hidden layer here, we should specify the input shape, right? So what's the input shape? Well, the input shape, it's as simple as the argument input shape, right? And then we passed in the build model function, all right. But obviously, this doesn't tell us much about the input shape itself. So let's go back, let's go back here to, to the main. OK. So now let's extract the input shape. So we'll create the input shape, which is gonna be a uh topple here, right? OK. And so the input shape is gonna be given by um X train and here this is gonna be equal to uh X trainin oops, sorry, not that. So X train dot shape. OK. And then uh one. So basically, like we are taking the uh this like shape here uh of the of the X strain. We could have like taken X validation or like X test for that matter. Uh But then we are taking like the shape of at index one and then we are going to take the extreme shape and index two. And uh finally, we're gonna take the shape as index three, right? So if you guys remember I told you that X strain over here, right? X train. Yeah, over here it's a four D uh array where we have like the number of samples and then uh we have like 100 30 like time bins. And then we have 13 MF CCS and one which is like the channel like the depth, right? So we know that each sample has this shape 100 30 by 13 by one, right? And so this is the shape that we want to use like as an input for our CNN. And so here we build like this input shape uh topple, we pass it in here in the BUILD model and then we pass it down here when we are uh creating like the first convolutional layer, right? And so this is the input shape, hope this is clear, right? OK. So now we have our first convolutional uh layer. So what we want to do next is add another layer here, but this is not gonna be a convolutional layer but a max pooling uh layer which is gonna down sample our uh input. OK. So this is gonna be a max pool two D, right. OK. And so we know from our previous video that max pooling has a bunch of settings that we should set there. So uh the first thing that we want to set is the uh so called like pool size or like the grid size. And here we're gonna use like grid a pooling uh a pool size of uh three by three. Then we want to specify the strides. So, and uh the strides vertical and the horizontal are gonna be uh two by two. And here we also want to add a padding and the type of padding, the zero padding uh that we'll use uh it's the same. So we're gonna just like use like padding uh throughout like all the edges like around all of the. Um hm Yeah, all of like the, the the convolutional like output that we get out of like this first convolutional layer uh cool. So now we have max pooling as well as like a convolutional uh convolutional layer. Uh And what we want to add here is a final thing is a final layer here and this layer it's basically batch normalization. So now batch normalization is a quite complicated like mathematical, I mean like the, the mathematical process beyond uh behind ba batch normalization, it's quite uh complicated. So and as they usually like say in these cases, and it's well beyond the scope of this uh introductory course, but all you should more or less like the intuition that you can have about botch normalization is that it's a process that standardizes, that normalizes the activations in a current layer and the activations that get presented to like the subsequent layer by doing so, the great thing, the great advantage is that we uh kind of like speed up training by a lot really. So the the models are gonna convert way faster. And then the other great thing is also like that the models are going to be way more reliable. Cool. Yeah, by the way, let me know like if you want to know more about batch normalization, but as I said, like this is like quite like complicated like mathematical topic. So I don't want to cover it in this series. But if I see like that, you guys like leave a lot of comments to know what this is, I may just like create a video about just batch normalization. So let me know uh cool. So this is basically like the, the overall first convolutional layer I would say. So now we want to build another couple of this. So the second one um is gonna be basically like the uh the, the same as this. Uh But uh in uh the third layer, we're gonna change a couple of like parameters here. So here we're gonna change the uh the size of the kernel and we're gonna move it to two by two and same thing here for uh the, the max pooling the pool size here, we're gonna move it like to uh two by two as well. Cool. So now we are done with the uh convolutional layers and now the next step is to flatten the output. And so, and we know that out of like this three convolutional layers, we're just uh expecting a two dimensional array. And so we want to flatten that into a one D array. So how do we do that again? This is like very, very simple with keras and tensor flow because it's as simple as calling Kas dot uh Layers dot flatter. And we, I think like we, I don't know like if we've seen this already like in a previous video, but if we haven't, it's as simple as this, right. OK. So now we flattened uh uh the, the output uh of the convolutions. And so the next thing that we want to do is add a dense layer, a fully connected layer for classification. And so, and here we'll do again a model uh dot art but this time we're gonna use a dense layer. So we'll do Kous dot Layers dot uh dense. And I'm sure like we've seen a lot of like dense dance layers like in the, in the previous videos. So now um here like in the dense layer, if you guys remember, we should specify how many neurons we want. And uh for this uh network, we're gonna use 64 neurons. And then we should specify the type of activation and the activation here again is gonna be R so rectified linear unit. Now, if you guys don't remember what A R is, what an activation function is, again, I have a video on that which hopefully should be over there. So just like click there and, and just like go learn that. So these are all like very important things that we need to learn to like master uh like deep learning. So if you don't know about that, go check that out. OK. So now let's move on. So we've a, we've flattened the convolution output. Uh we've fed that into a dense layer. But now I in order to avoid, avoid over fitting, sorry, I'm going to add an extra thing here. And again, this is another thing that we've seen and this is uh a drop out, right? And, and I'll set a drop out to 30 dropout probability to uh 30%. Again, if you don't remember what dropout is or how to uh combat how to solve um overfitting, I have a vat on that. Just go check that out should be over here, right? OK. Cool. So now we are at the output layer, right? And the output layer here is again a um dance layer. So we'll do Kous dot Layers dot dance. But now we want as many neurons as the number of genres that we want to predict. So here guys, let's go to the to this data thing. So here we have 10 different genres and so we want 10 different neurons and each neuron obviously is going to represent a different genre. And as the activation uh this time, we want to use soft maths and if you guys remember soft, what soft maths does is it creates a probability distribution kind of like scores like for each of these like 10 neurons, 10 possible like categories. And if we add up like all of these like values for the 10 different genres, then we're gonna get one. So how do we do predictions there? Well, we just like take the the index with the highest value and we're gonna map that onto like a relative genre. Cool. So this is the apple layer. So we are done. So as I said, as a promise, this was like a quite long process to build this network. But now we are done. So we have our nice CNN uh with three combinational layers. And after each com layer, we have max bulling and we've added also batch normalization. And uh out of after that, we, we flatten the results and then we feed like the uh the one dimensional array into a dense layer. And finally, we feed all of that into soft mark classifier good. Now we are done so we can return the model. That's like the thing that we want. Uh Yeah, right. We, we, we want the most OK. So this is the model over here. Now we need to compile the network. So we've done this like multiple times already guys. So I'm gonna try to power through this. So we want to specify the optimizer here. And so we're gonna uh just like do Kas dot optimizer and I'm gonna choose atom and I'm gonna specify the learning rate, which is gonna be equal to 0.0001 right? OK. But this is not the opti what, what have I written here? The opti miser, right? OK. And then we'll do a model dot uh compile and in the model dot compile, we, we, we need to pass a bunch of things. So the first thing that we need to pass again is the optimize it, right? And that's the one that we've just built. Then uh we want to pass the loss function, we need to specify that. And in this case, we'll use the pa category cross entropy function. So let me see if I've spelled this well, if I typed this, well, the spars categorical cross entropy. Yeah, it seems fine. And then we need to pass the accurate. Uh um I think it's called metrics. Sorry, it's the metrics and the metrics that we wanna um track here is accuracy. Cool. OK. So now we need to train the model and again, we've done this already multiple times. So I'm going to power through this. So we do a model uh dot fit and now we want to pass X train. So these are the inputs uh for like the train set, then we are going to pass the labels for the training set, then we want to pass the validation data. So this is the cross validation split that we've created. Uh And uh here uh we're gonna pass in the X of validation, right? And the Y validation, right? So now we have another couple of hyper parameters that we should uh specify. So the batch size 32 and then we need to specify the number of epochs that we want to like run this uh training for and we'll speci we'll, yeah, just put in 30. Now, these are like other like high level hyper parameters that we can trick. We're not gonna do this here. Uh But uh remember guys like that, you can trick like the batch size, the number of E books as well as like other hyper parameters to, to find what works best for your uh problem, right? OK. So I think we should be done like with, with training here. So now like the, the last thing that remains to do is to evaluate the CNN on the test set. Now, let's do that. And uh in order to do that, we, we can use a uh really handy function from a KIS. So that's gonna return us the test uh error as well as the test accuracy. And so we'll do a model dot uh evaluate over here and uh we're gonna pass in the X test. So the inputs for the, for the test split as well as the targets for the test split, which is not we train but Y test. Uh Right. And we're gonna save both uh equal one. Uh Right. OK. So now let's print uh the results here. And so we'll say um accuracy on test set ease and will just pass in to format uh test uh accuracy over here. Good. I think uh we, we've, we've done like a lot of work so far. So before we, we make like any predictions, uh I feel like we should just like run the script and hopefully if I haven't made like any mistakes and probably I have made a few here, uh We should be able like to, to see like uh this and see like if, if our CNN works. So this is exciting. So let's try this. So OK, so I'm running here. So it's gonna take some time if you guys remember also from previous video to load uh like the data. So hopefully this is gonna work. So that's great. Yes, it's working now. It's gonna take some time to, to do the whole training. So I'm gonna post the video and reprise like once like the whole thing is done and here we go with the results. So it's quite exciting because uh the accuracy that we're, we're able to get on the test set is 70% which is like a good, good improvement on the previous accuracy that we were able to get on the uh with the multi-layered perception architecture, which is nice. And, and again, like it shows like how like CNN S are very effective liquid audio data and we haven't done like very like crazy things like on anything like this is a very simple architecture but still like it's a nice result like 70% on a genre classification task with like 10 genres. I mean, it's starting to, to get like nice. OK. So now let's take a look at the accuracy that we have on the train set over here like in the last ebook and it's 74 then we have the accuracy like here on the, on the validation which is like 71. Well, I mean, it's good. It's like 70%. So we should be uh OK with this nice. OK. So now uh we need to do like the, the last bit, right? So just let me close this. OK. So we need to like make a prediction on a single sample. So, so we want to do like the so called inference, right? OK. So let's write a function for doing that. And so we'll, we'll just like call it predicts. Yeah, it's very uh straightforward and we'll pass in uh an X and A Y. So this is like the uh yeah, the input data for that uh sample and this is like the, the label. So because we're gonna compare the actual label to the actual genre against the uh predicted genre. OK. So we need to, to like get X and get uh Y so how do we do that? Well, we can just like take uh any sample from the test set. Really? So let's say like we, we're gonna take like the, the sample at index 100 from uh the, the test set. And so, and for y uh we're gonna seek Y test and so this is 100 now. So we are passing X and Y into uh predict here, but obviously, like we're getting an error here because predict isn't defined yet. So we need to define predict. OK. So let's do define predicts. And so we have X and Y over here. Nice. So um in order to do a prediction, so it's, it's very simple, right? So we, we just like uh take the model, I'm not passing in the model. Yes. So we need to pass the model here, right? So the train model, so we've trained the model, we need to pass it because otherwise, how are we gonna perform like the prediction? So we need the model. So we get, so here we need model as an argument. And so we'll, we'll do a model dot uh pre six, right? And then we'll pass in X. So we'll, we'll pass in uh the input data uh for that sample and then we're gonna get a prediction predictions or prediction, right. OK. So really here I'm lying to you because like X in itself is not gonna, it's not gonna be enough So we need to change this, right. So because uh X so if we analyze X so X uh is gonna be a two dimensional parade two dimensional array. Well, sorry, in this case, uh is gonna be a three dimensional array, right? So 100 30 by 13 by one, right? Uh But what uh model that predicts expects is a four dimensional array, right? And the fourth dimension should be like here at the beginning, this guy here. And it basically like uh is used like to specify like the, the number of samples that we are we want to predict. And that's done because we, when we use model do predict, usually we pass in a batch of samples that we want to predict. And so, and these are gonna be like, I mean, we, we need to specify all of these different samples and this means that we need an extra dimension for doing that. In our case, we're just gonna be like a one here, right? Uh OK. So how do we do that? Well, we've already seen how to like augment an array with an extra dimension. So we'll, we'll do that once again. So we'll take X and we'll see that we'll do a MP dot uh new axis and then uh we're just gonna pass in the dots. And so basically, we are inserting a new access like at the beginning of the array and then we are copying like all the rest. OK. And so now this should work. OK. So uh we make the prediction but what we should understand is that the prediction that we get is a uh two dimensional array. So like this prediction here, prediction is a two dimensional array. And we have values over here uh where like we have basically 10 values and the 10 values represent the different scores for the 10 different genres, right? And so this is like the the results like of the of the soft mats activation function. So we are not really like at the, at the point where we already have like a prediction. So we have like the predicted index. We need to extract that from these two dimensional array. So what we want to do here is to uh get the max the index where we have the max value. So extracts um index with max volume. So oops. So we'll do a pre see it index and uh we're gonna use a nice utility uh function from Nimai that's called marks. And uh we pass in uh prediction and we'll specify that we want to uh calculate the um the, the, the max on the axis number one, which is basically like on this guy here, right? And what we're gonna get out of this is a uh one dimensional array where we have a value like this like between zero and nine in this case. And that's gonna be like the index that's been predicted. And now we could potentially uh take this index and map it onto like a genre label and we could use like this mapping here. So like, for example, here we know that disco like is zero reggae one, but I'm not gonna do that because I mean, I don't want to, right? You guys can do that. Well, actually it's a nice like exercise for you. OK? But now we have the uh predicted index. So now let's do a uh print where uh we say uh what do we want to say here? So we wanna say the, so the expected um output or expected index is equal to are available and the predicted uh index is equal to another variable. And so let's fill in the variables here and the expected index is this Y variable over there Y argument and the predicted in index is just predicted index here. Cool. OK. So this should be working now. So what I'm gonna do is I'm gonna rerun the script and obviously it's gonna take some time because it's gonna like retrain everything. But then by the end of this, we're gonna try to predict the sample, the sample at index 100 like in the test set, right? And see if the, the model is predicting it correctly. So now let me run the scripts, I'll post the video and just go back, come back when uh we have a results and here we are back guys. So here we have our results. So the expected index for uh our sample was nine, which we know is uh yeah, let's take it here. It's metal, right? So this was a uh as a metal sample and the predicted index was nine good. OK. So the uh the model I performed correctly in this instance. Nice. So guys, we are done, this was like a quite intense video and I hope you like you really enjoyed that because now you know how to build like a CNN uh classifier. And this like music genre classifier is doing like pretty well overall. Uh So like for next video, we're gonna start looking into recurrent neural networks. So which are like another architecture, another type of architecture that's very important like with audio data, like music data more specifically because like we can interpret that as like time series, right? And so like next time it's gonna be all about like the theory behind R and M si really hope, you know, like you enjoyed this video. If that's the case, please remember to subscribe if you have any questions and you may have some now because like this was quite intense, just like write them like in the comments section below and I'll see you next time. Cheers."
            }
        ],
        "audio_segments": [
            {
                "id": 0,
                "transcript": "Hi, everybody and welcome to another video in the Deep Learning for audio with Python series. This time, we're gonna build a convolutional neural network for performing music genre classification. So basically, we're building on top of the previous video where we reviewed. Uh Well, we analyze what a CNN is and how it works as well as like the work that we've done on previous videos on uh music genre classification using a multi layer perception.",
                "start_time": "0.0",
                "end_time": "27.19"
            },
            {
                "id": 1,
                "transcript": "Now, this is gonna be like a quite intense video. So just like take it and uh relax uh but this is gonna be fun for real. OK. So what I want to do first is just like provide all the different steps, high level steps that we need to go through to build uh like this um CNM. And so uh let me just like start by doing if name is equal to main, right?",
                "start_time": "27.52",
                "end_time": "57.13"
            },
            {
                "id": 2,
                "transcript": "OK. And so here I'll just like jot down all the different steps. So let's start from the first one. So we want to create a train validation and test sets. Now we've already seen the train and test set. Uh We don't really know that much about validation set or cross validation. So, and we'll see that like in a second. But before let's go through like all the different steps here.",
                "start_time": "57.38",
                "end_time": "84.98"
            },
            {
                "id": 3,
                "transcript": "OK? So before anything else, we build like this different like train validation and test sets, then we want to actually uh build uh the, build the CNN net,",
                "start_time": "85.309",
                "end_time": "100.239"
            },
            {
                "id": 4,
                "transcript": "right? So once we've built the CNN network, we need to compile, compile the network",
                "start_time": "101.66",
                "end_time": "109.269"
            },
            {
                "id": 5,
                "transcript": "and then train the CNN.",
                "start_time": "110.19",
                "end_time": "114.41"
            },
            {
                "id": 6,
                "transcript": "And once we've trained it, we are gonna evaluate the uh CNN on the test set.",
                "start_time": "115.22",
                "end_time": "126.4"
            },
            {
                "id": 7,
                "transcript": "And finally, this is something like that. Many of you guys have asked me uh in the previous video. So you'd like to show you how to do inference with the model uh that we've trained. So we're gonna uh make a prediction, right? So we're gonna learn how to make predictions, make prediction. Um I'd say like on a sample, right? OK. So we have a lot on our hands here. So let's get started from this uh first thing.",
                "start_time": "127.449",
                "end_time": "156.809"
            },
            {
                "id": 8,
                "transcript": "So um what I want to um let you guys understand here is that so far we focused on only like a couple of like uh sets when we were doing like training on RDL and deep learning like models. So then we had like the train set that we use for training purposes and the test set that we use for evaluation.",
                "start_time": "157.07",
                "end_time": "178.91"
            },
            {
                "id": 9,
                "transcript": "But the evaluation that we were doing uh was basically, so we just had like the test set. And so that's all good. Like if you're not gonna do like, I mean a lot of stuff, a lot of like hyper parameter tricking. So if you are changing like the number of epochs that we use, the number uh of like yeah, the batch size, the number of layers in the architecture or the number of neurons per layers. But if you are changing all of these things to get better results,",
                "start_time": "179.039",
                "end_time": "207.199"
            },
            {
                "id": 10,
                "transcript": "you can just use like the test set because in a sense by doing that, you are um kind of like optimizing uh the the the results like of the model to uh the test data to this test set as well.",
                "start_time": "207.429",
                "end_time": "223.539"
            },
            {
                "id": 11,
                "transcript": "So what you want to do with the test set is just like create like split like from the main data set and keep it there until you're done with training and hyper parameter tweaking and then just use it in the end so that uh the model you, we are sure that has never seen that data before, right? And so, and this is where the validation set comes in.",
                "start_time": "223.85",
                "end_time": "248.83"
            },
            {
                "id": 12,
                "transcript": "So we can split like our data set into the training set, the validation and the test set. So we're gonna use the validation set for evaluating um like our model while we track all the hyper parameters and see how well like it does there. And so we optimize it also like on the validation set, but then we'll leave",
                "start_time": "249.059",
                "end_time": "271.054"
            },
            {
                "id": 13,
                "transcript": "test that in the end so that we've never seen, the model has never seen the data before, right? OK. So how do we get that? Yeah, the splits. Well, we are gonna build a custom function for D do. So what we expect here is uh first of all A X train and now with, with XS, I'm referring like to the inputs, right?",
                "start_time": "271.065",
                "end_time": "298.0"
            },
            {
                "id": 14,
                "transcript": "So input train and then we're gonna do an X uh validation and X uh test. And then we also wanna get the Y uh train,",
                "start_time": "298.45",
                "end_time": "312.549"
            },
            {
                "id": 15,
                "transcript": "the Y validation and the Y test.",
                "start_time": "313.529",
                "end_time": "318.279"
            },
            {
                "id": 16,
                "transcript": "Cool. So, and for why here I'm referring to the outputs or the targets. Cool. OK? So let's uh create like this uh this thing, right? So the, the, the, the function or let's just like use it. OK? So we are gonna use a function that's called prepare data sets.",
                "start_time": "318.799",
                "end_time": "341.799"
            },
            {
                "id": 17,
                "transcript": "Now, this is a custom function. So we need to uh define it and here we're gonna pass in a couple, a couple of arguments. So one is uh is gonna be called the test size and uh we'll put this like to 0.25 and the other one is gonna be called the uh validation size",
                "start_time": "342.44",
                "end_time": "362.399"
            },
            {
                "id": 18,
                "transcript": "and we'll put this uh to 0.2. Uh So what the test size uh like it tells us is basically how much of the training set we wanna use for um the, the test set, right? And for the validation size over here. We, we're basically saying like with that value that",
                "start_time": "362.91",
                "end_time": "384.019"
            },
            {
                "id": 19,
                "transcript": "20% of the training set that we've already separated from the test set is gonna be used for uh the validation set, right? OK. So now let's go build uh this function. So we'll define it over here.",
                "start_time": "384.25",
                "end_time": "399.429"
            },
            {
                "id": 20,
                "transcript": "So we'll say, hey, give me uh a prepare uh data sets and we already say thats uh we want a test size and a validation size arguments.",
                "start_time": "399.44",
                "end_time": "414.63"
            },
            {
                "id": 21,
                "transcript": "Cool. OK. So now what should we do here? So what we want to do here is first of all load in the data. So we'll do a, we want to uh load",
                "start_time": "415.35",
                "end_time": "430.69"
            },
            {
                "id": 22,
                "transcript": "uh data as the first step. Then as a second step, what we wanna do is basically create the sh trainin",
                "start_time": "431.5",
                "end_time": "442.309"
            },
            {
                "id": 23,
                "transcript": "test split. The first step here is create the train validation split and uh let's stop uh to this uh next steps for now, right. OK. So how do we load the data here? Uh This is straightforward and we already have a function here that we can use. And so I'm gonna use this one that, as we see here, returns X and Y for like all of our data",
                "start_time": "443.049",
                "end_time": "472.619"
            },
            {
                "id": 24,
                "transcript": "uh fetching it from uh the JSON file that we used. And it's this guy over here uh where we extracted all the MF CCS, all the labels and we have a mapping with the indexes and the relative um labels here. And so we're gonna uh like read that file and extract the X and the Y.",
                "start_time": "472.769",
                "end_time": "494.869"
            },
            {
                "id": 25,
                "transcript": "So the inputs and the outputs. So we'll do XY and we'll do a load data and I'm gonna pass this data path, that's a constant that I have over here. And this is my uh path. You may have something else to remember to uh just like input the right path there, right? OK. So now we have uh the data. So now what we wanna do is split these days into train and test uh sets.",
                "start_time": "495.32",
                "end_time": "524.58"
            },
            {
                "id": 26,
                "transcript": "Uh OK. So in order to do that, we need a function that we've come to know uh quite a lot by now. And that comes from uh psychic learn. And so from psych learn model selection, uh we want to import",
                "start_time": "525.52",
                "end_time": "543.679"
            },
            {
                "id": 27,
                "transcript": "train test split. This is a nice function that we can use to split a data into like a train and, and, and test split. OK. So here we expect X train,",
                "start_time": "544.299",
                "end_time": "557.26"
            },
            {
                "id": 28,
                "transcript": "then we expect uh X test and then Y uh",
                "start_time": "558.27",
                "end_time": "565.539"
            },
            {
                "id": 29,
                "transcript": "Y train and a Y test and we'll do a train test split and we'll pass in X and, and Y. And so this is gonna shuffle around the X and Y and then it's gonna split them and we, we should specify the proportion that we want to split this into. And so we have a test size argument here. Well, not surprisingly, we're gonna pass in our test size",
                "start_time": "566.07",
                "end_time": "594.169"
            },
            {
                "id": 30,
                "transcript": "uh argument that, that the prepared data sets uh function accepts, right? And so this way we should have our X train, X test way train way test. Nice. Now, we should build the train validation split. Ok. So what we'll do is again, we'll do X train here, we'll do uh uh X validation here.",
                "start_time": "594.32",
                "end_time": "620.21"
            },
            {
                "id": 31,
                "transcript": "And then here we again expect why train? And this time we'll have Y uh validation.",
                "start_time": "620.369",
                "end_time": "627.45"
            },
            {
                "id": 32,
                "transcript": "Again, we want to reuse the train to splits function. But this time we're going to pass in uh X train",
                "start_time": "628.049",
                "end_time": "636.349"
            },
            {
                "id": 33,
                "transcript": "and Y train, right? So we want to split the train set and split it into train and validation right, into two subsets. And again, like the, the test size, this time is gonna be our validation size. And so this is gonna be the percentage that's gonna be used for uh validation, right? And in this case, given we've given a 0.2 it's gonna be a 20% right?",
                "start_time": "637.13",
                "end_time": "665.83"
            },
            {
                "id": 34,
                "transcript": "OK. So now we are, we could say like that we are basically done, right? So because we have uh X train X uh test X validation, we train, we test, we validation so we could return all of these guys and we would be done. But unfortunately,",
                "start_time": "666.08",
                "end_time": "683.21"
            },
            {
                "id": 35,
                "transcript": "this is uh not the case. And the reason why this is not the case is but because uh tensorflow in this case is gonna uh for an X and N, sorry for a CNN, I don't know what an X and N is, but I mean, it could be like a new type of network, who knows? Right. OK. So for a CNN um tensorflow expects a 3d array uh for each",
                "start_time": "683.4",
                "end_time": "713.13"
            },
            {
                "id": 36,
                "transcript": "right. And so far, uh this X strain here basically has samples where each sample is a two D array which should have this shape if I'm not wrong. So where 100 30 is the number of like time bins that we have and at each of these time bins, we're taking the 13 MFCC uh values right now, if you guys remember from my previous view,",
                "start_time": "713.539",
                "end_time": "742.15"
            },
            {
                "id": 37,
                "transcript": "uh CNN and like image data where that we usually use with CNN S expect three dimensional um arrays. And that's because we have a third dimension which is the channel. And in this case, uh the channel is gonna be just a dimension uh just the mono dimensional, right? It's as if",
                "start_time": "742.159",
                "end_time": "766.02"
            },
            {
                "id": 38,
                "transcript": "like all your data was uh gray scale images. So where you have like one value uh for each pixel that's determined by X and Y, right? OK. If it was like an R GB, we would have like three here, right? But uh we, we need to like add these extra dimension over here. So how do we do that? Well, this is like quite simple to do",
                "start_time": "766.03",
                "end_time": "793.64"
            },
            {
                "id": 39,
                "transcript": "and uh so we'll do it on X train first. And so all we need to do here is take X train and given this is a nun pi array. So we're going to say that we,",
                "start_time": "794.38",
                "end_time": "806.69"
            },
            {
                "id": 40,
                "transcript": "we're gonna put like three dots there and we're gonna put in a NP dot near axis, right? So with this three dots, we're basically saying, hey, give me what I have so far in terms of like the, the, the X train array and then give me an extra axis after that, right? And now X train is gonna be a four D array. So why is that?",
                "start_time": "808.28",
                "end_time": "836.099"
            },
            {
                "id": 41,
                "transcript": "Well, because we have the number, it's not the boon, but it's the number of samples. And then we're gonna have 100 3013 and one, right? So here like the, the, the, the first dimension is just like the number of samples. So if we have 5000 samples, this is gonna be equal to 5000, right? OK. So X trainin is not the only",
                "start_time": "836.64",
                "end_time": "865.7"
            },
            {
                "id": 42,
                "transcript": "um",
                "start_time": "865.83",
                "end_time": "866.65"
            },
            {
                "id": 43,
                "transcript": "yeah, a ray that we need to uh add a a dimension on. So we also want to uh change uh X validation. And so we'll basically do the same here",
                "start_time": "867.609",
                "end_time": "880.34"
            },
            {
                "id": 44,
                "transcript": "and same thing for X test.",
                "start_time": "880.969",
                "end_time": "883.78"
            },
            {
                "id": 45,
                "transcript": "Cool.",
                "start_time": "884.82",
                "end_time": "885.58"
            },
            {
                "id": 46,
                "transcript": "OK. So here we have XTA",
                "start_time": "886.28",
                "end_time": "889.239"
            },
            {
                "id": 47,
                "transcript": "good. So now we should have all we need for like our training process and validation and testing, right? OK. So now we can return all of this test, split, all of this data set splits. So we'll take these guys here. So we have already written them. So I'm not gonna rewrite them",
                "start_time": "889.88",
                "end_time": "912.309"
            },
            {
                "id": 48,
                "transcript": "uh cool. So this should be done. So if this works correctly, this function works correctly. Now, we should be able to have all of our train validation and test sets down here, which is great. Now on to the next step building the CNN. So here we need to build the, the architecture itself, the network.",
                "start_time": "913.01",
                "end_time": "935.099"
            },
            {
                "id": 49,
                "transcript": "So what we, we're gonna use again, another like custom function for doing that. I'm not gonna write like all the instructions here like in the uh yeah here like in the main. And the main reason for that is because like, it's a lot of like uh stuff that we're gonna write. And so I don't want to like have a lot of mass. So I prefer just like to modular everything. By the way, this is a good, a very good advice if you have like a lot of instructions that go well, like together, even if you're doing just like simple scripting,",
                "start_time": "935.309",
                "end_time": "965.289"
            },
            {
                "id": 50,
                "transcript": "just try like to, to use like either functional programming or like object oriented programming. So that, I mean, you don't end up like with a lot of like instructions that are difficult to understand.",
                "start_time": "965.479",
                "end_time": "977.83"
            },
            {
                "id": 51,
                "transcript": "OK. So we'll do so we expect like the model itself. And so we'll create a uh function here that we'll call it uh built model. And now this function is gonna accept an argument uh that's called uh the input shape, right? So now we, we'll see what this is like in a second. But before that let's start building this function.",
                "start_time": "978.4",
                "end_time": "1006.409"
            },
            {
                "id": 52,
                "transcript": "So yeah, let me slide this down. OK. So yeah, we don't want all of that for sure. But we want uh to define BUILD model, we have the argument that's uh input shape, right? OK. So now we need to do like a bunch of things. So we want to, first of all uh create a model then this model",
                "start_time": "1007.179",
                "end_time": "1032.27"
            },
            {
                "id": 53,
                "transcript": "uh is gonna be a CNN with three convolutional layers followed by max pooling uh layers. So we are gonna write first con uh layer, then we are gonna have a second con uh layer",
                "start_time": "1032.448",
                "end_time": "1052.77"
            },
            {
                "id": 54,
                "transcript": "and a third com layer.",
                "start_time": "1053.52",
                "end_time": "1056.56"
            },
            {
                "id": 55,
                "transcript": "Then we're gonna flatten",
                "start_time": "1057.75",
                "end_time": "1060.41"
            },
            {
                "id": 56,
                "transcript": "uh the output of the convolutional layers",
                "start_time": "1061.689",
                "end_time": "1065.63"
            },
            {
                "id": 57,
                "transcript": "and fit",
                "start_time": "1066.38",
                "end_time": "1067.67"
            },
            {
                "id": 58,
                "transcript": "uh into dense layer. So we'll feed that into dense layer. And uh finally, we'll have an output layer that uses soft max cool. OK. So let's let's uh build all of this. But before we can do that, we need to import keras, right. OK. So we'll do a import",
                "start_time": "1068.569",
                "end_time": "1093.93"
            },
            {
                "id": 59,
                "transcript": "and we should say tensorflow dot uh carers and we'll import this as carers, right? So, yeah, I'm really busy. I don't want to write too many things. OK. So we need to build the model uh initially. So we'll do a model uh that's equal to uh we should say uh KIS and uh sequential. So this is a sequential model,",
                "start_time": "1094.54",
                "end_time": "1121.339"
            },
            {
                "id": 60,
                "transcript": "right? And now we want to build uh the first uh convolutional layer. So how do we do that. Well, we're gonna take the model and there's a great method that we can use on the, on the model. And it's called a here. And so here we can add a layer to a model that's basically like the, the, the idea like the semantics of model dot A.",
                "start_time": "1121.55",
                "end_time": "1144.829"
            },
            {
                "id": 61,
                "transcript": "OK. So what do we wanna uh add here? So we want to add a layer and specifically uh we want to add a con to D layer. So it's convolutional uh layer. Now, we need to pass quite uh a lot of values like to, to this uh layer.",
                "start_time": "1145.189",
                "end_time": "1166.06"
            },
            {
                "id": 62,
                "transcript": "So first of all, we should decide which uh not which, but how many kernels, how many filters we want in these convolutional layers. And so we'll say we want 32 filters, then we should decide uh the grid size of the kernel. And this is gonna be a quite customary three by three.",
                "start_time": "1166.229",
                "end_time": "1188.88"
            },
            {
                "id": 63,
                "transcript": "And then what we wanna uh specify here is the uh type of activation that we wanna use. So the activation function and we're gonna be using R",
                "start_time": "1189.359",
                "end_time": "1200.239"
            },
            {
                "id": 64,
                "transcript": "so rectified linear unit. Now, if you're not really familiar with all of these like weird terms that I'm using so uh like kernel uh great size convolution. So you can check out uh my previous video that introduced like the theory behind uh CNN si think it's gonna show up any time like over here. And so you can click that and check that out and then come back here because this is the fun stuff. OK.",
                "start_time": "1201.06",
                "end_time": "1230.68"
            },
            {
                "id": 65,
                "transcript": "So now we've specified the number of kernels, the uh the size of the grid, uh the size of the kernel and the activation function that we want to use. And since we are like the first hidden layer here, we should specify the input shape, right? So what's the input shape? Well, the input shape, it's as simple as the argument input shape, right? And then we passed in the build model function,",
                "start_time": "1230.839",
                "end_time": "1257.069"
            },
            {
                "id": 66,
                "transcript": "all right. But obviously, this doesn't tell us much about the input shape itself. So let's go back, let's go back here to, to the main.",
                "start_time": "1257.359",
                "end_time": "1266.589"
            },
            {
                "id": 67,
                "transcript": "OK. So now let's extract the input shape. So we'll create the input shape, which is gonna be a uh topple here, right? OK. And so the input shape is gonna be given by",
                "start_time": "1267.17",
                "end_time": "1285.439"
            },
            {
                "id": 68,
                "transcript": "um X train",
                "start_time": "1287.04",
                "end_time": "1289.739"
            },
            {
                "id": 69,
                "transcript": "and here this is gonna be equal to uh X trainin oops, sorry, not that. So X train dot shape. OK. And then uh one. So basically, like we are taking the uh this like shape here uh of the of the X strain. We could have like taken X validation or like X test for that matter.",
                "start_time": "1291.89",
                "end_time": "1319.01"
            },
            {
                "id": 70,
                "transcript": "Uh But then we are taking like the shape of at index one",
                "start_time": "1319.17",
                "end_time": "1324.92"
            },
            {
                "id": 71,
                "transcript": "and then we are going to take",
                "start_time": "1325.55",
                "end_time": "1328.479"
            },
            {
                "id": 72,
                "transcript": "the extreme shape and index two.",
                "start_time": "1330.069",
                "end_time": "1333.709"
            },
            {
                "id": 73,
                "transcript": "And uh finally, we're gonna take the shape as index three, right? So if you guys remember I told you that X strain over here, right?",
                "start_time": "1334.699",
                "end_time": "1349.89"
            },
            {
                "id": 74,
                "transcript": "X train. Yeah,",
                "start_time": "1351.459",
                "end_time": "1353.38"
            },
            {
                "id": 75,
                "transcript": "over here it's a four D uh array where we have like the number of samples and then uh we have like 100 30 like time bins. And then we have 13 MF CCS and one which is like the channel like the depth, right? So we know that each sample has this shape 100 30 by 13 by one, right? And so this is the shape that we want to use like as an input for our CNN.",
                "start_time": "1354.209",
                "end_time": "1382.199"
            },
            {
                "id": 76,
                "transcript": "And so here we build like this input shape uh topple, we pass it in here in the BUILD model and then we pass it down here when we are uh creating like the first convolutional layer, right? And so this is the input shape, hope this is clear, right? OK. So now we have our first convolutional",
                "start_time": "1382.78",
                "end_time": "1405.39"
            },
            {
                "id": 77,
                "transcript": "uh layer. So what we want to do next is add another layer here, but this is not gonna be a convolutional layer but a max pooling uh layer which is gonna down sample our uh input.",
                "start_time": "1405.52",
                "end_time": "1423.15"
            },
            {
                "id": 78,
                "transcript": "OK. So this is gonna be a max pool two D, right. OK. And so we know from our previous video that max pooling has a bunch of settings that we should set there. So uh the first thing that we want to set is the uh so called like pool size or like the grid size. And here we're gonna use like grid a pooling uh a pool size of uh three by three.",
                "start_time": "1423.869",
                "end_time": "1451.54"
            },
            {
                "id": 79,
                "transcript": "Then we want to specify the strides. So, and uh the strides vertical and the horizontal are gonna be uh two by two. And here we also want to add a padding and the type of padding, the zero padding uh that we'll use uh it's the same. So we're gonna just like use like padding uh throughout like all the edges like around all of the. Um",
                "start_time": "1451.93",
                "end_time": "1481.28"
            },
            {
                "id": 80,
                "transcript": "hm Yeah, all of like the, the the convolutional like output that we get out of like this first convolutional layer",
                "start_time": "1481.55",
                "end_time": "1490.489"
            },
            {
                "id": 81,
                "transcript": "uh cool. So now we have max pooling as well as like a convolutional uh convolutional layer. Uh And what we want to add here is a final thing is a final layer here and this layer it's basically",
                "start_time": "1491.02",
                "end_time": "1509.13"
            },
            {
                "id": 82,
                "transcript": "batch normalization. So now batch normalization is a quite complicated like mathematical, I mean like the, the mathematical process beyond uh behind ba batch normalization, it's quite uh complicated. So",
                "start_time": "1511.229",
                "end_time": "1528.13"
            },
            {
                "id": 83,
                "transcript": "and as they usually like say in these cases, and it's well beyond the scope of this uh introductory course, but all you should more or less like the intuition that you can have about botch normalization is that it's a process",
                "start_time": "1528.42",
                "end_time": "1543.8"
            },
            {
                "id": 84,
                "transcript": "that standardizes, that normalizes the activations in a current layer and the activations that get presented to like the subsequent layer by doing so, the great thing, the great advantage is that we uh kind of like speed up training by a lot really. So the the models are gonna convert way faster.",
                "start_time": "1543.81",
                "end_time": "1570.05"
            },
            {
                "id": 85,
                "transcript": "And then the other great thing is also like that the models are going to be way more reliable.",
                "start_time": "1570.209",
                "end_time": "1576.319"
            },
            {
                "id": 86,
                "transcript": "Cool.",
                "start_time": "1576.9",
                "end_time": "1577.839"
            },
            {
                "id": 87,
                "transcript": "Yeah, by the way, let me know like if you want to know more about batch normalization, but as I said, like this is like quite like complicated like mathematical topic. So I don't want to cover it in this series. But if I see like that, you guys like leave a lot of comments to know what this is, I may just like create a video about just batch normalization. So let me know",
                "start_time": "1579.229",
                "end_time": "1598.719"
            },
            {
                "id": 88,
                "transcript": "uh cool. So this is basically like the, the overall first convolutional layer I would say. So now we want to build another couple of this. So the second one",
                "start_time": "1599.329",
                "end_time": "1610.8"
            },
            {
                "id": 89,
                "transcript": "um is gonna be basically like the uh the, the same as this. Uh But uh in uh the third layer, we're gonna change a couple of like parameters here. So here we're gonna change the uh the size of the kernel and we're gonna move it to two by two and same thing here for uh the, the max pooling",
                "start_time": "1611.369",
                "end_time": "1636.27"
            },
            {
                "id": 90,
                "transcript": "the pool size here, we're gonna move it like to uh two by two as well. Cool. So now we are done with the uh convolutional layers and now the next step is to flatten the output. And so, and we know that out of like this three convolutional layers, we're just uh expecting a two dimensional array. And so we want to flatten that into a one D array.",
                "start_time": "1637.109",
                "end_time": "1664.39"
            },
            {
                "id": 91,
                "transcript": "So how do we do that again? This is like very, very simple with keras and tensor flow because it's as simple as calling Kas dot uh Layers dot flatter. And we, I think like we, I don't know like if we've seen this already like in a previous video, but if we haven't, it's as simple as this, right.",
                "start_time": "1664.54",
                "end_time": "1684.64"
            },
            {
                "id": 92,
                "transcript": "OK. So now we flattened uh uh the, the output uh of the convolutions. And so the next thing that we want to do is add a dense layer, a fully connected layer for classification.",
                "start_time": "1684.81",
                "end_time": "1698.05"
            },
            {
                "id": 93,
                "transcript": "And so, and here we'll do again a model uh dot art but this time we're gonna use a dense layer. So we'll do Kous dot Layers dot uh dense. And I'm sure like we've seen a lot of like dense dance layers like in the, in the previous videos. So now",
                "start_time": "1698.469",
                "end_time": "1720.81"
            },
            {
                "id": 94,
                "transcript": "um here like in the dense layer, if you guys remember, we should specify how many neurons we want. And uh for this uh network, we're gonna use 64 neurons. And then we should specify the type of activation and the activation here again is gonna be R so rectified linear unit. Now, if you guys don't remember what A R is, what an activation function is, again, I have a video on that which hopefully should be",
                "start_time": "1721.199",
                "end_time": "1750.68"
            },
            {
                "id": 95,
                "transcript": "over there. So just like click there and, and just like go learn that. So these are all like very important things that we need to learn to like master uh like deep learning. So if you don't know about that, go check that out. OK. So now let's move on. So we've a, we've flattened the convolution output.",
                "start_time": "1751.04",
                "end_time": "1773.31"
            },
            {
                "id": 96,
                "transcript": "Uh we've fed that into a dense layer. But now I in order to avoid, avoid over fitting, sorry, I'm going to add an extra thing here. And again, this is another thing that we've seen",
                "start_time": "1773.719",
                "end_time": "1791.099"
            },
            {
                "id": 97,
                "transcript": "and this is uh a drop out, right? And, and I'll set a drop out to 30 dropout probability to uh 30%. Again, if you don't remember what dropout is or how to uh combat how to solve um overfitting, I have a vat on that. Just go check that out should be over here, right?",
                "start_time": "1791.319",
                "end_time": "1816.15"
            },
            {
                "id": 98,
                "transcript": "OK. Cool. So now we are at the output layer, right? And the output layer here is again a um dance layer. So we'll do Kous dot Layers dot dance. But now",
                "start_time": "1817.25",
                "end_time": "1835.989"
            },
            {
                "id": 99,
                "transcript": "we want as many neurons as the number of genres that we want to predict. So here guys, let's go to the to this data thing. So here we have 10 different genres and so we want 10 different neurons and each neuron obviously is going to represent a different genre. And as the activation",
                "start_time": "1836.849",
                "end_time": "1855.699"
            },
            {
                "id": 100,
                "transcript": "uh this time, we want to use soft maths and if you guys remember soft, what soft maths does is it creates a probability distribution kind of like scores like for each of these like 10 neurons, 10 possible like categories. And if we add up like all of these like values for the 10 different genres, then we're gonna get one.",
                "start_time": "1855.939",
                "end_time": "1879.81"
            },
            {
                "id": 101,
                "transcript": "So how do we do predictions there? Well, we just like take the the index with the highest value and we're gonna map that onto like a relative genre.",
                "start_time": "1880.06",
                "end_time": "1889.05"
            },
            {
                "id": 102,
                "transcript": "Cool. So this is the apple layer. So we are done. So as I said, as a promise, this was like a quite long process to build this network. But now we are done. So we have our nice CNN uh with three combinational layers. And after each com layer, we have max bulling and we've added also batch normalization.",
                "start_time": "1889.719",
                "end_time": "1911.199"
            },
            {
                "id": 103,
                "transcript": "And uh out of after that, we, we flatten the results and then we feed like the uh the one dimensional array into a dense layer. And finally, we feed all of that into soft mark classifier",
                "start_time": "1911.449",
                "end_time": "1928.39"
            },
            {
                "id": 104,
                "transcript": "good. Now we are done so we can return the model. That's like the thing that we want. Uh Yeah, right. We, we, we want the most",
                "start_time": "1928.939",
                "end_time": "1938.359"
            },
            {
                "id": 105,
                "transcript": "OK. So this is the model over here. Now we need to compile the network. So we've done this like multiple times already guys. So I'm gonna try to power through this. So we want to specify the optimizer here. And so we're gonna uh just like do Kas dot optimizer and I'm gonna choose atom",
                "start_time": "1938.9",
                "end_time": "1962.66"
            },
            {
                "id": 106,
                "transcript": "and I'm gonna specify the learning rate, which is gonna be equal to 0.0001",
                "start_time": "1962.959",
                "end_time": "1970.26"
            },
            {
                "id": 107,
                "transcript": "right?",
                "start_time": "1970.989",
                "end_time": "1971.67"
            },
            {
                "id": 108,
                "transcript": "OK. But this is not the opti what, what have I written here? The opti miser,",
                "start_time": "1972.92",
                "end_time": "1981.15"
            },
            {
                "id": 109,
                "transcript": "right?",
                "start_time": "1981.89",
                "end_time": "1982.63"
            },
            {
                "id": 110,
                "transcript": "OK. And then we'll do a model dot uh compile",
                "start_time": "1983.209",
                "end_time": "1988.26"
            },
            {
                "id": 111,
                "transcript": "and in the model dot compile, we, we, we need to pass a bunch of things. So the first thing that we need to pass again is the optimize it, right? And that's the one that we've just built. Then uh we want to pass the loss function, we need to specify that. And in this case, we'll use the pa category",
                "start_time": "1988.829",
                "end_time": "2015.02"
            },
            {
                "id": 112,
                "transcript": "cross entropy function. So let me see if I've spelled this well, if I typed this, well, the spars categorical cross entropy. Yeah, it seems fine.",
                "start_time": "2015.43",
                "end_time": "2026.3"
            },
            {
                "id": 113,
                "transcript": "And then we need to pass the accurate. Uh um I think it's called metrics. Sorry, it's the metrics and the metrics that we wanna um track here is accuracy.",
                "start_time": "2026.869",
                "end_time": "2041.26"
            },
            {
                "id": 114,
                "transcript": "Cool. OK. So now we need to train the model and",
                "start_time": "2042.67",
                "end_time": "2048.59"
            },
            {
                "id": 115,
                "transcript": "again, we've done this already multiple times. So I'm going to power through this. So we do a model uh dot fit and now we want to pass X train. So these are the inputs uh for like the train set, then we are going to pass the labels for the training set, then we want to pass the validation data. So this is the cross validation split that we've created. Uh And uh here",
                "start_time": "2049.33",
                "end_time": "2077.53"
            },
            {
                "id": 116,
                "transcript": "uh we're gonna pass in the X of",
                "start_time": "2077.658",
                "end_time": "2081.36"
            },
            {
                "id": 117,
                "transcript": "validation, right? And the Y validation,",
                "start_time": "2082.06",
                "end_time": "2087.11"
            },
            {
                "id": 118,
                "transcript": "right? So now we have another couple of hyper parameters that we should uh specify. So the batch size 32 and then we need to specify the number of epochs that we want to like run this uh training for and we'll speci we'll, yeah, just put in 30. Now,",
                "start_time": "2087.928",
                "end_time": "2110.149"
            },
            {
                "id": 119,
                "transcript": "these are like other like high level hyper parameters that we can trick. We're not gonna do this here. Uh But uh remember guys like that, you can trick like the batch size, the number of E books as well as like other hyper parameters to, to find what works best for your uh problem,",
                "start_time": "2110.709",
                "end_time": "2129.57"
            },
            {
                "id": 120,
                "transcript": "right? OK. So I think we should be done like with, with training here. So now like the, the last thing that remains to do is to evaluate the CNN on the test set. Now, let's do that. And uh in order to do that, we, we can use",
                "start_time": "2129.82",
                "end_time": "2151.149"
            },
            {
                "id": 121,
                "transcript": "a uh really handy function from a KIS. So that's gonna return us the test uh error",
                "start_time": "2151.82",
                "end_time": "2161.6"
            },
            {
                "id": 122,
                "transcript": "as well as the test accuracy.",
                "start_time": "2162.209",
                "end_time": "2165.889"
            },
            {
                "id": 123,
                "transcript": "And so we'll do a model dot uh evaluate over here and uh we're gonna pass in the X test. So the inputs for the, for the test split as well as the targets for the test split, which is not we train but Y test. Uh Right. And we're gonna save both uh equal one.",
                "start_time": "2166.439",
                "end_time": "2190.379"
            },
            {
                "id": 124,
                "transcript": "Uh Right. OK. So now let's print uh the results here. And so we'll say um accuracy",
                "start_time": "2191.199",
                "end_time": "2202.05"
            },
            {
                "id": 125,
                "transcript": "on test set ease and will",
                "start_time": "2202.669",
                "end_time": "2208.409"
            },
            {
                "id": 126,
                "transcript": "just pass in to format uh test uh accuracy over here.",
                "start_time": "2209.79",
                "end_time": "2215.12"
            },
            {
                "id": 127,
                "transcript": "Good.",
                "start_time": "2216.07",
                "end_time": "2217.01"
            },
            {
                "id": 128,
                "transcript": "I think uh we, we've, we've done like a lot of work so far. So before we, we make like any predictions, uh I feel like we should just like run the script and hopefully if I haven't made like any mistakes and probably I have made a few here, uh We should be able like to, to see like uh this and see like if, if our CNN works. So this is exciting. So let's try this. So",
                "start_time": "2219.05",
                "end_time": "2246.479"
            },
            {
                "id": 129,
                "transcript": "OK, so I'm running here. So it's gonna take some time if you guys remember also from previous video to load uh like the data. So hopefully this is gonna work. So",
                "start_time": "2247.419",
                "end_time": "2258.5"
            },
            {
                "id": 130,
                "transcript": "that's great. Yes, it's working now. It's gonna take some time to, to do the whole training. So I'm gonna post the video and reprise like once like the whole thing is done",
                "start_time": "2259.62",
                "end_time": "2270.35"
            },
            {
                "id": 131,
                "transcript": "and here we go with the results. So it's quite exciting because uh the accuracy that we're, we're able to get on the test set is 70% which is like a good, good improvement on the previous accuracy that we were able to get on the uh with the multi-layered perception architecture, which is nice. And, and again, like it shows like how like CNN S are very effective liquid audio data",
                "start_time": "2271.469",
                "end_time": "2299.169"
            },
            {
                "id": 132,
                "transcript": "and we haven't done like very like crazy things like on anything like this is a very simple architecture but still like it's a nice result like 70% on a genre classification task with like 10 genres. I mean, it's starting to, to get like nice.",
                "start_time": "2299.51",
                "end_time": "2314.739"
            },
            {
                "id": 133,
                "transcript": "OK. So now let's take a look at the accuracy that we have on the train set over here like in the last ebook and it's 74 then we have the accuracy like here on the, on the validation which is like 71. Well, I mean,",
                "start_time": "2314.989",
                "end_time": "2330.25"
            },
            {
                "id": 134,
                "transcript": "it's good. It's like 70%. So we should be uh OK with this nice. OK. So now uh we need to do like the, the last bit, right? So just let me close this. OK. So we need to like make a prediction on a single sample. So, so we want to do like the so called inference, right? OK. So let's write a function for doing that. And so we'll,",
                "start_time": "2330.83",
                "end_time": "2359.209"
            },
            {
                "id": 135,
                "transcript": "we'll just like call it predicts. Yeah, it's very uh straightforward and we'll pass in uh an X and A Y. So this is like the uh yeah, the input data for that uh sample and this is like the, the label. So because we're gonna compare the actual label to the actual genre against the uh predicted genre.",
                "start_time": "2359.949",
                "end_time": "2385.11"
            },
            {
                "id": 136,
                "transcript": "OK. So we need to, to like get X and get uh Y so how do we do that? Well, we can just like take uh any sample from the test set. Really? So let's say like we, we're gonna take like the, the sample at index 100 from uh the, the test set. And so, and for y uh we're gonna seek Y test and so",
                "start_time": "2385.33",
                "end_time": "2414.29"
            },
            {
                "id": 137,
                "transcript": "this is 100 now.",
                "start_time": "2414.85",
                "end_time": "2416.84"
            },
            {
                "id": 138,
                "transcript": "So we are passing X and Y into uh predict here, but obviously, like we're getting an error here because predict isn't defined yet. So we need to define predict. OK. So let's do define predicts. And so we have X and Y over here.",
                "start_time": "2418.03",
                "end_time": "2438.76"
            },
            {
                "id": 139,
                "transcript": "Nice. So um in order to do a prediction, so it's, it's very simple, right? So we, we just like uh take the model,",
                "start_time": "2439.389",
                "end_time": "2451.51"
            },
            {
                "id": 140,
                "transcript": "I'm not passing in the model. Yes. So we need to pass the model here, right? So the train model, so we've trained the model, we need to pass it because otherwise, how are we gonna perform like the prediction? So we need the model. So we get, so here we need model as an argument. And so we'll, we'll do a model dot uh pre six, right?",
                "start_time": "2452.229",
                "end_time": "2481.149"
            },
            {
                "id": 141,
                "transcript": "And then we'll pass in X. So we'll, we'll pass in uh the input data uh for that sample and then we're gonna get a prediction",
                "start_time": "2481.31",
                "end_time": "2493.81"
            },
            {
                "id": 142,
                "transcript": "predictions or prediction, right.",
                "start_time": "2496.459",
                "end_time": "2500.29"
            },
            {
                "id": 143,
                "transcript": "OK. So",
                "start_time": "2500.81",
                "end_time": "2502.27"
            },
            {
                "id": 144,
                "transcript": "really here I'm lying to you because like X in itself is not gonna, it's not gonna be enough So we need to change this, right. So because uh X so if we analyze X so X uh is gonna be a two dimensional parade two dimensional array.",
                "start_time": "2503.07",
                "end_time": "2526.489"
            },
            {
                "id": 145,
                "transcript": "Well, sorry, in this case, uh is gonna be a three dimensional array, right? So 100 30 by 13 by one, right? Uh But what uh model that predicts expects is a four dimensional array, right? And the fourth dimension should be like here at the beginning,",
                "start_time": "2528.429",
                "end_time": "2556.12"
            },
            {
                "id": 146,
                "transcript": "this guy here. And it basically like uh is used like to specify like the, the number of samples that we are we want to predict. And that's done because we, when we use model do predict, usually we pass in a batch of samples that we want to predict. And so, and these are gonna be like, I mean, we, we need to specify all of these different samples and this means that we need an extra dimension for doing that. In our case, we're just gonna be like a one",
                "start_time": "2556.489",
                "end_time": "2585.87"
            },
            {
                "id": 147,
                "transcript": "here, right?",
                "start_time": "2586.949",
                "end_time": "2588.27"
            },
            {
                "id": 148,
                "transcript": "Uh OK. So how do we do that? Well, we've already seen how to like augment an array with an extra dimension. So we'll, we'll do that once again. So we'll take X and we'll see that we'll do a MP dot uh new axis and then uh we're just gonna pass in the dots. And so basically, we are inserting a new",
                "start_time": "2589.05",
                "end_time": "2614.939"
            },
            {
                "id": 149,
                "transcript": "access like at the beginning of the array and then we are copying like all the rest. OK. And so now this should work. OK. So uh we make the prediction but what we should understand is that the prediction that we get is a uh two dimensional array. So like this prediction here, prediction is a two dimensional array.",
                "start_time": "2615.199",
                "end_time": "2643.709"
            },
            {
                "id": 150,
                "transcript": "And we have values",
                "start_time": "2644.659",
                "end_time": "2647.06"
            },
            {
                "id": 151,
                "transcript": "over here",
                "start_time": "2647.949",
                "end_time": "2648.85"
            },
            {
                "id": 152,
                "transcript": "uh where like we have basically 10 values and the 10 values represent the different scores for the 10 different genres, right? And so this is like the the results like of the of the soft mats",
                "start_time": "2649.409",
                "end_time": "2666.75"
            },
            {
                "id": 153,
                "transcript": "activation function. So we are not really like at the, at the point where we already have like a prediction. So we have like the predicted index. We need to extract that from these two dimensional array. So what we want to do here is to uh get the max the index where we have the max value. So extracts",
                "start_time": "2667.35",
                "end_time": "2691.36"
            },
            {
                "id": 154,
                "transcript": "um index with max volume. So",
                "start_time": "2691.739",
                "end_time": "2697.8"
            },
            {
                "id": 155,
                "transcript": "oops. So we'll do a pre",
                "start_time": "2698.37",
                "end_time": "2702.8"
            },
            {
                "id": 156,
                "transcript": "see it",
                "start_time": "2703.489",
                "end_time": "2704.56"
            },
            {
                "id": 157,
                "transcript": "index",
                "start_time": "2705.209",
                "end_time": "2706.179"
            },
            {
                "id": 158,
                "transcript": "and uh we're gonna use a nice utility uh function from Nimai that's called marks. And uh we pass in uh prediction",
                "start_time": "2707.239",
                "end_time": "2719.929"
            },
            {
                "id": 159,
                "transcript": "and we'll specify that we want to uh calculate the um the, the, the max on the axis number one, which is basically like on this guy here, right? And what we're gonna get out of this is a uh one dimensional array where we have a value like this like between zero and nine in this case. And that's gonna be like the index that's",
                "start_time": "2720.939",
                "end_time": "2749.834"
            },
            {
                "id": 160,
                "transcript": "been predicted. And now we could potentially uh take this index and map it onto like a genre label and we could use like this mapping here. So like, for example, here we know that disco like is zero reggae one, but I'm not gonna do that because I mean, I don't want to, right? You guys can do that. Well, actually it's a nice like exercise for you. OK? But now we have the uh predicted index. So now let's do a uh print",
                "start_time": "2749.844",
                "end_time": "2777.82"
            },
            {
                "id": 161,
                "transcript": "where uh we say uh what do we want to say here? So we wanna say the, so the expected",
                "start_time": "2778.09",
                "end_time": "2788.82"
            },
            {
                "id": 162,
                "transcript": "um output or expected index",
                "start_time": "2789.75",
                "end_time": "2793.11"
            },
            {
                "id": 163,
                "transcript": "is equal",
                "start_time": "2794.04",
                "end_time": "2795.659"
            },
            {
                "id": 164,
                "transcript": "to are available and the predicted",
                "start_time": "2796.219",
                "end_time": "2801.639"
            },
            {
                "id": 165,
                "transcript": "uh index is equal to another variable. And so let's fill in the variables here and the expected index is this Y variable over there Y argument and the predicted in index is just predicted index here. Cool. OK. So this should be working now. So what I'm gonna do is I'm gonna",
                "start_time": "2802.389",
                "end_time": "2828.969"
            },
            {
                "id": 166,
                "transcript": "rerun the script and obviously it's gonna take some time because it's gonna like retrain everything. But then by the end of this, we're gonna try to predict the sample, the sample at index 100 like in the test set, right? And see if the, the model is predicting it correctly. So now let me",
                "start_time": "2829.61",
                "end_time": "2850.584"
            },
            {
                "id": 167,
                "transcript": "run the scripts, I'll post the video and just go back, come back when uh we have a results and here we are back guys. So here we have our results. So the expected index for uh our sample was nine, which we know is uh yeah, let's take it here. It's metal, right? So this was a",
                "start_time": "2850.594",
                "end_time": "2875.61"
            },
            {
                "id": 168,
                "transcript": "uh as a metal sample and the predicted index was nine good. OK. So the uh the model I performed correctly in this instance. Nice. So guys, we are done, this was like a quite intense video and I hope you like you really enjoyed that because now you know how to build like a CNN",
                "start_time": "2875.81",
                "end_time": "2899.78"
            },
            {
                "id": 169,
                "transcript": "uh classifier. And this like music genre classifier is doing like pretty well overall. Uh So like for next video, we're gonna start looking into recurrent neural networks. So which are like another architecture, another type of architecture that's very important like with audio data, like music data more specifically because like we can interpret that as like time series, right?",
                "start_time": "2899.949",
                "end_time": "2925.909"
            },
            {
                "id": 170,
                "transcript": "And so like next time it's gonna be all about like the theory behind R and M si really hope, you know, like you enjoyed this video. If that's the case, please remember to subscribe if you have any questions and you may have some now because like this was quite intense, just like",
                "start_time": "2926.169",
                "end_time": "2943.05"
            },
            {
                "id": 171,
                "transcript": "write them like in the comments section below and I'll see you next time. Cheers.",
                "start_time": "2943.689",
                "end_time": "2949.26"
            }
        ]
    }
}