{
    "jobName": "transcript-job-audio-assistant",
    "accountId": "337909742319",
    "status": "COMPLETED",
    "results": {
        "transcripts": [
            {
                "transcript": "Hi, everybody and welcome to another video in the Deep Learning for Rodeo with Python series. This time around, we're gonna talk about artificial neurons and we're gonna both understand the theory behind it and also implement them in Python. So a little bit more of a focus on what we'll be learning. So we'll have a quick look at a biological neurons. Obviously, it's not gonna be a neuro class by any means, it's just gonna be like some very introductory uh things about biological neurons. And then we're gonna move on to artificial neurons and see a little bit how the math works behind it and understanding the uh the theory there and then we'll move on and we're gonna basically implement an artificial neuron from scratch in Python. So let's get started. Uh the picture you have here is that of a biological neuron. So as you see, this is a quite complex system with a bunch of like different things, but we are mainly interested in three aspects of it. The dendrites, which are all of these filaments like down here. And basically these are input modules which are important for the neuron because they get uh signals from other neurons and they input it to the cell body, which is the operational center of the neuron. And, and basically the cell body does some kind of computation on this signal which is electric signal, it modulates it and then it passes it on along the Axion to all of these different synaptic terminals down here which are connected to other neurons. So through synapses and through the Axion terminals, basically, um what the neuron does is connect, being connected to all other neurons in this sense, the neuron can be seen as an individual that stays within a very complex system within a network where you have loads and loads of neurons connected together. Now, if you take a neuron by itself, that isn't, it's obviously remarkable, but it's not super powerful, it becomes super powerful when you put together billions of neurons. And the result that you have is basically the brain there and the power of the brain doesn't reside in the structure of the neurons themselves. But rather in the incredible number of connections we have among all of our neurons in our brains. And thanks like to, to these neurons and these connections. And we are talking here about trillions of connections of billions and billions of neurons. We can walk, we can play the piano and we can solve Sudoku and play chess, for example, right. So now let's move on to the artificial neuron. And basically the the the story goes that we looked at the uh neuron at the biological neuron and we use a ve a very simplified version of it to create artificial neurons. And in artificial neurons here, as you can see, you have a series of inputs. So it is X one, X two, X three with certain weights associated to this different inputs like W 1 W-2 W three. And as you can imagine here, uh this is the equivalent of the dendrites that we have in the biological neuron. Then we have the neuron itself uh which does a couple of things. It does uh computation in the form of a sum and an activation. And this part of the neuron in a sense can be equated to the, to the cell body. And finally, you have like the output of the neuron, right. So as I just said, so the neuron does a couple of computation, one is called the sum or the nets input and the other one, it's the activation. So let's take a look at the sum first. So here we have H which stands for the net input and H is nothing. It is just uh the sum over uh the, all the inputs multiplied by their respective weights. So in other words over here, uh H in this case is basically X one by W one plus X 2 W-2 plus X three, W three. So the first phase in the first phase, the uh artificial neuron does this sum and, and it arrives at a net input, then we have the second phase of the neuron where we have the activation itself. And so basically the output Y is a function of the activation function F where we pass in H which is the net input. So now there are a gazillion different activation functions in neural networks. But we'll be looking at one in particular right now because it's quite simple and it's very common as well called the sigmoid function. And so here on the right, you have the function Excel itself and on the left you have the graph of the function. So why is this a good function for being like an activation function? Well, first of all, it's bounded between zero and one. Then as you see here, it's a very smooth function. It doesn't have any discontinuity and this is great because it can be uh differentiated. So you can calculate the derivatives of this function quite easily. And so what this function does is basically modulating all the uh inputs and the net input into an output that's uh limited between zero and one, right. So if we take that function and we plug it into uh here, so into like this function over here, you'll see that we have this equation and this equation is basically the equation of a neuron starting from the inputs uh moving all the way to the output itself cool. So this is somewhat like simple to understand, but it's always like uh nicer to have like examples to understand what's going on, like really in detail. So let's take an example here. And so here we have three inputs again with our very simple neurons. So the first one is 0.5 then we have 0.3 and then 0.2 and then we have um the respect the respective weights over here. So 0.4 naught 0.7 and N 0.2. So now let's calculate the output. Why? By going through the two phases of computations of a neuron. So the sum and the activation. So let's calculate the sum first. And if you guys remember the, activate the sum over here is calculated by multiplying X one by um W one plus X 2 W-2 plus uh X three W three, which in our case is, is basically 0.5 by 0.4. It's these two guys over here and then 0.3 by uh 0.7 plus 0.2 uh by 0.2. So if you run the map over there, you'll find that the input, the net input is equal to 0.45. Cool. So now we have our net input, let's uh arrive at the output by using the activation function. And as we said, we're going to use the sigmoid activation function. And so basically we are plugging in this 0.45 which is our input into the activation function. And then we have the result which is no 0.61 that's the output of uh the neuron in this particular case. Now, we have an idea of how an artificial neuron uh works. So it's time to implement one from scratch in Python. Now we'll implement the artificial neuron in Python. And I'm using py charm as uh my idea of choice. Obviously, you can use whatever you want. If you want to use Jupiter notebooks, like, please feel free to do that. I'm just using Python because like I'm used it and I love it, right. So let's get started here. So the first thing that I'll do is I'll just ensure that we can run the scripts easily. So, and then what I want to do is basically replicate the um structure of the neuron that we had. So the inputs, the weights and then the calculations that we did. So the inputs will be uh represented by a list, a simple list. And then we'll have the weights which again are going to be represented by lists. So if you guys remember the example, artificial neuron or the exa mle parameters for the simple artificial neuron I used uh before, you'll remember probably that we have 0.50 0.3 and 0.2 respectively for X one, X two and uh X three. And then for the weight, we would just associate uh like the numbers respecting the SA me indexes as the input. So for W one, we would have uh 0.4. And then for W-2, we would have naught 0.7. And finally, uh 0.2 for uh W uh three. And so this way, we've basically uh recreated uh in this very simple um data format, the inputs and the weights cool. Now, the next uh phase is to calculate the output and the output is given by the activate function where we pass I both inputs and wait. And obviously, you'll see here that the activate function doesn't exist because we haven't defined it yet. So it's not going to work. Um But this is a function that takes inputs and weights as um as parameters as arguments. And obviously this is the, the computational unit of the neuron itself. And it's that function that's going to be responsible for doing uh the net for performing the net input and then the activation function itself. And then as the last step over here, we're going to do a print and we're gonna just print the output so that we can sit. Now, obviously, if you, I'm gonna run this, as you can see here, I'm gonna get an error because activate obviously hasn't been defined. So we need to define activate, which is the, the core function for our um uh for our neuron. So as we said, we define activate and we pass uh two arguments. One is called inputs. The other one is called weights and so activate, does two things and it just replicates the two phases of a neuron that we've seen in the theoretical part of things. And so the first thing is just perform net input. And the second side of things is to uh perform the activation, right? So how do we calculate the net input? So we have to loop obviously uh through inputs and weights and we have to multiply them. And so we need to multiply basically the um inputs and weights on the SA me index. And so how do we do that? Well, it's quite simple. So we, we create a for loop and we have the X and WS and I uh we use the zip function over here and we pass in inputs and weights, right? Like this. So if you guys are not familiar with the zip function, like it's a very nice function because it enables you like to unpack two lists and to uh just like pass them to like these variables over here, index by index. And so like here, for example, when we go through the loop for the first time, this X is gonna be basically inputs at index zero. And this uh W over here is gonna be weights at the index zero, right? So once we have this, then we'll have the H that's going to be, we're going to just like sum, add up the multiplication of X by uh W to H which is our net input. Obviously H is not defined here. So we need to predefined it, declare it over here. And so H is gonna start from zero and then we're gonna loop through all the inputs and weights and we're gonna multiply them and add them up to the net input. And so basically, now we're done. So the next step, once we have our H which is our net input is to perform the activation itself. So how do we do that? Well, uh it's kind of like really simple because the activation is, in our case said that we're gonna use a sigmoid function activation. So we call sigmoid and we pass INH and now we can just return this and this is the output of the neuron itself. And this is like what we're gonna get down here. And then as a final thing, we just like print this output. Now again, sigmoid hasn't been um declare it defined anywhere. So we need to define this function. And so let's define it over here. So we define sigmoid, it's a function of X. And if you uh guys remember, so this is, we can define this as 1.0 divided by one plus the exponential, which is math X of minus X over here. Now we d we haven't defined this. Uh we haven't included uh this math module. So we need to include it because otherwise we're gonna get an error and so we're gonna import math. Uh Now the error is gone and so here we can return. Why? Perfect. So now we have all of the different elements in place. And so now when we run the script, we should be able to get an output and then print the output if you guys remember by using these inputs. And these, we, we got, when we run the, the math there, we got an output for the neuron of 0.61 something like that. So let's see if we are lucky and we're going to get like the same value. And as you see here, we do have the very same value. So uh the results of this uh computation is 0.61 and that's correct. So here you have it, it's your first neuron implemented from scratch in Python. Cool. So this was it for implementing an artificial neuron in Python. So before living, I want to give you some takeaway points. So as we've seen, artificial neurons are loosely inspired to biologically neurons and neurons are computational unions and precisely what they do is they receive certain inputs and they modulate that input and using an activation function, they arrive at an output. Cool. So this was it for uh artificial neurons. So now we have like an understanding of how uh like this uh units of computation work. So what's next Well, before getting into neural networks themselves, we have to understand a little bit more of math and specifically we are going to be looking at matrix operations and vector operations. So if you've enjoyed the video, please subscribe to the channel. Just leave a like and I guess I'll see you the next time. Cheers."
            }
        ],
        "audio_segments": [
            {
                "id": 0,
                "transcript": "Hi, everybody and welcome to another video in the Deep Learning for Rodeo with Python series. This time around, we're gonna talk about artificial neurons and we're gonna both understand the theory behind it and also implement them in Python. So a little bit more of a focus on what we'll be learning. So we'll have a quick look at a biological neurons. Obviously, it's not gonna be a neuro",
                "start_time": "7.78",
                "end_time": "33.615"
            },
            {
                "id": 1,
                "transcript": "class by any means, it's just gonna be like some very introductory uh things about biological neurons. And then we're gonna move on to artificial neurons and see a little bit how the math works behind it and understanding the uh the theory there and then we'll move on and we're gonna basically implement an artificial neuron from scratch in Python. So let's get started.",
                "start_time": "33.625",
                "end_time": "59.47"
            },
            {
                "id": 2,
                "transcript": "Uh the picture you have here is that of a biological neuron. So as you see, this is a quite complex system with a bunch of like different things, but we are mainly interested in three aspects of it. The dendrites, which are all of these filaments like down here. And basically these are",
                "start_time": "60.47",
                "end_time": "82.769"
            },
            {
                "id": 3,
                "transcript": "input modules which are important for the neuron because they get uh signals from other neurons and they input it to the cell body, which is the operational center of the neuron. And, and basically the cell body does",
                "start_time": "82.779",
                "end_time": "105.08"
            },
            {
                "id": 4,
                "transcript": "some kind of computation on this signal which is electric signal, it modulates it and then it passes it on along the Axion to all of these different synaptic terminals down here which are connected to other neurons. So through synapses and through the Axion terminals, basically,",
                "start_time": "105.089",
                "end_time": "127.4"
            },
            {
                "id": 5,
                "transcript": "um what the neuron does is connect, being connected to all other neurons in this sense, the neuron can be seen as an individual that stays within a very complex system within a network where you have loads and loads of neurons connected together.",
                "start_time": "127.639",
                "end_time": "146.699"
            },
            {
                "id": 6,
                "transcript": "Now, if you take a neuron by itself, that isn't, it's obviously remarkable, but it's not super powerful, it becomes super powerful when",
                "start_time": "146.869",
                "end_time": "158.55"
            },
            {
                "id": 7,
                "transcript": "you put together billions of neurons. And the result that you have is basically the brain there and the power of the brain doesn't reside in the structure of the neurons themselves. But rather in the incredible number of connections we have among all of our neurons",
                "start_time": "158.699",
                "end_time": "177.744"
            },
            {
                "id": 8,
                "transcript": "in our brains. And thanks like to, to these neurons and these connections. And we are talking here about trillions of connections of billions and billions of neurons. We can walk, we can play the piano and we can solve Sudoku and play chess, for example, right.",
                "start_time": "177.755",
                "end_time": "197.52"
            },
            {
                "id": 9,
                "transcript": "So now let's move on to the artificial neuron. And basically the the the story goes that we looked at the uh neuron at the biological neuron and we use a ve a very simplified version of it",
                "start_time": "197.74",
                "end_time": "215.294"
            },
            {
                "id": 10,
                "transcript": "to create artificial neurons. And in artificial neurons here, as you can see, you have a series of inputs. So it is X one, X two, X three with certain weights associated to this different inputs like W 1 W-2 W three. And",
                "start_time": "215.304",
                "end_time": "232.869"
            },
            {
                "id": 11,
                "transcript": "as you can imagine here, uh this is the equivalent of the dendrites that we have in the biological neuron. Then we have the neuron itself uh which does a couple of things. It does uh computation in the form of a sum and an activation.",
                "start_time": "233.179",
                "end_time": "254.169"
            },
            {
                "id": 12,
                "transcript": "And this part of the neuron in a sense can be equated to the, to the cell body. And finally, you have like the output of the neuron,",
                "start_time": "254.339",
                "end_time": "265.609"
            },
            {
                "id": 13,
                "transcript": "right. So as I just said, so the neuron does a couple of computation, one is called the sum or the nets input and the other one, it's the activation. So let's take a look at the sum first. So here we have H which stands for the",
                "start_time": "265.98",
                "end_time": "284.589"
            },
            {
                "id": 14,
                "transcript": "net input and H is nothing. It is just uh the sum over uh the, all the inputs multiplied by their respective weights. So in other words over here, uh H in this case is basically X one by W one plus X 2 W-2 plus X three, W three.",
                "start_time": "284.6",
                "end_time": "311.6"
            },
            {
                "id": 15,
                "transcript": "So the first phase in the first phase, the uh artificial neuron does this sum and, and it arrives at a net input, then",
                "start_time": "311.899",
                "end_time": "322.399"
            },
            {
                "id": 16,
                "transcript": "we have the second phase of the neuron where we have the activation itself. And so basically the output Y is a function of the activation function F where we pass in H which is the net input. So now there are a gazillion different activation functions in neural networks. But we'll be looking at one in particular right now because it's quite simple and it's very common as well",
                "start_time": "322.57",
                "end_time": "351.404"
            },
            {
                "id": 17,
                "transcript": "called the sigmoid function. And so here on the right, you have the function Excel itself and on the left you have the graph of the function. So why is this a good function for being like an activation function? Well, first of all, it's bounded between zero and one. Then as you see here, it's a very smooth function. It doesn't have any discontinuity and this is great because it can be",
                "start_time": "351.415",
                "end_time": "380.26"
            },
            {
                "id": 18,
                "transcript": "uh differentiated. So you can calculate the derivatives of this function quite easily. And so what this function does is basically modulating all the uh inputs and the net input into an output that's uh limited between zero and one,",
                "start_time": "380.489",
                "end_time": "400.91"
            },
            {
                "id": 19,
                "transcript": "right. So if we take that function and we plug it into uh here, so into like this function over here, you'll see that we have this equation and this equation is basically the equation of a neuron starting from the inputs uh moving all the way to the output itself cool.",
                "start_time": "401.44",
                "end_time": "428.39"
            },
            {
                "id": 20,
                "transcript": "So this is somewhat like simple to understand, but it's always like uh nicer to have like examples to understand what's going on, like really in detail. So let's take an example here. And so here we have three inputs again with our very simple neurons.",
                "start_time": "428.839",
                "end_time": "448.5"
            },
            {
                "id": 21,
                "transcript": "So the first one is 0.5 then we have 0.3 and then 0.2 and then we have um the respect the respective weights over here. So 0.4 naught 0.7 and N 0.2.",
                "start_time": "448.67",
                "end_time": "461.959"
            },
            {
                "id": 22,
                "transcript": "So now let's calculate the output. Why? By going through the two phases of computations of a neuron. So the sum and the activation. So let's calculate the sum first. And if you guys remember the, activate the sum over here is calculated by multiplying X one by",
                "start_time": "462.109",
                "end_time": "485.67"
            },
            {
                "id": 23,
                "transcript": "um W one plus X 2 W-2 plus uh X three W three, which in our case is, is basically 0.5 by 0.4. It's these two guys over here and then 0.3 by uh 0.7 plus 0.2",
                "start_time": "485.91",
                "end_time": "506.98"
            },
            {
                "id": 24,
                "transcript": "uh by 0.2. So if you run the map over there, you'll find that the input, the net input is equal to 0.45. Cool. So now we have our net input,",
                "start_time": "507.119",
                "end_time": "521.799"
            },
            {
                "id": 25,
                "transcript": "let's uh arrive at the output by using the activation function. And as we said, we're going to use the sigmoid activation function. And so basically we are plugging in this 0.45 which is our input into the activation function.",
                "start_time": "521.968",
                "end_time": "539.77"
            },
            {
                "id": 26,
                "transcript": "And then we have the result which is no 0.61 that's the output of uh the neuron in this particular case. Now, we have an idea of how an artificial neuron uh works. So it's time to implement one from scratch in Python.",
                "start_time": "539.909",
                "end_time": "562.309"
            },
            {
                "id": 27,
                "transcript": "Now we'll implement the artificial neuron in Python. And I'm using py charm as uh my idea of choice. Obviously, you can use whatever you want. If you want to use Jupiter notebooks, like, please feel free to do that. I'm just using Python because like I'm used it and I love it, right. So let's get started here. So the first thing that I'll do is I'll just",
                "start_time": "562.59",
                "end_time": "590.65"
            },
            {
                "id": 28,
                "transcript": "ensure that we can run",
                "start_time": "591.299",
                "end_time": "594.609"
            },
            {
                "id": 29,
                "transcript": "the scripts easily. So, and then what I want to do is basically replicate the um structure of the neuron that we had. So the inputs, the weights and then the calculations that we did. So the inputs will be",
                "start_time": "595.39",
                "end_time": "614.89"
            },
            {
                "id": 30,
                "transcript": "uh represented by a list, a simple list. And then we'll have the weights which again are going to be represented by lists. So if you guys remember the example, artificial neuron",
                "start_time": "615.14",
                "end_time": "630.645"
            },
            {
                "id": 31,
                "transcript": "or the exa mle parameters for the simple artificial neuron I used uh before, you'll remember probably that we have 0.50 0.3 and 0.2 respectively for X one, X two and uh X three. And then for the weight, we would just associate uh like the numbers respecting the SA me indexes as the input. So for W one, we would have",
                "start_time": "630.655",
                "end_time": "660.409"
            },
            {
                "id": 32,
                "transcript": "uh 0.4. And then for W-2, we would have naught 0.7. And finally, uh 0.2 for uh W uh three. And so this way, we've basically uh recreated uh in this very simple um data format, the inputs and the weights cool. Now, the next uh phase is to calculate the output",
                "start_time": "660.63",
                "end_time": "690.469"
            },
            {
                "id": 33,
                "transcript": "and the output is given by the activate function where we pass I both inputs and",
                "start_time": "690.979",
                "end_time": "700.179"
            },
            {
                "id": 34,
                "transcript": "wait.",
                "start_time": "700.69",
                "end_time": "701.69"
            },
            {
                "id": 35,
                "transcript": "And obviously, you'll see here that the activate function doesn't exist because we haven't defined it yet. So it's not going to work.",
                "start_time": "702.809",
                "end_time": "710.799"
            },
            {
                "id": 36,
                "transcript": "Um But this is a function that takes inputs and weights as um as parameters as arguments. And obviously this is the, the computational unit of the neuron itself. And it's that function that's going to be responsible for doing uh the net for performing the net input and then the activation function itself. And then as the last step over here, we're going to do a print",
                "start_time": "710.989",
                "end_time": "739.25"
            },
            {
                "id": 37,
                "transcript": "and we're gonna just print the output so that we can sit. Now, obviously, if you, I'm gonna run this, as you can see here, I'm gonna get an error because activate obviously hasn't been defined. So we need to define activate, which is the, the core function for our",
                "start_time": "739.789",
                "end_time": "758.58"
            },
            {
                "id": 38,
                "transcript": "um uh for our neuron. So as we said, we define activate and we pass uh two arguments. One is called inputs. The other one is called weights and so activate, does two things and it just replicates the two phases of a neuron that we've seen in the theoretical part of things. And so the first thing is just perform",
                "start_time": "759.239",
                "end_time": "786.89"
            },
            {
                "id": 39,
                "transcript": "net input. And the second side of things is to uh perform the activation,",
                "start_time": "787.409",
                "end_time": "797.45"
            },
            {
                "id": 40,
                "transcript": "right? So how do we calculate the net input? So we have to loop obviously",
                "start_time": "798.599",
                "end_time": "807.409"
            },
            {
                "id": 41,
                "transcript": "uh through inputs and weights and we have to multiply them. And so we need to multiply basically the um inputs and weights on the SA me index. And so how do we do that? Well, it's quite simple. So we, we create a for loop and we have the X and WS and I",
                "start_time": "807.859",
                "end_time": "831.989"
            },
            {
                "id": 42,
                "transcript": "uh we use the zip function over here and we pass in inputs and weights, right? Like this. So if you guys are not familiar with the zip function, like it's a very nice function because it enables you like to unpack two lists and to",
                "start_time": "832.489",
                "end_time": "852.34"
            },
            {
                "id": 43,
                "transcript": "uh just like pass them to like these variables over here, index by index. And so like here, for example, when we go through the loop for the first time, this X is gonna be basically inputs at index zero. And this uh W over here is gonna be weights at the index zero, right? So once we have this, then we'll have the H",
                "start_time": "852.549",
                "end_time": "880.01"
            },
            {
                "id": 44,
                "transcript": "that's going to be, we're going to just like sum, add up the multiplication of X by",
                "start_time": "880.28",
                "end_time": "889.59"
            },
            {
                "id": 45,
                "transcript": "uh W to H which is our net input. Obviously H is not defined here. So we need to predefined it, declare it over here.",
                "start_time": "890.099",
                "end_time": "900.969"
            },
            {
                "id": 46,
                "transcript": "And so H is gonna start from zero and then we're gonna loop through all the inputs and weights and we're gonna multiply them and add them up to the net input. And so basically, now we're done. So the next step, once we have our H which is our net input is to perform the activation itself. So how do we do that? Well, uh it's kind of like really simple because the activation is, in our case",
                "start_time": "901.479",
                "end_time": "929.695"
            },
            {
                "id": 47,
                "transcript": "said that we're gonna use a sigmoid function activation. So we call sigmoid and we pass INH and now we can just return this and this is the output of the neuron itself. And this is like what we're gonna get down here. And then as a final thing, we just like print this output. Now again, sigmoid hasn't been",
                "start_time": "929.705",
                "end_time": "952.969"
            },
            {
                "id": 48,
                "transcript": "um declare it defined anywhere. So we need to define this function. And so let's define it over here. So we define sigmoid, it's a function of X.",
                "start_time": "953.159",
                "end_time": "968.65"
            },
            {
                "id": 49,
                "transcript": "And",
                "start_time": "969.219",
                "end_time": "970.479"
            },
            {
                "id": 50,
                "transcript": "if you",
                "start_time": "971.2",
                "end_time": "972.219"
            },
            {
                "id": 51,
                "transcript": "uh guys remember, so this is, we can define this as 1.0 divided by one plus",
                "start_time": "972.84",
                "end_time": "987.669"
            },
            {
                "id": 52,
                "transcript": "the exponential,",
                "start_time": "988.2",
                "end_time": "989.859"
            },
            {
                "id": 53,
                "transcript": "which is math X of minus X over here.",
                "start_time": "990.4",
                "end_time": "999.89"
            },
            {
                "id": 54,
                "transcript": "Now we d we haven't defined this. Uh we haven't included uh this math module. So we need to include it because otherwise we're gonna get an error and so we're gonna import math. Uh Now the error",
                "start_time": "1000.539",
                "end_time": "1016.19"
            },
            {
                "id": 55,
                "transcript": "is gone and so here we can return. Why? Perfect. So now we have all of the different elements in place. And so now when we run the script, we should be able to get an output and then print the output if you guys remember by using these inputs. And these,",
                "start_time": "1016.63",
                "end_time": "1037.469"
            },
            {
                "id": 56,
                "transcript": "we, we got, when we run the, the math there, we got an output for the neuron of 0.61 something like that. So let's see if we are lucky and we're going to get like the same value. And as you see here, we do have the very same value. So uh the results of this",
                "start_time": "1037.479",
                "end_time": "1060.63"
            },
            {
                "id": 57,
                "transcript": "uh computation is 0.61 and that's correct. So here you have it, it's your first neuron implemented from scratch in Python.",
                "start_time": "1060.88",
                "end_time": "1072.859"
            },
            {
                "id": 58,
                "transcript": "Cool. So this was it for implementing an artificial neuron in Python. So before living, I want to give you some takeaway points. So as we've seen, artificial neurons are loosely inspired to biologically",
                "start_time": "1073.479",
                "end_time": "1090.069"
            },
            {
                "id": 59,
                "transcript": "neurons and neurons are computational unions and precisely what they do is they receive certain inputs and they modulate that input and using an activation function, they arrive at an output.",
                "start_time": "1090.079",
                "end_time": "1106.68"
            },
            {
                "id": 60,
                "transcript": "Cool. So this was it for uh artificial neurons. So now we have like an understanding of how uh like this uh units of computation work. So what's next Well, before getting into neural networks themselves, we have to understand a little bit more of math and specifically we are going to be looking at matrix operations and vector operations. So",
                "start_time": "1107.099",
                "end_time": "1134.939"
            },
            {
                "id": 61,
                "transcript": "if you've enjoyed the video, please subscribe to the channel. Just leave a like and I guess I'll see you the next time. Cheers.",
                "start_time": "1135.189",
                "end_time": "1144.689"
            }
        ]
    }
}