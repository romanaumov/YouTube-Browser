{
    "jobName": "transcript-job-audio-assistant",
    "accountId": "337909742319",
    "status": "COMPLETED",
    "results": {
        "transcripts": [
            {
                "transcript": "Hi, everybody and welcome to a new video in the Deep Learning for audio with Python series. This time we're gonna introduce basic concepts about audio data and signal processing. Specifically, we're gonna look into waveforms, sound concepts like pitch loudness and things that are a little bit more advanced, like spectrograms, fourier transform and MF CCS. And we're gonna need all of these elements because these are the bases we'll need for implementing audio and music, deep learning models. Cool. So a disclaimer is needed here. So I'm not, this is not a comprehensive video on audio, digital signal processing rather. It, it will give you just like the the basic foundations you'll need for like deep learning in this field. But if you want to know more about this fascinating topic, like let me know in the comments section and I may make a few videos on the topic moving forward. Cool. So let's get started. So first question. So what sound well, sound is produced when there's an object that vibrates and these vibrations and determine the oscillation of air molecules, which basically creates an alternation of air pressure. And this high pressure alternated with low pressure causes a wave and we can represent this wave using a nice wave form. And in this case, we have like a very nice wave that oscillates. And we can represent it using an amplitude and a time because at the end of the day, this is a wave is just like a, a point that oscillates with different like amplitude in different points cool. So uh there are like a few important uh elements of a wave or a sound wave. So one is the period and the period uh gives us an idea when we have like the same, the starts like of the same uh wave. So for example, here, like we have a peak and then we go back like to the next peak. And this is like the, the period which is like the interval before like we see uh that peak again. Now um the period is strictly correlated with a frequency. Indeed a frequency is the inverse of period. So the higher uh the period, the lower the frequency and the lower the period, the higher uh the frequency now to uh describe uh a sound wave, we also need another uh information about another thing which is indeed a amplitude and amplitude is given by the distance uh of a point uh from like zero amplitude, right? In this case, we can represent this sound wave as with a, with a sine function. And here we have a mathematical representation of this a sound wave and it's given by the A, by A which is the amplitude multiplied by the sine function uh calculated in two pi F which stands for frequency time uh plus P. This phi is a Greek letter which stands for phase well phase. Uh what phase does to a waveform? It's basically like it shifts it to the right or to the left uh cool. OK. So now we have like a simple mathematical representation of a of a, of a waveform. So now let's look at how like sound wave uh like these two fundamental elements like of sound waves are like frequency and amplitude are connected with pitch and loudness. So frequency and pitch are connected together. Uh basically what happens is that higher frequencies are perceived as higher pitch, but pitch is not a physical uh observable. It is like a perceptual is the way we perceive uh like a frequency and we process it right. And so uh basically, the idea here is that, and you can see it here with this two sound waves like in red. So when you have uh like a longer periods, you have basically lower frequencies and here with shorter periods, you have like higher frequencies. So basically, we would perceive these um sound wave on the bottom left as higher pitch than the one like on uh like on the top part. OK. So now let's move on to amplitude and loudness. Well, um there's a correlation, obviously, there's a connection between amplitude and loudness, but it's by no means like linear and it's very complicated. But all in all uh larger amplitudes uh are perceived as louder, right? So for example, if we compare uh these two sound waves like on the right. So these two blue sound waves like the one on the top uh is quieter than the one like on the bottom. Cool. So now a thing that I think like it's important to strike here is that when we talk about uh like acoustic sound waves. So for example, like the sound of my voice or the sound of a of a piano playing, these are continuous wave forms, right? So, so they are analog waveforms, but obviously, we can't really store analog waveforms. We need a way of digitalis those. And for doing that, we have this analog digital conversion uh process or a DC. So when we do analog digital conversion, uh we basically do perform two steps. The first one is called uh sampling and the second one is called quantization. So during a sampling, uh what we do is we just like sample the signal at specific uh time intervals and then we quantize the amplitude given and, and we represent that with a limited number of bits. So let's see this in action with this example here. So as you'll see, we haven't read a nice continuous analog uh sound wave and now we're gonna sample it at these like blue points here which are all at the same interval. And the interval is given by the sample rate, which is basically uh the amount of like samples that we have like in a uh in a second. Cool. Uh So um what we do like at each uh sample is we project the, the value of the amplitude of the analog uh sound wave to the closer quantized uh bit we have here like on the left, right. So for example, if you, if you look at this point here, so as you'll see, so like the amplitude, it's probably like around 6.6 something like that. But we, we don't have 6.6 right? And so we're just gonna project that to the closer bit we have, which is seven. And so we'll store this information here like with, with a seven, right. So now the uh obviously, as you'll see here, there are, there are gonna be like quite some like errors that accumulate throughout uh like the A DC process because of like the, the sampling process itself and the quantization. But the more bits we have to store the amplitude and the better the quality of the sound uh will be. So we have two, I would say like matrix here when we do a DC. So one is called sample rate. And the other one is called bit F. For example, with the CD RM, we have a sample rate of 44,000 and 100 heads, which basically means that we take more than 40,000 amplitude points in a second, right? And the bit depth is given by 16 bits uh for each channel, right? Uh So the more like nostalgic uh like if you guys are who like really love like video game music or video games, like retro games may remember the so called eight bit music like Super Mario or Final Fantasy, like the first ones, right? And so that music is called eight bit because the bit depth was eight bit. And obviously the quality of that sound was kind of like not that that great compared to what with what we have now, but still like it was like really, really nice, cool. So this is a DC. So now let's move on and let's take a look at Real World sound waves. So it turns out the Real World sound waves are not as simple as like the sine wave that we've seen before. So here, for example, we have a, a waveform for a piano key. So we just like strike uh a piano key and we, we wait until like the sound fades out basically here after nine seconds. Cool. So this is like a messy uh sound that there's a lot of like complexity in it. So the question we could ask and which is like super legitimate is like, what can we know about this sound cause like, I mean, it doesn't seem like we can know much, but actually, it turns out that nature has given us like an incredible way of knowing quite a lot about complex sounds. And that's given through a fourier transform. And basically what we do with a fourier transform is like the process of decomposing a periodic sound into a sum of sine waves which all vibrate oscillate like at different frequencies. So think about that, this is like quite incredible. So we can describe a very complex sound as long as it's periodic as a sum as the super imposition of a bunch of different sine waves and different frequencies. Like it's quite remarkable, isn't it cool? So, but let's like try to like uh visualize this because this could feel I know a little bit abstract. So let's start like with this uh sound wave over here. Now this uh sound wave is given by the super imposition of these two sine waves, right? So if we sum them, we're gonna get this, which is quite cool. So let's see this like uh mathematically. So over here, so if we call this sound wave, uh the red sound wave s then we see that that is given by the uh the sine wave relative like to, to this guy here plus the sound wave, the wave like relative to this guy over here, which is quite cool. And if we, if we take a look over here. So we can describe, as we said before, like this two sine waves like with the amplitude with the frequency and with the phase which in this case is zero cool. But uh when we do uh a fourier transform, we are particularly interested in the amplitudes themselves. And why is that the case? Because like the amplitude tells us how much uh a specific frequency contributes to the complex sound, right? So the higher the amplitude and the more I know that that specific uh frequency is contributing to the complex sound, I want to decompose, right. So in this case, we see that uh the frequency uh 1.5 is the one that contributes the most to this sound over here. Uh which is because like the amplitude is 1.5 which is way more than 0.5 like for the case of frequency four, right? And so, so you may be wondering, but what's the great thing about this? Well, it, it's fantastic because now we know like the different elements which uh kind of like contribute to create a complex sound. It's as if like you think of like for example, like a dish say for example, like you have a uh some pasta spaghetti like tomato spaghetti, right? So the waveform is just like that dish in itself. And it's difficult like to understand like all the different uh parts of it. But then with a fourier transform, we can divide those um elements, those ingredients and understand that probably we had like 200 g of spaghetti. Then we had like a little bit of garlic. We had uh five leaves for example of uh basilica, a basil, right? And we had 100 g of tomatoes, right? So basically, we can decompose the whole dish in it, in its ingredients. And it's the same thing we can do with the fourier transform, we can decompose a complex sound and understand how different frequencies are contributing to that, right? So let's go back to the waveform of our piano key and perform a fourier transform here. And so what you'll get if you perform a fourier transform is this guy over here which is called a power spectrum. So and the spectrum basically gives us the magnitude as a function of frequency. So here we know that there's a peak of like magnitude of power around 3000. Yeah, I would say like 500 like over here. So the uh the uh this frequency is very much represented like in this sound over here, right? So now, so when we do like this fourier transfer, basically we move from the time domain towards the frequency domain. What does that mean? Well, if you take a look at this waveform here, so you'll see that here we have the amplitude as a function of time. So we are in the time domain. But then when we apply the fourier transform, we move in the frequency domain because here we have on the x axis, the frequency and the magnitude is a function of the frequency itself, right? Cool. And so if, because this happens, we lose information about uh time. So it's as if uh this spectrum power spectrum here was a snapshot of all the elements uh which concur to form this sound over this like nine seconds, right? And so basically what this uh spectrum is telling us is telling us that uh these different frequencies have different uh powers. But throughout all of the, all of the um all of the sound here. So it's a snapshot, it's static which could be seen like as a problem because obviously audio and music data like it is a time series, right? So things change in time. And so we want to know about how things change in time and it seems that with the fourier transform, we, we can't really do that. So we are missing on a lot of information, right? But obviously, there's a solution to that and the solution it's called the short time fourier transform or SDFT. And so what the short time fourier transform does, it computes several fourier transforms at different intervals. And in doing so it preserves information about uh time and the way uh sounds uh like evolves like over time, right? And so the different intervals at which we perform uh the fourier transform uh is given by the frame size. And so a frame is a bunch of samples. And so we fix the number of samples. And we say, OK, so let's consider only like for example, 202 hun 2048 samples and do the, the fourier transform there. And then let's move on to let's shift and move on like to, to the rest like of the waveform. And what happens here is that we are given a spectrogram which is a representation that gives us information about uh like magnitude as a, as a function of frequency and time. So let's take a look at the spectrogram of the um piano key that we, we say like uh before, right? So like that sound wave cool. So here, as you can see we are back in business because we have time now here on the, on the uh x axis. But we also have frequency on the y axis and we have a third axis which is basically given by the color and the color is telling us how much a given frequence like is present uh in uh the sound at a given time. So for example, here we see that like towards like the beginning uh above like 4000 Hertz, we really don't have like much uh uh contribution like at all. So it seems like that the, the energy is kind of like um I would say like uh around a very like low frequency here could be like 500 something like that like Hertz, right? And as you see here, uh this uh spectrogram resembles a little bit of the waveform of the piano key as well because as you see, like in time, like all of these frequencies are kind of like fading out like the energy for these frequencies because like the time the the sound of the piano key is just like fading out uh all the time cool. OK. So now we have like an idea of like what a spectrogram is, but let's see how we perform a short time fourier transform a little bit like more in detail. Um just like to understand like how this works. So here we start obviously from a um a waveform like the the mm sound wave. And then what we do next is we basically like focus only on uh one frame which is given by a number of samples here. And then what we do next is we calculate the, the fourier transform over here. And then we take this information and we just project it in the spectrogram. So we, we put it like in the, in the first interval here in the spectrogram uh at, at time zero basically. And then we, we pass like the, the frequency here and then we pass the, the magnitude here. Uh And basically, we, we can visualize it like as a, as a function of, of color. So now two things here. So here I'm using like decibels and uh like with decibels, basically, we, we apply like a logarithmic like a function like to, to the magnitude itself. And uh the other thing that I wanted to say is that uh when we uh perform the fourier transform, what we actually perform is the FFT which is the fast fourier transform, which is a kind of like a uh a variation of the fourier transform which is used for performing um like the fourier transform like way faster, right? OK. So now we are at the end of the, of the first uh iteration. So what do we do next? Well, we just like slide here um like on the, on the sound wave. And now we take like the second like the same like uh the same uh kind of like interval but the second frame. So we are like shifting to the right. And now we do the same thing. So we calculate the spectrum there through the fast fourier transform and then we project that into the spectrogram and then we move on. So we have the third, the fourth until we get to the end. And once we are at the end, we have our own spectrogram which is fantastic news. Cool. So now you may be wondering well, but why did we have to learn about uh like spectrograms like in all of this? Well, it turns out like that spectrograms are fundamental uh for uh performing like deep learning uh like applications like on audio data well. And actually uh like the whole preprocessing pipeline for audio data for deep learning is based on spectrograms. So not surprisingly, uh when we have a data set, we start with a bunch of wave files. So basically of like a wave forms or like sound waves, right? And then we pass those in into a short time fourier transform and we get a spectrogram and then we use that spectrogram as an input for our deep learning model over here. Nice. So this is a super nice way of getting uh like a valuable representation for our like deep learning uh model. So we just like focus on the spectrogram. Now this is like very different from uh what used to happen in the past when we were focusing more on uh traditional machine learning um like algorithms like a logistic regression or super machines, like in those cases, like the preprocessing pipeline for audio data was quite different. So let's take a look at that. So the whole thing was way more about feature engineering. So it turns out that from a from uh like a waveform, you can take a lot of like different features like uh from those. And so what we used to do before was like taking like those features. And um and for doing that, uh we would basically use either a directly like a waveform or perform a fourier transform. And then using like the the spectrum and you would use like a waveform for extracting a time domain features. And you would use a spectrogram for extracting uh frequency uh domain features. An example of like a time domain feature is amplitude envelope. An example of like frequency domain feature like spectral center or like spectral flux. But the point that I want to make here is that we start from a waveform and then we, we get a bunch of features we and we should decide which features we want to get. And then we combine these features, we aggregate them somehow using like some statistical means or even like some unsupervised learning uh like models. And then we, we use those and we fed those features into an ML algorithm like logistic regression or like super vector machine. So with the advance of like deep learning, the whole process became a little bit more straightforward because we don't need to uh like a concern that much about feature engineering because we just use the spectrum right. And so we don't use like all of these other things. This is why like a deep learning uh model uh like for audio, like in this case, it's called like an end to end model because basically you just use some basic information without uh worrying too much about extracting specific features. Cool. Now I want to introduce the another feature which is fundamental for deep learning. And this is like as important if not more important than the spectrogram itself. And this feature is called the mal frequency subs subs coefficients. Now uh to extract like this features like it's quite complicated and I'm not gonna get like into the details because again, we we don't need them. So we need to understand like the intuition between like a spectrogram and MF CCS. But we don't need to like understand the implementation details here. But what we need to do however, is to understands like what uh the MF CCS are and how we can like use them from a very high level perspective. Well, MF CCS capture timbrel and textural aspects of sound. So if you have, for example, like a piano and a violin uh playing like the same melody, uh you would have potentially like the, the same like peach content, the same frequency and same like rhythm um more or less there depending on the performance. But what would change is like timbre the quality of sound and the MF CCS are capable of capturing that information. And for uh extracting MF CCS, we, we perform uh a fourier transform and we move like from the time domain into like the frequency domain. So MF CCS are basically like frequency domain uh feature. But the great thing, the great advantage of MF CCS over spectrograms is that they approximate the human auditory system that try to model the way we perceive like frequency, right? And so that's like very important. Like if you then want to do like deep learning stuff to have like some data that represents the way we uh kind of like process like audio. Now, uh the results like of uh extracting MF CCS is a bunch of coefficients. It's an MFCC vector. And so you can specify a number of like different coefficients. Usually in all your music applications you want to use between 13 to 40 coefficients. And then again, you are gonna calculate all of these coefficients at each frame. So that you have um an idea of how like the MF CCS are evolving over time, right? Cool. So now let's take a look at uh like uh an MFCC like representation here and this is like very similar to a spectrogram, isn't it? Right? So here on uh the AX ax axis, we have time. And then again, this is the MF CCS calculated for the same uh sound wave like that we, so up until now, so like the, the uh the piano key, right? OK. So here we have time. So it is nine seconds. And here on the Y axis, we have the different MFCC coefficients. And here you see, so this is one, this is two, this is three. And here I think I have like 13 coefficients. And now again, like the value of the coefficient is represented with this uh like uh colors over here and as usual, like the, the more red, the redder, the um a value like over here and like the, the, the, the, the bigger, like the value itself, right? Cool. So uh what can we say about MF CCS? So where do we use them? Well, it turns out that uh MF CCS are fantastic for a number of different audio applications. So they've originally been introduced for speech recognition and they are still used like for speech recognition today quite extensively. And but around the 2000, they've also been introduced in music analysis. And so uh we can use MF CCS for music genre classification and music instrument classification as well. So regarding music genre classification in a few videos, I think a couple of videos we are gonna use MF CCS for classifying uh a bunch of like different uh tracks that we'll have and decide which genres like they are and we'll use MF CCS for that. Cool. So now again, so as you've seen, I haven't gone into like the deep mathematical details or implementation details, both of like the fourier transform and MF CCS because we don't need them. But again, if you are like interested in knowing more, just like let me know in the comments section. And once I'm done like with the series, uh I could actually like make a few videos about this and if you are very interested in that I could actually create a series about all your digital signal processing, which is a fascinating topic in itself and very complex. Cool. OK. So now how do we use like MF CCS like in the uh preprocessing pipeline? Like for a data? Well, it's basically the same thing that we do like for a spectrogram. So we start from a, a waveform, a sound form a sound wave, then we extract MF CCS. Now we have these MF CCS and we pass the MF CCS directly into our deep learning um uh network, right? So again, using MF CCS is another way of like doing end to end uh deep learning with audio and it's like a very efficient one, effective one cool. So this was it for this video. So what's up next? Well, uh it turns out that like in this video, we spent quite a lot of time talking about theoretical stuff. And now as usual, so as we usually do, we, we want just like to turn like this theoretical uh information into like implementation. So in the next video, we'll perform fast fourier transforms a short term, short time via transform. Uh with Python, we'll look at spectrograms at spectra and we'll extract MF CCS. But again, we won't implement uh these extractors like from scratch. But R will get familiar with a fantastic audio library in Python that's called uh Li Brosa. And that's like a library that you really want to know if you want to use if you want to like do stuff like in um audio with deep learning, right? To prepare your data. Cool. So this is it for this video? Yeah, I really hope you enjoyed it. And if that's the case, as usual, just uh subscribe and hit the notification bell and if you have any questions, feel free like to to post them in the comments section below and I'll see you the next time. Cheers."
            }
        ],
        "audio_segments": [
            {
                "id": 0,
                "transcript": "Hi, everybody and welcome to a new video in the Deep Learning for audio with Python series. This time we're gonna introduce basic concepts about audio data and signal processing.",
                "start_time": "0.23",
                "end_time": "11.189"
            },
            {
                "id": 1,
                "transcript": "Specifically, we're gonna look into waveforms, sound concepts like pitch loudness and things that are a little bit more advanced, like spectrograms, fourier transform and MF CCS. And we're gonna need all of these elements because these are the bases we'll need for implementing audio and music, deep learning models. Cool.",
                "start_time": "11.5",
                "end_time": "37.83"
            },
            {
                "id": 2,
                "transcript": "So a disclaimer is needed here. So I'm not, this is not a comprehensive video on audio, digital signal processing rather. It, it will give you just like the the basic foundations you'll need for like deep learning in this field. But if you want to know more about this fascinating topic, like let me know in the comments section and I may make a few videos on the topic moving forward. Cool. So let's get started. So first question. So what sound well,",
                "start_time": "38.159",
                "end_time": "67.319"
            },
            {
                "id": 3,
                "transcript": "sound is produced when there's an object that vibrates and these vibrations and determine the oscillation of air molecules, which basically creates an alternation of air pressure. And this high pressure alternated with low pressure causes a wave and we can represent this wave using a nice wave form.",
                "start_time": "67.51",
                "end_time": "94.209"
            },
            {
                "id": 4,
                "transcript": "And in this case, we have like a very nice wave that oscillates. And we can represent it using an amplitude and a time because at the end of the day, this is a wave is just like a, a point that oscillates with different like amplitude in different points cool. So",
                "start_time": "94.36",
                "end_time": "117.0"
            },
            {
                "id": 5,
                "transcript": "uh there are like a few important uh elements of a wave or a sound wave. So one is the period and the period uh gives us an idea when we have like the same, the starts like of the same uh wave. So for example, here, like we have a peak and then we go back like to the next peak. And this is like the, the period which is like the interval before like we see uh that peak again.",
                "start_time": "117.319",
                "end_time": "145.1"
            },
            {
                "id": 6,
                "transcript": "Now um the period is strictly correlated with a frequency. Indeed a frequency is the inverse of period. So the higher uh the period, the lower the frequency and the lower the period, the higher uh the frequency",
                "start_time": "145.32",
                "end_time": "163.41"
            },
            {
                "id": 7,
                "transcript": "now to uh describe uh a sound wave, we also need another uh information about another thing which is indeed a amplitude and amplitude is given by the distance uh of a point uh from like zero amplitude, right? In this case, we can represent this sound wave as with a, with a sine function.",
                "start_time": "164.11",
                "end_time": "188.75"
            },
            {
                "id": 8,
                "transcript": "And here we have a mathematical representation of this a sound wave and it's given by the A, by A which is the amplitude multiplied by the sine function uh calculated in two pi F which stands for frequency time",
                "start_time": "189.6",
                "end_time": "205.979"
            },
            {
                "id": 9,
                "transcript": "uh plus P. This phi is a Greek letter which stands for phase well phase. Uh what phase does to a waveform? It's basically like it shifts it to the right or to the left",
                "start_time": "206.179",
                "end_time": "219.66"
            },
            {
                "id": 10,
                "transcript": "uh cool. OK. So now we have like a simple mathematical representation of a of a, of a waveform. So now let's look at how like sound wave uh like these two fundamental elements like of sound waves are like frequency and amplitude are connected with pitch and loudness. So frequency and pitch are connected together.",
                "start_time": "220.33",
                "end_time": "244.979"
            },
            {
                "id": 11,
                "transcript": "Uh basically what happens is that higher frequencies are perceived as higher pitch, but pitch is not a physical uh observable. It is like a perceptual is the way we perceive uh like a frequency and we process it right. And so uh basically, the idea here is that, and you can see it here with this two sound waves like in red.",
                "start_time": "245.339",
                "end_time": "271.679"
            },
            {
                "id": 12,
                "transcript": "So when you have uh like a longer periods, you have basically lower frequencies and here with shorter periods, you have like higher frequencies. So basically, we would perceive these um sound wave on the bottom left as higher pitch than the one like on uh like on the top part.",
                "start_time": "272.119",
                "end_time": "295.32"
            },
            {
                "id": 13,
                "transcript": "OK. So now let's move on to amplitude and loudness. Well, um there's a correlation, obviously, there's a connection between amplitude and loudness, but it's by no means like linear and it's very complicated. But all in all uh larger amplitudes uh are perceived as louder, right? So for example, if we compare uh these two sound waves like on the right. So these two blue sound waves like the one on the top",
                "start_time": "295.619",
                "end_time": "325.35"
            },
            {
                "id": 14,
                "transcript": "uh is quieter than the one like on the bottom.",
                "start_time": "325.559",
                "end_time": "329.929"
            },
            {
                "id": 15,
                "transcript": "Cool. So now a thing that I think like it's important to strike here is that when we talk about uh like acoustic sound waves. So for example, like the sound of my voice or the sound of a of a piano playing, these are continuous wave forms, right?",
                "start_time": "330.5",
                "end_time": "347.519"
            },
            {
                "id": 16,
                "transcript": "So, so they are analog waveforms, but obviously, we can't really store analog waveforms. We need a way of digitalis those. And for doing that, we have this analog digital conversion uh process or a DC. So when we do analog digital conversion,",
                "start_time": "347.779",
                "end_time": "370.82"
            },
            {
                "id": 17,
                "transcript": "uh we basically do perform two steps. The first one is called uh sampling and the second one is called quantization. So during a sampling, uh what we do is we just like sample the signal at specific uh time intervals and then we quantize the amplitude given and, and we represent that with a limited number of bits. So let's see this in action with this example here.",
                "start_time": "370.959",
                "end_time": "399.029"
            },
            {
                "id": 18,
                "transcript": "So as you'll see, we haven't read a nice continuous analog uh sound wave and now we're gonna sample it at these like blue points here which are all at the same interval. And the interval is given by the sample rate, which is basically uh the amount of like samples that we have like in a uh in a second. Cool. Uh So",
                "start_time": "399.23",
                "end_time": "426.69"
            },
            {
                "id": 19,
                "transcript": "um what we do like at each uh sample is we project the, the value of the amplitude of the analog uh sound wave to the closer quantized uh bit we have here like on the left, right.",
                "start_time": "427.38",
                "end_time": "446.07"
            },
            {
                "id": 20,
                "transcript": "So for example, if you, if you look at this point here,",
                "start_time": "446.589",
                "end_time": "449.779"
            },
            {
                "id": 21,
                "transcript": "so as you'll see, so like the amplitude, it's probably like around 6.6 something like that. But we, we don't have 6.6 right? And so we're just gonna project that to the closer bit we have, which is seven. And so we'll store this information here like with, with a seven, right. So now the",
                "start_time": "450.619",
                "end_time": "472.559"
            },
            {
                "id": 22,
                "transcript": "uh obviously, as you'll see here, there are, there are gonna be like quite some like errors that accumulate throughout uh like the A DC process because of like the, the sampling process itself and the quantization. But the more bits we have to store the amplitude and the better the quality of the sound uh will be. So we have two,",
                "start_time": "472.769",
                "end_time": "497.769"
            },
            {
                "id": 23,
                "transcript": "I would say like matrix here when we do a DC. So one is called sample rate. And the other one is called bit F. For example, with the CD RM, we have a sample rate of 44,000 and 100 heads, which basically means that we take more than 40,000 amplitude points in a second, right? And the bit depth is given by 16 bits uh for each channel, right?",
                "start_time": "498.2",
                "end_time": "527.099"
            },
            {
                "id": 24,
                "transcript": "Uh So the more like nostalgic uh like if you guys are who like really love like video game music or video games, like retro games may remember the so called eight bit music like Super Mario or Final Fantasy, like the first ones, right? And so that music is called eight bit because the bit depth was eight bit.",
                "start_time": "527.94",
                "end_time": "551.21"
            },
            {
                "id": 25,
                "transcript": "And obviously the quality of that sound was kind of like not that that great compared to what with what we have now, but still like it was like really, really nice, cool. So this is a DC. So now let's move on and let's take a look at Real World sound waves. So",
                "start_time": "551.609",
                "end_time": "570.659"
            },
            {
                "id": 26,
                "transcript": "it turns out the Real World sound waves are not as simple as like the sine wave that we've seen before. So here, for example, we have a, a waveform for a piano key. So we just like strike uh a piano key and we, we wait until like the sound fades out basically here after nine seconds. Cool. So this is like a messy uh sound",
                "start_time": "570.869",
                "end_time": "593.619"
            },
            {
                "id": 27,
                "transcript": "that there's a lot of like complexity in it. So the question we could ask and which is like super legitimate is like, what can we know about this sound cause like,",
                "start_time": "593.83",
                "end_time": "603.919"
            },
            {
                "id": 28,
                "transcript": "I mean, it doesn't seem like we can know much, but actually, it turns out that nature has given us like an incredible way of knowing quite a lot about complex sounds. And that's given through a fourier transform.",
                "start_time": "604.559",
                "end_time": "620.53"
            },
            {
                "id": 29,
                "transcript": "And basically what we do with a fourier transform is like the process of decomposing a periodic sound into a sum of sine waves which all vibrate oscillate like at different frequencies.",
                "start_time": "620.799",
                "end_time": "637.049"
            },
            {
                "id": 30,
                "transcript": "So think about that, this is like quite incredible. So we can describe a very complex sound as long as it's periodic",
                "start_time": "637.82",
                "end_time": "647.77"
            },
            {
                "id": 31,
                "transcript": "as a sum as the super imposition of a bunch of different sine waves and different frequencies. Like it's quite remarkable, isn't it cool? So, but let's like try to like uh visualize this because this could feel I know a little bit abstract. So let's start like with this uh sound wave over here.",
                "start_time": "648.469",
                "end_time": "666.58"
            },
            {
                "id": 32,
                "transcript": "Now this uh sound wave is given by the super imposition of these two sine waves, right? So if we sum them, we're gonna get this, which is quite cool. So let's see this like uh mathematically.",
                "start_time": "666.94",
                "end_time": "683.89"
            },
            {
                "id": 33,
                "transcript": "So over here, so if we call this sound wave, uh the red sound wave s then we see that that is given by the uh the sine wave relative like to, to this guy here plus the sound wave, the wave like relative to this",
                "start_time": "684.08",
                "end_time": "702.08"
            },
            {
                "id": 34,
                "transcript": "guy over here,",
                "start_time": "702.64",
                "end_time": "703.65"
            },
            {
                "id": 35,
                "transcript": "which is quite cool. And if we, if we take a look over here. So we can describe, as we said before, like this two sine waves like with the amplitude with the frequency and with the phase which in this case is zero cool. But uh when we do uh a fourier transform, we are particularly interested in the amplitudes themselves. And why is that the case? Because like the amplitude tells us how much",
                "start_time": "704.549",
                "end_time": "734.44"
            },
            {
                "id": 36,
                "transcript": "uh a specific frequency contributes to the complex sound, right? So the higher the amplitude and the more I know that that specific uh frequency is contributing to the complex sound, I want to decompose, right. So in this case, we see that uh the frequency uh 1.5 is the one that contributes the most to this sound over here.",
                "start_time": "735.419",
                "end_time": "765.2"
            },
            {
                "id": 37,
                "transcript": "Uh which is because like the amplitude is 1.5 which is way more than 0.5 like for the case of frequency four, right? And so, so you may be wondering, but what's the great thing about this? Well, it, it's fantastic because now we know like the different",
                "start_time": "765.479",
                "end_time": "785.15"
            },
            {
                "id": 38,
                "transcript": "elements which uh kind of like contribute to create a complex sound. It's as if like you think of like for example, like a dish say for example, like you have a uh some pasta spaghetti like tomato spaghetti, right? So the waveform is just like that",
                "start_time": "785.32",
                "end_time": "804.07"
            },
            {
                "id": 39,
                "transcript": "dish in itself. And it's difficult like to understand like all the different uh parts of it. But then with a fourier transform, we can divide those um elements, those ingredients and understand that probably we had like 200 g of spaghetti. Then we had like a little bit of garlic. We had",
                "start_time": "804.799",
                "end_time": "829.039"
            },
            {
                "id": 40,
                "transcript": "uh five leaves for example of uh basilica, a basil, right? And we had 100 g of tomatoes, right? So basically, we can decompose the whole dish in it, in its ingredients. And it's the same thing we can do with the fourier transform, we can decompose a complex sound and understand how different frequencies are contributing to that,",
                "start_time": "829.239",
                "end_time": "856.27"
            },
            {
                "id": 41,
                "transcript": "right? So let's go back to the waveform of our piano key and perform a fourier transform here. And so what you'll get if you perform a fourier transform is this guy over here which is called a power spectrum. So and the spectrum basically gives us the magnitude as a function of frequency. So here we know that there's a peak",
                "start_time": "856.63",
                "end_time": "880.34"
            },
            {
                "id": 42,
                "transcript": "of like magnitude of power around 3000. Yeah, I would say like 500 like over here. So the uh the uh this frequency is very much represented like in this sound over here, right? So now,",
                "start_time": "880.659",
                "end_time": "897.32"
            },
            {
                "id": 43,
                "transcript": "so when we do like this fourier transfer, basically we move from the time domain towards the frequency domain. What does that mean? Well, if you take a look at this waveform here, so you'll see that here we have the amplitude as a function of time. So we are in the time domain. But then when we apply the fourier transform, we move in the frequency domain because here we have on the x axis, the frequency",
                "start_time": "898.099",
                "end_time": "924.359"
            },
            {
                "id": 44,
                "transcript": "and the magnitude is a function of the frequency itself, right?",
                "start_time": "924.89",
                "end_time": "930.489"
            },
            {
                "id": 45,
                "transcript": "Cool. And so if, because this happens, we lose information about uh time. So it's as if uh this spectrum power spectrum here was a snapshot of all the elements uh which concur to form this sound over this like nine seconds, right?",
                "start_time": "931.099",
                "end_time": "955.559"
            },
            {
                "id": 46,
                "transcript": "And so basically what this uh spectrum is telling us is telling us that uh these different frequencies have different uh powers. But throughout all of the, all of the um all of the sound here. So it's a snapshot, it's static which could be seen like as a problem because obviously audio and music data like it is a time series, right?",
                "start_time": "955.83",
                "end_time": "984.359"
            },
            {
                "id": 47,
                "transcript": "So things change in time. And so we want to know about how things change in time and it seems that with the fourier transform, we, we can't really do that. So we are missing on a lot of information, right?",
                "start_time": "984.479",
                "end_time": "996.51"
            },
            {
                "id": 48,
                "transcript": "But obviously, there's a solution to that and the solution it's called the short time fourier transform or SDFT. And so what the short time fourier transform does, it computes several fourier transforms at different intervals. And in doing so it preserves information about uh time and the way",
                "start_time": "997.15",
                "end_time": "1024.0"
            },
            {
                "id": 49,
                "transcript": "uh sounds uh like evolves like over time, right? And so the different intervals at which we perform uh the fourier transform uh is given by the frame size. And so a frame is a bunch of samples. And so we fix the number of samples. And we say, OK, so let's consider only like for example, 202 hun 2048 samples",
                "start_time": "1024.3",
                "end_time": "1051.15"
            },
            {
                "id": 50,
                "transcript": "and do the, the fourier transform there. And then let's move on to let's shift and move on like to, to the rest like of the waveform. And what happens here is that we are given a spectrogram which is",
                "start_time": "1051.489",
                "end_time": "1066.68"
            },
            {
                "id": 51,
                "transcript": "a representation that gives us information about uh like magnitude as a, as a function of frequency and time. So let's take a look at the spectrogram of the um piano key that we, we say like uh before, right? So like that sound wave cool. So here, as you can see we are",
                "start_time": "1067.31",
                "end_time": "1088.689"
            },
            {
                "id": 52,
                "transcript": "back in business because we have time now here on the, on the uh x axis. But we also have frequency on the y axis and we have a third axis which is basically given by the color and the color is telling us how much a given frequence like is present uh in uh the sound at a given time. So for example, here we see that like towards like the beginning",
                "start_time": "1089.469",
                "end_time": "1118.439"
            },
            {
                "id": 53,
                "transcript": "uh above like 4000 Hertz, we really don't have like much uh uh contribution like at all. So it seems like that the, the energy is kind of like um",
                "start_time": "1118.619",
                "end_time": "1132.75"
            },
            {
                "id": 54,
                "transcript": "I would say like uh around",
                "start_time": "1133.27",
                "end_time": "1137.189"
            },
            {
                "id": 55,
                "transcript": "a very like low frequency here could be like 500 something like that like Hertz, right? And as you see here, uh this uh spectrogram resembles a little bit of the waveform of the piano key as well because as you see, like in time, like all of these frequencies are kind of like fading out like the energy for these frequencies because like the time the the sound",
                "start_time": "1137.709",
                "end_time": "1166.339"
            },
            {
                "id": 56,
                "transcript": "of the piano key is just like fading out uh all the time cool. OK. So now we have like an idea of like what a spectrogram is, but let's see how we perform a short time fourier transform a little bit like more in detail. Um just like to understand like how this works. So here we start obviously from a um a waveform like the the mm",
                "start_time": "1166.66",
                "end_time": "1195.26"
            },
            {
                "id": 57,
                "transcript": "sound wave. And then what we do next is we basically like focus only on uh one frame which is given by a number of samples here. And then what we do next is we calculate the, the fourier transform over here. And then we take this information and we just project it in the spectrogram.",
                "start_time": "1195.469",
                "end_time": "1222.369"
            },
            {
                "id": 58,
                "transcript": "So we, we put it like in the, in the first interval here in the spectrogram uh at, at time zero basically. And then we, we pass like the, the frequency here and then we pass the, the magnitude here. Uh And basically, we, we can visualize it like as a, as a function of, of color.",
                "start_time": "1222.579",
                "end_time": "1245.069"
            },
            {
                "id": 59,
                "transcript": "So now two things here. So here I'm using like decibels and uh like with decibels, basically, we, we apply like a logarithmic like a function like to, to the magnitude itself.",
                "start_time": "1245.63",
                "end_time": "1261.89"
            },
            {
                "id": 60,
                "transcript": "And uh the other thing that I wanted to say is that uh when we uh perform the fourier transform, what we actually perform is the FFT which is the fast fourier transform, which is a kind of like a uh a variation of the fourier transform which is used for performing um like the fourier transform like way faster, right?",
                "start_time": "1262.489",
                "end_time": "1287.5"
            },
            {
                "id": 61,
                "transcript": "OK. So now we are at the end of the, of the first uh iteration. So what do we do next? Well, we just like slide here um like on the, on the sound wave. And now we take like the second like the same like uh the same uh kind of like interval but the second frame. So we are like shifting to the right.",
                "start_time": "1287.77",
                "end_time": "1312.29"
            },
            {
                "id": 62,
                "transcript": "And now we do the same thing. So we calculate the spectrum there through the fast fourier transform and then we project that into the spectrogram and then we move on. So we have the third, the fourth until we get to the end. And once we are at the end, we have our own spectrogram which is fantastic news.",
                "start_time": "1312.459",
                "end_time": "1335.359"
            },
            {
                "id": 63,
                "transcript": "Cool. So now you may be wondering well, but why did we have to learn about uh like spectrograms like in all of this? Well, it turns out like that spectrograms are fundamental uh for uh performing like deep learning uh like applications like on audio data",
                "start_time": "1335.91",
                "end_time": "1357.369"
            },
            {
                "id": 64,
                "transcript": "well. And actually uh like the whole preprocessing pipeline for audio data for deep learning is based on spectrograms. So not surprisingly, uh when we have a data set, we start with a bunch of wave files. So basically of like a wave forms or like sound waves, right?",
                "start_time": "1357.63",
                "end_time": "1377.16"
            },
            {
                "id": 65,
                "transcript": "And then we pass those in into a short time fourier transform and we get a spectrogram and then we use that spectrogram as an input for our deep learning model over here. Nice. So this is a super nice way of getting uh like a valuable representation for our like deep learning uh model. So we just like focus on the spectrogram. Now",
                "start_time": "1377.42",
                "end_time": "1406.449"
            },
            {
                "id": 66,
                "transcript": "this is like very different from uh what used to happen in the past when we were focusing more on uh traditional machine learning um like algorithms like a logistic regression or super machines, like in those cases, like the preprocessing pipeline for audio data was quite different. So let's take a look at that. So the whole thing was way more about feature engineering. So",
                "start_time": "1406.79",
                "end_time": "1434.069"
            },
            {
                "id": 67,
                "transcript": "it turns out that from a from uh like a waveform, you can take a lot of like different features like uh from those. And so what we used to do before was like taking like those features. And um and for doing that, uh we would basically use either a directly like a waveform or perform a fourier transform. And then using like the the spectrum",
                "start_time": "1434.219",
                "end_time": "1462.959"
            },
            {
                "id": 68,
                "transcript": "and you would use like a waveform for extracting a time domain features. And you would use a spectrogram for extracting uh frequency uh domain features. An example of like a time domain feature is amplitude envelope. An example of like frequency domain feature like spectral center or like spectral flux.",
                "start_time": "1463.209",
                "end_time": "1482.614"
            },
            {
                "id": 69,
                "transcript": "But the point that I want to make here is that we start from a waveform and then we, we get a bunch of features we and we should decide which features we want to get. And then we combine these features, we aggregate them somehow using like some statistical means or even like some unsupervised learning uh like models. And then we, we use those",
                "start_time": "1482.625",
                "end_time": "1506.06"
            },
            {
                "id": 70,
                "transcript": "and we fed those features into an ML algorithm like logistic regression or like super vector machine. So with the advance of like deep learning, the whole process became a little bit more straightforward because we don't need to uh like a concern that much about feature engineering because we just use the spectrum right. And so we don't use like all of these other things. This is why like a deep learning",
                "start_time": "1506.42",
                "end_time": "1533.56"
            },
            {
                "id": 71,
                "transcript": "uh model uh like for audio, like in this case, it's called like an end to end model because basically you just use some basic information without",
                "start_time": "1533.689",
                "end_time": "1542.219"
            },
            {
                "id": 72,
                "transcript": "uh worrying too much about extracting specific features. Cool. Now I want to introduce the another feature which is fundamental for deep learning. And this is like as important if not more important than the spectrogram itself. And this feature is called the mal frequency subs subs coefficients. Now",
                "start_time": "1542.979",
                "end_time": "1565.5"
            },
            {
                "id": 73,
                "transcript": "uh to extract like this features like it's quite complicated and I'm not gonna get like into the details because again, we we don't need them. So we need to understand like the intuition between like a spectrogram and MF CCS. But we don't need to like understand the implementation details here. But what we need to do however, is to",
                "start_time": "1565.67",
                "end_time": "1587.699"
            },
            {
                "id": 74,
                "transcript": "understands like what uh the MF CCS are and how we can like use them from a very high level perspective. Well, MF CCS capture timbrel and textural aspects of sound. So if you have, for example, like a piano and a violin uh playing like the same melody, uh you would have potentially like the, the same like peach content, the same frequency and same like rhythm",
                "start_time": "1587.859",
                "end_time": "1615.76"
            },
            {
                "id": 75,
                "transcript": "um more or less there depending on the performance. But what would change is like timbre the quality of sound and the MF CCS are capable of capturing that information.",
                "start_time": "1615.91",
                "end_time": "1627.93"
            },
            {
                "id": 76,
                "transcript": "And for uh extracting MF CCS, we, we perform uh a fourier transform and we move like from the time domain into like the frequency domain. So MF CCS are basically like frequency domain uh feature. But the great thing, the great advantage of MF CCS over spectrograms is that they approximate the human auditory system that try to model the way",
                "start_time": "1628.42",
                "end_time": "1656.619"
            },
            {
                "id": 77,
                "transcript": "we perceive like frequency, right? And so that's like very important. Like if you then want to do like deep learning stuff to have like some data that represents the way we uh kind of like process like audio.",
                "start_time": "1656.76",
                "end_time": "1670.04"
            },
            {
                "id": 78,
                "transcript": "Now, uh the results like of uh extracting MF CCS is a bunch of coefficients. It's an MFCC vector. And so you can specify a number of like different coefficients. Usually in all your music applications you want to use between 13 to 40 coefficients. And then again, you are gonna calculate all of these coefficients at each frame. So that you have um an idea of how like the MF CCS are evolving over time,",
                "start_time": "1670.43",
                "end_time": "1700.39"
            },
            {
                "id": 79,
                "transcript": "right? Cool. So now let's take a look at uh like uh an MFCC like representation here and this is like very similar to a spectrogram, isn't it? Right? So here on uh the AX ax axis, we have time. And then again, this is the MF CCS calculated for the same uh sound wave like that we, so up until now, so like the, the uh the piano key, right?",
                "start_time": "1700.66",
                "end_time": "1725.55"
            },
            {
                "id": 80,
                "transcript": "OK. So here we have time. So it is nine seconds. And here on the Y axis, we have the different MFCC coefficients. And here you see, so this is one, this is two, this is three. And here I think I have like 13 coefficients. And now again, like the value of the coefficient is represented with this",
                "start_time": "1725.939",
                "end_time": "1743.8"
            },
            {
                "id": 81,
                "transcript": "uh like uh colors over here and as usual, like the, the more red, the redder, the um a value like over here and like the, the, the, the, the bigger, like the value itself, right? Cool. So",
                "start_time": "1744.38",
                "end_time": "1763.329"
            },
            {
                "id": 82,
                "transcript": "uh what can we say about MF CCS? So where do we use them? Well, it turns out that uh MF CCS are fantastic for a number of different audio applications. So they've originally been introduced for speech recognition and they are still used like for speech recognition today quite extensively. And but around the 2000, they've also been introduced in music analysis. And so",
                "start_time": "1763.68",
                "end_time": "1791.479"
            },
            {
                "id": 83,
                "transcript": "uh we can use MF CCS for music genre classification and music instrument classification as well. So regarding music genre classification in a few videos, I think a couple of videos we are gonna use MF CCS for classifying uh a bunch of like different uh tracks that we'll have and decide which genres like they are and we'll use MF CCS for that.",
                "start_time": "1791.64",
                "end_time": "1815.9"
            },
            {
                "id": 84,
                "transcript": "Cool. So now again, so as you've seen, I haven't gone into like the deep mathematical details or implementation details, both of like the fourier transform and MF CCS because we don't need them. But again, if you are like interested in knowing more, just like let me know in the comments section. And once I'm done like with the series,",
                "start_time": "1816.67",
                "end_time": "1837.989"
            },
            {
                "id": 85,
                "transcript": "uh I could actually like make a few videos about this and if you are very interested in that I could actually create a series about all your digital signal processing, which is a fascinating topic in itself and very complex.",
                "start_time": "1838.26",
                "end_time": "1851.349"
            },
            {
                "id": 86,
                "transcript": "Cool. OK. So now how do we use like MF CCS like in the uh preprocessing pipeline? Like for a data? Well, it's basically the same thing that we do like for a spectrogram. So we start from a, a waveform, a sound form",
                "start_time": "1851.88",
                "end_time": "1871.609"
            },
            {
                "id": 87,
                "transcript": "a sound wave, then we extract MF CCS. Now we have these MF CCS and we pass the MF CCS directly into our deep learning um uh network, right?",
                "start_time": "1871.89",
                "end_time": "1885.479"
            },
            {
                "id": 88,
                "transcript": "So again, using MF CCS is another way of like doing end to end uh deep learning with audio and it's like a very efficient one, effective one cool. So this was it for this video. So what's up next? Well,",
                "start_time": "1886.0",
                "end_time": "1901.93"
            },
            {
                "id": 89,
                "transcript": "uh it turns out that like in this video, we spent quite a lot of time talking about theoretical stuff. And now as usual, so as we usually do, we, we want just like to turn like this theoretical uh information into like implementation. So in the next video, we'll perform fast fourier transforms a short term,",
                "start_time": "1902.27",
                "end_time": "1922.64"
            },
            {
                "id": 90,
                "transcript": "short time via transform. Uh with Python, we'll look at spectrograms at spectra and we'll extract MF CCS. But again, we won't implement uh these extractors like from scratch. But R will get familiar with a fantastic audio library in Python that's called uh Li Brosa. And that's like a library that you really want to know if you want to use if you want to like do stuff like in um audio",
                "start_time": "1922.89",
                "end_time": "1950.41"
            },
            {
                "id": 91,
                "transcript": "with deep learning, right? To prepare your data. Cool. So this is it for this video? Yeah, I really hope you enjoyed it. And if that's the case, as usual, just uh subscribe and hit the notification bell and if you have any questions, feel free like to to post them in the comments section below and I'll see you the next time. Cheers.",
                "start_time": "1950.63",
                "end_time": "1973.829"
            }
        ]
    }
}