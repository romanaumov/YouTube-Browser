{
    "jobName": "transcript-job-audio-assistant",
    "accountId": "337909742319",
    "status": "COMPLETED",
    "results": {
        "transcripts": [
            {
                "transcript": "Hi, everybody and welcome to a new video in the Deep Learning for audio with Python series. This time we're gonna implement a neural network from scratch specifically, we're gonna implement a multi layer perception. So let's get started. So the first thing that we want to do is importing numpy the nun P if you're not familiar with it is a um scientific library for Python that's used in most uh libraries if you don't have nun pie installed. So you, you just go and go to your terminal and pip install it. It's quite straightforward, right? So uh the thing that we want to build here is a class that's called M LP or multi layer perception. So here we're gonna have all the attributes and methods that we can use to actually represent a multi layer of perception. So what's the first thing that we need to do here? Well, it's the constructor. So we want the constructor for the M LP class. And so we are gonna have this in it over here and we want to pass it three different attributes. So one it's the number of uh inputs, then we want to pass it the number of hidden layers. And finally, we want to pass it the number of outputs over here. And so we're gonna store these arguments internally in uh attributes and this not surprisingly are gonna be called uh self dots numb inputs. So, and this is gonna be the numb inputs over here. Then we're gonna have the number of hidden layers over here and this is gonna be number of hidden and then we're gonna have this num outputs outputs and we have the number of outputs over here. Well, so this way we just store internally to the instance all of this uh information about the, the number of neurons in the input layer in the uh hidden layers and the in the output layers. So we can pass in some default values over here. Uh because uh that's gonna be like easier like when we implement an M LP. So we don't need to pass all of this information every time. So we could say that our default number of inputs. So number of neurons in the input layer is gonna be three for example. And for the hidden layers, it is gonna be slightly different because we want to pass in a list and this is gonna be a list of the integers and it's integer is gonna represent the number of neurons in a hidden layer. So uh basically, in this case, we have a neuron with two hidden layers. So the first hidden layer has three neurons and the second layer has a hidden layer has five neurons. And then we finally, we have the number of outputs. And so these are the output neurons, let's say we have two, for example, right. So now we want to create a an internal uh representation of the layers. So how do we do that? Well, we want this as a as a list. And in order to do that, we are gonna uh cast to at least type this self dot number of inputs over here, then we are gonna concatenate that with the number of hidden layers. And then we're gonna concatenate this with the number of output layers. So what this does is basically getting a list where each item in the list represents the number of neurons in a layer. And the layers obviously move from uh like zero to, to the, to the zero index to the uh number of layers that we have. OK. So next, what do we want to do? Well, we need to initiate a random weights. So a a neuron network is isn't a neural network if you don't have weights uh for all the connections between the neurons in a subsequent layers. So how do we do that? Well, first of all, let's initiate a, an internal like an attribute called weight. And this is gonna be an empty list for the time being. And then we want to iterate through all the layers and then create a matrix uh weight for each uh pair of layers. So how do we do that? Well, we do a full look in a range of the length uh layers minus uh one. So, and then at each uh step, we're gonna build this weight matrix W and W is gonna be equal to the uh nun dot random dot run. And here will pass in layers I layers I and here we have layers I plus one. OK. So what have I done here? So uh I'm basically creating a matrix uh which is this W here using this method uh from the numpy library which is random dot brand. And what I uh what this like um method does, it, it creates uh a random uh arrays and the arrays can have different uh dimensions. So in this case, we have like two dimensions. So and the two dimensions are given here by these values. And so here, what I'm saying is that I want a two D array which is basically a matrix. And the, the, the number of rows that I want is the current uh layer I'm in. And the number of columns that I want is the uh the the number of neurons that I have in the subsequent layer. And if you remember the weight matrix, this works because on the rows, we have all the connections of a neuron from a previous layer with the subsequent layer. So we want the for the number of rows to be equal to the number of neurons that we have like in a, in a layer. And for the number of columns we want to have uh the number of neurons that we have in the subsequent layers. And we can obtain that by using this expression here. Right. And so, and what this random random does is obviously it creates this uh to the uh array. So this matrix and it initiates all the elements of the, the matrix randomly between zero and one, right. So what else remains to do? Well, we we need to store this information and say how do we do that? Well, super simple. So we get the weights and we do append W and so here uh weight is gonna be a list uh with as many um items as the number of weight mattresses in the layers, which is the number of layers minus one. Because if you remember if we have a network with three layers, we're gonna have two weight mattresses because the weight mattresses are always between two subsequent layers. So like the first um weight matrix is gonna be between uh layer one and layer two. And the second uh um weight matrix is gonna be between layer two and layer three, right. So we have the constructor and with this construct, we are able to build um a representation of a simple multi-layered perception. So what do we do need to do next. Well, we need to do the actual computation. So we need to do a forward propagation. So we'll create a new method called forward proper gate. And this method uh accepts as uh as an argument the inputs. And what do we do with this forward propagate? So if you remember um what forward propagation does it at each layer, how the the signal like travels through? Uh it does like two things. So the neurons first do like a a net input and then they do uh uh an activation. So the first part of this is going to be activations is equal to inputs. So the first time, so for the first layer, the activations are basically the inputs, then we want to do a full loop and we want to loop through all the weight mattresses. And this basically means looping through all the the layers in the network. So if we do four WN self dot waits and then here we should perform a couple of things. So first thing is to um do the cal calculation. So calculate the net inputs for a given layer. And then the second thing that we want to do is to uh calculate the activations, right? So how do we do that? Yeah, with NP is quite simple because for the nets inputs, if you remember guys, what we should do is a matrix multiplication between the activations of the previous layer with the weight matrices with the weight matrix. So how do we do that? It's quite simple. So we do an N uh NP dot uh dot And this method is basically performs a uh dot product between two batches. But if we have two matrices, it performs a matrix multiplication. So in this case, we pass in activations which is a vector and then we pass in the uh the weights W right. And this is a matrix. And so uh basically here and what MP dot dot does it uh performs uh this matrix multiplication between activations and weights and so we can get the net inputs. OK. So now we have the net inputs. What's the next phase? Well, we should calculate the activation. So how do we do that? Yeah, it's quite straightforward because uh in this case, we want a, an M LP with a sigmoid activation function. So we just need to pass the uh net inputs to the sigmoid function. So how do we do that? Super simple? So we pass the net inputs to the Sigma function. Now you may be wondering but yeah, but we don't have a uh underscore Sigma uh method. Well, you, you're right, we, we need to implement that and if you remember, we already implemented this in a, in a previous video and it's quite straightforward and it will take us like two seconds to do that. OK. So now what we do is after we've gone through all of the loop, uh then we want to return the activations right. So let's review this before moving forward. So with forward propagate, we start with the inputs and this is a vector. And for the first layer, the activations are basically the inputs. And then what we do, we do a forward propagation. So we just like move from one layer to the next one, from left to right. And so we, we do for doing that, we do a four, a four loop uh through all the work we iterate through all the weights. And then at each layer, we do two things, we calculate the net inputs first. And we do that with a matrix multiplication between the activation and the weights. And then we actually calculate the activations. And we do that by passing the net inputs to the sigmoid activation function. And then finally, after we are at the end of this journey, we just return the activations great. So what remains to do here? Well, we should implement this underscore sigmoid function. So here we just want to pass X and this is super straightforward. We did it already. And the act the sigmoid function is basically 1/1 plus NP dot The exponential to the uh minus X right. Here we go super simple, right? So now we have all the elements it plays for our M LP class. And so now we can do a full uh forward propagation. And once we pass in some inputs, we can calculate like the outputs after like the mop has done its magic. So let's do that. And so in order to do that, let's uh ensure that uh we are running the script as like the main. So and we do that if name is equal to name and then we'll just move on down here. Cool. So what should we do here? So obviously, the first thing we want to do uh is to uh create an M LP. Obviously, we need a neural network. First of all, then we want to create uh some inputs, then we want to perform the forward prop propagation. And finally, we'll just like uh print the results, print oh what's that mistake there? Print the results? Cool. Okay. So let's do this. So how do we create an M LP? Super simple? So M LP is equal to the M LP class, the we call just the M LP constructor. So if we want to change the uh number of layers that we have and the number of neurons in each layer, we can do that. Uh But we'll just use this default values here. So I'm not gonna bother. OK. So let's create some inputs here. So uh inputs. So inputs is should be a vector and the vector should be, should have the same number of items as the number of neurons in the input layer. So we're gonna create some uh random inputs here. And so how do we do that again? We know our old friend random grand from the NPI LIB library here. This is a one dimensional array. So it's a vector basically. And so the di the dimension of uh the vector should be M LP dots number of inputs. So the number of neurons in the inputs, nice. We have the inputs. So we want to get the outputs out. And so to do that, it's super simple because we will just do a forward propagate and we'll pass in all the inputs. And so by now, we should have a uh a vector, an output vector. And now we want to print like what we've done. OK. So let's do print uh network. The net work output is and will pass uh the outputs here. And OK. Yeah, let's also pass in the, let's also print the inputs. So the network input is, and here obviously, we need to change this to inputs. And down here we have the outputs. So cool. So now everything should be in place. So we've built our uh M LP objects, we created some random inputs, we passed that to the network which uh performed a forward propagation. So we have the outputs and now we've uh should be able to both uh print the inputs and the outputs. So what remains to, to do here is we should just run this and hopefully, if I've done all correctly, uh we should be able to see some results go oh nice. So the network input is this three values over here. And uh it's three and it's correct because we are expecting three inputs free. Uh We, because we have three neurons in the input layers and then the output is made up of two values here. So this two values here. And again, it's correct because we are expecting two values for the outputs because we have two neurons in the upper layer. Nice. So basically, you should just like uh congratulate yourself now because you've just finished implementing a neural network from scratch. So you now know more than most people like out there about how to actually create a neural network like from scratch. And now you may be wondering, OK, but there's an elephant in the room here and that's that we haven't uh trained like our network at all. So how do we do that? Well, this is a huge topic in itself and it's gonna be the focus of my next video where I'm gonna basically explain the theory behind back propagation and gradient descent uh which are like the two approaches, the two techniques which we used to uh train network with uh some uh training data set. So yeah, next time we're gonna do that. So I hope you enjoyed this video if you did. Uh And if you want to get like more videos, remember to subscribe and hit the notification bell. Uh because that way you're gonna get like all the videos once I upload them and if you like again the video just like also leave a like and I guess I'll see you next time. Cheers."
            }
        ],
        "audio_segments": [
            {
                "id": 0,
                "transcript": "Hi, everybody and welcome to a new video in the Deep Learning for audio with Python series. This time we're gonna implement a neural network from scratch specifically, we're gonna implement a multi layer perception. So let's get started. So the first thing that we want to do is importing numpy",
                "start_time": "0.0",
                "end_time": "20.29"
            },
            {
                "id": 1,
                "transcript": "the nun P if you're not familiar with it is a um scientific library for Python that's used in most uh libraries if you don't have nun pie installed. So you, you just go and go to your terminal and pip install it. It's quite straightforward, right?",
                "start_time": "20.709",
                "end_time": "39.0"
            },
            {
                "id": 2,
                "transcript": "So uh the thing that we want to build here is a class that's called M LP or multi layer perception. So here we're gonna have all the attributes and methods that we can use to actually represent a multi layer of perception.",
                "start_time": "39.799",
                "end_time": "58.389"
            },
            {
                "id": 3,
                "transcript": "So what's the first thing that we need to do here? Well, it's the constructor. So we want the constructor for the M LP class. And so we are gonna",
                "start_time": "58.709",
                "end_time": "70.05"
            },
            {
                "id": 4,
                "transcript": "have this in it over here and we want to pass it three different attributes. So one it's the number of",
                "start_time": "70.62",
                "end_time": "81.9"
            },
            {
                "id": 5,
                "transcript": "uh inputs, then we want to pass it the number of hidden layers.",
                "start_time": "82.709",
                "end_time": "89.36"
            },
            {
                "id": 6,
                "transcript": "And finally, we want to pass it the number of outputs over here. And so we're gonna store these arguments internally in uh attributes and this not surprisingly are gonna be called uh self dots numb inputs. So, and this is gonna be the numb inputs over here. Then we're gonna have the",
                "start_time": "90.349",
                "end_time": "118.76"
            },
            {
                "id": 7,
                "transcript": "number of",
                "start_time": "119.23",
                "end_time": "120.76"
            },
            {
                "id": 8,
                "transcript": "hidden layers over here and this is gonna be number of hidden and then we're gonna have this num outputs",
                "start_time": "121.47",
                "end_time": "131.869"
            },
            {
                "id": 9,
                "transcript": "outputs and we have the number of outputs over here. Well, so this way we just store internally to the instance all of this uh information about the, the number of neurons in the input layer in the uh hidden layers and the in the output layers. So we can pass in some default values over here.",
                "start_time": "132.71",
                "end_time": "157.449"
            },
            {
                "id": 10,
                "transcript": "Uh because uh that's gonna be like easier like when we implement an M LP. So we don't need to pass all of this information every time. So we could say that our default number of inputs. So number of neurons in the input layer is gonna be three for example. And for the hidden layers, it is gonna be slightly different because we want to pass in a list and this is gonna be a list of the integers and it's",
                "start_time": "157.589",
                "end_time": "186.429"
            },
            {
                "id": 11,
                "transcript": "integer is gonna represent the number of neurons in a hidden layer. So uh basically, in this case, we have a neuron with two hidden layers. So the first hidden layer has three neurons and the second layer has a hidden layer has five neurons. And then we finally, we have the number of outputs. And so these are the output neurons, let's say we have two, for example, right. So",
                "start_time": "187.57",
                "end_time": "214.139"
            },
            {
                "id": 12,
                "transcript": "now we want to create a an internal uh representation of the layers. So how do we do that? Well, we want this as a as a list. And in order to do that, we are gonna",
                "start_time": "214.38",
                "end_time": "231.74"
            },
            {
                "id": 13,
                "transcript": "uh cast to at least type this self",
                "start_time": "232.559",
                "end_time": "237.649"
            },
            {
                "id": 14,
                "transcript": "dot number of inputs over here, then we are gonna concatenate that with the number of hidden layers. And then we're gonna concatenate this with the number of output layers. So",
                "start_time": "238.199",
                "end_time": "255.809"
            },
            {
                "id": 15,
                "transcript": "what this does is basically getting a list where each item in the list represents the number of neurons in a layer. And the layers obviously move from uh like zero to, to the, to the zero index to the uh number of layers that we have. OK. So next, what do we want to do? Well, we need to initiate a random weights.",
                "start_time": "256.049",
                "end_time": "285.16"
            },
            {
                "id": 16,
                "transcript": "So a a neuron network is isn't a neural network if you don't have weights uh for all the connections between the neurons in a subsequent layers. So how do we do that? Well, first of all, let's initiate",
                "start_time": "285.309",
                "end_time": "301.41"
            },
            {
                "id": 17,
                "transcript": "a,",
                "start_time": "302.119",
                "end_time": "302.91"
            },
            {
                "id": 18,
                "transcript": "an internal like an attribute called weight. And this is gonna be an empty list for the time being. And then we want to iterate through all the layers and then create a matrix uh weight for each uh pair of layers. So how do we do that? Well, we do a full look in a range of the length",
                "start_time": "303.519",
                "end_time": "332.679"
            },
            {
                "id": 19,
                "transcript": "uh layers minus uh one.",
                "start_time": "333.01",
                "end_time": "338.1"
            },
            {
                "id": 20,
                "transcript": "So, and then at each uh step, we're gonna build this weight matrix W and W is gonna be equal to the uh nun",
                "start_time": "339.269",
                "end_time": "355.1"
            },
            {
                "id": 21,
                "transcript": "dot random dot run.",
                "start_time": "355.82",
                "end_time": "359.709"
            },
            {
                "id": 22,
                "transcript": "And here will pass in layers I layers I and here we have layers",
                "start_time": "360.23",
                "end_time": "372.66"
            },
            {
                "id": 23,
                "transcript": "I plus one. OK. So what have I done here? So uh I'm basically creating a matrix",
                "start_time": "373.239",
                "end_time": "384.589"
            },
            {
                "id": 24,
                "transcript": "uh which is this W here using this method uh from the numpy library which is random dot brand. And what I uh what this like um method does, it, it creates uh a random uh arrays and the arrays can have different uh dimensions. So in this case, we have like two dimensions. So and the two dimensions are given here by these values.",
                "start_time": "384.98",
                "end_time": "412.489"
            },
            {
                "id": 25,
                "transcript": "And so here, what I'm saying is that I want a two D array which is basically a matrix. And the, the, the number of rows that I want is the current uh layer I'm in. And the number of columns that I want is the uh the the number of neurons that I have in the subsequent layer.",
                "start_time": "412.72",
                "end_time": "435.079"
            },
            {
                "id": 26,
                "transcript": "And if you remember the weight matrix, this works because on the rows, we have all the connections of a neuron from a previous layer with the subsequent layer. So we want the for the",
                "start_time": "435.309",
                "end_time": "452.47"
            },
            {
                "id": 27,
                "transcript": "number of rows to be equal to the number of neurons that we have like in a, in a layer. And for the number of columns we want to have uh the number of neurons that we have in the subsequent layers. And we can obtain that by using this expression here. Right.",
                "start_time": "452.48",
                "end_time": "470.57"
            },
            {
                "id": 28,
                "transcript": "And so, and what this random random does is obviously it creates this uh to the uh array. So this matrix and it initiates all the elements of the, the matrix randomly between zero and one,",
                "start_time": "470.91",
                "end_time": "485.42"
            },
            {
                "id": 29,
                "transcript": "right. So what else remains to do? Well, we we need to store this information and say how do we do that? Well, super simple. So we get the weights and we do append W",
                "start_time": "485.97",
                "end_time": "498.79"
            },
            {
                "id": 30,
                "transcript": "and so here uh weight is gonna be a list uh with as many um items as the number of weight mattresses in the layers, which is the number of layers minus one. Because if you remember if we have a network with three layers, we're gonna have two weight mattresses",
                "start_time": "499.119",
                "end_time": "523.218"
            },
            {
                "id": 31,
                "transcript": "because the weight mattresses are always between two subsequent layers. So like the first um weight matrix is gonna be between uh layer one and layer two. And the second uh um weight matrix is gonna be between layer two and layer three,",
                "start_time": "523.53",
                "end_time": "541.7"
            },
            {
                "id": 32,
                "transcript": "right. So we have the constructor and with this construct, we are able to build um a representation of a simple multi-layered perception. So what do we do need to do next. Well, we need to do the actual computation. So we need to do a forward propagation. So we'll create a new method called forward proper gate.",
                "start_time": "541.869",
                "end_time": "568.89"
            },
            {
                "id": 33,
                "transcript": "And this method uh accepts as uh as an argument the inputs.",
                "start_time": "569.96",
                "end_time": "578.45"
            },
            {
                "id": 34,
                "transcript": "And",
                "start_time": "579.19",
                "end_time": "580.39"
            },
            {
                "id": 35,
                "transcript": "what do we do with this forward propagate? So if you remember um what forward propagation does it at each layer, how the the signal like travels through? Uh it does like two things. So the neurons first do like a a net input and then they do uh uh an activation. So the first part of this is going to be",
                "start_time": "580.909",
                "end_time": "607.2"
            },
            {
                "id": 36,
                "transcript": "activations is equal to inputs. So the first time, so for the first layer, the activations are basically the inputs, then we want to do a full loop",
                "start_time": "609.19",
                "end_time": "623.58"
            },
            {
                "id": 37,
                "transcript": "and we want to loop through all the weight mattresses. And this basically means looping through all the the layers in the network. So if we do four WN self",
                "start_time": "624.2",
                "end_time": "640.179"
            },
            {
                "id": 38,
                "transcript": "dot waits and then here we should perform a couple of things. So first thing is to",
                "start_time": "641.01",
                "end_time": "651.09"
            },
            {
                "id": 39,
                "transcript": "um do the cal calculation. So calculate the net inputs for a given layer. And then the second thing that we want to do is to uh calculate",
                "start_time": "651.599",
                "end_time": "667.58"
            },
            {
                "id": 40,
                "transcript": "the activations, right? So how do we do that? Yeah, with NP is quite simple because for the nets inputs, if you remember guys, what we should do is a matrix multiplication between the activations of the previous layer with the weight matrices",
                "start_time": "668.38",
                "end_time": "693.84"
            },
            {
                "id": 41,
                "transcript": "with the weight matrix. So how do we do that? It's quite simple. So we do an N uh NP dot uh dot And this method is basically performs a uh dot product between two batches. But if we have two matrices, it performs a matrix multiplication.",
                "start_time": "694.099",
                "end_time": "711.89"
            },
            {
                "id": 42,
                "transcript": "So in this case, we pass in activations which is a vector and then we pass in the",
                "start_time": "712.419",
                "end_time": "720.13"
            },
            {
                "id": 43,
                "transcript": "uh the weights W right. And this is a matrix. And so uh basically here and what MP dot dot does it uh performs uh this matrix multiplication between activations and weights and so we can get the net inputs. OK. So now we have the net inputs. What's the next phase? Well, we should calculate the activation. So how do we do that?",
                "start_time": "720.84",
                "end_time": "748.26"
            },
            {
                "id": 44,
                "transcript": "Yeah, it's quite straightforward because uh in this case, we want a, an M LP with a sigmoid activation function. So we just need to pass the uh net inputs to the sigmoid function. So how do we do that? Super simple? So we pass the net inputs to the Sigma function.",
                "start_time": "748.7",
                "end_time": "771.969"
            },
            {
                "id": 45,
                "transcript": "Now you may be wondering but yeah, but we don't have a uh underscore Sigma uh method. Well, you, you're right, we, we need to implement that and if you remember, we already implemented this in a, in a previous video and it's quite straightforward and it will take us like two seconds to do that. OK. So now what we do is after we've gone through all of the loop, uh then we want to return the activations",
                "start_time": "772.27",
                "end_time": "802.2"
            },
            {
                "id": 46,
                "transcript": "right.",
                "start_time": "802.45",
                "end_time": "803.52"
            },
            {
                "id": 47,
                "transcript": "So let's review this before moving forward. So with forward propagate, we start with the inputs and this is a vector. And for the first layer, the activations are basically the inputs. And then what we do, we do a forward propagation. So we just like move from one layer to the next one, from left to right. And so we, we do for doing that, we do a four, a four loop uh through all the work",
                "start_time": "804.039",
                "end_time": "833.075"
            },
            {
                "id": 48,
                "transcript": "we iterate through all the weights. And then at each layer, we do two things, we calculate the net inputs first. And we do that with a matrix multiplication between the activation and the weights. And then we actually calculate the activations. And we do that by passing the net inputs to the sigmoid activation function. And then finally, after we are at the end of this journey, we just return the activations great. So",
                "start_time": "833.085",
                "end_time": "860.549"
            },
            {
                "id": 49,
                "transcript": "what remains to do here? Well, we should implement this underscore sigmoid function.",
                "start_time": "860.78",
                "end_time": "867.77"
            },
            {
                "id": 50,
                "transcript": "So here we just want to pass X and this is super straightforward. We did it already. And the act the sigmoid function is basically 1/1 plus NP dot The exponential to the uh minus X right. Here we go super simple, right? So now we have all the elements",
                "start_time": "868.78",
                "end_time": "896.7"
            },
            {
                "id": 51,
                "transcript": "it plays for our M LP class. And so now we can do a full uh forward propagation. And once we pass in some inputs, we can calculate like the outputs after like the mop has done its magic. So let's do that. And so in order to do that, let's uh ensure that uh we are running the script as like the main.",
                "start_time": "896.909",
                "end_time": "926.359"
            },
            {
                "id": 52,
                "transcript": "So and we do that if name is equal to name and then we'll just move on down here. Cool. So what should we do here? So obviously, the first thing we want to do",
                "start_time": "926.679",
                "end_time": "942.919"
            },
            {
                "id": 53,
                "transcript": "uh is to uh create an M LP. Obviously, we need a neural network. First of all, then we want to create uh some inputs,",
                "start_time": "943.52",
                "end_time": "956.65"
            },
            {
                "id": 54,
                "transcript": "then we want to perform the forward prop propagation. And finally, we'll just like uh print the results, print oh what's that mistake there? Print the results? Cool. Okay. So let's do this. So how do we create an M LP? Super simple? So M LP is equal to",
                "start_time": "957.64",
                "end_time": "984.51"
            },
            {
                "id": 55,
                "transcript": "the M LP class, the we call just the M LP constructor. So if we want to change the uh number of layers that we have and the number of neurons in each layer, we can do that. Uh But we'll just use this default values here. So I'm not gonna bother. OK. So let's create some inputs here. So",
                "start_time": "985.929",
                "end_time": "1007.229"
            },
            {
                "id": 56,
                "transcript": "uh inputs. So inputs is should be a vector and the vector should be, should have the same number of items as the number of neurons in the input layer.",
                "start_time": "1008.0",
                "end_time": "1021.5"
            },
            {
                "id": 57,
                "transcript": "So we're gonna create some uh random inputs here. And so how do we do that again? We know our old friend random grand from the NPI LIB library here. This is a one dimensional array. So it's a vector basically. And so the di the dimension of uh the vector should be M LP dots number of inputs. So the number of neurons in the inputs, nice. We have the inputs.",
                "start_time": "1021.969",
                "end_time": "1049.369"
            },
            {
                "id": 58,
                "transcript": "So we want to get the outputs out. And so to do that, it's super simple because we will just do a forward propagate and we'll pass in all the inputs. And so by now, we should have a uh a vector, an output vector.",
                "start_time": "1049.569",
                "end_time": "1071.959"
            },
            {
                "id": 59,
                "transcript": "And now we want to print like what we've done. OK. So let's do print uh network.",
                "start_time": "1072.17",
                "end_time": "1080.459"
            },
            {
                "id": 60,
                "transcript": "The net work output is and will pass uh the outputs here. And",
                "start_time": "1081.29",
                "end_time": "1094.229"
            },
            {
                "id": 61,
                "transcript": "OK. Yeah, let's also pass in the, let's also print the inputs. So the network input is, and here obviously, we need to change this to inputs. And down here we have the outputs. So cool. So now everything should be in place. So we've built our uh M LP objects, we created some random inputs, we passed that to the network which",
                "start_time": "1095.119",
                "end_time": "1119.88"
            },
            {
                "id": 62,
                "transcript": "uh performed a forward propagation. So we have the outputs and now we've uh should be able to both uh print the inputs and the outputs. So what remains to, to do here is we should just run this and hopefully, if I've done all correctly, uh we should be able to see some results",
                "start_time": "1120.14",
                "end_time": "1140.119"
            },
            {
                "id": 63,
                "transcript": "go oh nice. So the network input is this three values over here. And uh it's three and it's correct because we are expecting three inputs free. Uh We, because we have three neurons in the input layers",
                "start_time": "1140.729",
                "end_time": "1160.869"
            },
            {
                "id": 64,
                "transcript": "and then the output is made up of two values here. So this two values here. And again, it's correct because we are expecting two values for the outputs because we have two neurons in the upper layer. Nice. So basically, you should just like uh congratulate yourself now because you've",
                "start_time": "1161.119",
                "end_time": "1187.199"
            },
            {
                "id": 65,
                "transcript": "just finished implementing a neural network from scratch. So you now know more than most people like out there about how to actually create a neural network like from scratch. And now you may be wondering, OK, but there's an elephant in the room here and that's that we haven't uh trained like our network at all. So how do we do that?",
                "start_time": "1187.339",
                "end_time": "1214.0"
            },
            {
                "id": 66,
                "transcript": "Well, this is a huge topic in itself and it's gonna be the focus of my next video where I'm gonna basically explain the theory behind back propagation and gradient descent uh which are like the two approaches, the two techniques which we used to uh train network with uh some uh training data set.",
                "start_time": "1214.13",
                "end_time": "1235.67"
            },
            {
                "id": 67,
                "transcript": "So yeah, next time we're gonna do that. So I hope you enjoyed this video if you did. Uh And if you want to get like more videos, remember to subscribe and hit the notification bell. Uh because that way you're gonna get like all the videos once I upload them and if you like again the video just like also leave a like and I guess I'll see you next time. Cheers.",
                "start_time": "1236.239",
                "end_time": "1262.589"
            }
        ]
    }
}