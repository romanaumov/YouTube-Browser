{
    "jobName": "transcript-job-audio-assistant",
    "accountId": "337909742319",
    "status": "COMPLETED",
    "results": {
        "transcripts": [
            {
                "transcript": "Hi everybody and welcome to yet another video in the deep learning for audio with Python series. This time, it's super exciting because we are looking into convolutional neural networks and we'll try to explain how they work on a theoretical level, right? So what are CNN S? Well CNN S are quite advanced type of um neural network. And they've been mainly used for processing images. And over time, we've found out that they are way better at performing liquid images than the equivalent, for example, like multi-layered perception uh architecture. And the the great thing about CNN S is that they have at the same time way less parameters than dense layers. So when analyzing images, we have this double advantage, so they perform better and they have less parameters than the multi layer perception. But now you may be wondering, but why is that the case? Well, it turns out that image data is somehow uh structured. So the pixels are not just like randomly like uh positions like in an image, but usually there are certain emergent structures. So for example, you have structures like edges shapes, you have invariants to translation, you have scale invariants So for example, a square remains a square, square re regardless of how big the square is, right? And in a sense like what we do with CNN S is we try to emulate the way we see stuff and we perceive like images, right? And uh by doing so what we do when we, when we see like stuff, it's basically we extract uh basic features. So for example, we we are able to extract when we see something like vertical bars or horizontal bars. And in a sense, all the different components of A CNN try to uh learn to extract different uh types of features. So uh all of this process uh in A CNN is done relying on a, on a couple of components mainly. So one is the one that provides the name to CNN. So it's called convolution and the other one is called uh pooling. So uh in the remaining part of this video, we'll take a look at this two processes, these two components like in some somewhat in detail. OK. So let's start with convolution. So at the center of convolution, we have the idea of a kernel, uh we can call it a kernel or you can also call it a filter. And now uh like over the next few slides, you'll understand why that's the case. But in its simplest um wave like of understanding it, a kernel is no more than a grid, a weights like this over here, you have a three by three kernel with a bunch of weights. And the idea is that we apply the kernel to the image, right. So we have an image and then we apply the kernel. Now I'm gonna specify how, what this means like mathematically like in a second. But before get getting that, I want to give you like an overview like of these kernels and convolutions, right? So uh the uh other ideas that's uh kernels like in filters have been traditionally used in uh image processing. So like for detecting edges, for example, for or for creating effects like a blur or things like that, right? Uh But now they, they, we found out that in deep learning, they are very, very useful for processing images as well. Cool. OK. So now let's uh try to understand how convolutions work starting from an image. Now this is a gray scale image, which basically means that we have a bunch of pixels that um can vary between zero and 255 where zero is basically black and 255 is white. And so we can take this uh the image of this cat and we can translate it into like this greed uh of pixels and these values which are supposed to be between zero and 253 55 represents like the values for each pixel, right? So I shouldn't notice here. I've used quite like small values just like, yeah, because it's easier uh right. OK. So now let's move on. So we have this translation from an image to its pixel representation. Cool. So we said that we should apply a kernel to the image, right. So we have the pixel representation of the image, we have a kernel here. And the results of the convolution is a uh itself like a an output uh which is agreed. And in the case of like the, the settings that I'm gonna use, we're gonna get an output that basically has the same size in terms of width and height of the original image over here. Now let's try to understand how convolution works. So what we do basically is we overlay the kernel on top of the image. And so, for example, here you'll see that we are overlaying the kernel on top of like this initial uh red square here on the image and we center it around uh like the center like of the kernel. And here it corresponds like to these values to this four that's highlighted in green. Now, uh when we do that, uh we get a, a value and the value of the convolution is then input in the, in this like output grid over here, which is basically like at the same index of the original uh image uh where we've centered the kernel on, right. So how do we get to a value there? Well, what we do basically is we just apply the dot product uh having like the two like vectors like the uh the image and the kernel itself. And for the image we just like consider all of these values here in the red square uh which are like correspondence uh kind of like to the kernel. So we have like the same number of bodies there, right? And so what we do and we should know how to do A dot Products. Because if, if you don't remember, you should go just and watch back like my video on linear algebra introduction to linear algebra. But basically what we do here is we take each value and from the image and we multiply that for the correspondence uh value at the, at the same index of the kernel. So like in this case, we have five by one and then we uh add to that to multiplied by zero, then we add to that three by zero, then we add two plus two. Well, I mean, you get the gist and the last number like the last value that we have is this zero multiplied by minus one over here. Now, uh when we uh take all of these like uh different like expressions and we run the math, we end up with 18, which is the value for this convolution. Now not 100% sure that 18 is right? Because I was like, I did things like quite quickly but yeah, you can try that out and see like if 18 is actually like the, the right results for this product. But regardless, let's assume 18 is correct. OK. So we'll take that 18 and we'll just it in here in the output grid over here, right? So how do we continue here? Well, it's quite simple because now we slide the kernel on the image here and we slide to the right and now we center the uh the kernel on this like index here with the the one highlighted in green in the image, right? And we redo the the dot products and we get 10, we move on, we slide and we get like another value and it's minus three, we slide again and we get five. Now we go down, right? We go down one and we go back to the start and we have like another value and we continue like this until basically we can uh arrive at all the numbers for like this square here like highlighted in green. Now I've left uh question marks because I didn't want to do like the calculation. But this is a great uh way for you to practice with convolution. So you can just like run the numbers there and uh yeah and write perhaps like the results like in the comments, right? But you may see that there's an issue here, right? Because we have the values of the convolutions just like for this internal square but the edges over here don't get a number. And why is that an issue? Well, it's why, why, why is it an issue like to run that uh map on the, on the edges? Right? And the reason is like uh quickly explained by uh just opposing like the the kernel on the edges here. So for example, like if we center like the kernel on this 00 like index over here, right? Uh We see that uh yeah, we have a part like of this uh kernel that can't be like applied to like any anything really because we don't have values over there. So what, what, what can we do there? Well, there are a couple of solutions. So one it's kind of super straightforward, we say wait, well, wait. So we, we can't apply the kernel there. So we, we'll just ignore it. So we'll just ignore the edges, right? Yeah, this is a solution in itself. But the problem with that is that we are missing, we are losing some information, right. So all the edges on the image, the other solution uh which is the one that we usually use in deep learning is applying a type of padding called zero padding. So we come up with an edge with an artificial edge that has all zeros for the purpose of calculation, right? And so if we do that, then all of a sudden we can calculate the, we can perform the convolution also like on the edges of the image, right. And so if we uh run the calculations also like on the edges, we end up with an output like this where we basically have a uh the output convolution which is this uh grid uh that where like the k the kernel has been like applied on the image. And we have like all the different results. Again, question marks where I haven't done like the, the the calculation cool. OK. So this is like the basic idea of a convolution. So, but let's think of a kernel. And so, and what that is uh on a, on a semantic level. Well, we can think of a kernel or a filter as a feature detector. So basically, we can have a bunch of like uh different kernels that detects different uh features. So for example, this kernel here uh which has like these ones like on this uh diagonal here, uh it's able to detect oblique lines like diagonals, right lines, right. Whereas this type of kernel here could be used to uh identify vertical lines. And so this is a vertical line uh detector, right? So now when we have a convolutional neural network, as I mentioned earlier, what we do is we, we kind of like extract features uh using uh these kernels. But now the great thing about CNN is that we don't uh hard wire those, those kernels rather we learn them in the process so we learn the, so the the network itself learns the kernels that it needs to extract in order to perform well, in some classification of whatever task we may think of. Right? And what does it mean to learn a kernel? Well, we, what the uh what we do basically is we learn the values, the weights of the kernel, all of these numbers here, right? And so when we train a CNN, basically, we are training uh these values in the kernels cool. OK. So uh when we um handle like convolution, we have a bunch of like architectural decisions that we can take to decide which type of convolutions uh like to use, right? And so uh here I have like a few uh things, a few settings that we should specify uh when we build our architecture. So we should specify the grid size of the convolution, the stride, the depth and the number of kernels. So let's look into this one by one. Let's start from grid size. Well, this is like very intuitive, right. So uh the grid size is just the number of pixels for the height and the width of the grid. So in this case, for example, we have a three by three kernel and in this case, we have a five by five kernel because we have just like 55 values like for, for the width and five values like for, for each height, right. So, and obviously like, let's remember that uh each uh like of these guys basically, like it's equivalent like to, to a pixel and it analyzes just like one pixel when it performs uh when we perform convolutions, right? So you'll notice that I'm using um grid sizes with odd numbers. So why is that the case? Well, that's the case. Uh because when we have odd numbers, we have a central um a value that we can use like as a, as a center as a reference uh on the image when we start and run the uh the, the convolution, right? And so it's, it's usually like you, you'll see odd numbers. Now um there, there's usually you'll also see um square kind of like uh kernels. But you can potentially use also like nouns square like rectangular uh uh grids, right? For example, a one by three or a three by one kernel, right? So this was about great size. Now, let's move on to another parameter stride. So what's the stride? Well, the stride is quite simple. It's the step size that we use for sliding the kernel on the image, right? So, and the stride itself again is indicated in pixels. So let's try to look at a stride. Uh So here we have um our image and we have a three by three kernel that we are applying. Now, we let's say we have a stride, a one. So we'll just move like this. So we start here, then we'll move this by one pixel and then we'll keep moving by another pixel and so on and so forth. And so as you can see, we are sliding just by one pixel and this is a stride of one. Now, let's take, let's try a stride of two. This again, like is also like quite simple to understand because like we start in this position and then we jump by two pixels right. Here we go. So this is a stride of two. Now, uh this you can specify both the horizontal stride and the vertical stride. So uh when you arrive like at the end of the image, like how much you want to go down in the image say like by one pixel or two pixels. And the important thing to understand is that again, the stride uh doesn't necessarily need to be uh like the same for like the horizontal value and the vertical value, we can have a stride of two horizontally and a stride of one vertically, for example. But usually you tend to use like uh the same uh stride uh for like the vertical part and the horizontal cool. So these are so grid size and straight are very important uh like settings for uh the kernel. But then we have another one which is a depth even though I would say like this is like this comes like uh it's more constrained. You don't have like much like leverage like there and you can decide what you want to do. But the basic idea is that if you have a gray scale image, the depth is equal to one. But if you have a color image that's like for example, represented in, in our R GB, what that basically means is that that image that each pixel for uh a color image has three values, one for the red, one for the green and one for uh the blue collar, right? And so what we do in terms of kernels is we have a kernel. Uh So the the kernel like is divided like into three like parts, three grids. And so we have a grid for the red channel, we have a grid for the green channel and the third grid for the blue channel. And so, and these are independent, right? So a kernel in the case of R GB uh image data, it's gonna have uh a three dimensional uh it's, it's a three dimensional array, right. So where like the, the first two dimensions uh represent the, for example, the width and the height of the, of the kernel and the third dimension here it's the depth. So in this case, in this example, we have a three by three kernel which again, we should multiply by three which because that's the depth basically it's like 32 dimensional uh grids. So one for red, one for green and one for blue. And so the total number of weights in this case is equal to 27 because it's basically three by three uh by three, right? OK. So let's remember guys. So if we are dealing with uh R GB data, we're gonna have a three dimensional array as our kernel where the first two dimensions are the width and the height. And the third dimension is the depth or it's also called the channel, right? Cos in R GB we have three channels great. So now on to the last uh setting that we want to look into, so this is the number of kernels, but this is really not that related to like the the kernel itself, but it's related to the to the convolutional layer. Now, convolutional layer can have and usually has multiple kernels. But let's remember that each kernel outputs a single two dimensional array and it's that output convolution that we've calculated before, right? And so um one question could be, so how many outputs do we have from a convolutional layer? Well, we have as many two D arrays as the number of kernels, right? So if we have for example, five kernels in a convolutional layer, so we're gonna have 52 D arrays because we're gonna apply each kernel to the input image and we're gonna get five separate output uh to the to the arrays, right? OK. So this is the number of kernels. Now we've learned quite a lot about uh convolution. Now we need to look at the other side of the coin. Well, I should say like the other part that's usually used in CNN that's called pooling. Now, the, the the the most difficult part is behind this because pooling is quite intuitive. So what pooling does like at the end of the day is just like down sampling an image. So it's basically shrinks an image and we do that in a sense in a similar manner to convolution in the sense that we overlay a grid on top of an image. And then in terms of pooling, we have like different options. And the, the, the two that are like the most used are like max pooling and average pooling even though I should say that. Um Yeah, I'd say like in deep learning, like max pooling like is the, is the main one right now, as I said, like pulling is quite uh intrusive and simple and it doesn't have like any parameters. So we don't learn anything in terms of, of pooling. It's a simple like mathematical operation that we perform. OK. So let's uh take a look at the different pooling settings that we'll have. And so we have like very simple ones again, like there's the grid size that determines like how big like the uh the pooling grid is gonna be, we have the stride again, which is like the, the, the step uh the step size used for sliding the grid on top of an image and then we have the type of pooling. So max pooling or average poling. OK. So now we know like we have like an overview of pooling. Uh Now let's take a look at the at how pooling really works. OK. So here we'll take a look at a max pooling where we have a two by two grid and we are gonna be using a stride of two, both horizontally and vertically, right? OK. So here we have our two by two um pulling greed. And so, and, and we just like go like on top of like the input and so you can image like you can image that this is an image, right? And, and so here uh we have like these values, right? And given we are doing max pooling, we just pick the, the greatest value and we log it in the output grid over here, right? OK. And so in by doing so we are down sampling uh the original input because basically we are just like getting one parameter out of like these four parameters. Now, how are we gonna continue doing this? Well, if you've like followed uh like attentively like up until now. So you should know that. Now, I'm just gonna slide the grid with a stride of two. So I'm moving two pixels like on the right. And then again, here I'm gonna do the same thing. I'm gonna pick the highest number, which in this case is 10 and I'm gonna log it over here. Right. We continue and we pick 12 and finally we pick seven. Now, um, there's obviously a mathematical relationship between the size of the output, the size of the input and the, the stride, right? And the grade size, right? Ok. So I'm not gonna give you like the general rule but, uh that you can, and you should definitely like uh understand by yourself. But like in this case where we have a uh two by two max pooling reed uh which try to, you'll see that we are basically uh hal like the width and the height of the input. So here we have a four by four like input um grid and the output grid in this case is a two by two, right? OK. So we're basically done. So we know what pooling is. So now we we should uh put together like everything we've learned and understand that a CNN uh just like uses like yeah, pooling and uh convolutions, right? So convolutional layers and pooling layers uh and uh and it uses it like by putting more of this like uh together. So let's take a look at at a typical CNN architecture. So here we have the input, which is kind it's a simple image, right? And then here we have a feature learning um I would say like face like in the in the CNN where we have a bunch of like convolutional layers followed by pooling layers. And so now the um the the network can be like as deep as you want really. So you can have up to like 50 100 layers, if not more, right? And after like the feature learning like phase, usually you get a uh fully connected uh like layer. So here at the end of like this uh the feature learning like section of the CNN, we usually like flatten the results. So we, we move it like from two D to like a one D uh vector. And then we, we pass that information into like one or more fully connected layers. And in the end, we have a soft max classifier which provides us with a uh dis probability distribution on top of a number of like different categories. So like uh like in this case, we are trying to like in this particular example, we are trying to classify different types of like uh transportation vehicles. And so here soft max is gonna give us like values for car track, van, bicycle train, right? Or airplane, for example, right? But let's take a look at this feature learning uh uh like segment of the CNN, which is like the most interesting one for our purposes, right. So what happens here is basically that at each uh convolution um uh a convolutional layer, basically what we're doing is we're trying to extract features, right? And basically, the idea is that at the beginning towards like the beginning of this feature learning segment, we extract uh very uh like low level features. So it could be like edges. Uh but then uh moving forward uh like from edges, we can use, we can leverage them, for example, to uh to arrive at shapes, then going deeper in the network from shapes, we can arrive at objects. So for example, like a car could be, I don't know, like uh could be represented say by like a few circles uh which could be like the um the tires uh and a and a rectangle, for example, that could be like, I mean the whole uh like space like of the of the car uh like itself, right? So and but this is like important to understand. So we we start with low level features and then while we move from one convolution layer to the next we abstract higher level features. Cool. And so this is like an intuition how a CNN architecture uh works? Cool. OK. So now you may be wondering, OK, so we've spent so much time on talking about how wonderful CNN S are with images. So what about audio? After all, we are talking about deep learning for audio like in this uh uh series, right? Yeah. But uh the great thing is that uh we can think of audio in a sense itself as a, as an image, right? So if you guys remember, so what we usually use when we um use deep learning like audio. Uh we we use like spectrograms, we use MF CCS all of this type of like features, but now we've already like visualize them like as an image. And at the end of the day, these are big, we can be interpreted as images, right? Both the spectrogram and the MF CCS, right? So uh in the case of a spectrogram, for example, time, the different time beings and the frequencies that we have here can be folded like the X and Y indexes for the the pixels of of an image and the amplitude can be thought of the the the value associated to each pixel, right. So in a sense, a spectrogram is a two dimensional array which is really comparable to to an image. And the great thing is that a spectrogram and audio in general has I would say like structures in them that are like in a sense like similar to like images. So like the the uh the data itself like is somehow like correlated. It's it's not like completely like random. And so because of like those structures that we can identify in spectrogram and MS CCS CNN work really really well because they are able to extract features while we uh like apply them when we apply like convolutions like on the uh on the audio data, right? So now that we have an intuition of how like all your data can be used in CN MS. Let's try to look at a specific example where we look at MF CCS for uh data, right? And so now the question uh that I want to ask you is like given like these uh settings for uh these like MF CCS like and for all of these audio data, what's gonna be like the, the data shape, right? So we have 13 MFCC, we have a hop length of 512 samples. Now, if you don't re remember what a hop length is or an MFCC uh like is just like go back to my previous videos where I introduced like all of these audio features. Cool. So we said hop length 512 samples and then we have the total number of samples in a audio file that we are analyzing. That's conveniently 51,200. Now, the question is what's the data shape that we, we, we're gonna expect our CNN to be fed with and right. So the data shape is 100 by 13 by one. So let's analyze why that's the case. So here we have uh 100 different like time windows at which we take 13 values which are the, the 13 MF CCS that we are extracting, right? And 100 comes by dividing the overall number of samples in the audio file by the hop length which is like the, the sliding window that we use uh like to calculate the MF CCS. And so again, like the two, these two values are like quite understandable. Uh because we have, we, we understand them saying yeah, we have 100 time windows at which we've taken the 13 coefficients uh uh the first in MF CCS. Um But why do we have a third dimension here? So by one, do you guys remember depth? Yeah. So in this case, we have a depth which is equal to one. So basically all your data can be um kind of like compared to like gray scale images where the depth is equal to one. So we don't have like R GB like representation of audio data. It's just like gray scale, we just have depth, just one channel, right? And so you may be wondering well, but why do we need to like give like this third dimension? Isn't that redundant? Well, it could be but again, like CNN S are like supposed to work like with uh images and usually images, right are color images most often time. And so they have like channels. So the depth is very important and right? And uh in this case, I'm giving you like this data shape here because this is like what tensorflow is gonna like accept uh as yeah for learning purposes. And so like it's good to get like into that like frame of mind. Wow. So this was intense. But at the same time now, you should have a quite clear understanding of a convolutional neural network, what its components are, what's like uh the the processes that come into place, how uh like convolutional pooling layers like work and how the overall architecture like is built together. So we should just ask ourselves as usual at the end of these videos and say what's up next? Well, as usual, we'll take all of this theoretical uh information and in the next video we're gonna uh put that into practice. So what we'll do is implementing a music genre classifier. But this time we are gonna be using a CNN. So stay tuned for that. I hope you really enjoyed the uh the this video. If that's the case, just like the video and if you want to know more and have like more videos like this and never miss one, please consider subscribing and I guess I'll see you next time. Cheers."
            }
        ],
        "audio_segments": [
            {
                "id": 0,
                "transcript": "Hi everybody and welcome to yet another video in the deep learning for audio with Python series. This time, it's super exciting because we are looking into convolutional neural networks and we'll try to explain how they work on a theoretical level,",
                "start_time": "0.0",
                "end_time": "15.539"
            },
            {
                "id": 1,
                "transcript": "right? So what are CNN S? Well CNN S are quite advanced type of um neural network. And they've been mainly used for processing images. And over time, we've found out that they are way better at performing liquid images than the equivalent, for example, like multi-layered perception uh architecture.",
                "start_time": "15.729",
                "end_time": "39.4"
            },
            {
                "id": 2,
                "transcript": "And the the great thing about CNN S is that they have at the same time way less parameters than dense layers. So when analyzing images, we have this double advantage, so they perform better and they have less parameters than the multi layer perception. But now you may be wondering, but why is that the case? Well, it turns out that image data is somehow uh structured.",
                "start_time": "39.54",
                "end_time": "65.308"
            },
            {
                "id": 3,
                "transcript": "So the pixels are not just like randomly like uh positions like in an image, but usually there are certain emergent structures. So for example, you have structures like edges shapes, you have invariants to translation, you have scale invariants So for example, a square remains a square, square re regardless of how big the square is, right?",
                "start_time": "65.58",
                "end_time": "91.279"
            },
            {
                "id": 4,
                "transcript": "And in a sense like what we do with CNN S is we try to emulate the way we see stuff and we perceive like images, right?",
                "start_time": "91.51",
                "end_time": "103.029"
            },
            {
                "id": 5,
                "transcript": "And uh by doing so what we do when we, when we see like stuff, it's basically we extract uh basic features. So for example, we we are able to extract when we see something like vertical bars or horizontal bars. And in a sense, all the different components of A CNN try to uh learn to extract different uh types of features. So",
                "start_time": "103.3",
                "end_time": "129.83"
            },
            {
                "id": 6,
                "transcript": "uh all of this process uh in A CNN is done relying on a, on a couple of components mainly. So one is the one that provides the name to CNN. So it's called convolution and the other one is called uh pooling. So uh in the remaining part of this video, we'll take a look at this two processes, these two components like in some somewhat in detail. OK. So let's start with convolution.",
                "start_time": "130.44",
                "end_time": "159.02"
            },
            {
                "id": 7,
                "transcript": "So at the center of convolution, we have the idea of a kernel, uh we can call it a kernel or you can also call it a filter. And now uh like over the next few slides, you'll understand why that's the case. But in its simplest um wave like of understanding it, a kernel is no more than a grid, a weights like this over here, you have a three by three",
                "start_time": "159.169",
                "end_time": "185.264"
            },
            {
                "id": 8,
                "transcript": "kernel with a bunch of weights. And the idea is that we apply the kernel to the image, right. So we have an image and then we apply the kernel. Now I'm gonna specify how, what this means like mathematically like in a second. But before get getting that, I want to give you like an overview like of these kernels and convolutions, right? So",
                "start_time": "185.274",
                "end_time": "207.399"
            },
            {
                "id": 9,
                "transcript": "uh the uh other ideas that's uh kernels like in filters have been traditionally used in uh image processing. So like for detecting edges, for example, for or for creating effects like a blur or things like that, right? Uh But now they, they, we found out that in deep learning, they are very, very useful for processing images as well.",
                "start_time": "207.759",
                "end_time": "232.07"
            },
            {
                "id": 10,
                "transcript": "Cool. OK. So now let's uh try to understand how convolutions work starting from an image. Now this is a gray scale image, which basically means that we have a bunch of pixels that um can vary between zero and 255 where zero is basically black and 255 is white. And so",
                "start_time": "232.259",
                "end_time": "256.214"
            },
            {
                "id": 11,
                "transcript": "we can take this uh the image of this cat and we can translate it into like this greed uh of pixels and these values which are supposed to be between zero and 253 55 represents like the values for each pixel, right? So I shouldn't notice here. I've used quite like small values just like, yeah, because it's easier",
                "start_time": "256.225",
                "end_time": "280.19"
            },
            {
                "id": 12,
                "transcript": "uh right. OK. So now let's move on. So we have this translation from an image to its pixel representation. Cool. So we said that we should apply a kernel to the image, right. So we have the pixel representation of the image, we have a kernel here. And the results of the convolution",
                "start_time": "280.579",
                "end_time": "302.51"
            },
            {
                "id": 13,
                "transcript": "is a uh itself like a an output uh which is agreed. And in the case of like the, the settings that I'm gonna use, we're gonna get an output that basically has the same size in terms of width and height of the original image over here. Now let's try to understand how convolution works. So",
                "start_time": "302.709",
                "end_time": "326.23"
            },
            {
                "id": 14,
                "transcript": "what we do basically is we overlay the kernel on top of the image. And so, for example, here you'll see that we are overlaying the kernel on top of like this initial uh red square here on the image and we center it around uh like the center like of the kernel. And here it corresponds like to these values to this four that's highlighted in green.",
                "start_time": "326.6",
                "end_time": "354.869"
            },
            {
                "id": 15,
                "transcript": "Now, uh when we do that, uh we get a, a value and the value of the convolution is then input in the, in this like output grid over here, which is basically like at the same index of the original uh image uh where we've centered the kernel on, right. So how do we get to a value there? Well, what we do basically is we just apply the dot product",
                "start_time": "355.07",
                "end_time": "384.76"
            },
            {
                "id": 16,
                "transcript": "uh having like the two like vectors like the uh the image and the kernel itself. And for the image we just like consider all of these values here in the red square",
                "start_time": "385.029",
                "end_time": "400.16"
            },
            {
                "id": 17,
                "transcript": "uh which are like correspondence uh kind of like to the kernel. So we have like the same number of bodies there, right? And so what we do and we should know how to do A dot Products. Because if, if you don't remember, you should go just and watch back like my video on linear algebra introduction to linear algebra.",
                "start_time": "400.359",
                "end_time": "417.39"
            },
            {
                "id": 18,
                "transcript": "But basically what we do here is we take each value and from the image and we multiply that for the correspondence uh value at the, at the same index of the kernel. So like in this case, we have five by one and then we uh add to that to multiplied by zero, then",
                "start_time": "417.619",
                "end_time": "440.739"
            },
            {
                "id": 19,
                "transcript": "we add to that three by zero, then we add two plus two. Well, I mean, you get the gist and the last number like the last value that we have is this zero multiplied by minus one over here. Now, uh when we uh take all of these like uh different like expressions and we run the math, we end up with",
                "start_time": "440.98",
                "end_time": "464.055"
            },
            {
                "id": 20,
                "transcript": "18, which is the value for this convolution. Now not 100% sure that 18 is right? Because I was like, I did things like quite quickly but yeah, you can try that out and see like if 18 is actually like the, the right results for this product. But regardless, let's assume 18 is correct. OK. So we'll take that 18 and we'll just",
                "start_time": "464.066",
                "end_time": "487.141"
            },
            {
                "id": 21,
                "transcript": "it in here in the output grid over here, right? So how do we continue here? Well, it's quite simple because now we slide the kernel on the image here and we slide to the right and now we center the uh the kernel on this like index here with the the one highlighted in green in the image, right? And we redo the the dot products and we get 10, we move on,",
                "start_time": "487.152",
                "end_time": "516.19"
            },
            {
                "id": 22,
                "transcript": "we slide and we get like another value and it's minus three, we slide again and we get five. Now we go",
                "start_time": "516.33",
                "end_time": "523.63"
            },
            {
                "id": 23,
                "transcript": "down, right? We go down one and we go back to the start and we have like another value and we continue like this until basically we can uh arrive at all the numbers for like this square here like highlighted in green. Now I've left uh question marks because I didn't want to do like the calculation. But this is a great",
                "start_time": "523.859",
                "end_time": "550.5"
            },
            {
                "id": 24,
                "transcript": "uh way for you to practice with convolution. So you can just like run the numbers there and uh yeah and write perhaps like the results like in the comments, right? But you may see that there's an issue here, right? Because we have the values of the convolutions just like for this internal square but the edges over here don't get a number.",
                "start_time": "550.679",
                "end_time": "573.659"
            },
            {
                "id": 25,
                "transcript": "And why is that an issue? Well, it's why, why, why is it an issue like to run that uh map on the, on the edges? Right? And the reason is like uh quickly explained by uh just opposing like the the kernel on the edges here. So for example, like if we center like the kernel on this 00 like index over here, right?",
                "start_time": "573.969",
                "end_time": "596.299"
            },
            {
                "id": 26,
                "transcript": "Uh We see that uh yeah, we have a part like of this uh kernel that can't be like applied to like any anything really because we don't have values over there. So what, what, what can we do there? Well, there are a couple of solutions.",
                "start_time": "596.489",
                "end_time": "613.71"
            },
            {
                "id": 27,
                "transcript": "So one it's kind of super straightforward, we say wait, well, wait. So we, we can't apply the kernel there. So we, we'll just ignore it. So we'll just ignore the edges, right? Yeah, this is a solution in itself. But the problem with that is that we are missing, we are losing some information, right. So all the edges on the image, the other solution uh which is the one that we usually use in deep learning is applying",
                "start_time": "613.719",
                "end_time": "641.78"
            },
            {
                "id": 28,
                "transcript": "a type of padding called zero padding. So we come up with an edge with an artificial edge that has all zeros for the purpose of calculation, right? And so if we do that, then all of a sudden we can calculate the, we can perform the convolution also like on the edges of the image, right.",
                "start_time": "642.03",
                "end_time": "665.34"
            },
            {
                "id": 29,
                "transcript": "And so if we uh run the calculations also like on the edges, we end up with an output like this where we basically have a uh the output convolution which is this uh grid uh that where like the k the kernel has been like applied on the image. And we have like all the different results. Again, question marks where I haven't done like the, the the calculation cool.",
                "start_time": "665.549",
                "end_time": "692.28"
            },
            {
                "id": 30,
                "transcript": "OK. So this is like the basic idea of a convolution. So, but let's think of a kernel. And so, and what that is uh on a, on a semantic level. Well, we can think of a kernel or a filter as a feature detector.",
                "start_time": "692.609",
                "end_time": "714.13"
            },
            {
                "id": 31,
                "transcript": "So basically, we can have a bunch of like uh different kernels that detects different uh features. So for example, this kernel here",
                "start_time": "714.349",
                "end_time": "724.02"
            },
            {
                "id": 32,
                "transcript": "uh which has like these ones like on this uh diagonal here, uh it's able to detect oblique lines like diagonals, right lines, right. Whereas this type of kernel here could be used to uh identify vertical lines. And so this is a vertical line uh detector, right? So now when we have a convolutional neural network,",
                "start_time": "724.26",
                "end_time": "751.5"
            },
            {
                "id": 33,
                "transcript": "as I mentioned earlier, what we do is we, we kind of like extract features uh using uh these kernels. But now the great thing about CNN is that we don't uh hard wire those, those kernels",
                "start_time": "751.739",
                "end_time": "767.19"
            },
            {
                "id": 34,
                "transcript": "rather we learn them in the process so we learn the, so the the network itself learns the kernels that it needs to extract in order to perform well, in some classification of whatever task we may think of. Right? And what does it mean to learn a kernel? Well, we, what the uh what we do basically is we learn the values, the weights of the kernel, all of these numbers here, right?",
                "start_time": "767.2",
                "end_time": "797.179"
            },
            {
                "id": 35,
                "transcript": "And so when we train a CNN, basically, we are training uh these values in the kernels cool.",
                "start_time": "797.39",
                "end_time": "805.52"
            },
            {
                "id": 36,
                "transcript": "OK. So uh when we um handle like convolution, we have a bunch of like architectural decisions that we can take to decide which type of convolutions uh like to use, right? And so uh here I have like a few uh things, a few settings that we should specify uh when we build our architecture.",
                "start_time": "806.15",
                "end_time": "831.659"
            },
            {
                "id": 37,
                "transcript": "So we should specify the grid size of the convolution, the stride, the depth and the number of kernels. So let's look into this one by one. Let's start from grid size. Well, this is like very intuitive, right. So uh the grid size is just the number of pixels for the height and the width of the grid. So in this case, for example, we have a three by three kernel",
                "start_time": "831.799",
                "end_time": "858.75"
            },
            {
                "id": 38,
                "transcript": "and in this case, we have a five by five kernel because we have just like 55 values like for, for the width and five values like for, for each height, right. So, and obviously like, let's remember that uh each uh like of these guys basically, like it's equivalent like to, to a pixel and it analyzes just like one pixel when it performs uh when we perform convolutions,",
                "start_time": "859.099",
                "end_time": "886.859"
            },
            {
                "id": 39,
                "transcript": "right? So you'll notice that I'm using um grid sizes with odd numbers. So why is that the case? Well, that's the case. Uh because when we have odd numbers, we have a central",
                "start_time": "886.869",
                "end_time": "902.26"
            },
            {
                "id": 40,
                "transcript": "um a value that we can use like as a, as a center as a reference uh on the image when we start and run the uh the, the convolution, right? And so it's, it's usually like you, you'll see odd numbers. Now um there, there's usually you'll also see um square",
                "start_time": "902.84",
                "end_time": "925.669"
            },
            {
                "id": 41,
                "transcript": "kind of like uh kernels. But you can potentially use also like nouns square like rectangular uh uh grids, right? For example, a one by three or a three by one kernel,",
                "start_time": "926.169",
                "end_time": "939.26"
            },
            {
                "id": 42,
                "transcript": "right? So this was about great size. Now, let's move on to another parameter stride. So what's the stride? Well, the stride is quite simple. It's the step size that we use for sliding the kernel on the image,",
                "start_time": "939.88",
                "end_time": "957.45"
            },
            {
                "id": 43,
                "transcript": "right? So, and the stride itself again is indicated in pixels. So let's try to look at a stride. Uh So here we have um our image and we have a three by three kernel that we are applying. Now, we let's say we have a stride, a one.",
                "start_time": "958.13",
                "end_time": "976.599"
            },
            {
                "id": 44,
                "transcript": "So we'll just move like this. So we start here, then we'll move this by one pixel and then we'll keep moving by another pixel and so on and so forth. And so as you can see, we are sliding just by one pixel and this is a stride of one. Now, let's take, let's try a stride of two. This again, like is also like quite simple to understand because like we start in this position and then we jump by two pixels right. Here we go.",
                "start_time": "976.75",
                "end_time": "1005.729"
            },
            {
                "id": 45,
                "transcript": "So this is a stride of two. Now, uh this you can specify both the horizontal stride and the vertical stride. So uh when you arrive like at the end of the image, like how much you want to go down in the image say like by one pixel or two pixels. And the important thing to understand is that again, the stride",
                "start_time": "1005.94",
                "end_time": "1026.51"
            },
            {
                "id": 46,
                "transcript": "uh doesn't necessarily need to be uh like the same for like the horizontal value and the vertical value, we can have a stride of two horizontally and a stride of one vertically, for example. But usually you tend to use like uh the same uh stride",
                "start_time": "1026.68",
                "end_time": "1041.92"
            },
            {
                "id": 47,
                "transcript": "uh for like the vertical part and the horizontal cool. So these are so grid size and straight are very important uh like settings for uh the kernel.",
                "start_time": "1042.469",
                "end_time": "1053.29"
            },
            {
                "id": 48,
                "transcript": "But then we have another one which is a depth even though I would say like this is like this comes like uh it's more constrained. You don't have like much like leverage like there and you can decide what you want to do. But the basic idea is that if you have a gray scale image, the depth is equal to one. But if you have a color image that's like for example, represented in, in our",
                "start_time": "1053.43",
                "end_time": "1077.776"
            },
            {
                "id": 49,
                "transcript": "R GB, what that basically means is that that image that each pixel for uh a color image has three values, one for the red, one for the green and one for uh the blue collar, right? And so what we do in terms of kernels is we have a kernel. Uh So the the kernel like is divided",
                "start_time": "1077.786",
                "end_time": "1102.131"
            },
            {
                "id": 50,
                "transcript": "like into three like parts, three grids. And so we have a grid for the red channel, we have a grid for the green channel and the third grid for the blue channel. And so, and these are independent, right? So a kernel in the case of R GB uh image data,",
                "start_time": "1102.141",
                "end_time": "1126.5"
            },
            {
                "id": 51,
                "transcript": "it's gonna have uh a three dimensional uh it's, it's a three dimensional array, right. So where like the, the first two dimensions uh represent the, for example, the width and the height of the, of the kernel and the",
                "start_time": "1126.77",
                "end_time": "1142.29"
            },
            {
                "id": 52,
                "transcript": "third dimension here it's the depth. So in this case, in this example, we have a three by three kernel which again, we should multiply by three which because that's the depth basically it's like 32 dimensional uh grids. So one for red, one for green and one for blue. And so the total number of weights in this case is equal to 27 because it's basically three by three",
                "start_time": "1142.3",
                "end_time": "1171.849"
            },
            {
                "id": 53,
                "transcript": "uh by three, right? OK. So let's remember guys. So if we are dealing with uh R GB data, we're gonna have a three dimensional array as our kernel where the first two dimensions are the width and the height. And the third dimension is the depth or it's also called the channel, right? Cos in R GB we have three channels",
                "start_time": "1172.56",
                "end_time": "1198.17"
            },
            {
                "id": 54,
                "transcript": "great. So now on to the last uh setting that we want to look into, so this is the number of kernels, but this is really not that related to like the the kernel itself, but it's related to the to the convolutional layer. Now,",
                "start_time": "1198.5",
                "end_time": "1216.579"
            },
            {
                "id": 55,
                "transcript": "convolutional layer can have and usually has multiple kernels. But let's remember that each kernel outputs a single two dimensional array and it's that output convolution that we've calculated before, right? And so",
                "start_time": "1216.589",
                "end_time": "1234.68"
            },
            {
                "id": 56,
                "transcript": "um one question could be, so how many outputs do we have from a convolutional layer? Well, we have as many two D arrays as the number of kernels, right? So if we have for example, five kernels",
                "start_time": "1235.05",
                "end_time": "1251.545"
            },
            {
                "id": 57,
                "transcript": "in a convolutional layer, so we're gonna have 52 D arrays because we're gonna apply each kernel to the input image and we're gonna get five separate output uh to the to the arrays, right?",
                "start_time": "1251.555",
                "end_time": "1268.06"
            },
            {
                "id": 58,
                "transcript": "OK. So this is the number of kernels. Now we've learned quite a lot about uh convolution. Now we need to look at the other side of the coin. Well, I should say like the other part that's usually used in CNN that's called pooling. Now, the, the the the most difficult part is behind this because pooling is quite intuitive.",
                "start_time": "1268.719",
                "end_time": "1292.839"
            },
            {
                "id": 59,
                "transcript": "So what pooling does like at the end of the day is just like down sampling an image. So it's basically shrinks an image and we do that in a sense in a similar manner to convolution in the sense that we overlay a grid on top of an image.",
                "start_time": "1293.05",
                "end_time": "1310.65"
            },
            {
                "id": 60,
                "transcript": "And then in terms of pooling, we have like different options. And the, the, the two that are like the most used are like max pooling and average pooling even though I should say that. Um Yeah, I'd say like in deep learning, like max pooling like is the, is the main one right",
                "start_time": "1310.93",
                "end_time": "1326.939"
            },
            {
                "id": 61,
                "transcript": "now, as I said, like pulling is quite uh intrusive and simple and it doesn't have like any parameters. So we don't learn anything in terms of, of pooling. It's a simple like mathematical operation that we perform. OK. So let's uh take a look at the different pooling settings that we'll have. And so",
                "start_time": "1327.15",
                "end_time": "1345.92"
            },
            {
                "id": 62,
                "transcript": "we have like very simple ones again, like there's the grid size that determines like how big like the uh the pooling grid is gonna be, we have the stride again, which is like the, the, the step uh the step size used for sliding the grid on top of an image and then we have the type of pooling. So max pooling or average poling. OK. So now we know like we have like an overview of pooling.",
                "start_time": "1346.439",
                "end_time": "1373.4"
            },
            {
                "id": 63,
                "transcript": "Uh Now let's take a look at the at how pooling really works. OK. So here we'll take a look at a max pooling where we have a two by two grid and we are gonna be using a stride of two, both horizontally and vertically, right? OK. So here we have our two by two",
                "start_time": "1373.53",
                "end_time": "1396.579"
            },
            {
                "id": 64,
                "transcript": "um pulling greed. And so, and, and we just like go like on top of like the input and so you can image like you can image that this is an image, right? And, and so here uh we have like these values, right? And given we are doing max pooling, we just pick the, the greatest value",
                "start_time": "1396.8",
                "end_time": "1421.3"
            },
            {
                "id": 65,
                "transcript": "and we log it in the output grid over here, right? OK. And so in by doing so we are down sampling uh the original input because basically we are just like getting one parameter out of like these four parameters. Now, how are we gonna continue doing this? Well, if you've like followed uh like attentively like up until now. So you should know that. Now, I'm just gonna slide",
                "start_time": "1421.51",
                "end_time": "1449.28"
            },
            {
                "id": 66,
                "transcript": "the grid with a stride of two. So I'm moving two pixels like on the right. And then again, here I'm gonna do the same thing. I'm gonna pick the highest number, which in this case is 10 and I'm gonna log it over here. Right. We continue and we pick 12 and finally we pick seven.",
                "start_time": "1449.479",
                "end_time": "1467.89"
            },
            {
                "id": 67,
                "transcript": "Now, um, there's obviously a mathematical relationship between the size of the output, the size of the input and the, the stride, right? And the grade size, right?",
                "start_time": "1468.089",
                "end_time": "1482.369"
            },
            {
                "id": 68,
                "transcript": "Ok. So I'm not gonna give you like the general rule but, uh that you can, and you should definitely like uh understand by yourself. But like in this case where we have a uh two by two max pooling reed uh which try to, you'll see that we are basically uh hal like the width and the height of the input. So here we have a four by four like input",
                "start_time": "1482.54",
                "end_time": "1510.28"
            },
            {
                "id": 69,
                "transcript": "um grid and the output grid in this case is a two by two, right?",
                "start_time": "1510.599",
                "end_time": "1515.63"
            },
            {
                "id": 70,
                "transcript": "OK. So we're basically done. So we know what pooling is. So now we we should uh put together like everything we've learned and understand that a CNN uh just like uses like yeah, pooling",
                "start_time": "1516.29",
                "end_time": "1534.68"
            },
            {
                "id": 71,
                "transcript": "and uh convolutions, right? So convolutional layers and pooling layers uh and uh and it uses it like by putting more of this like uh together. So let's take a look at at a typical CNN architecture. So here we have the input, which is kind it's a simple image, right? And then here we have a feature learning um",
                "start_time": "1535.199",
                "end_time": "1560.0"
            },
            {
                "id": 72,
                "transcript": "I would say like face like in the in the CNN where we have a bunch of like convolutional layers followed by pooling layers. And so now the um the the network can be like as deep as you want really. So you can have up to like 50 100 layers, if not more, right? And after like the feature learning like phase, usually you get a",
                "start_time": "1560.3",
                "end_time": "1586.319"
            },
            {
                "id": 73,
                "transcript": "uh fully connected uh like layer. So here at the end of like this uh the feature learning like section of the CNN, we usually like flatten the results. So we, we move it like from two D to like a one D uh vector. And",
                "start_time": "1586.489",
                "end_time": "1602.439"
            },
            {
                "id": 74,
                "transcript": "then we, we pass that information into like one or more fully connected layers. And in the end, we have a soft max classifier which provides us with a uh dis probability distribution on top of a number of like different categories. So like",
                "start_time": "1602.449",
                "end_time": "1618.41"
            },
            {
                "id": 75,
                "transcript": "uh like in this case, we are trying to like in this particular example, we are trying to classify different types of like uh transportation vehicles. And so here soft max is gonna give us like values for car track, van, bicycle train, right? Or airplane, for example, right? But let's take a look at this feature learning uh uh like segment of the CNN, which is like the most interesting one for our purposes, right.",
                "start_time": "1618.63",
                "end_time": "1646.219"
            },
            {
                "id": 76,
                "transcript": "So what happens here is basically that at each uh convolution um uh a convolutional layer, basically what we're doing is we're trying to extract features, right? And basically, the idea is that at the beginning towards like the beginning of this feature learning segment, we extract uh very uh like low level features. So it could be like edges. Uh but then",
                "start_time": "1646.369",
                "end_time": "1671.89"
            },
            {
                "id": 77,
                "transcript": "uh moving forward uh like from edges, we can use, we can leverage them, for example, to uh to arrive at shapes, then going deeper in the network from shapes, we can arrive at objects. So for example, like a car could be,",
                "start_time": "1672.05",
                "end_time": "1689.17"
            },
            {
                "id": 78,
                "transcript": "I don't know, like uh could be represented say by like a few circles uh which could be like the um the tires uh and a and a rectangle, for example, that could be like, I mean the whole uh like space like of the of the car uh like itself, right? So",
                "start_time": "1689.18",
                "end_time": "1707.27"
            },
            {
                "id": 79,
                "transcript": "and but this is like important to understand. So we we start with low level features and then while we move from one convolution layer to the next we abstract higher level features. Cool. And so this is like an intuition how a CNN architecture uh works? Cool.",
                "start_time": "1707.459",
                "end_time": "1729.81"
            },
            {
                "id": 80,
                "transcript": "OK. So now you may be wondering, OK, so we've spent so much time on talking about how wonderful CNN S are with images. So what about audio? After all, we are talking about deep learning for audio like in this uh uh series, right? Yeah. But uh the great thing is that uh we can think of audio in a sense itself as a, as an image, right? So if you guys remember,",
                "start_time": "1729.939",
                "end_time": "1758.8"
            },
            {
                "id": 81,
                "transcript": "so what we usually use when we um use deep learning like audio. Uh we we use like spectrograms, we use MF CCS all of this type of like features, but now we've already like visualize them like as an image. And at the end of the day, these are big, we can be interpreted as images, right? Both the spectrogram and the MF CCS, right?",
                "start_time": "1759.02",
                "end_time": "1784.339"
            },
            {
                "id": 82,
                "transcript": "So uh in the case of a spectrogram, for example, time, the different time beings and the frequencies that we have here can be folded like the X and Y indexes for the the pixels of of an image",
                "start_time": "1784.54",
                "end_time": "1800.68"
            },
            {
                "id": 83,
                "transcript": "and the amplitude can be thought of the the the value associated to each pixel, right. So in a sense, a spectrogram is a two dimensional array which is really comparable to to an image. And the great thing is that a spectrogram and audio in general has",
                "start_time": "1800.969",
                "end_time": "1822.28"
            },
            {
                "id": 84,
                "transcript": "I would say like structures in them",
                "start_time": "1823.199",
                "end_time": "1826.839"
            },
            {
                "id": 85,
                "transcript": "that are like in a sense like similar to like images. So like the the uh the data itself like is somehow like correlated. It's it's not like completely like random. And so because of like those structures that we can identify in spectrogram and MS CCS CNN work really really well because they are able to extract features while we uh like apply them when we apply like convolutions like on the uh on the audio data,",
                "start_time": "1826.979",
                "end_time": "1854.619"
            },
            {
                "id": 86,
                "transcript": "right? So now that we have an intuition of how like all your data can be used in CN MS. Let's try to look at a specific example where we look at MF CCS for uh data, right? And so now the question",
                "start_time": "1854.859",
                "end_time": "1869.839"
            },
            {
                "id": 87,
                "transcript": "uh that I want to ask you is like given like these uh settings for uh these like MF CCS like and for all of these audio data, what's gonna be like the, the data shape, right? So we have 13 MFCC,",
                "start_time": "1870.13",
                "end_time": "1886.435"
            },
            {
                "id": 88,
                "transcript": "we have a hop length of 512 samples. Now, if you don't re remember what a hop length is or an MFCC uh like is just like go back to my previous videos where I introduced like all of these audio features.",
                "start_time": "1886.444",
                "end_time": "1902.76"
            },
            {
                "id": 89,
                "transcript": "Cool. So we said hop length 512 samples and then we have the total number of samples in a audio file that we are analyzing. That's conveniently 51,200. Now, the question is what's the data shape that we, we, we're gonna expect our CNN to be fed with",
                "start_time": "1903.069",
                "end_time": "1924.729"
            },
            {
                "id": 90,
                "transcript": "and right. So the data shape is 100 by 13 by one. So let's analyze why that's the case. So here we have uh 100 different like time windows at which we take 13 values which are the, the 13 MF CCS that we are extracting, right?",
                "start_time": "1926.359",
                "end_time": "1949.339"
            },
            {
                "id": 91,
                "transcript": "And 100 comes by dividing the overall number of samples in the audio file by the hop length which is like the, the sliding window that we use uh like to calculate the MF CCS. And so again, like the two, these two values are like quite understandable. Uh because we have, we, we understand them saying yeah, we have 100 time windows at which we've taken the 13 coefficients",
                "start_time": "1949.56",
                "end_time": "1978.05"
            },
            {
                "id": 92,
                "transcript": "uh uh the first in MF CCS. Um But why do we have a third dimension here? So by one, do you guys remember depth?",
                "start_time": "1978.29",
                "end_time": "1987.089"
            },
            {
                "id": 93,
                "transcript": "Yeah. So in this case, we have a depth which is equal to one. So basically all your data can be um kind of like compared to like gray scale images where the depth is equal to one. So we don't have like R GB like representation of audio data. It's just like gray scale, we just have depth, just one channel,",
                "start_time": "1987.77",
                "end_time": "2011.04"
            },
            {
                "id": 94,
                "transcript": "right? And so you may be wondering well, but why do we need to like give like this third dimension? Isn't that redundant? Well, it could be but again, like CNN S are like supposed to work like with uh images and usually images, right are color images most often time. And so they have like channels. So the depth is very important",
                "start_time": "2011.199",
                "end_time": "2032.05"
            },
            {
                "id": 95,
                "transcript": "and right? And uh in this case, I'm giving you like this data shape here because this is like what tensorflow is gonna like accept uh as yeah for learning purposes. And so like it's good to get like into that like frame of mind.",
                "start_time": "2032.209",
                "end_time": "2048.87"
            },
            {
                "id": 96,
                "transcript": "Wow. So this was intense. But at the same time now, you should have a quite clear understanding of a convolutional neural network, what its components are, what's like uh the the processes that come into place, how uh like convolutional pooling layers like work and how the overall architecture like is built together. So",
                "start_time": "2049.678",
                "end_time": "2077.709"
            },
            {
                "id": 97,
                "transcript": "we should just ask ourselves as usual at the end of these videos and say what's up next? Well, as usual, we'll take all of this theoretical uh information and in the next video we're gonna uh put that into practice. So what we'll do is implementing a music genre classifier. But this time we are gonna be using a CNN.",
                "start_time": "2078.78",
                "end_time": "2103.629"
            },
            {
                "id": 98,
                "transcript": "So stay tuned for that. I hope you really enjoyed the uh the this video. If that's the case, just like the video and if you want to know more and have like more videos like this and never miss one, please consider subscribing and I guess I'll see you next time. Cheers.",
                "start_time": "2103.84",
                "end_time": "2122.86"
            }
        ]
    }
}