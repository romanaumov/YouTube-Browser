[
    {
        "id": "64f029d5",
        "text": "Hi, everybody and welcome to a new video in the Deep Learning for Radio with Python series. This time we're gonna talk about training a neural network. And specifically, we're gonna look at the theory behind it. So we'll look at two very important algorithms backward propagation and gradient the sound cool. So let's get started with the idea of training a neural network. So the high level overview. So what do we do when we train a network? Well, we basically trick all the weights of the connections among different neurons so that we can have predictions that are really good. So how do we do that? Well, we basically uh feed some training data. So inputs and target values to the network, then we look at the predictions and we calculate the error between the predictions and the expected outcome. And then we use that information for iteratively adjusting the weights I know like this feels like very high level. So we're gonna now look at how that's done precisely cool. So again, from a quite high level overview, there are two main steps here. So when we train a network, we pass a sample, an example",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "a0ae13fb",
        "text": "So what do we do when we train a network? Well, we basically trick all the weights of the connections among different neurons so that we can have predictions that are really good. So how do we do that? Well, we basically uh feed some training data. So inputs and target values to the network, then we look at the predictions and we calculate the error between the predictions and the expected outcome. And then we use that information for iteratively adjusting the weights I know like this feels like very high level. So we're gonna now look at how that's done precisely cool. So again, from a quite high level overview, there are two main steps here. So when we train a network, we pass a sample, an example uh input and that input uh does we use it like for doing two things? So we have the input traveling all the way through to the rightmost uh layer and we get the prediction there. So we have a feed forward propagation forward pass",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=25s",
        "start_time": "25.049"
    },
    {
        "id": "9e91fe9c",
        "text": "And then we use that information for iteratively adjusting the weights I know like this feels like very high level. So we're gonna now look at how that's done precisely cool. So again, from a quite high level overview, there are two main steps here. So when we train a network, we pass a sample, an example uh input and that input uh does we use it like for doing two things? So we have the input traveling all the way through to the rightmost uh layer and we get the prediction there. So we have a feed forward propagation forward pass and then we calculate an error, we use that information and we back propagate that signal error signal back to the uh initial layers. So from right to left cool. So now let's break down this two big steps. So forward propagation and back propagation into it's like subst steps. So",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=53s",
        "start_time": "53.389"
    },
    {
        "id": "c6005b6b",
        "text": "uh input and that input uh does we use it like for doing two things? So we have the input traveling all the way through to the rightmost uh layer and we get the prediction there. So we have a feed forward propagation forward pass and then we calculate an error, we use that information and we back propagate that signal error signal back to the uh initial layers. So from right to left cool. So now let's break down this two big steps. So forward propagation and back propagation into it's like subst steps. So when we have this forward propagation, what we do basically is we get the prediction and in the end we calculate the error,",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=79s",
        "start_time": "79.819"
    },
    {
        "id": "0b930b21",
        "text": "and then we calculate an error, we use that information and we back propagate that signal error signal back to the uh initial layers. So from right to left cool. So now let's break down this two big steps. So forward propagation and back propagation into it's like subst steps. So when we have this forward propagation, what we do basically is we get the prediction and in the end we calculate the error, then when we have the uh error traveling back. So the back propagation side, we do two things. We calculate the gradient of the error function uh with regard to the weight. And we use that information to update the uh parameters because the gradient is gonna tell us in which direction we should go",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=101s",
        "start_time": "101.11"
    },
    {
        "id": "992341d4",
        "text": "when we have this forward propagation, what we do basically is we get the prediction and in the end we calculate the error, then when we have the uh error traveling back. So the back propagation side, we do two things. We calculate the gradient of the error function uh with regard to the weight. And we use that information to update the uh parameters because the gradient is gonna tell us in which direction we should go uh in which direction we should treat the weights in order to improve the predictions. Cool. So now what I'm gonna do is I'm gonna go one by one through all of these steps and explain them and explain the math behind it. So get prediction.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=126s",
        "start_time": "126.94"
    },
    {
        "id": "64533201",
        "text": "then when we have the uh error traveling back. So the back propagation side, we do two things. We calculate the gradient of the error function uh with regard to the weight. And we use that information to update the uh parameters because the gradient is gonna tell us in which direction we should go uh in which direction we should treat the weights in order to improve the predictions. Cool. So now what I'm gonna do is I'm gonna go one by one through all of these steps and explain them and explain the math behind it. So get prediction. Uh we, we saw this already. So uh we have a previous video where I went through uh how to calculate uh competition in neural networks. So if you're interested in the specifics of this just go",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=135s",
        "start_time": "135.289"
    },
    {
        "id": "0faf66e6",
        "text": "uh in which direction we should treat the weights in order to improve the predictions. Cool. So now what I'm gonna do is I'm gonna go one by one through all of these steps and explain them and explain the math behind it. So get prediction. Uh we, we saw this already. So uh we have a previous video where I went through uh how to calculate uh competition in neural networks. So if you're interested in the specifics of this just go uh back to that one and watch that video cool. But in a nutshell, we have inputs, these inputs get propagated to uh uh the subsequent layers like on um the right and and then until they arrive like at the last layer where we get a prediction cool. So then we have the next step which is calculating the error. And for calculating the error, we need an error function.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=160s",
        "start_time": "160.559"
    },
    {
        "id": "c1473b7c",
        "text": "Uh we, we saw this already. So uh we have a previous video where I went through uh how to calculate uh competition in neural networks. So if you're interested in the specifics of this just go uh back to that one and watch that video cool. But in a nutshell, we have inputs, these inputs get propagated to uh uh the subsequent layers like on um the right and and then until they arrive like at the last layer where we get a prediction cool. So then we have the next step which is calculating the error. And for calculating the error, we need an error function. Uh The error function is a function of two variables. So the prediction itself and why, which in this case indicates the expected uh outcome. Cool.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=180s",
        "start_time": "180.35"
    },
    {
        "id": "32682620",
        "text": "uh back to that one and watch that video cool. But in a nutshell, we have inputs, these inputs get propagated to uh uh the subsequent layers like on um the right and and then until they arrive like at the last layer where we get a prediction cool. So then we have the next step which is calculating the error. And for calculating the error, we need an error function. Uh The error function is a function of two variables. So the prediction itself and why, which in this case indicates the expected uh outcome. Cool. So uh be aware that many people instead of using the expression error function, use loss function. So you can use them like interchangeably I prefer using error because I feel like it's more like it's more it's clear like to understand what we're talking about. Cool. So we've calculated the error. So now we need to go to the next step.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=194s",
        "start_time": "194.24"
    },
    {
        "id": "223a756c",
        "text": "Uh The error function is a function of two variables. So the prediction itself and why, which in this case indicates the expected uh outcome. Cool. So uh be aware that many people instead of using the expression error function, use loss function. So you can use them like interchangeably I prefer using error because I feel like it's more like it's more it's clear like to understand what we're talking about. Cool. So we've calculated the error. So now we need to go to the next step. But before we go to the next step, I wanted to mention that like in this example, we are using the um quadratic uh error function which is given by this nice formula over here. Nice. OK. So now we can move to the next step which is calculating the gradient of the error function. So what we want to do is calculate the derivative of e of the error function with regard to",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=220s",
        "start_time": "220.899"
    },
    {
        "id": "223149a9",
        "text": "So uh be aware that many people instead of using the expression error function, use loss function. So you can use them like interchangeably I prefer using error because I feel like it's more like it's more it's clear like to understand what we're talking about. Cool. So we've calculated the error. So now we need to go to the next step. But before we go to the next step, I wanted to mention that like in this example, we are using the um quadratic uh error function which is given by this nice formula over here. Nice. OK. So now we can move to the next step which is calculating the gradient of the error function. So what we want to do is calculate the derivative of e of the error function with regard to uh w so all the weights. So, and if you guys remember W 1 W-2, these are the weight um uh mattresses. And so these are the weights for the connections between like different neurons in different layers. Nice.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=234s",
        "start_time": "234.07"
    },
    {
        "id": "c8faaa30",
        "text": "But before we go to the next step, I wanted to mention that like in this example, we are using the um quadratic uh error function which is given by this nice formula over here. Nice. OK. So now we can move to the next step which is calculating the gradient of the error function. So what we want to do is calculate the derivative of e of the error function with regard to uh w so all the weights. So, and if you guys remember W 1 W-2, these are the weight um uh mattresses. And so these are the weights for the connections between like different neurons in different layers. Nice. Cool. So, but how do we do that? Well, first, we should think of the uh of a neural network as a very complex function uh which is dependent on two variables. So the XS which are like this input vector here. Uh X so the inputs and W so all the weights. So",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=257s",
        "start_time": "257.399"
    },
    {
        "id": "aea30678",
        "text": "uh w so all the weights. So, and if you guys remember W 1 W-2, these are the weight um uh mattresses. And so these are the weights for the connections between like different neurons in different layers. Nice. Cool. So, but how do we do that? Well, first, we should think of the uh of a neural network as a very complex function uh which is dependent on two variables. So the XS which are like this input vector here. Uh X so the inputs and W so all the weights. So if you remember the error function is a function of P and Y so P being the prediction, but the prediction obviously is a function is the results of the um neural network. So we can substitute this F of XW uh for P. And by doing so we find out that E",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=285s",
        "start_time": "285.32"
    },
    {
        "id": "c47e149c",
        "text": "Cool. So, but how do we do that? Well, first, we should think of the uh of a neural network as a very complex function uh which is dependent on two variables. So the XS which are like this input vector here. Uh X so the inputs and W so all the weights. So if you remember the error function is a function of P and Y so P being the prediction, but the prediction obviously is a function is the results of the um neural network. So we can substitute this F of XW uh for P. And by doing so we find out that E the error function is a function of the weight itself. So it makes sense to calculate the, the gradient of the error function of the weights because it depends on the weight cool. So having like all of this knowledge we can move on and just like review all the elements of a neural network. I hope by now you're very familiar with all of this like elements. But if you're not, I'll give you a quick overview. So we have the",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=300s",
        "start_time": "300.22"
    },
    {
        "id": "fec0e82a",
        "text": "if you remember the error function is a function of P and Y so P being the prediction, but the prediction obviously is a function is the results of the um neural network. So we can substitute this F of XW uh for P. And by doing so we find out that E the error function is a function of the weight itself. So it makes sense to calculate the, the gradient of the error function of the weights because it depends on the weight cool. So having like all of this knowledge we can move on and just like review all the elements of a neural network. I hope by now you're very familiar with all of this like elements. But if you're not, I'll give you a quick overview. So we have the axis which are obviously the inputs, then we have like the activations. A one is equal to the inputs. W one we show this, it's the, the weights for the first layer H two is the net input vector.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=321s",
        "start_time": "321.079"
    },
    {
        "id": "7ad13e8e",
        "text": "the error function is a function of the weight itself. So it makes sense to calculate the, the gradient of the error function of the weights because it depends on the weight cool. So having like all of this knowledge we can move on and just like review all the elements of a neural network. I hope by now you're very familiar with all of this like elements. But if you're not, I'll give you a quick overview. So we have the axis which are obviously the inputs, then we have like the activations. A one is equal to the inputs. W one we show this, it's the, the weights for the first layer H two is the net input vector. So it's all the net inputs for layer two. And then we just repeat a 2 W-2 H three. And then finally, a three in this case, which is the prediction itself nice, we have this information we know about the error function and its dependencies. And so what we can do is we can calculate the derivative of E with respect to W-2.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=342s",
        "start_time": "342.38"
    },
    {
        "id": "69746409",
        "text": "axis which are obviously the inputs, then we have like the activations. A one is equal to the inputs. W one we show this, it's the, the weights for the first layer H two is the net input vector. So it's all the net inputs for layer two. And then we just repeat a 2 W-2 H three. And then finally, a three in this case, which is the prediction itself nice, we have this information we know about the error function and its dependencies. And so what we can do is we can calculate the derivative of E with respect to W-2. And we can use a very nice rule from calculus which I'm not gonna um review here, but it's called the chain rule. And if we use the chain rule, uh we can come up with this formula",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=370s",
        "start_time": "370.269"
    },
    {
        "id": "8af9b1ac",
        "text": "So it's all the net inputs for layer two. And then we just repeat a 2 W-2 H three. And then finally, a three in this case, which is the prediction itself nice, we have this information we know about the error function and its dependencies. And so what we can do is we can calculate the derivative of E with respect to W-2. And we can use a very nice rule from calculus which I'm not gonna um review here, but it's called the chain rule. And if we use the chain rule, uh we can come up with this formula uh which rewrites the derivative of the uh E uh error function with regard to W-2. So we have three blocks, three derivatives for coming up with the, with this main derivative here of the error over W-2. And he, yeah, this feels like a little bit intimidating. And I know and as I said, like this is gonna be a quick math heavy",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=385s",
        "start_time": "385.769"
    },
    {
        "id": "1bb7d9ac",
        "text": "And we can use a very nice rule from calculus which I'm not gonna um review here, but it's called the chain rule. And if we use the chain rule, uh we can come up with this formula uh which rewrites the derivative of the uh E uh error function with regard to W-2. So we have three blocks, three derivatives for coming up with the, with this main derivative here of the error over W-2. And he, yeah, this feels like a little bit intimidating. And I know and as I said, like this is gonna be a quick math heavy uh video, but uh I'm gonna try to make this uh calculations as clear and as simple as possible. So you can follow and understand what's going on. Cool. So let's calculate this three derivatives here. Let's start from the first one, the derivative of E over a three. Let's remember that we are using the",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=409s",
        "start_time": "409.989"
    },
    {
        "id": "f96a9d66",
        "text": "uh which rewrites the derivative of the uh E uh error function with regard to W-2. So we have three blocks, three derivatives for coming up with the, with this main derivative here of the error over W-2. And he, yeah, this feels like a little bit intimidating. And I know and as I said, like this is gonna be a quick math heavy uh video, but uh I'm gonna try to make this uh calculations as clear and as simple as possible. So you can follow and understand what's going on. Cool. So let's calculate this three derivatives here. Let's start from the first one, the derivative of E over a three. Let's remember that we are using the uh quadratic error uh function here as the error function. And so if we perform the derivative of E uh with respect to a three, we end up with these results. So it's basically a three minus Y. So it's the uh prediction uh minus the actual expected outcome",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=424s",
        "start_time": "424.35"
    },
    {
        "id": "4471d26e",
        "text": "uh video, but uh I'm gonna try to make this uh calculations as clear and as simple as possible. So you can follow and understand what's going on. Cool. So let's calculate this three derivatives here. Let's start from the first one, the derivative of E over a three. Let's remember that we are using the uh quadratic error uh function here as the error function. And so if we perform the derivative of E uh with respect to a three, we end up with these results. So it's basically a three minus Y. So it's the uh prediction uh minus the actual expected outcome good. So that's the first block, second block, the derivative of A three over H three. Now, for the activations uh in this example, and as like in the examples, for the previous video, we are gonna use the sigmoid function. And so the A three, the mm uh the prediction here a three is gonna be the sigmoid function calculated in H three",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=449s",
        "start_time": "449.399"
    },
    {
        "id": "e5b5b5d1",
        "text": "uh quadratic error uh function here as the error function. And so if we perform the derivative of E uh with respect to a three, we end up with these results. So it's basically a three minus Y. So it's the uh prediction uh minus the actual expected outcome good. So that's the first block, second block, the derivative of A three over H three. Now, for the activations uh in this example, and as like in the examples, for the previous video, we are gonna use the sigmoid function. And so the A three, the mm uh the prediction here a three is gonna be the sigmoid function calculated in H three cool. So now if we want to calculate the derivative of A three over H three, we're gonna end up with the first derivative of the sigmoid function",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=471s",
        "start_time": "471.279"
    },
    {
        "id": "d587e734",
        "text": "good. So that's the first block, second block, the derivative of A three over H three. Now, for the activations uh in this example, and as like in the examples, for the previous video, we are gonna use the sigmoid function. And so the A three, the mm uh the prediction here a three is gonna be the sigmoid function calculated in H three cool. So now if we want to calculate the derivative of A three over H three, we're gonna end up with the first derivative of the sigmoid function calculated in H free. I'm using this Sigma uh symbol here uh to indicate the sigmoid function and the derivative of the sigmoid function is given by this formula here. So it's the Sigma function itself by one minus the sigmoid function. And in this case, it's calculated in H three nice.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=495s",
        "start_time": "495.929"
    },
    {
        "id": "6b4e8984",
        "text": "cool. So now if we want to calculate the derivative of A three over H three, we're gonna end up with the first derivative of the sigmoid function calculated in H free. I'm using this Sigma uh symbol here uh to indicate the sigmoid function and the derivative of the sigmoid function is given by this formula here. So it's the Sigma function itself by one minus the sigmoid function. And in this case, it's calculated in H three nice. So we have the second element, let's move on to the third element. So it's the derivative of H three over W-2. So H three, you guys should know this by now it's given by the matrix multiplication of the uh activation vector by the uh weight matrix uh of the second layer",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=522s",
        "start_time": "522.51"
    },
    {
        "id": "824df1b1",
        "text": "calculated in H free. I'm using this Sigma uh symbol here uh to indicate the sigmoid function and the derivative of the sigmoid function is given by this formula here. So it's the Sigma function itself by one minus the sigmoid function. And in this case, it's calculated in H three nice. So we have the second element, let's move on to the third element. So it's the derivative of H three over W-2. So H three, you guys should know this by now it's given by the matrix multiplication of the uh activation vector by the uh weight matrix uh of the second layer nice. So this is basically a linear transformation. And so if we do a derivative of H three over W-2, we are gonna end up with A two with the activations cool. Now we have all the three elements that we need to rewrite uh the uh derivative of E over W-2. So now let's rewrite this, let's fill this up. So first elements,",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=534s",
        "start_time": "534.849"
    },
    {
        "id": "2db45ea2",
        "text": "So we have the second element, let's move on to the third element. So it's the derivative of H three over W-2. So H three, you guys should know this by now it's given by the matrix multiplication of the uh activation vector by the uh weight matrix uh of the second layer nice. So this is basically a linear transformation. And so if we do a derivative of H three over W-2, we are gonna end up with A two with the activations cool. Now we have all the three elements that we need to rewrite uh the uh derivative of E over W-2. So now let's rewrite this, let's fill this up. So first elements, so the first derivative here is a three minus Y, then the derivative of the A three over H three is basically the derivative of the Sigma function calculated in H three. And finally, the last block is equal to a two nice. We have our first um element to calculate like the gradient. So we have our error function,",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=557s",
        "start_time": "557.28"
    },
    {
        "id": "b78bd515",
        "text": "nice. So this is basically a linear transformation. And so if we do a derivative of H three over W-2, we are gonna end up with A two with the activations cool. Now we have all the three elements that we need to rewrite uh the uh derivative of E over W-2. So now let's rewrite this, let's fill this up. So first elements, so the first derivative here is a three minus Y, then the derivative of the A three over H three is basically the derivative of the Sigma function calculated in H three. And finally, the last block is equal to a two nice. We have our first um element to calculate like the gradient. So we have our error function, uh the derivative of the error function calculated in W-2. Now, what should we do next? Well, we should calculate the error, the derivative of the error function with respect to W one. So we are basically moving from the second layer back towards the first layer. So back propagation, we are going back from right to left. Now",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=578s",
        "start_time": "578.09"
    },
    {
        "id": "e128aed4",
        "text": "so the first derivative here is a three minus Y, then the derivative of the A three over H three is basically the derivative of the Sigma function calculated in H three. And finally, the last block is equal to a two nice. We have our first um element to calculate like the gradient. So we have our error function, uh the derivative of the error function calculated in W-2. Now, what should we do next? Well, we should calculate the error, the derivative of the error function with respect to W one. So we are basically moving from the second layer back towards the first layer. So back propagation, we are going back from right to left. Now um the derivative of E with respect to W one is equal to this other be here. But it's not too different like from what we've seen like before, we've just like reused the, the same like um expansion rules, rewriting rules using the chain rules uh chain rule from calculus of the E of the derivative of E calculated with respect to W-2",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=604s",
        "start_time": "604.049"
    },
    {
        "id": "b054766c",
        "text": "uh the derivative of the error function calculated in W-2. Now, what should we do next? Well, we should calculate the error, the derivative of the error function with respect to W one. So we are basically moving from the second layer back towards the first layer. So back propagation, we are going back from right to left. Now um the derivative of E with respect to W one is equal to this other be here. But it's not too different like from what we've seen like before, we've just like reused the, the same like um expansion rules, rewriting rules using the chain rules uh chain rule from calculus of the E of the derivative of E calculated with respect to W-2 cool. So we can get these first element here. So the derivative of E with respect to A two and using uh the chain rule again, we can rewrite it as this formula down here with this three blocks. Now, if we take all of this and we plug in",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=633s",
        "start_time": "633.349"
    },
    {
        "id": "890e429a",
        "text": "um the derivative of E with respect to W one is equal to this other be here. But it's not too different like from what we've seen like before, we've just like reused the, the same like um expansion rules, rewriting rules using the chain rules uh chain rule from calculus of the E of the derivative of E calculated with respect to W-2 cool. So we can get these first element here. So the derivative of E with respect to A two and using uh the chain rule again, we can rewrite it as this formula down here with this three blocks. Now, if we take all of this and we plug in in here, we're gonna end up with this beast formula for the derivative of V uh with regards to W one which is given by all of these five elements. Now don't be scared. So it's quite easy to calculate. And good news is that we've already calculated these first two elements when we calculated the derivative of V with respect to W-2.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=656s",
        "start_time": "656.94"
    },
    {
        "id": "763100d2",
        "text": "cool. So we can get these first element here. So the derivative of E with respect to A two and using uh the chain rule again, we can rewrite it as this formula down here with this three blocks. Now, if we take all of this and we plug in in here, we're gonna end up with this beast formula for the derivative of V uh with regards to W one which is given by all of these five elements. Now don't be scared. So it's quite easy to calculate. And good news is that we've already calculated these first two elements when we calculated the derivative of V with respect to W-2. So we should now focus on this three blocks, three derivatives now here. So let's",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=681s",
        "start_time": "681.859"
    },
    {
        "id": "3de9bc28",
        "text": "in here, we're gonna end up with this beast formula for the derivative of V uh with regards to W one which is given by all of these five elements. Now don't be scared. So it's quite easy to calculate. And good news is that we've already calculated these first two elements when we calculated the derivative of V with respect to W-2. So we should now focus on this three blocks, three derivatives now here. So let's uh let's start calculating the first one of this three. So the derivative of H three with respect to A two. So H three again is given by uh the matrix multiplication of A two and W-2. And when we calculate the derivative here of H three over A two, we end up with only W-2. So the weight matrix for layer two, now let's move on next derivative. So it's a derivative of a two calculated over H two.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=705s",
        "start_time": "705.869"
    },
    {
        "id": "b288ea45",
        "text": "So we should now focus on this three blocks, three derivatives now here. So let's uh let's start calculating the first one of this three. So the derivative of H three with respect to A two. So H three again is given by uh the matrix multiplication of A two and W-2. And when we calculate the derivative here of H three over A two, we end up with only W-2. So the weight matrix for layer two, now let's move on next derivative. So it's a derivative of a two calculated over H two. So again, a two is the basically given by the sigmoid function uh calculated in H two. And so when we do the derivative of A two over H two, again, we have the first derivative of the sigmoid function. But this time calculated in H two",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=731s",
        "start_time": "731.96"
    },
    {
        "id": "fbdcd515",
        "text": "uh let's start calculating the first one of this three. So the derivative of H three with respect to A two. So H three again is given by uh the matrix multiplication of A two and W-2. And when we calculate the derivative here of H three over A two, we end up with only W-2. So the weight matrix for layer two, now let's move on next derivative. So it's a derivative of a two calculated over H two. So again, a two is the basically given by the sigmoid function uh calculated in H two. And so when we do the derivative of A two over H two, again, we have the first derivative of the sigmoid function. But this time calculated in H two good. Now final element. So the derivative of H two calculated over W one. So H two is given by the matrix multiplication of the input vector X with the uh weight matrix one nice. So the derivative of H two over W one is just the input vector X",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=739s",
        "start_time": "739.38"
    },
    {
        "id": "fc507b6a",
        "text": "So again, a two is the basically given by the sigmoid function uh calculated in H two. And so when we do the derivative of A two over H two, again, we have the first derivative of the sigmoid function. But this time calculated in H two good. Now final element. So the derivative of H two calculated over W one. So H two is given by the matrix multiplication of the input vector X with the uh weight matrix one nice. So the derivative of H two over W one is just the input vector X cool. We have all the five elements to rewrite the derivative of E over W uh over W one. So let's rewrite that",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=769s",
        "start_time": "769.25"
    },
    {
        "id": "7bb77fd8",
        "text": "good. Now final element. So the derivative of H two calculated over W one. So H two is given by the matrix multiplication of the input vector X with the uh weight matrix one nice. So the derivative of H two over W one is just the input vector X cool. We have all the five elements to rewrite the derivative of E over W uh over W one. So let's rewrite that first element here. So the derivative of the E over A three can be rewritten as a three minus Y. Then we have the second element here. So which is the derivative of a three over H three. And this is the",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=788s",
        "start_time": "788.299"
    },
    {
        "id": "806dbf08",
        "text": "cool. We have all the five elements to rewrite the derivative of E over W uh over W one. So let's rewrite that first element here. So the derivative of the E over A three can be rewritten as a three minus Y. Then we have the second element here. So which is the derivative of a three over H three. And this is the uh first derivative of the Sigma function calculated in H three. So the third block is basically just W-2. The fourth block is again the uh the first derivative of the Sigma function calculated in H two. And finally, we have for the fifth block our input vector X excellent. So now we have all the elements for our gradient which are the",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=813s",
        "start_time": "813.539"
    },
    {
        "id": "af78521d",
        "text": "first element here. So the derivative of the E over A three can be rewritten as a three minus Y. Then we have the second element here. So which is the derivative of a three over H three. And this is the uh first derivative of the Sigma function calculated in H three. So the third block is basically just W-2. The fourth block is again the uh the first derivative of the Sigma function calculated in H two. And finally, we have for the fifth block our input vector X excellent. So now we have all the elements for our gradient which are the derivative of the E with respect to W-2 and the derivative of the E with respect to W one. So do you see a pattern emerging here? Yeah, I guess like you can see something. So look at these two elements here. A three minus Y and multiplied by Sigma, the first derivative of the Sigma F function calculated in H three. So they are basically the same as the ones we have here.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=825s",
        "start_time": "825.599"
    },
    {
        "id": "7231ef6d",
        "text": "uh first derivative of the Sigma function calculated in H three. So the third block is basically just W-2. The fourth block is again the uh the first derivative of the Sigma function calculated in H two. And finally, we have for the fifth block our input vector X excellent. So now we have all the elements for our gradient which are the derivative of the E with respect to W-2 and the derivative of the E with respect to W one. So do you see a pattern emerging here? Yeah, I guess like you can see something. So look at these two elements here. A three minus Y and multiplied by Sigma, the first derivative of the Sigma F function calculated in H three. So they are basically the same as the ones we have here. This is great news because it means that we for calculating um the derivatives of previous layers here, we can reuse some stuff that we've calculated uh in layers that are more like towards like the right towards the end of the network. And this is the whole point of like a back propagation. So we for calculating um",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=840s",
        "start_time": "840.679"
    },
    {
        "id": "c9798820",
        "text": "derivative of the E with respect to W-2 and the derivative of the E with respect to W one. So do you see a pattern emerging here? Yeah, I guess like you can see something. So look at these two elements here. A three minus Y and multiplied by Sigma, the first derivative of the Sigma F function calculated in H three. So they are basically the same as the ones we have here. This is great news because it means that we for calculating um the derivatives of previous layers here, we can reuse some stuff that we've calculated uh in layers that are more like towards like the right towards the end of the network. And this is the whole point of like a back propagation. So we for calculating um uh the the the derivatives of like the error function in previous",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=868s",
        "start_time": "868.559"
    },
    {
        "id": "81bf3003",
        "text": "This is great news because it means that we for calculating um the derivatives of previous layers here, we can reuse some stuff that we've calculated uh in layers that are more like towards like the right towards the end of the network. And this is the whole point of like a back propagation. So we for calculating um uh the the the derivatives of like the error function in previous uh layers with respect to the weights, we can use elements that we've already calculated. So let's see this like in action again now that we know like all the pieces of the puzzle cool. So we said that the first phase here for doing back prop is to calculate the error, then once we have the errors, we use those errors to calculate the first derivative of the error with respect to W-2.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=897s",
        "start_time": "897.83"
    },
    {
        "id": "d8bbaf15",
        "text": "uh the the the derivatives of like the error function in previous uh layers with respect to the weights, we can use elements that we've already calculated. So let's see this like in action again now that we know like all the pieces of the puzzle cool. So we said that the first phase here for doing back prop is to calculate the error, then once we have the errors, we use those errors to calculate the first derivative of the error with respect to W-2. And then we back propagate that we use that information to calculate the derivative of the error function with respect to W one. So we're basically moving all the way backwards from right to left. And so this is the whole point of",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=926s",
        "start_time": "926.63"
    },
    {
        "id": "06798a77",
        "text": "uh layers with respect to the weights, we can use elements that we've already calculated. So let's see this like in action again now that we know like all the pieces of the puzzle cool. So we said that the first phase here for doing back prop is to calculate the error, then once we have the errors, we use those errors to calculate the first derivative of the error with respect to W-2. And then we back propagate that we use that information to calculate the derivative of the error function with respect to W one. So we're basically moving all the way backwards from right to left. And so this is the whole point of propagating back the error signal. Nice. So now we have an understanding of back propagation. So you should congratulate yourself because I feel there are not many people out there also machine learning practitioners who really understands like the math behind uh back propagation. Now you do nice.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=933s",
        "start_time": "933.28"
    },
    {
        "id": "1b0242e1",
        "text": "And then we back propagate that we use that information to calculate the derivative of the error function with respect to W one. So we're basically moving all the way backwards from right to left. And so this is the whole point of propagating back the error signal. Nice. So now we have an understanding of back propagation. So you should congratulate yourself because I feel there are not many people out there also machine learning practitioners who really understands like the math behind uh back propagation. Now you do nice. So let's go back, let's say the higher level perspective. So the the training steps. So we've seen the first three steps. So we get the prop predictions, we calculate the error and then we calculate the gradient of the error function over the weight. Now, the last thing that remains to do is to actually update the parameters to update the weight. So how do we do that? Well, for doing that, we need",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=959s",
        "start_time": "959.38"
    },
    {
        "id": "e45df34b",
        "text": "propagating back the error signal. Nice. So now we have an understanding of back propagation. So you should congratulate yourself because I feel there are not many people out there also machine learning practitioners who really understands like the math behind uh back propagation. Now you do nice. So let's go back, let's say the higher level perspective. So the the training steps. So we've seen the first three steps. So we get the prop predictions, we calculate the error and then we calculate the gradient of the error function over the weight. Now, the last thing that remains to do is to actually update the parameters to update the weight. So how do we do that? Well, for doing that, we need a very important algorithm that's called gradient descent that's also used in traditional machine learning. So how does gradient descent work? Well with gradient descent, we take a step in the opposite direction to the gradient.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=979s",
        "start_time": "979.59"
    },
    {
        "id": "ce232456",
        "text": "So let's go back, let's say the higher level perspective. So the the training steps. So we've seen the first three steps. So we get the prop predictions, we calculate the error and then we calculate the gradient of the error function over the weight. Now, the last thing that remains to do is to actually update the parameters to update the weight. So how do we do that? Well, for doing that, we need a very important algorithm that's called gradient descent that's also used in traditional machine learning. So how does gradient descent work? Well with gradient descent, we take a step in the opposite direction to the gradient. So the, the, the, the step or the size of the step is basically the learning rate and the learning rate is a parameter uh that we use. And we can trick when we do training with our neural networks to obtain better results. OK. But",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1002s",
        "start_time": "1002.89"
    },
    {
        "id": "349bcda5",
        "text": "a very important algorithm that's called gradient descent that's also used in traditional machine learning. So how does gradient descent work? Well with gradient descent, we take a step in the opposite direction to the gradient. So the, the, the, the step or the size of the step is basically the learning rate and the learning rate is a parameter uh that we use. And we can trick when we do training with our neural networks to obtain better results. OK. But we now have an understanding like a back propagation and like of gradient descent. But how does gradient descent like really work? So what what, what, what does it mean to take a step in the opposite direction to the gradient? Cool for doing that? We need this uh graph down here. So on the X axis, we have the weights",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1029s",
        "start_time": "1029.359"
    },
    {
        "id": "0ae4b09c",
        "text": "So the, the, the, the step or the size of the step is basically the learning rate and the learning rate is a parameter uh that we use. And we can trick when we do training with our neural networks to obtain better results. OK. But we now have an understanding like a back propagation and like of gradient descent. But how does gradient descent like really work? So what what, what, what does it mean to take a step in the opposite direction to the gradient? Cool for doing that? We need this uh graph down here. So on the X axis, we have the weights and on the y axis we have the errors nice. And here we have the error function which is E as a function of W obviously. And it's this orange curve here. So let's say we start from this position here. So after",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1047s",
        "start_time": "1047.41"
    },
    {
        "id": "9bc1ac47",
        "text": "we now have an understanding like a back propagation and like of gradient descent. But how does gradient descent like really work? So what what, what, what does it mean to take a step in the opposite direction to the gradient? Cool for doing that? We need this uh graph down here. So on the X axis, we have the weights and on the y axis we have the errors nice. And here we have the error function which is E as a function of W obviously. And it's this orange curve here. So let's say we start from this position here. So after uh we've uh trained uh like our network, uh for example, we've passed in like the first sample we've calculated the gradient and we've, and we know that we are like at this point like with the errors and with the weight cool. So here we need to calculate the gradient and the gradient is basically this purple line here. And the gradient what does intuitively is to give us information the direction",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1067s",
        "start_time": "1067.079"
    },
    {
        "id": "15a92b5a",
        "text": "and on the y axis we have the errors nice. And here we have the error function which is E as a function of W obviously. And it's this orange curve here. So let's say we start from this position here. So after uh we've uh trained uh like our network, uh for example, we've passed in like the first sample we've calculated the gradient and we've, and we know that we are like at this point like with the errors and with the weight cool. So here we need to calculate the gradient and the gradient is basically this purple line here. And the gradient what does intuitively is to give us information the direction uh in which the function uh increases the fastest right cool. So if we know the gradient, what we want to do is go in the opposite direction to the gradient because we want to go down in uh the, the function because we want to minimize this function. So we want to go down here to this global minimum where the error of our network is minimized. And so we take the gradient",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1089s",
        "start_time": "1089.959"
    },
    {
        "id": "2216de2c",
        "text": "uh we've uh trained uh like our network, uh for example, we've passed in like the first sample we've calculated the gradient and we've, and we know that we are like at this point like with the errors and with the weight cool. So here we need to calculate the gradient and the gradient is basically this purple line here. And the gradient what does intuitively is to give us information the direction uh in which the function uh increases the fastest right cool. So if we know the gradient, what we want to do is go in the opposite direction to the gradient because we want to go down in uh the, the function because we want to minimize this function. So we want to go down here to this global minimum where the error of our network is minimized. And so we take the gradient and we basically uh take a step in the opposite direction to the gradient where we have like the gradient calculated for all the weights in the network. And so we take a step that's given by the learning rate. So",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1106s",
        "start_time": "1106.189"
    },
    {
        "id": "9ba3c845",
        "text": "uh in which the function uh increases the fastest right cool. So if we know the gradient, what we want to do is go in the opposite direction to the gradient because we want to go down in uh the, the function because we want to minimize this function. So we want to go down here to this global minimum where the error of our network is minimized. And so we take the gradient and we basically uh take a step in the opposite direction to the gradient where we have like the gradient calculated for all the weights in the network. And so we take a step that's given by the learning rate. So uh we calculate the gradient here like when we are like at this 0.1 and we jump down here to, to this point. Now we we have like uh another like sample in uh like the, the uh the network for example. And it does like forward propagation, like backward propagation. And then when we have like the gradient uh of the of the net of the weight of the network again,",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1135s",
        "start_time": "1135.599"
    },
    {
        "id": "0f593c63",
        "text": "and we basically uh take a step in the opposite direction to the gradient where we have like the gradient calculated for all the weights in the network. And so we take a step that's given by the learning rate. So uh we calculate the gradient here like when we are like at this 0.1 and we jump down here to, to this point. Now we we have like uh another like sample in uh like the, the uh the network for example. And it does like forward propagation, like backward propagation. And then when we have like the gradient uh of the of the net of the weight of the network again, uh we just like take a step in the opposite direction and we go down to three, down to 456 until we reach the global minimum. And so this is the whole point of training. So we want to",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1163s",
        "start_time": "1163.619"
    },
    {
        "id": "b85a885a",
        "text": "uh we calculate the gradient here like when we are like at this 0.1 and we jump down here to, to this point. Now we we have like uh another like sample in uh like the, the uh the network for example. And it does like forward propagation, like backward propagation. And then when we have like the gradient uh of the of the net of the weight of the network again, uh we just like take a step in the opposite direction and we go down to three, down to 456 until we reach the global minimum. And so this is the whole point of training. So we want to uh tweak the weights in such a way that we can get the minimum possible error. And when we have the minimum possible error, it means that we have good predictions because the predictions are quite similar to the act actual outputs to the correct outputs",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1180s",
        "start_time": "1180.81"
    },
    {
        "id": "b9c47fa3",
        "text": "uh we just like take a step in the opposite direction and we go down to three, down to 456 until we reach the global minimum. And so this is the whole point of training. So we want to uh tweak the weights in such a way that we can get the minimum possible error. And when we have the minimum possible error, it means that we have good predictions because the predictions are quite similar to the act actual outputs to the correct outputs good. So this was like quite intense. But now I hope you have understanding of both back propagation and grand in descent and how we use them for neural networks. Cool. So what remains to do? So what should we do",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1206s",
        "start_time": "1206.56"
    },
    {
        "id": "7fce1c37",
        "text": "uh tweak the weights in such a way that we can get the minimum possible error. And when we have the minimum possible error, it means that we have good predictions because the predictions are quite similar to the act actual outputs to the correct outputs good. So this was like quite intense. But now I hope you have understanding of both back propagation and grand in descent and how we use them for neural networks. Cool. So what remains to do? So what should we do like next in the next video? So we'll take all of this theoretical knowledge and we're gonna turn it into implementation. So we're gonna implement back propagation and grid in the center and we're gonna expand the multi-layered perception class that we've already built so that we can train our network from scratch good.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1219s",
        "start_time": "1219.01"
    },
    {
        "id": "9e952d2d",
        "text": "good. So this was like quite intense. But now I hope you have understanding of both back propagation and grand in descent and how we use them for neural networks. Cool. So what remains to do? So what should we do like next in the next video? So we'll take all of this theoretical knowledge and we're gonna turn it into implementation. So we're gonna implement back propagation and grid in the center and we're gonna expand the multi-layered perception class that we've already built so that we can train our network from scratch good. So this was the video for today. So I hope you enjoyed it. And if that's the case, please subscribe and hit the notification bell if you have any questions or doubts uh about like the content of this video because it was like quite tough. Please leave a comment, leave a questions in the comments section below and I hope I'll see you next time. Cheers.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1238s",
        "start_time": "1238.29"
    }
]