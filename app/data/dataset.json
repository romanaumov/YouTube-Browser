[
    {
        "id": "d8917aa5",
        "text": "Hi, everybody and welcome to a new exciting video in the audio signal processing for machine learning series. This time, we'll look into a very important audio feature. In other words, Mal frequency seal coefficient or if we use their acronym MF CCS. But before we get started with this super cool topic, I want to remind you about the sound of the Ice L community. So if you sign up there, you can get feedback, share projects and share ideas with a community of people who are interested in A I audio A I music and audio signal processing. So I really invite you to check this community out and I'll leave you the link and the sign up link to the Slack workspace in the description box below. Now let's move on to the cool stuff. But before we get to M I want just like to remind you about what we did in the previous couple of videos and we focused on male spectrograms. Now male spectrograms are going to be like an important building block to understanding MF CCS. So if you are really not that familiar with that, I highly suggest you to go check out my previous couple of videos on male spectrograms. OK? But now let's get started with MFC",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "3a71bea5",
        "text": "if you sign up there, you can get feedback, share projects and share ideas with a community of people who are interested in A I audio A I music and audio signal processing. So I really invite you to check this community out and I'll leave you the link and the sign up link to the Slack workspace in the description box below. Now let's move on to the cool stuff. But before we get to M I want just like to remind you about what we did in the previous couple of videos and we focused on male spectrograms. Now male spectrograms are going to be like an important building block to understanding MF CCS. So if you are really not that familiar with that, I highly suggest you to go check out my previous couple of videos on male spectrograms. OK? But now let's get started with MFC see that as a set built on top of the concept of male spectrum to a certain extent. OK. So now we have this audio feature, it's called male frequency seor coefficients, right? So in this feature, we have many different words. So now let's try to uh understand which word means what. OK. So male frequency, well, male frequency, as I said, refer",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=25s",
        "start_time": "25.379"
    },
    {
        "id": "228ad9fe",
        "text": "I want just like to remind you about what we did in the previous couple of videos and we focused on male spectrograms. Now male spectrograms are going to be like an important building block to understanding MF CCS. So if you are really not that familiar with that, I highly suggest you to go check out my previous couple of videos on male spectrograms. OK? But now let's get started with MFC see that as a set built on top of the concept of male spectrum to a certain extent. OK. So now we have this audio feature, it's called male frequency seor coefficients, right? So in this feature, we have many different words. So now let's try to uh understand which word means what. OK. So male frequency, well, male frequency, as I said, refer somewhat to the concept of a male spectrogram. Basically, the idea is that we are using the male scale here, which is a a perceptually relevant uh scale for pit and there's something that has to do with male spectrograms and male scale like in MFCC. OK. So we know that and we know that what male spectrograms are from previous videos. OK. So now let's move on and the last point is coefficients.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=50s",
        "start_time": "50.759"
    },
    {
        "id": "86e69438",
        "text": "see that as a set built on top of the concept of male spectrum to a certain extent. OK. So now we have this audio feature, it's called male frequency seor coefficients, right? So in this feature, we have many different words. So now let's try to uh understand which word means what. OK. So male frequency, well, male frequency, as I said, refer somewhat to the concept of a male spectrogram. Basically, the idea is that we are using the male scale here, which is a a perceptually relevant uh scale for pit and there's something that has to do with male spectrograms and male scale like in MFCC. OK. So we know that and we know that what male spectrograms are from previous videos. OK. So now let's move on and the last point is coefficients. Well, this isn't really like that difficult to understand because the idea that probably you may guess like from, from like this name is that out of these features, you're gonna get a number of coefficients, a number of values and those coefficients will describe some characteristic of a piece of sound, right? That's all it is, right.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=76s",
        "start_time": "76.139"
    },
    {
        "id": "fc9b6852",
        "text": "somewhat to the concept of a male spectrogram. Basically, the idea is that we are using the male scale here, which is a a perceptually relevant uh scale for pit and there's something that has to do with male spectrograms and male scale like in MFCC. OK. So we know that and we know that what male spectrograms are from previous videos. OK. So now let's move on and the last point is coefficients. Well, this isn't really like that difficult to understand because the idea that probably you may guess like from, from like this name is that out of these features, you're gonna get a number of coefficients, a number of values and those coefficients will describe some characteristic of a piece of sound, right? That's all it is, right. OK. And finally, we have probably the most interesting part here. That's a Septra, right? So this is a weird word, right? And Septra is the adjective. But if we want to move to the noun, the noun is Septra.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=101s",
        "start_time": "101.519"
    },
    {
        "id": "94668a5a",
        "text": "Well, this isn't really like that difficult to understand because the idea that probably you may guess like from, from like this name is that out of these features, you're gonna get a number of coefficients, a number of values and those coefficients will describe some characteristic of a piece of sound, right? That's all it is, right. OK. And finally, we have probably the most interesting part here. That's a Septra, right? So this is a weird word, right? And Septra is the adjective. But if we want to move to the noun, the noun is Septra. OK. Does this word ring a bell at all?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=131s",
        "start_time": "131.33"
    },
    {
        "id": "69f44ebb",
        "text": "OK. And finally, we have probably the most interesting part here. That's a Septra, right? So this is a weird word, right? And Septra is the adjective. But if we want to move to the noun, the noun is Septra. OK. Does this word ring a bell at all? No, if not, I'll give you a hint.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=154s",
        "start_time": "154.779"
    },
    {
        "id": "64850c8b",
        "text": "OK. Does this word ring a bell at all? No, if not, I'll give you a hint. Ss just like focus on this like four letters here.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=171s",
        "start_time": "171.77"
    },
    {
        "id": "0395ced2",
        "text": "No, if not, I'll give you a hint. Ss just like focus on this like four letters here. Any idea?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=176s",
        "start_time": "176.899"
    },
    {
        "id": "f21d8838",
        "text": "Ss just like focus on this like four letters here. Any idea? If not, I'll give you the answer.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=180s",
        "start_time": "180.58"
    },
    {
        "id": "7b8914af",
        "text": "Any idea? If not, I'll give you the answer. It's spectrum, right? So if you, if you just like take steps and you spell it like backwards, you'll have spec and spectrum. OK. So Seps is somewhat related to spectrum. OK. So here we have clearly a wordplay and so it's gonna take us like some time to understand why this is like relevant and why researchers who came up with the idea of sere",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=186s",
        "start_time": "186.35"
    },
    {
        "id": "85998817",
        "text": "If not, I'll give you the answer. It's spectrum, right? So if you, if you just like take steps and you spell it like backwards, you'll have spec and spectrum. OK. So Seps is somewhat related to spectrum. OK. So here we have clearly a wordplay and so it's gonna take us like some time to understand why this is like relevant and why researchers who came up with the idea of sere um used like this word and they had this kind of like wordplay on spectrum. So I suggest you just like to bear with me because this is gonna be like a quite intense and in depth session to understand ses stream. And then once we understand SEPS stream, we're gonna use like this concept to build MF CCS or to see how we can build MF ccs on top of seps.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=188s",
        "start_time": "188.25"
    },
    {
        "id": "aacf8e8a",
        "text": "It's spectrum, right? So if you, if you just like take steps and you spell it like backwards, you'll have spec and spectrum. OK. So Seps is somewhat related to spectrum. OK. So here we have clearly a wordplay and so it's gonna take us like some time to understand why this is like relevant and why researchers who came up with the idea of sere um used like this word and they had this kind of like wordplay on spectrum. So I suggest you just like to bear with me because this is gonna be like a quite intense and in depth session to understand ses stream. And then once we understand SEPS stream, we're gonna use like this concept to build MF CCS or to see how we can build MF ccs on top of seps. OK? So now let's put subs stream and spectrum like down there. But when we are talking about sere, it's not only subst the the weird words that we, we have or that these researchers who came up with this idea came out with. So there are a bunch of other concepts there. So that's the concept of we lifting and Ramon, for example. Now, I guess like you, you, you, you, you you have an idea of how like to",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=191s",
        "start_time": "191.199"
    },
    {
        "id": "b01923a7",
        "text": "um used like this word and they had this kind of like wordplay on spectrum. So I suggest you just like to bear with me because this is gonna be like a quite intense and in depth session to understand ses stream. And then once we understand SEPS stream, we're gonna use like this concept to build MF CCS or to see how we can build MF ccs on top of seps. OK? So now let's put subs stream and spectrum like down there. But when we are talking about sere, it's not only subst the the weird words that we, we have or that these researchers who came up with this idea came out with. So there are a bunch of other concepts there. So that's the concept of we lifting and Ramon, for example. Now, I guess like you, you, you, you, you you have an idea of how like to translating like these things into stuff that makes sense. And indeed, right. Quiery is a wordplay on a frequency lifting is connected to some sort of filtering and dr is connected to harmonic. OK. So now we are entering the world of septum where we don't have frequencies, but we have Quis, we don't have filtering, but we have lifting and we don't have harmonics, but we have Ramons",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=220s",
        "start_time": "220.72"
    },
    {
        "id": "517d56a4",
        "text": "OK? So now let's put subs stream and spectrum like down there. But when we are talking about sere, it's not only subst the the weird words that we, we have or that these researchers who came up with this idea came out with. So there are a bunch of other concepts there. So that's the concept of we lifting and Ramon, for example. Now, I guess like you, you, you, you, you you have an idea of how like to translating like these things into stuff that makes sense. And indeed, right. Quiery is a wordplay on a frequency lifting is connected to some sort of filtering and dr is connected to harmonic. OK. So now we are entering the world of septum where we don't have frequencies, but we have Quis, we don't have filtering, but we have lifting and we don't have harmonics, but we have Ramons sounds a little bit weird, right. Yeah. And it is so bear with me to understand what all of these things like really mean. OK. So now uh let's get like a an historical understanding of the, the concept of SEPS where like it it came out from and how it developed over time.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=246s",
        "start_time": "246.669"
    },
    {
        "id": "586aa48f",
        "text": "translating like these things into stuff that makes sense. And indeed, right. Quiery is a wordplay on a frequency lifting is connected to some sort of filtering and dr is connected to harmonic. OK. So now we are entering the world of septum where we don't have frequencies, but we have Quis, we don't have filtering, but we have lifting and we don't have harmonics, but we have Ramons sounds a little bit weird, right. Yeah. And it is so bear with me to understand what all of these things like really mean. OK. So now uh let's get like a an historical understanding of the, the concept of SEPS where like it it came out from and how it developed over time. So researchers, I believe at the mit during the sixties came out with this concept of a subst and they used it to study specifically um ecos in seismic signals. Then other researchers noticed that like this concept of subst could be nicely applied to speech processing.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=275s",
        "start_time": "275.529"
    },
    {
        "id": "33c48a5e",
        "text": "sounds a little bit weird, right. Yeah. And it is so bear with me to understand what all of these things like really mean. OK. So now uh let's get like a an historical understanding of the, the concept of SEPS where like it it came out from and how it developed over time. So researchers, I believe at the mit during the sixties came out with this concept of a subst and they used it to study specifically um ecos in seismic signals. Then other researchers noticed that like this concept of subst could be nicely applied to speech processing. And indeed, towards the end of the sixties, uh SEPS stream started to be used like in the speech processing community. And during the seventies and onwards, it became the kind of like audio, audio feature of choice for a speech recognition, speech identification and all sorts of speech processing related uh problems. And that remained like that for a long time until I think like the advent of deep learning. So very recent stuff then.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=304s",
        "start_time": "304.76"
    },
    {
        "id": "455c37eb",
        "text": "So researchers, I believe at the mit during the sixties came out with this concept of a subst and they used it to study specifically um ecos in seismic signals. Then other researchers noticed that like this concept of subst could be nicely applied to speech processing. And indeed, towards the end of the sixties, uh SEPS stream started to be used like in the speech processing community. And during the seventies and onwards, it became the kind of like audio, audio feature of choice for a speech recognition, speech identification and all sorts of speech processing related uh problems. And that remained like that for a long time until I think like the advent of deep learning. So very recent stuff then. OK. And in the two thousands um seps stream started to be adopted in the form of MF CCS also in music processing and specifically in music information retrieval.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=327s",
        "start_time": "327.6"
    },
    {
        "id": "2739e490",
        "text": "And indeed, towards the end of the sixties, uh SEPS stream started to be used like in the speech processing community. And during the seventies and onwards, it became the kind of like audio, audio feature of choice for a speech recognition, speech identification and all sorts of speech processing related uh problems. And that remained like that for a long time until I think like the advent of deep learning. So very recent stuff then. OK. And in the two thousands um seps stream started to be adopted in the form of MF CCS also in music processing and specifically in music information retrieval. So as you see now we have an audio feature, a Mys mystery audio feature that can serve many different purposes or many different applications. It works well for seismic signals. It works great for audio uh I mean speech processing and it also works really well for music processing.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=351s",
        "start_time": "351.98"
    },
    {
        "id": "62779160",
        "text": "OK. And in the two thousands um seps stream started to be adopted in the form of MF CCS also in music processing and specifically in music information retrieval. So as you see now we have an audio feature, a Mys mystery audio feature that can serve many different purposes or many different applications. It works well for seismic signals. It works great for audio uh I mean speech processing and it also works really well for music processing. OK. So now it's time to understand like this mysterious um audio feature a little bit more. So and here we'll do like uh uh uh we'll try to understand this like in a few different on a few different levels. So the first level will be a mathematical formalization of the concept",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=381s",
        "start_time": "381.66"
    },
    {
        "id": "b7f52101",
        "text": "So as you see now we have an audio feature, a Mys mystery audio feature that can serve many different purposes or many different applications. It works well for seismic signals. It works great for audio uh I mean speech processing and it also works really well for music processing. OK. So now it's time to understand like this mysterious um audio feature a little bit more. So and here we'll do like uh uh uh we'll try to understand this like in a few different on a few different levels. So the first level will be a mathematical formalization of the concept ses stream. Then we'll look into the visualization of SEPS stream so that hopefully you can understand what's going on there for real. And finally, we'll look at SEPS stream in the context of speech and there probably you'll have like the better intuition out of all of these approaches. OK. Let's get started with the math behind it.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=395s",
        "start_time": "395.72"
    },
    {
        "id": "2dc10cf7",
        "text": "OK. So now it's time to understand like this mysterious um audio feature a little bit more. So and here we'll do like uh uh uh we'll try to understand this like in a few different on a few different levels. So the first level will be a mathematical formalization of the concept ses stream. Then we'll look into the visualization of SEPS stream so that hopefully you can understand what's going on there for real. And finally, we'll look at SEPS stream in the context of speech and there probably you'll have like the better intuition out of all of these approaches. OK. Let's get started with the math behind it. So how do we compute the SES stream? Well, we compute it like this. So now here we have like our ses strum and we indicate that like S capital T and uh the sere is provided by like this formula here. So let's get started with XFT. Well, XFT is just like a normal uh signal in the time domain, right, it's just like normal waveform then",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=417s",
        "start_time": "417.899"
    },
    {
        "id": "26975784",
        "text": "ses stream. Then we'll look into the visualization of SEPS stream so that hopefully you can understand what's going on there for real. And finally, we'll look at SEPS stream in the context of speech and there probably you'll have like the better intuition out of all of these approaches. OK. Let's get started with the math behind it. So how do we compute the SES stream? Well, we compute it like this. So now here we have like our ses strum and we indicate that like S capital T and uh the sere is provided by like this formula here. So let's get started with XFT. Well, XFT is just like a normal uh signal in the time domain, right, it's just like normal waveform then out of this normal waveform, what we do is we take the uh discrete fourier transform, which here I've indicated with this capital F. And so when we do that, we come up with a spectrum and we move from the time domain to the frequency domain. OK. Now the next step that we want to do is apply a logarithm to the spectrum.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=437s",
        "start_time": "437.089"
    },
    {
        "id": "8d0ffc7d",
        "text": "So how do we compute the SES stream? Well, we compute it like this. So now here we have like our ses strum and we indicate that like S capital T and uh the sere is provided by like this formula here. So let's get started with XFT. Well, XFT is just like a normal uh signal in the time domain, right, it's just like normal waveform then out of this normal waveform, what we do is we take the uh discrete fourier transform, which here I've indicated with this capital F. And so when we do that, we come up with a spectrum and we move from the time domain to the frequency domain. OK. Now the next step that we want to do is apply a logarithm to the spectrum. And in this way, we get the log amplitude spectrum. So in other words, we are applying the logarithm on the amplitude of the spectrum. Now, uh if you if you are not familiar with the fourier transform or logarithm logarithm amplitude spectrum, all of this kind of stuff I highly suggest you to go check out my previous videos on the fourier transform",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=456s",
        "start_time": "456.51"
    },
    {
        "id": "b2abbe78",
        "text": "out of this normal waveform, what we do is we take the uh discrete fourier transform, which here I've indicated with this capital F. And so when we do that, we come up with a spectrum and we move from the time domain to the frequency domain. OK. Now the next step that we want to do is apply a logarithm to the spectrum. And in this way, we get the log amplitude spectrum. So in other words, we are applying the logarithm on the amplitude of the spectrum. Now, uh if you if you are not familiar with the fourier transform or logarithm logarithm amplitude spectrum, all of this kind of stuff I highly suggest you to go check out my previous videos on the fourier transform because all of these things I've addressed them time and again, like in my previous videos, OK. Good. So we said we start from the signal, we take the, the fourier transform. So we, we move to a spectrum, we take the log a spectrum.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=480s",
        "start_time": "480.57"
    },
    {
        "id": "eae64cbd",
        "text": "And in this way, we get the log amplitude spectrum. So in other words, we are applying the logarithm on the amplitude of the spectrum. Now, uh if you if you are not familiar with the fourier transform or logarithm logarithm amplitude spectrum, all of this kind of stuff I highly suggest you to go check out my previous videos on the fourier transform because all of these things I've addressed them time and again, like in my previous videos, OK. Good. So we said we start from the signal, we take the, the fourier transform. So we, we move to a spectrum, we take the log a spectrum. And finally, at this point, we do the f the kind of the key step to get to a subst which is basically applying an inverse fourier transform to a log amplitude spectrum. And when we do that, we come up with a subst",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=505s",
        "start_time": "505.859"
    },
    {
        "id": "685f28d2",
        "text": "because all of these things I've addressed them time and again, like in my previous videos, OK. Good. So we said we start from the signal, we take the, the fourier transform. So we, we move to a spectrum, we take the log a spectrum. And finally, at this point, we do the f the kind of the key step to get to a subst which is basically applying an inverse fourier transform to a log amplitude spectrum. And when we do that, we come up with a subst well, if you think about this, what we are actually doing is we are taking a spectrum specifically a log amplitude spectrum and then we are calculating a spectrum of a spectrum, right? So and that's because we are applying the inverse for a transform at this point. OK. So we could have called",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=529s",
        "start_time": "529.88"
    },
    {
        "id": "dd3f62de",
        "text": "And finally, at this point, we do the f the kind of the key step to get to a subst which is basically applying an inverse fourier transform to a log amplitude spectrum. And when we do that, we come up with a subst well, if you think about this, what we are actually doing is we are taking a spectrum specifically a log amplitude spectrum and then we are calculating a spectrum of a spectrum, right? So and that's because we are applying the inverse for a transform at this point. OK. So we could have called the ses stream spectrum of a spectrum, right? But that wouldn't sound really cool. So what the researchers decided to do is use like this wordplay. And so they decided to use like this sere and the reason why they, they use like this, they just like took like the first like four letters in spectrum and",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=546s",
        "start_time": "546.989"
    },
    {
        "id": "bfa8406b",
        "text": "well, if you think about this, what we are actually doing is we are taking a spectrum specifically a log amplitude spectrum and then we are calculating a spectrum of a spectrum, right? So and that's because we are applying the inverse for a transform at this point. OK. So we could have called the ses stream spectrum of a spectrum, right? But that wouldn't sound really cool. So what the researchers decided to do is use like this wordplay. And so they decided to use like this sere and the reason why they, they use like this, they just like took like the first like four letters in spectrum and kind of like use them like backwards is gonna be clear like in a few moments. So bear with me. OK. So now we we know like that on a very high level the spectrum, well, the septum is the spectrum of a spectrum. OK? But now let's try to visualize this concept because this is gonna help us understand what's going on here. OK. So we will try to",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=567s",
        "start_time": "567.989"
    },
    {
        "id": "c7164f1e",
        "text": "the ses stream spectrum of a spectrum, right? But that wouldn't sound really cool. So what the researchers decided to do is use like this wordplay. And so they decided to use like this sere and the reason why they, they use like this, they just like took like the first like four letters in spectrum and kind of like use them like backwards is gonna be clear like in a few moments. So bear with me. OK. So now we we know like that on a very high level the spectrum, well, the septum is the spectrum of a spectrum. OK? But now let's try to visualize this concept because this is gonna help us understand what's going on here. OK. So we will try to visualize all those different steps that we saw in the mathematical formalization. So we start with a normal waveform. We are in the time domain, we have like very short amount of sound just like 40 milliseconds, for example here. And then what we want to do here as a first step is taking the discrete fourier transform.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=593s",
        "start_time": "593.51"
    },
    {
        "id": "2b0a584d",
        "text": "kind of like use them like backwards is gonna be clear like in a few moments. So bear with me. OK. So now we we know like that on a very high level the spectrum, well, the septum is the spectrum of a spectrum. OK? But now let's try to visualize this concept because this is gonna help us understand what's going on here. OK. So we will try to visualize all those different steps that we saw in the mathematical formalization. So we start with a normal waveform. We are in the time domain, we have like very short amount of sound just like 40 milliseconds, for example here. And then what we want to do here as a first step is taking the discrete fourier transform. And what we get out of that is our usual power spectrum where on the X axis we have frequency and on the y axis we have power. So we've seen this time and again, time and again during this uh series. And basically what we've done here is moving like from the time domain to the frequency domain. And um the values that we have for each frequency tells us",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=614s",
        "start_time": "614.89"
    },
    {
        "id": "5ae9deb1",
        "text": "visualize all those different steps that we saw in the mathematical formalization. So we start with a normal waveform. We are in the time domain, we have like very short amount of sound just like 40 milliseconds, for example here. And then what we want to do here as a first step is taking the discrete fourier transform. And what we get out of that is our usual power spectrum where on the X axis we have frequency and on the y axis we have power. So we've seen this time and again, time and again during this uh series. And basically what we've done here is moving like from the time domain to the frequency domain. And um the values that we have for each frequency tells us how much each frequency component is present in the original signal in the original waveform, right? OK. So this is like the, the first step. Now, the other step is that of um applying logarithm to power spectrum.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=638s",
        "start_time": "638.909"
    },
    {
        "id": "a77330b6",
        "text": "And what we get out of that is our usual power spectrum where on the X axis we have frequency and on the y axis we have power. So we've seen this time and again, time and again during this uh series. And basically what we've done here is moving like from the time domain to the frequency domain. And um the values that we have for each frequency tells us how much each frequency component is present in the original signal in the original waveform, right? OK. So this is like the, the first step. Now, the other step is that of um applying logarithm to power spectrum. And we, yeah, let me just like shift like the power spec spectrum here on the left. And now we can apply the logarithm. And so here what we do is a simple transformation. So we take like all the amplitudes and we apply like a logarithm so that we get",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=660s",
        "start_time": "660.88"
    },
    {
        "id": "0bd021d6",
        "text": "how much each frequency component is present in the original signal in the original waveform, right? OK. So this is like the, the first step. Now, the other step is that of um applying logarithm to power spectrum. And we, yeah, let me just like shift like the power spec spectrum here on the left. And now we can apply the logarithm. And so here what we do is a simple transformation. So we take like all the amplitudes and we apply like a logarithm so that we get decibels right on the Y axis as the the unit of reference and on the X axis still we have frequency, right? Because the um transformation was only like on the y axis really. OK. So now we have the log power spectrum. Now let me shift this like onto the left once again.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=685s",
        "start_time": "685.289"
    },
    {
        "id": "bed6da37",
        "text": "And we, yeah, let me just like shift like the power spec spectrum here on the left. And now we can apply the logarithm. And so here what we do is a simple transformation. So we take like all the amplitudes and we apply like a logarithm so that we get decibels right on the Y axis as the the unit of reference and on the X axis still we have frequency, right? Because the um transformation was only like on the y axis really. OK. So now we have the log power spectrum. Now let me shift this like onto the left once again. So what about the log power spectrum? So first of all, it is a continuous signal, right? Second point it has some periodic structures, right? And this periodic structures are present because the log power spectrum has like some harmonic components or like the original signal has some harmonic components that gets that become kind of like",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=704s",
        "start_time": "704.28"
    },
    {
        "id": "6667a62e",
        "text": "decibels right on the Y axis as the the unit of reference and on the X axis still we have frequency, right? Because the um transformation was only like on the y axis really. OK. So now we have the log power spectrum. Now let me shift this like onto the left once again. So what about the log power spectrum? So first of all, it is a continuous signal, right? Second point it has some periodic structures, right? And this periodic structures are present because the log power spectrum has like some harmonic components or like the original signal has some harmonic components that gets that become kind of like are periodic in the spectrum. And so when we have a signal, even if it's just like a spectrum",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=721s",
        "start_time": "721.09"
    },
    {
        "id": "e0cd7c61",
        "text": "So what about the log power spectrum? So first of all, it is a continuous signal, right? Second point it has some periodic structures, right? And this periodic structures are present because the log power spectrum has like some harmonic components or like the original signal has some harmonic components that gets that become kind of like are periodic in the spectrum. And so when we have a signal, even if it's just like a spectrum that has some uh periodicity. What we can do is apply a transformation like a fourier transform to understand like the different components and try to find like which frequencies right are present in the signal. So in other words, what we can do here is treat this log power spectrum as a signal at a time domain signal.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=739s",
        "start_time": "739.849"
    },
    {
        "id": "1aabda10",
        "text": "are periodic in the spectrum. And so when we have a signal, even if it's just like a spectrum that has some uh periodicity. What we can do is apply a transformation like a fourier transform to understand like the different components and try to find like which frequencies right are present in the signal. So in other words, what we can do here is treat this log power spectrum as a signal at a time domain signal. And we can apply a fourier transform like transformation, right. And specifically, we'll be applying an inverse fourier transform. And what we'll get is a spectrum of this signal which is the spectrum. So in other words, it is the spectrum of a sex A spectrum which is the ses",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=766s",
        "start_time": "766.84"
    },
    {
        "id": "91e53add",
        "text": "that has some uh periodicity. What we can do is apply a transformation like a fourier transform to understand like the different components and try to find like which frequencies right are present in the signal. So in other words, what we can do here is treat this log power spectrum as a signal at a time domain signal. And we can apply a fourier transform like transformation, right. And specifically, we'll be applying an inverse fourier transform. And what we'll get is a spectrum of this signal which is the spectrum. So in other words, it is the spectrum of a sex A spectrum which is the ses and here we go, we apply the inverse fourier transform and we get the subs which is the spectrum of a spectrum. Now the cool thing that or the thing that we should think about is what do we actually have on the X axis, right? Because now we have the spectrum of a spectrum. I mean if you",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=775s",
        "start_time": "775.369"
    },
    {
        "id": "ea78f5a7",
        "text": "And we can apply a fourier transform like transformation, right. And specifically, we'll be applying an inverse fourier transform. And what we'll get is a spectrum of this signal which is the spectrum. So in other words, it is the spectrum of a sex A spectrum which is the ses and here we go, we apply the inverse fourier transform and we get the subs which is the spectrum of a spectrum. Now the cool thing that or the thing that we should think about is what do we actually have on the X axis, right? Because now we have the spectrum of a spectrum. I mean if you in the time domain and you and you take like a fourier transform, then you move in the frequency domain. So on the X axis, you'll have frequencies obviously. But if you start from a signal that has like frequencies on the x axis, what do you actually get on the x axis of the transformation? Right? And",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=803s",
        "start_time": "803.489"
    },
    {
        "id": "898caa49",
        "text": "and here we go, we apply the inverse fourier transform and we get the subs which is the spectrum of a spectrum. Now the cool thing that or the thing that we should think about is what do we actually have on the X axis, right? Because now we have the spectrum of a spectrum. I mean if you in the time domain and you and you take like a fourier transform, then you move in the frequency domain. So on the X axis, you'll have frequencies obviously. But if you start from a signal that has like frequencies on the x axis, what do you actually get on the x axis of the transformation? Right? And the the answer is that we take we get like some sort of pseudo frequency axis. And this pseudo frequency axis was termed by the researchers as quiery. And the unit of reference here is milliseconds or seconds. Now let me",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=824s",
        "start_time": "824.219"
    },
    {
        "id": "e2c5e3b5",
        "text": "in the time domain and you and you take like a fourier transform, then you move in the frequency domain. So on the X axis, you'll have frequencies obviously. But if you start from a signal that has like frequencies on the x axis, what do you actually get on the x axis of the transformation? Right? And the the answer is that we take we get like some sort of pseudo frequency axis. And this pseudo frequency axis was termed by the researchers as quiery. And the unit of reference here is milliseconds or seconds. Now let me show you why we are talking about ferency and SRE So why this word workplace makes sense? And the reason is because we are starting from the time domain originally with the way form, then we go to the frequency domain with the initial discrete fourier transform. Now we apply another",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=842s",
        "start_time": "842.789"
    },
    {
        "id": "d5f56a2e",
        "text": "the the answer is that we take we get like some sort of pseudo frequency axis. And this pseudo frequency axis was termed by the researchers as quiery. And the unit of reference here is milliseconds or seconds. Now let me show you why we are talking about ferency and SRE So why this word workplace makes sense? And the reason is because we are starting from the time domain originally with the way form, then we go to the frequency domain with the initial discrete fourier transform. Now we apply another discrete uh we apply like an inverse discrete fourier transform at this point. And we go back to um somewhat something that resembles like a frequency domain, but it's not really a frequency domain, right? And so they just like decided to take like the opposite of that. So it's not frequency, it's queer and this is not a spectrum. This is a subs, right. OK. So here you have like the intuition,",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=862s",
        "start_time": "862.75"
    },
    {
        "id": "4bae3745",
        "text": "show you why we are talking about ferency and SRE So why this word workplace makes sense? And the reason is because we are starting from the time domain originally with the way form, then we go to the frequency domain with the initial discrete fourier transform. Now we apply another discrete uh we apply like an inverse discrete fourier transform at this point. And we go back to um somewhat something that resembles like a frequency domain, but it's not really a frequency domain, right? And so they just like decided to take like the opposite of that. So it's not frequency, it's queer and this is not a spectrum. This is a subs, right. OK. So here you have like the intuition, OK. So what are like all of these values here? Right. So in the ses stream um visualization, right, we have certain peaks and here we have like a very high peak there. So what do they represent? Right. And so",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=883s",
        "start_time": "883.859"
    },
    {
        "id": "6de45185",
        "text": "discrete uh we apply like an inverse discrete fourier transform at this point. And we go back to um somewhat something that resembles like a frequency domain, but it's not really a frequency domain, right? And so they just like decided to take like the opposite of that. So it's not frequency, it's queer and this is not a spectrum. This is a subs, right. OK. So here you have like the intuition, OK. So what are like all of these values here? Right. So in the ses stream um visualization, right, we have certain peaks and here we have like a very high peak there. So what do they represent? Right. And so these basically represent how present these different preferences are in the log power spectrum, right. OK. So here we have like this huge pick and that is the first harmonic. Uh I bet like you guess you, you realize this like yourself, this is like the equivalent of a harmonic, right?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=903s",
        "start_time": "903.719"
    },
    {
        "id": "054d129d",
        "text": "OK. So what are like all of these values here? Right. So in the ses stream um visualization, right, we have certain peaks and here we have like a very high peak there. So what do they represent? Right. And so these basically represent how present these different preferences are in the log power spectrum, right. OK. So here we have like this huge pick and that is the first harmonic. Uh I bet like you guess you, you realize this like yourself, this is like the equivalent of a harmonic, right? And this is the a Raonic that provides us informa or this is like, let's put it this way, this is the quefrency where uh that is associated with the fundamental frequency of the original signal of the original waveform. And indeed, one way of using uh SEPS, seps I should say is one application is for pitch detection",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=932s",
        "start_time": "932.51"
    },
    {
        "id": "96c4b2f2",
        "text": "these basically represent how present these different preferences are in the log power spectrum, right. OK. So here we have like this huge pick and that is the first harmonic. Uh I bet like you guess you, you realize this like yourself, this is like the equivalent of a harmonic, right? And this is the a Raonic that provides us informa or this is like, let's put it this way, this is the quefrency where uh that is associated with the fundamental frequency of the original signal of the original waveform. And indeed, one way of using uh SEPS, seps I should say is one application is for pitch detection because you take like the log power spectrum and then you take the se the se stream and the peak that you're gonna have like this is gonna be like the first Ramon. And you can use that to then move back to the frequency domain and then understand where you have like the fundamental uh",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=948s",
        "start_time": "948.619"
    },
    {
        "id": "93521942",
        "text": "And this is the a Raonic that provides us informa or this is like, let's put it this way, this is the quefrency where uh that is associated with the fundamental frequency of the original signal of the original waveform. And indeed, one way of using uh SEPS, seps I should say is one application is for pitch detection because you take like the log power spectrum and then you take the se the se stream and the peak that you're gonna have like this is gonna be like the first Ramon. And you can use that to then move back to the frequency domain and then understand where you have like the fundamental uh f in the original signal. And so, and why is this like such a peak? Well, this is a peak because this reflects the harmonic structure of the original signal that gets some that gets like represented in a periodic way here in the log power spectrum. So this is like the the key idea there, right? OK. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=975s",
        "start_time": "975.559"
    },
    {
        "id": "96271a1b",
        "text": "because you take like the log power spectrum and then you take the se the se stream and the peak that you're gonna have like this is gonna be like the first Ramon. And you can use that to then move back to the frequency domain and then understand where you have like the fundamental uh f in the original signal. And so, and why is this like such a peak? Well, this is a peak because this reflects the harmonic structure of the original signal that gets some that gets like represented in a periodic way here in the log power spectrum. So this is like the the key idea there, right? OK. So I guess like now we have a uh an understanding of the math behind the Seps stream and we also have an understanding of what the septum looks like, but I think like what's what still is missing here is understanding. So having like an intuition of the septum. So and why it is so important? Why should we butter taking the inverse this grid fourier transform of the log power spectrum? Why should we butter?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1004s",
        "start_time": "1004.979"
    },
    {
        "id": "9ecf6bca",
        "text": "f in the original signal. And so, and why is this like such a peak? Well, this is a peak because this reflects the harmonic structure of the original signal that gets some that gets like represented in a periodic way here in the log power spectrum. So this is like the the key idea there, right? OK. So I guess like now we have a uh an understanding of the math behind the Seps stream and we also have an understanding of what the septum looks like, but I think like what's what still is missing here is understanding. So having like an intuition of the septum. So and why it is so important? Why should we butter taking the inverse this grid fourier transform of the log power spectrum? Why should we butter? Right? OK. So for understanding that we have to take a little detour into how speech works and into speech processing really? Ok. And so I want to context contextualize seps stream in",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1024s",
        "start_time": "1024.109"
    },
    {
        "id": "ebdeb101",
        "text": "I guess like now we have a uh an understanding of the math behind the Seps stream and we also have an understanding of what the septum looks like, but I think like what's what still is missing here is understanding. So having like an intuition of the septum. So and why it is so important? Why should we butter taking the inverse this grid fourier transform of the log power spectrum? Why should we butter? Right? OK. So for understanding that we have to take a little detour into how speech works and into speech processing really? Ok. And so I want to context contextualize seps stream in within speech. And so the first thing that we need to do is understand how we produce speech. And a key element to understanding how humans produce speech is the vocal tract. So the vocal tract is kind of a very complex systems that has like multiple elements. So it has like the tongue, it has the teeth, it has the nasal cavity, your throat. And the basic idea is that",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1050s",
        "start_time": "1050.64"
    },
    {
        "id": "fbda3148",
        "text": "Right? OK. So for understanding that we have to take a little detour into how speech works and into speech processing really? Ok. And so I want to context contextualize seps stream in within speech. And so the first thing that we need to do is understand how we produce speech. And a key element to understanding how humans produce speech is the vocal tract. So the vocal tract is kind of a very complex systems that has like multiple elements. So it has like the tongue, it has the teeth, it has the nasal cavity, your throat. And the basic idea is that uh depending on how you shape your vocal tract, you're gonna produce different sounds different. What like linguist, linguists, I believe it's called like call uh phones or like different constants, different vowels. It really depends on how you put your tongue, how you, you stretch your throat or you contract it, right? And",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1080s",
        "start_time": "1080.28"
    },
    {
        "id": "92c2cde3",
        "text": "within speech. And so the first thing that we need to do is understand how we produce speech. And a key element to understanding how humans produce speech is the vocal tract. So the vocal tract is kind of a very complex systems that has like multiple elements. So it has like the tongue, it has the teeth, it has the nasal cavity, your throat. And the basic idea is that uh depending on how you shape your vocal tract, you're gonna produce different sounds different. What like linguist, linguists, I believe it's called like call uh phones or like different constants, different vowels. It really depends on how you put your tongue, how you, you stretch your throat or you contract it, right? And but if we think about this, uh that in terms of like digital signal processing, we can think of the vocal tract as a filter. In other words, words, the vocal tract acts as a filter. So how do we actually generate, produce speech? Well, this is like quite fascinating and I'll give you like a, a simplification of what like the real thing is, but it's gonna be instrumental to understand sere fully. OK.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1096s",
        "start_time": "1096.875"
    },
    {
        "id": "cba32321",
        "text": "uh depending on how you shape your vocal tract, you're gonna produce different sounds different. What like linguist, linguists, I believe it's called like call uh phones or like different constants, different vowels. It really depends on how you put your tongue, how you, you stretch your throat or you contract it, right? And but if we think about this, uh that in terms of like digital signal processing, we can think of the vocal tract as a filter. In other words, words, the vocal tract acts as a filter. So how do we actually generate, produce speech? Well, this is like quite fascinating and I'll give you like a, a simplification of what like the real thing is, but it's gonna be instrumental to understand sere fully. OK. So speech generation acts in a kind of like pipeline form. So initially you have what we call a glottal pulse and this is like a signal noisy signal, high pitched signal that gets generated by the vocal folds, right? And that signal passes through the vocal tracks and the vocal track acts as a filter on",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1125s",
        "start_time": "1125.3"
    },
    {
        "id": "d53e31d0",
        "text": "but if we think about this, uh that in terms of like digital signal processing, we can think of the vocal tract as a filter. In other words, words, the vocal tract acts as a filter. So how do we actually generate, produce speech? Well, this is like quite fascinating and I'll give you like a, a simplification of what like the real thing is, but it's gonna be instrumental to understand sere fully. OK. So speech generation acts in a kind of like pipeline form. So initially you have what we call a glottal pulse and this is like a signal noisy signal, high pitched signal that gets generated by the vocal folds, right? And that signal passes through the vocal tracks and the vocal track acts as a filter on the glottal pulse. And by filtering like the initial signal, it creates the speech signal. Now, the basic idea once again is that depending on how you shape your vocal tract, then you're gonna have like a different speech signal starting from the more or less the same glottal pulse. Now, the the intuition here is that the glottal pulse carries information",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1149s",
        "start_time": "1149.64"
    },
    {
        "id": "ba3fddf8",
        "text": "So speech generation acts in a kind of like pipeline form. So initially you have what we call a glottal pulse and this is like a signal noisy signal, high pitched signal that gets generated by the vocal folds, right? And that signal passes through the vocal tracks and the vocal track acts as a filter on the glottal pulse. And by filtering like the initial signal, it creates the speech signal. Now, the basic idea once again is that depending on how you shape your vocal tract, then you're gonna have like a different speech signal starting from the more or less the same glottal pulse. Now, the the intuition here is that the glottal pulse carries information about pitch or a high frequency kind of like information. Whereas like the vocal tracks or I should say like the the the frequency response provided by the vocal tracks by this filter",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1179s",
        "start_time": "1179.079"
    },
    {
        "id": "5f449f47",
        "text": "the glottal pulse. And by filtering like the initial signal, it creates the speech signal. Now, the basic idea once again is that depending on how you shape your vocal tract, then you're gonna have like a different speech signal starting from the more or less the same glottal pulse. Now, the the intuition here is that the glottal pulse carries information about pitch or a high frequency kind of like information. Whereas like the vocal tracks or I should say like the the the frequency response provided by the vocal tracks by this filter pro is gonna kind of like carry information about the, the tre of, of the sound of the speech and specifically the timbre when we talk about speech is like the actual phones that you utter that you produce, right, the different consonants or the different um uh vowels that you can produce. OK.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1205s",
        "start_time": "1205.449"
    },
    {
        "id": "d663726f",
        "text": "about pitch or a high frequency kind of like information. Whereas like the vocal tracks or I should say like the the the frequency response provided by the vocal tracks by this filter pro is gonna kind of like carry information about the, the tre of, of the sound of the speech and specifically the timbre when we talk about speech is like the actual phones that you utter that you produce, right, the different consonants or the different um uh vowels that you can produce. OK. So this is kind of like the high level idea. Now, let's take a look at a kind of visualization of all of this. So we start with a speech signal that looks like this, right? OK. So, and here we are like uh representing like this. Well, it's not really like a speech signal in the time domain is a speech is the log spectrum log amplitude spectrum of a short amount of speech. OK. And it looks like this. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1235s",
        "start_time": "1235.209"
    },
    {
        "id": "b32957df",
        "text": "pro is gonna kind of like carry information about the, the tre of, of the sound of the speech and specifically the timbre when we talk about speech is like the actual phones that you utter that you produce, right, the different consonants or the different um uh vowels that you can produce. OK. So this is kind of like the high level idea. Now, let's take a look at a kind of visualization of all of this. So we start with a speech signal that looks like this, right? OK. So, and here we are like uh representing like this. Well, it's not really like a speech signal in the time domain is a speech is the log spectrum log amplitude spectrum of a short amount of speech. OK. And it looks like this. So we can think of this like as a, as a search like as a log amplitude spectrum. But now one thing that we could do is kind of like try to smoothen the this signal here, right? And so how can we do that? Well, we can take the envelope. And what we actually do is we take the so called spectral envelope. And now we already say like a similar idea in the time domain when we,",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1248s",
        "start_time": "1248.969"
    },
    {
        "id": "002de93a",
        "text": "So this is kind of like the high level idea. Now, let's take a look at a kind of visualization of all of this. So we start with a speech signal that looks like this, right? OK. So, and here we are like uh representing like this. Well, it's not really like a speech signal in the time domain is a speech is the log spectrum log amplitude spectrum of a short amount of speech. OK. And it looks like this. So we can think of this like as a, as a search like as a log amplitude spectrum. But now one thing that we could do is kind of like try to smoothen the this signal here, right? And so how can we do that? Well, we can take the envelope. And what we actually do is we take the so called spectral envelope. And now we already say like a similar idea in the time domain when we, I discuss the amplitude envelope and I have a couple of videos on that one is like fully theoretical. So you can understand what the amplitude and so how to calculate the amplitude envelope. And then I have another video where I actually implement the amplitude envelope obviously in the time domain uh with Python from scratch.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1271s",
        "start_time": "1271.52"
    },
    {
        "id": "4e452f4a",
        "text": "we can think of this like as a, as a search like as a log amplitude spectrum. But now one thing that we could do is kind of like try to smoothen the this signal here, right? And so how can we do that? Well, we can take the envelope. And what we actually do is we take the so called spectral envelope. And now we already say like a similar idea in the time domain when we, I discuss the amplitude envelope and I have a couple of videos on that one is like fully theoretical. So you can understand what the amplitude and so how to calculate the amplitude envelope. And then I have another video where I actually implement the amplitude envelope obviously in the time domain uh with Python from scratch. But, but basically like we, we take that idea and we put it here like in the spectral domain in the frequency domain. And so here we have like the spectral envelope basically like movements like all the complexity or like the the the quickly changing like information like here like in this signal, right?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1300s",
        "start_time": "1300.849"
    },
    {
        "id": "154fc0cb",
        "text": "I discuss the amplitude envelope and I have a couple of videos on that one is like fully theoretical. So you can understand what the amplitude and so how to calculate the amplitude envelope. And then I have another video where I actually implement the amplitude envelope obviously in the time domain uh with Python from scratch. But, but basically like we, we take that idea and we put it here like in the spectral domain in the frequency domain. And so here we have like the spectral envelope basically like movements like all the complexity or like the the the quickly changing like information like here like in this signal, right? OK. Now what's cool about this? Well, it turns out that there's like something that's extremely important in how we perceive speech and sound that the spectral envelope captures. And it's these peaks in red that you see there. So those peaks are called for months.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1330s",
        "start_time": "1330.88"
    },
    {
        "id": "57c43306",
        "text": "But, but basically like we, we take that idea and we put it here like in the spectral domain in the frequency domain. And so here we have like the spectral envelope basically like movements like all the complexity or like the the the quickly changing like information like here like in this signal, right? OK. Now what's cool about this? Well, it turns out that there's like something that's extremely important in how we perceive speech and sound that the spectral envelope captures. And it's these peaks in red that you see there. So those peaks are called for months. Now, foreman are responsible for, for kind of like ID for carrying the identity of sound. So yeah, identity of sound sounds really wishy washy. So what's that? Well, that is like the timer.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1349s",
        "start_time": "1349.729"
    },
    {
        "id": "596bb15e",
        "text": "OK. Now what's cool about this? Well, it turns out that there's like something that's extremely important in how we perceive speech and sound that the spectral envelope captures. And it's these peaks in red that you see there. So those peaks are called for months. Now, foreman are responsible for, for kind of like ID for carrying the identity of sound. So yeah, identity of sound sounds really wishy washy. So what's that? Well, that is like the timer. So depending on the performance that you have in a speech signal, then you're gonna perceive certain phones instead of others. In other words, the spectral envelope provides us information about timer about the uh the different like phones that we have in speech. So this is extremely important because like this is like a feature that we want to isolate to do speech processing. OK.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1368s",
        "start_time": "1368.88"
    },
    {
        "id": "ef279ef3",
        "text": "Now, foreman are responsible for, for kind of like ID for carrying the identity of sound. So yeah, identity of sound sounds really wishy washy. So what's that? Well, that is like the timer. So depending on the performance that you have in a speech signal, then you're gonna perceive certain phones instead of others. In other words, the spectral envelope provides us information about timer about the uh the different like phones that we have in speech. So this is extremely important because like this is like a feature that we want to isolate to do speech processing. OK. OK. So the spectral envelope turns out is something like very similar to the vocal tract frequency response, right? So and this is like the the the input response like of the vocal tract depending on how we shape the vocal tract. And it's gonna give us like a signal like this",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1392s",
        "start_time": "1392.319"
    },
    {
        "id": "a1685319",
        "text": "So depending on the performance that you have in a speech signal, then you're gonna perceive certain phones instead of others. In other words, the spectral envelope provides us information about timer about the uh the different like phones that we have in speech. So this is extremely important because like this is like a feature that we want to isolate to do speech processing. OK. OK. So the spectral envelope turns out is something like very similar to the vocal tract frequency response, right? So and this is like the the the input response like of the vocal tract depending on how we shape the vocal tract. And it's gonna give us like a signal like this that resembles like this spectral envelope here, right? And depending on how you shape your vocal tract, uh you're gonna have a slightly different vocal tract frequency response with different forms, right? And that is gonna determine uh different tres, different identities of sound. OK. So this iii I hope like you're starting to understand how important like this is, right? And now",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1407s",
        "start_time": "1407.8"
    },
    {
        "id": "80504159",
        "text": "OK. So the spectral envelope turns out is something like very similar to the vocal tract frequency response, right? So and this is like the the the input response like of the vocal tract depending on how we shape the vocal tract. And it's gonna give us like a signal like this that resembles like this spectral envelope here, right? And depending on how you shape your vocal tract, uh you're gonna have a slightly different vocal tract frequency response with different forms, right? And that is gonna determine uh different tres, different identities of sound. OK. So this iii I hope like you're starting to understand how important like this is, right? And now if we think about like uh if we take like this uh initial signal, so now we have like the this modern version of the signal, right? So now we can kind of like subtract the two and what remains is something like this, right? And it's a lot like a quickly change, changing",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1438s",
        "start_time": "1438.01"
    },
    {
        "id": "ac2968a5",
        "text": "that resembles like this spectral envelope here, right? And depending on how you shape your vocal tract, uh you're gonna have a slightly different vocal tract frequency response with different forms, right? And that is gonna determine uh different tres, different identities of sound. OK. So this iii I hope like you're starting to understand how important like this is, right? And now if we think about like uh if we take like this uh initial signal, so now we have like the this modern version of the signal, right? So now we can kind of like subtract the two and what remains is something like this, right? And it's a lot like a quickly change, changing information here and we can call this like the spectral detail. And the cool thing is that the spectral detail maps really nicely into the glut of pulse. Ok.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1460s",
        "start_time": "1460.75"
    },
    {
        "id": "64709d42",
        "text": "if we think about like uh if we take like this uh initial signal, so now we have like the this modern version of the signal, right? So now we can kind of like subtract the two and what remains is something like this, right? And it's a lot like a quickly change, changing information here and we can call this like the spectral detail. And the cool thing is that the spectral detail maps really nicely into the glut of pulse. Ok. Wow, that, that, that, that this is like really fascinating stuff. So we have like an initial speech signal and so we can decompose that like into two parts. So one carries information about four months and slowly changing spectral features. And that is like the vocal tract frequency response. And it's basically like the filter that we have with our vocal tract or the response of that filter.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1489s",
        "start_time": "1489.069"
    },
    {
        "id": "63e525ec",
        "text": "information here and we can call this like the spectral detail. And the cool thing is that the spectral detail maps really nicely into the glut of pulse. Ok. Wow, that, that, that, that this is like really fascinating stuff. So we have like an initial speech signal and so we can decompose that like into two parts. So one carries information about four months and slowly changing spectral features. And that is like the vocal tract frequency response. And it's basically like the filter that we have with our vocal tract or the response of that filter. And then the remaining part of the signal is the initial blot of pulse. OK. So the carrier of pitch information and yeah. OK. So now what should we do? So let's move on and try to formalize a speech. Um And, and we can say that speech",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1509s",
        "start_time": "1509.219"
    },
    {
        "id": "d9616f53",
        "text": "Wow, that, that, that, that this is like really fascinating stuff. So we have like an initial speech signal and so we can decompose that like into two parts. So one carries information about four months and slowly changing spectral features. And that is like the vocal tract frequency response. And it's basically like the filter that we have with our vocal tract or the response of that filter. And then the remaining part of the signal is the initial blot of pulse. OK. So the carrier of pitch information and yeah. OK. So now what should we do? So let's move on and try to formalize a speech. Um And, and we can say that speech uh can be interpreted as a convolution of the vocal tracts frequency response with the glottal pulse. So this is kind of like",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1524s",
        "start_time": "1524.369"
    },
    {
        "id": "d46f4d15",
        "text": "And then the remaining part of the signal is the initial blot of pulse. OK. So the carrier of pitch information and yeah. OK. So now what should we do? So let's move on and try to formalize a speech. Um And, and we can say that speech uh can be interpreted as a convolution of the vocal tracts frequency response with the glottal pulse. So this is kind of like formalization of what we just said in a qualitative way like up until now. So now let's take a look at the math behind this because like this is very important to understanding why. For example, we take like the logarithm, right. OK. So let's start. So XFT is our speech signal. OK.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1548s",
        "start_time": "1548.339"
    },
    {
        "id": "4d8e44a5",
        "text": "uh can be interpreted as a convolution of the vocal tracts frequency response with the glottal pulse. So this is kind of like formalization of what we just said in a qualitative way like up until now. So now let's take a look at the math behind this because like this is very important to understanding why. For example, we take like the logarithm, right. OK. So let's start. So XFT is our speech signal. OK. And here we have like this convolution. So we are uh convolving the uh vocal tracks of frequency response with the glottal pulse. And here we are in the time domain. So these, these are all like waveforms. OK. So now if we move to the frequency domain and we do this obviously by applying a fourier transform,",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1571s",
        "start_time": "1571.069"
    },
    {
        "id": "5b5102f3",
        "text": "formalization of what we just said in a qualitative way like up until now. So now let's take a look at the math behind this because like this is very important to understanding why. For example, we take like the logarithm, right. OK. So let's start. So XFT is our speech signal. OK. And here we have like this convolution. So we are uh convolving the uh vocal tracks of frequency response with the glottal pulse. And here we are in the time domain. So these, these are all like waveforms. OK. So now if we move to the frequency domain and we do this obviously by applying a fourier transform, uh so what we know is that the, the, the spectrum, the speech or I should say the spectrum associated to the speech signal is equal to the multiplication of these two spectra. The one that comes out of the glottal pulse and the other one that comes out from the vocal tract frequency response. OK. So let's put this one up there. What we can do is now",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1583s",
        "start_time": "1583.319"
    },
    {
        "id": "a78a9361",
        "text": "And here we have like this convolution. So we are uh convolving the uh vocal tracks of frequency response with the glottal pulse. And here we are in the time domain. So these, these are all like waveforms. OK. So now if we move to the frequency domain and we do this obviously by applying a fourier transform, uh so what we know is that the, the, the spectrum, the speech or I should say the spectrum associated to the speech signal is equal to the multiplication of these two spectra. The one that comes out of the glottal pulse and the other one that comes out from the vocal tract frequency response. OK. So let's put this one up there. What we can do is now take the logarithm and so we'll apply the logarithm to both sides of this equation. OK? Like this. Now we can use the properties of the logarithm and rewrite this formula like this",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1605s",
        "start_time": "1605.489"
    },
    {
        "id": "6695a686",
        "text": "uh so what we know is that the, the, the spectrum, the speech or I should say the spectrum associated to the speech signal is equal to the multiplication of these two spectra. The one that comes out of the glottal pulse and the other one that comes out from the vocal tract frequency response. OK. So let's put this one up there. What we can do is now take the logarithm and so we'll apply the logarithm to both sides of this equation. OK? Like this. Now we can use the properties of the logarithm and rewrite this formula like this good. So, and what's the advantage of this? Well, the great advantage now is that we can treat the vocal tract frequency response as separate from the glottal pulse, right? There are two separate elements that we are just adding up. So using the logarithm has the advantage of treating like these two elements as separate. So we can just like add them up and then we get the speech or we can just like focus on one of these two things alone. OK?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1631s",
        "start_time": "1631.02"
    },
    {
        "id": "febea675",
        "text": "take the logarithm and so we'll apply the logarithm to both sides of this equation. OK? Like this. Now we can use the properties of the logarithm and rewrite this formula like this good. So, and what's the advantage of this? Well, the great advantage now is that we can treat the vocal tract frequency response as separate from the glottal pulse, right? There are two separate elements that we are just adding up. So using the logarithm has the advantage of treating like these two elements as separate. So we can just like add them up and then we get the speech or we can just like focus on one of these two things alone. OK? I hope you're getting now why we use like the log amplitude spectrum and not just the, the, the normal like spectrum uh when we, we get the, when we calculate the subs, OK. So now we should um yeah, now, yeah, let's let's take a look like at this different elements. So now let's try to map them to the different elements of speech that we talked about. So again,",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1659s",
        "start_time": "1659.55"
    },
    {
        "id": "c4d883b9",
        "text": "good. So, and what's the advantage of this? Well, the great advantage now is that we can treat the vocal tract frequency response as separate from the glottal pulse, right? There are two separate elements that we are just adding up. So using the logarithm has the advantage of treating like these two elements as separate. So we can just like add them up and then we get the speech or we can just like focus on one of these two things alone. OK? I hope you're getting now why we use like the log amplitude spectrum and not just the, the, the normal like spectrum uh when we, we get the, when we calculate the subs, OK. So now we should um yeah, now, yeah, let's let's take a look like at this different elements. So now let's try to map them to the different elements of speech that we talked about. So again, so this is like the uh this first element here is just like the, the speech or we, we should say it's the log spectrum of the the speech signal. Then we have here like in orange, the vocal tract frequency response here and here we have the glottal pulse. And so these are like all the different elements. Now, what we should ask is",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1675s",
        "start_time": "1675.339"
    },
    {
        "id": "7fea3942",
        "text": "I hope you're getting now why we use like the log amplitude spectrum and not just the, the, the normal like spectrum uh when we, we get the, when we calculate the subs, OK. So now we should um yeah, now, yeah, let's let's take a look like at this different elements. So now let's try to map them to the different elements of speech that we talked about. So again, so this is like the uh this first element here is just like the, the speech or we, we should say it's the log spectrum of the the speech signal. Then we have here like in orange, the vocal tract frequency response here and here we have the glottal pulse. And so these are like all the different elements. Now, what we should ask is uh what's the goal like a speech processing? So what should we",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1705s",
        "start_time": "1705.369"
    },
    {
        "id": "326c6b55",
        "text": "so this is like the uh this first element here is just like the, the speech or we, we should say it's the log spectrum of the the speech signal. Then we have here like in orange, the vocal tract frequency response here and here we have the glottal pulse. And so these are like all the different elements. Now, what we should ask is uh what's the goal like a speech processing? So what should we should we do? So what, what, what should we achieve to get like from a speech signal? And the point is really is that like when we",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1732s",
        "start_time": "1732.38"
    },
    {
        "id": "7ce08526",
        "text": "uh what's the goal like a speech processing? So what should we should we do? So what, what, what should we achieve to get like from a speech signal? And the point is really is that like when we work with speech signal, we really don't have the luxury of having the vocal tract frequency response separated from the glottal pulse, right? Because we just get the speech signal and the speech signal is this massive signal like this, right?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1759s",
        "start_time": "1759.64"
    },
    {
        "id": "4b052fce",
        "text": "should we do? So what, what, what should we achieve to get like from a speech signal? And the point is really is that like when we work with speech signal, we really don't have the luxury of having the vocal tract frequency response separated from the glottal pulse, right? Because we just get the speech signal and the speech signal is this massive signal like this, right? And so the ultimate goal, so the one thing that we want to achieve is to separate the initial speech uh signal into two components. The one that's connected with the vocal tract frequency response and the other one that's just connected with the uh glottal pulse, right? OK. But are we really interested in the glottal pulse?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1765s",
        "start_time": "1765.239"
    },
    {
        "id": "ead1ce07",
        "text": "work with speech signal, we really don't have the luxury of having the vocal tract frequency response separated from the glottal pulse, right? Because we just get the speech signal and the speech signal is this massive signal like this, right? And so the ultimate goal, so the one thing that we want to achieve is to separate the initial speech uh signal into two components. The one that's connected with the vocal tract frequency response and the other one that's just connected with the uh glottal pulse, right? OK. But are we really interested in the glottal pulse? Well, really not that much in terms of uh audio, well, speech processing. And that's because yeah, pitch is important but not really that important. What we really care about is the identity of sound. So it's the Forys is the timer is the formance, right? And the formance and all of this stuff is carried by this component of the speech signal. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1773s",
        "start_time": "1773.91"
    },
    {
        "id": "a47c7ec7",
        "text": "And so the ultimate goal, so the one thing that we want to achieve is to separate the initial speech uh signal into two components. The one that's connected with the vocal tract frequency response and the other one that's just connected with the uh glottal pulse, right? OK. But are we really interested in the glottal pulse? Well, really not that much in terms of uh audio, well, speech processing. And that's because yeah, pitch is important but not really that important. What we really care about is the identity of sound. So it's the Forys is the timer is the formance, right? And the formance and all of this stuff is carried by this component of the speech signal. So what we want to get at is a set of features that enables us to work only with this part of the speech so that we can just like throw out the glottal port because we don't need that for audio process for speech processing or speech recognition, right?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1788s",
        "start_time": "1788.31"
    },
    {
        "id": "18c08338",
        "text": "Well, really not that much in terms of uh audio, well, speech processing. And that's because yeah, pitch is important but not really that important. What we really care about is the identity of sound. So it's the Forys is the timer is the formance, right? And the formance and all of this stuff is carried by this component of the speech signal. So what we want to get at is a set of features that enables us to work only with this part of the speech so that we can just like throw out the glottal port because we don't need that for audio process for speech processing or speech recognition, right? OK. So we should find a um process through which we can start from a speech signal like this or log spectrum speech like this and then move and isolate the vocal tract frequency response component. How can we achieve that? Well, septum comes to rescue",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1815s",
        "start_time": "1815.89"
    },
    {
        "id": "3ff0cb37",
        "text": "what we want to get at is a set of features that enables us to work only with this part of the speech so that we can just like throw out the glottal port because we don't need that for audio process for speech processing or speech recognition, right? OK. So we should find a um process through which we can start from a speech signal like this or log spectrum speech like this and then move and isolate the vocal tract frequency response component. How can we achieve that? Well, septum comes to rescue here guys, we have the visualization for a three log spectra. So uh up here you have the log spectral relative to uh speech and then down here you have like the two different components. OK. So now if we want to take the the ses stream, what we should do is apply the inverse discrete fourier transform to this speech spectral signal. So if we do, so we move from the frequency domain to the quefrency domain. But if we want to like",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1843s",
        "start_time": "1843.16"
    },
    {
        "id": "0b123dd5",
        "text": "OK. So we should find a um process through which we can start from a speech signal like this or log spectrum speech like this and then move and isolate the vocal tract frequency response component. How can we achieve that? Well, septum comes to rescue here guys, we have the visualization for a three log spectra. So uh up here you have the log spectral relative to uh speech and then down here you have like the two different components. OK. So now if we want to take the the ses stream, what we should do is apply the inverse discrete fourier transform to this speech spectral signal. So if we do, so we move from the frequency domain to the quefrency domain. But if we want to like see like the details of how to do that. Well, basically what we do is we take like sine waves uh with different frequencies and we try to feed them onto the spectral signal up here. And basically what we are, what we want to do is try to decompose uh that signal into its quefrency components and see how present the different QF components are. OK. So we start with low frequency sine waves. And",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1863s",
        "start_time": "1863.89"
    },
    {
        "id": "449bb055",
        "text": "here guys, we have the visualization for a three log spectra. So uh up here you have the log spectral relative to uh speech and then down here you have like the two different components. OK. So now if we want to take the the ses stream, what we should do is apply the inverse discrete fourier transform to this speech spectral signal. So if we do, so we move from the frequency domain to the quefrency domain. But if we want to like see like the details of how to do that. Well, basically what we do is we take like sine waves uh with different frequencies and we try to feed them onto the spectral signal up here. And basically what we are, what we want to do is try to decompose uh that signal into its quefrency components and see how present the different QF components are. OK. So we start with low frequency sine waves. And uh if you like for example, like take a look at this speech spectral signal here, right? You see and that you we have like four peaks here. And it's easier to see down here in the spectral relative spectral envelope. So you have a peak 1234. So probably a sine wave that has a frequency of four hands is gonna do a pretty good job at approximating this uh spectral signal. And so",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1885s",
        "start_time": "1885.91"
    },
    {
        "id": "c3b8fe43",
        "text": "see like the details of how to do that. Well, basically what we do is we take like sine waves uh with different frequencies and we try to feed them onto the spectral signal up here. And basically what we are, what we want to do is try to decompose uh that signal into its quefrency components and see how present the different QF components are. OK. So we start with low frequency sine waves. And uh if you like for example, like take a look at this speech spectral signal here, right? You see and that you we have like four peaks here. And it's easier to see down here in the spectral relative spectral envelope. So you have a peak 1234. So probably a sine wave that has a frequency of four hands is gonna do a pretty good job at approximating this uh spectral signal. And so what that means is that when we move to the quefrency domain, we're gonna get a high value with respect to the frequency that is at four Hertz right now. The cool thing here is that the um like all the Lowrey values are gonna represent the slowly changing spectral information",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1915s",
        "start_time": "1915.555"
    },
    {
        "id": "0526197c",
        "text": "uh if you like for example, like take a look at this speech spectral signal here, right? You see and that you we have like four peaks here. And it's easier to see down here in the spectral relative spectral envelope. So you have a peak 1234. So probably a sine wave that has a frequency of four hands is gonna do a pretty good job at approximating this uh spectral signal. And so what that means is that when we move to the quefrency domain, we're gonna get a high value with respect to the frequency that is at four Hertz right now. The cool thing here is that the um like all the Lowrey values are gonna represent the slowly changing spectral information in the the speech spectral signal here. In other words, like here in the low end of the frequency axis, we're gonna get all the values that are relative and all the information that's relative to the spectral envelope. So it's gonna carry information about foreman, the relative phones and timbre. Now the moment we go up, we increase the, the Hertz and we get up like uh here like on the prey",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1945s",
        "start_time": "1945.349"
    },
    {
        "id": "b39588f1",
        "text": "what that means is that when we move to the quefrency domain, we're gonna get a high value with respect to the frequency that is at four Hertz right now. The cool thing here is that the um like all the Lowrey values are gonna represent the slowly changing spectral information in the the speech spectral signal here. In other words, like here in the low end of the frequency axis, we're gonna get all the values that are relative and all the information that's relative to the spectral envelope. So it's gonna carry information about foreman, the relative phones and timbre. Now the moment we go up, we increase the, the Hertz and we get up like uh here like on the prey uh axis. What's gonna happen is that we are gonna start to approximate the the spectral detail. So the fast uh changing information on the speech spectral signal.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1972s",
        "start_time": "1972.086"
    },
    {
        "id": "b9e6718c",
        "text": "in the the speech spectral signal here. In other words, like here in the low end of the frequency axis, we're gonna get all the values that are relative and all the information that's relative to the spectral envelope. So it's gonna carry information about foreman, the relative phones and timbre. Now the moment we go up, we increase the, the Hertz and we get up like uh here like on the prey uh axis. What's gonna happen is that we are gonna start to approximate the the spectral detail. So the fast uh changing information on the speech spectral signal. So here, for example, we could say that a sine wave at 100 Hertz perhaps is going to do a good job at approximating this spectral details here that are obviously part of this speech spectral signal up here. And so in other words, the great thing of moving from the frequency domain from the log spectrum to the qui",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=1998s",
        "start_time": "1998.822"
    },
    {
        "id": "4df68b88",
        "text": "uh axis. What's gonna happen is that we are gonna start to approximate the the spectral detail. So the fast uh changing information on the speech spectral signal. So here, for example, we could say that a sine wave at 100 Hertz perhaps is going to do a good job at approximating this spectral details here that are obviously part of this speech spectral signal up here. And so in other words, the great thing of moving from the frequency domain from the log spectrum to the qui domain. In other words, the subs stream is that we're gonna have a natural physical separation of the information that's relative to the spectral envelope. Or in other words, the uh vocal trapped um frequency response and the information that's connected to the spectral details or glottal pulse. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2025s",
        "start_time": "2025.729"
    },
    {
        "id": "86cd2a12",
        "text": "So here, for example, we could say that a sine wave at 100 Hertz perhaps is going to do a good job at approximating this spectral details here that are obviously part of this speech spectral signal up here. And so in other words, the great thing of moving from the frequency domain from the log spectrum to the qui domain. In other words, the subs stream is that we're gonna have a natural physical separation of the information that's relative to the spectral envelope. Or in other words, the uh vocal trapped um frequency response and the information that's connected to the spectral details or glottal pulse. So on the frequency uh axis, all the information relative to the spectral envelope is in the low and and the information relative to the spectral details is in the higher part of the frequency axis, we can capture all of this information through the mathematical formalization. And here you can see that the SES which is capsule X of T is given by the sum of two components. So all the SES coefficients that are relative to the glottal pools",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2039s",
        "start_time": "2039.64"
    },
    {
        "id": "65a29132",
        "text": "domain. In other words, the subs stream is that we're gonna have a natural physical separation of the information that's relative to the spectral envelope. Or in other words, the uh vocal trapped um frequency response and the information that's connected to the spectral details or glottal pulse. So on the frequency uh axis, all the information relative to the spectral envelope is in the low and and the information relative to the spectral details is in the higher part of the frequency axis, we can capture all of this information through the mathematical formalization. And here you can see that the SES which is capsule X of T is given by the sum of two components. So all the SES coefficients that are relative to the glottal pools uh added to all the ses coefficients that are connected to the spectral envelope. Now, if you remember our goal and the reason why we moved to the spectrum and to the SES is because we want to just focus on the features relative to the spectral envelope. So how do we do that? Well, here comes the last weird words that we introduced",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2061s",
        "start_time": "2061.353"
    },
    {
        "id": "8416027d",
        "text": "on the frequency uh axis, all the information relative to the spectral envelope is in the low and and the information relative to the spectral details is in the higher part of the frequency axis, we can capture all of this information through the mathematical formalization. And here you can see that the SES which is capsule X of T is given by the sum of two components. So all the SES coefficients that are relative to the glottal pools uh added to all the ses coefficients that are connected to the spectral envelope. Now, if you remember our goal and the reason why we moved to the spectrum and to the SES is because we want to just focus on the features relative to the spectral envelope. So how do we do that? Well, here comes the last weird words that we introduced earlier. In other words, the lifting or a lifter, what we want to use here is a low pass lifter, which is basically a nice way of saying that we want a low pass filter that's just gonna remove all the values are related to the high uh equivalences. OK. And so once we do that, we remain only",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2083s",
        "start_time": "2083.064"
    },
    {
        "id": "1e0511fc",
        "text": "uh added to all the ses coefficients that are connected to the spectral envelope. Now, if you remember our goal and the reason why we moved to the spectrum and to the SES is because we want to just focus on the features relative to the spectral envelope. So how do we do that? Well, here comes the last weird words that we introduced earlier. In other words, the lifting or a lifter, what we want to use here is a low pass lifter, which is basically a nice way of saying that we want a low pass filter that's just gonna remove all the values are related to the high uh equivalences. OK. And so once we do that, we remain only if the SES coefficients connected to the spectral envelope, which is the stuff that we wanted. Now that we know about the subs we can move on and understand what male frequency subs coefficients are. The cool thing is that MFCC is built on top of sere. So that is gonna be a piece of cake for us. The best way we can understand how MFC work is by looking at how we can compute them.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2112s",
        "start_time": "2112.969"
    },
    {
        "id": "3a8fadfd",
        "text": "earlier. In other words, the lifting or a lifter, what we want to use here is a low pass lifter, which is basically a nice way of saying that we want a low pass filter that's just gonna remove all the values are related to the high uh equivalences. OK. And so once we do that, we remain only if the SES coefficients connected to the spectral envelope, which is the stuff that we wanted. Now that we know about the subs we can move on and understand what male frequency subs coefficients are. The cool thing is that MFCC is built on top of sere. So that is gonna be a piece of cake for us. The best way we can understand how MFC work is by looking at how we can compute them. And this is a multi step process. Many of the steps are shared by how we compute SERE and MFCC. So let's get started. We begin with a simple waveform. So signal in the time domain, as usual we apply the full transform and we get a spectrum out of that. Next step is to apply a logarithm to the amplitude so that we get a log spectrum.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2138s",
        "start_time": "2138.939"
    },
    {
        "id": "cf516287",
        "text": "if the SES coefficients connected to the spectral envelope, which is the stuff that we wanted. Now that we know about the subs we can move on and understand what male frequency subs coefficients are. The cool thing is that MFCC is built on top of sere. So that is gonna be a piece of cake for us. The best way we can understand how MFC work is by looking at how we can compute them. And this is a multi step process. Many of the steps are shared by how we compute SERE and MFCC. So let's get started. We begin with a simple waveform. So signal in the time domain, as usual we apply the full transform and we get a spectrum out of that. Next step is to apply a logarithm to the amplitude so that we get a log spectrum. And up until this point, the process for getting subs stream and male frequency subst coefficients is actually the same right. But here we have the first divergence. So what we do next is applying mel scaling. What this means is that we take the log spectrum and we apply the mall filter banks which are these triangular filters like this, right? So if you followed along",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2164s",
        "start_time": "2164.909"
    },
    {
        "id": "48ee3631",
        "text": "And this is a multi step process. Many of the steps are shared by how we compute SERE and MFCC. So let's get started. We begin with a simple waveform. So signal in the time domain, as usual we apply the full transform and we get a spectrum out of that. Next step is to apply a logarithm to the amplitude so that we get a log spectrum. And up until this point, the process for getting subs stream and male frequency subst coefficients is actually the same right. But here we have the first divergence. So what we do next is applying mel scaling. What this means is that we take the log spectrum and we apply the mall filter banks which are these triangular filters like this, right? So if you followed along my series, you should be familiar with this image because I've used it in the previous couple of videos when we were talking about male spectrums. Now, if you're not familiar about with like male spectrograms or male scale, I highly suggest you once again to go check out my previous videos. But",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2191s",
        "start_time": "2191.07"
    },
    {
        "id": "26d35bf2",
        "text": "And up until this point, the process for getting subs stream and male frequency subst coefficients is actually the same right. But here we have the first divergence. So what we do next is applying mel scaling. What this means is that we take the log spectrum and we apply the mall filter banks which are these triangular filters like this, right? So if you followed along my series, you should be familiar with this image because I've used it in the previous couple of videos when we were talking about male spectrums. Now, if you're not familiar about with like male spectrograms or male scale, I highly suggest you once again to go check out my previous videos. But at the end of the this step, we have now a male spectrum, we now enter the final step for getting MF CCS which is instead of applying the equivalent of an inverse uh discrete fourier transform for the SES. And in this case, that one transformation is the discrete cosine transform.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2216s",
        "start_time": "2216.899"
    },
    {
        "id": "44f87231",
        "text": "my series, you should be familiar with this image because I've used it in the previous couple of videos when we were talking about male spectrums. Now, if you're not familiar about with like male spectrograms or male scale, I highly suggest you once again to go check out my previous videos. But at the end of the this step, we have now a male spectrum, we now enter the final step for getting MF CCS which is instead of applying the equivalent of an inverse uh discrete fourier transform for the SES. And in this case, that one transformation is the discrete cosine transform. Now, I'm not gonna get into the details of why we're using the discrete cosine transform instead of the inverse fourier transform. I'll do that like in a few moments. But for now, all you need to understand is that once we apply the discrete cosine cosine transform is that we get a number of coefficients, a number of values or MF CCS which are the ones that we are interested in. OK.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2243s",
        "start_time": "2243.475"
    },
    {
        "id": "8c74382e",
        "text": "at the end of the this step, we have now a male spectrum, we now enter the final step for getting MF CCS which is instead of applying the equivalent of an inverse uh discrete fourier transform for the SES. And in this case, that one transformation is the discrete cosine transform. Now, I'm not gonna get into the details of why we're using the discrete cosine transform instead of the inverse fourier transform. I'll do that like in a few moments. But for now, all you need to understand is that once we apply the discrete cosine cosine transform is that we get a number of coefficients, a number of values or MF CCS which are the ones that we are interested in. OK. So one thing I want to draw your attention to is the type of transformations or the type of like steps that we are uh using like in this multi step process for getting MF CCS. And the cool thing about",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2262s",
        "start_time": "2262.489"
    },
    {
        "id": "32c038bd",
        "text": "Now, I'm not gonna get into the details of why we're using the discrete cosine transform instead of the inverse fourier transform. I'll do that like in a few moments. But for now, all you need to understand is that once we apply the discrete cosine cosine transform is that we get a number of coefficients, a number of values or MF CCS which are the ones that we are interested in. OK. So one thing I want to draw your attention to is the type of transformations or the type of like steps that we are uh using like in this multi step process for getting MF CCS. And the cool thing about is that at each step, we have a process that's somewhat perceptually informed, it's perceptually relevant. So let me explain what I mean by that. So we start with the signal away from, OK. At that point, we get the, we apply a group for transform so that we can move to the time domain all good. And well, at this point, we apply a logarithm",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2286s",
        "start_time": "2286.1"
    },
    {
        "id": "26fed299",
        "text": "So one thing I want to draw your attention to is the type of transformations or the type of like steps that we are uh using like in this multi step process for getting MF CCS. And the cool thing about is that at each step, we have a process that's somewhat perceptually informed, it's perceptually relevant. So let me explain what I mean by that. So we start with the signal away from, OK. At that point, we get the, we apply a group for transform so that we can move to the time domain all good. And well, at this point, we apply a logarithm on the amplitude. And this is something that's perceptually relevant. And that's because you may be familiar with this because like you, you've seen it like in earlier videos that I had on this on this series. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2309s",
        "start_time": "2309.939"
    },
    {
        "id": "71796896",
        "text": "is that at each step, we have a process that's somewhat perceptually informed, it's perceptually relevant. So let me explain what I mean by that. So we start with the signal away from, OK. At that point, we get the, we apply a group for transform so that we can move to the time domain all good. And well, at this point, we apply a logarithm on the amplitude. And this is something that's perceptually relevant. And that's because you may be familiar with this because like you, you've seen it like in earlier videos that I had on this on this series. So we don't perceive amplitude or loudness linearly but rather logarithmically. So by applying a logarithm at this point, we're putting like a step that's like perceptually relevant. The next step is similar to that, right? Because when",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2325s",
        "start_time": "2325.645"
    },
    {
        "id": "f106d5b0",
        "text": "on the amplitude. And this is something that's perceptually relevant. And that's because you may be familiar with this because like you, you've seen it like in earlier videos that I had on this on this series. So we don't perceive amplitude or loudness linearly but rather logarithmically. So by applying a logarithm at this point, we're putting like a step that's like perceptually relevant. The next step is similar to that, right? Because when we apply male scaling, we are basically passing from a linear frequency representation to a male based uh representation which is perceptually relevant in the realm of frequencies, right.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2351s",
        "start_time": "2351.639"
    },
    {
        "id": "5ef19568",
        "text": "we don't perceive amplitude or loudness linearly but rather logarithmically. So by applying a logarithm at this point, we're putting like a step that's like perceptually relevant. The next step is similar to that, right? Because when we apply male scaling, we are basically passing from a linear frequency representation to a male based uh representation which is perceptually relevant in the realm of frequencies, right. And finally, when we apply the discrete cosine transform, that's kind of similar to applying the inverse fourier transform uh to get the subs stream, what we get out of that is information about the uh different values that kind of uh",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2366s",
        "start_time": "2366.649"
    },
    {
        "id": "a3c9c0dd",
        "text": "we apply male scaling, we are basically passing from a linear frequency representation to a male based uh representation which is perceptually relevant in the realm of frequencies, right. And finally, when we apply the discrete cosine transform, that's kind of similar to applying the inverse fourier transform uh to get the subs stream, what we get out of that is information about the uh different values that kind of uh construct like form the the different formats or the timer or like the basic information about the spectrum that we need in order to understand uh like speech, understand full names and just like recognize speech. Really. The question we should now ask is",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2386s",
        "start_time": "2386.129"
    },
    {
        "id": "60bc90e4",
        "text": "And finally, when we apply the discrete cosine transform, that's kind of similar to applying the inverse fourier transform uh to get the subs stream, what we get out of that is information about the uh different values that kind of uh construct like form the the different formats or the timer or like the basic information about the spectrum that we need in order to understand uh like speech, understand full names and just like recognize speech. Really. The question we should now ask is why using the discrete cosine transform, can we just use the inverse fourier transform? Well, it turns out there are a bunch of reasons why we prefer to use a discrete cosine transform for getting MF CS. So the first one is that a discrete cosine transform is a simplified version of a fourier transform. And one of the reasons is that",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2399s",
        "start_time": "2399.739"
    },
    {
        "id": "92391d41",
        "text": "construct like form the the different formats or the timer or like the basic information about the spectrum that we need in order to understand uh like speech, understand full names and just like recognize speech. Really. The question we should now ask is why using the discrete cosine transform, can we just use the inverse fourier transform? Well, it turns out there are a bunch of reasons why we prefer to use a discrete cosine transform for getting MF CS. So the first one is that a discrete cosine transform is a simplified version of a fourier transform. And one of the reasons is that the discrete cosine transform gives us back real valued coefficients. And this is different from what a fourier transform does. So if you're not familiar with the fourier transform, I highly suggest you to go check out this video there. You'll find that a fourier transform returns complex numbers, but we don't really need complex coefficients here. Real value coefficients are more than enough for our purposes with MFTC. So discrete cost and transform is way",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2417s",
        "start_time": "2417.75"
    },
    {
        "id": "83710d46",
        "text": "why using the discrete cosine transform, can we just use the inverse fourier transform? Well, it turns out there are a bunch of reasons why we prefer to use a discrete cosine transform for getting MF CS. So the first one is that a discrete cosine transform is a simplified version of a fourier transform. And one of the reasons is that the discrete cosine transform gives us back real valued coefficients. And this is different from what a fourier transform does. So if you're not familiar with the fourier transform, I highly suggest you to go check out this video there. You'll find that a fourier transform returns complex numbers, but we don't really need complex coefficients here. Real value coefficients are more than enough for our purposes with MFTC. So discrete cost and transform is way simpler to handle with than a fourier transform. OK. So now one thing that I want to show you guys is how we move, how we can apply this group cosine transform and move from the logarithm spectrum to the NF CCS. And basically like here, the idea is that we get like cosines like with different free",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2435s",
        "start_time": "2435.935"
    },
    {
        "id": "68b5bcb4",
        "text": "the discrete cosine transform gives us back real valued coefficients. And this is different from what a fourier transform does. So if you're not familiar with the fourier transform, I highly suggest you to go check out this video there. You'll find that a fourier transform returns complex numbers, but we don't really need complex coefficients here. Real value coefficients are more than enough for our purposes with MFTC. So discrete cost and transform is way simpler to handle with than a fourier transform. OK. So now one thing that I want to show you guys is how we move, how we can apply this group cosine transform and move from the logarithm spectrum to the NF CCS. And basically like here, the idea is that we get like cosines like with different free frequencies and we try to fit them to uh the the lock spectrum, right? And uh each cosine is gonna have like a different frequency and it's gonna basically come up with a value that value is how well like that cosine with that specific frequency",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2456s",
        "start_time": "2456.409"
    },
    {
        "id": "7bc7d302",
        "text": "simpler to handle with than a fourier transform. OK. So now one thing that I want to show you guys is how we move, how we can apply this group cosine transform and move from the logarithm spectrum to the NF CCS. And basically like here, the idea is that we get like cosines like with different free frequencies and we try to fit them to uh the the lock spectrum, right? And uh each cosine is gonna have like a different frequency and it's gonna basically come up with a value that value is how well like that cosine with that specific frequency um fits the original log spectrum. And that value is an MFC, the higher like the index of the MFC and the higher like the, the signal, the cosine signal that we pass that we try to fit to the log spectrum. OK.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2485s",
        "start_time": "2485.35"
    },
    {
        "id": "f4281d8c",
        "text": "frequencies and we try to fit them to uh the the lock spectrum, right? And uh each cosine is gonna have like a different frequency and it's gonna basically come up with a value that value is how well like that cosine with that specific frequency um fits the original log spectrum. And that value is an MFC, the higher like the index of the MFC and the higher like the, the signal, the cosine signal that we pass that we try to fit to the log spectrum. OK. Good. So now I hope like you have like this idea of how to apply like this district cosine transform. Now, moving on another advantage of uh the district cosine transform is that it enables us to the correlate energy in different male bands. OK. So what's this all about? OK. And here we have once again the male filter bands and these are like triangular filters. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2508s",
        "start_time": "2508.889"
    },
    {
        "id": "cbac4ca3",
        "text": "um fits the original log spectrum. And that value is an MFC, the higher like the index of the MFC and the higher like the, the signal, the cosine signal that we pass that we try to fit to the log spectrum. OK. Good. So now I hope like you have like this idea of how to apply like this district cosine transform. Now, moving on another advantage of uh the district cosine transform is that it enables us to the correlate energy in different male bands. OK. So what's this all about? OK. And here we have once again the male filter bands and these are like triangular filters. So you can see like the center like of a male bench, for example, like this one here, right, which is we can say like this is mel bench number two is somewhat correlated with what comes after it, the subsequent mail bench and the previous mail bench, right? And you can see it here like there's some overlap. And that means that information is somewhat like correlation",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2528s",
        "start_time": "2528.35"
    },
    {
        "id": "888f1075",
        "text": "Good. So now I hope like you have like this idea of how to apply like this district cosine transform. Now, moving on another advantage of uh the district cosine transform is that it enables us to the correlate energy in different male bands. OK. So what's this all about? OK. And here we have once again the male filter bands and these are like triangular filters. So you can see like the center like of a male bench, for example, like this one here, right, which is we can say like this is mel bench number two is somewhat correlated with what comes after it, the subsequent mail bench and the previous mail bench, right? And you can see it here like there's some overlap. And that means that information is somewhat like correlation it is shared across multiple um male bands. Now, when we apply the discrete cosine transform, what we do is we correlate the energy in the different male bands, which is a really good thing to have because with machine learning algorithms, we want uh features that are as least correlated as possible. OK.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2549s",
        "start_time": "2549.33"
    },
    {
        "id": "521f5c60",
        "text": "you can see like the center like of a male bench, for example, like this one here, right, which is we can say like this is mel bench number two is somewhat correlated with what comes after it, the subsequent mail bench and the previous mail bench, right? And you can see it here like there's some overlap. And that means that information is somewhat like correlation it is shared across multiple um male bands. Now, when we apply the discrete cosine transform, what we do is we correlate the energy in the different male bands, which is a really good thing to have because with machine learning algorithms, we want uh features that are as least correlated as possible. OK. So one final thing that comes with the discrete cosine transform is that it reduces the number of dimensions that we use to represent the log spectrum. In other words, we can think of the discrete cosine transform as a dimension",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2575s",
        "start_time": "2575.889"
    },
    {
        "id": "3067b017",
        "text": "it is shared across multiple um male bands. Now, when we apply the discrete cosine transform, what we do is we correlate the energy in the different male bands, which is a really good thing to have because with machine learning algorithms, we want uh features that are as least correlated as possible. OK. So one final thing that comes with the discrete cosine transform is that it reduces the number of dimensions that we use to represent the log spectrum. In other words, we can think of the discrete cosine transform as a dimension reduction algorithm that takes like the input which is this log spectrum and it provides us back with um like a a feature or set of features like that is that has like a smaller dimensional less dimensions. OK. Good. So now I guess like one important question that you may have is how many coefficients should that take? How many MF CCS?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2599s",
        "start_time": "2599.54"
    },
    {
        "id": "4ed527c6",
        "text": "So one final thing that comes with the discrete cosine transform is that it reduces the number of dimensions that we use to represent the log spectrum. In other words, we can think of the discrete cosine transform as a dimension reduction algorithm that takes like the input which is this log spectrum and it provides us back with um like a a feature or set of features like that is that has like a smaller dimensional less dimensions. OK. Good. So now I guess like one important question that you may have is how many coefficients should that take? How many MF CCS? Now traditionally, we focus on the first, we consider the 1st 12 to 13 coefficients. Why do we take this? Right? We take the first coefficients because these are the ones that keep the most relevant information, which is the information about performance and spectral envelope. This is like the same stuff that we had with the SEPS. If you recall on the qui",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2624s",
        "start_time": "2624.6"
    },
    {
        "id": "df70ce3c",
        "text": "reduction algorithm that takes like the input which is this log spectrum and it provides us back with um like a a feature or set of features like that is that has like a smaller dimensional less dimensions. OK. Good. So now I guess like one important question that you may have is how many coefficients should that take? How many MF CCS? Now traditionally, we focus on the first, we consider the 1st 12 to 13 coefficients. Why do we take this? Right? We take the first coefficients because these are the ones that keep the most relevant information, which is the information about performance and spectral envelope. This is like the same stuff that we had with the SEPS. If you recall on the qui access the equivalency values like that are like on the lower end are the ones that provide information about like the spectral envelope, right?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2642s",
        "start_time": "2642.594"
    },
    {
        "id": "4554ee13",
        "text": "Now traditionally, we focus on the first, we consider the 1st 12 to 13 coefficients. Why do we take this? Right? We take the first coefficients because these are the ones that keep the most relevant information, which is the information about performance and spectral envelope. This is like the same stuff that we had with the SEPS. If you recall on the qui access the equivalency values like that are like on the lower end are the ones that provide information about like the spectral envelope, right? The quiery values on the higher end are the ones that provide information about the the glottal pulse we're not interested in the glottal pulse we are interested in the spectral envelope or in other words, interested in the vocal trait um frequency response because that provides us information about the the stuff that perceptually the most relevant the phones, the foreman. OK. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2669s",
        "start_time": "2669.479"
    },
    {
        "id": "6464ab09",
        "text": "access the equivalency values like that are like on the lower end are the ones that provide information about like the spectral envelope, right? The quiery values on the higher end are the ones that provide information about the the glottal pulse we're not interested in the glottal pulse we are interested in the spectral envelope or in other words, interested in the vocal trait um frequency response because that provides us information about the the stuff that perceptually the most relevant the phones, the foreman. OK. So the moment you take like the the the initial coefficients you are taking information about like those forms, higher coefficients provide us information about uh fast changing spectral uh details information, right. And we don't really need that that much for a speech recognition. We're more interested in cellular performance as we said multiple times. So all of this to say that of course, you can take more anesthesia but that",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2695s",
        "start_time": "2695.87"
    },
    {
        "id": "944df337",
        "text": "The quiery values on the higher end are the ones that provide information about the the glottal pulse we're not interested in the glottal pulse we are interested in the spectral envelope or in other words, interested in the vocal trait um frequency response because that provides us information about the the stuff that perceptually the most relevant the phones, the foreman. OK. So the moment you take like the the the initial coefficients you are taking information about like those forms, higher coefficients provide us information about uh fast changing spectral uh details information, right. And we don't really need that that much for a speech recognition. We're more interested in cellular performance as we said multiple times. So all of this to say that of course, you can take more anesthesia but that it's not necessarily gonna improve the quality of your algorithms like that much. But there's another strategy that's probably gonna like boost the accuracy of your ML uh machine learning algorithms quite a lot and it's taking the 1st and 2nd derivatives of MF CCS or in other words, taking the delta and the delta delta of MF CCS.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2707s",
        "start_time": "2707.709"
    },
    {
        "id": "91385019",
        "text": "the moment you take like the the the initial coefficients you are taking information about like those forms, higher coefficients provide us information about uh fast changing spectral uh details information, right. And we don't really need that that much for a speech recognition. We're more interested in cellular performance as we said multiple times. So all of this to say that of course, you can take more anesthesia but that it's not necessarily gonna improve the quality of your algorithms like that much. But there's another strategy that's probably gonna like boost the accuracy of your ML uh machine learning algorithms quite a lot and it's taking the 1st and 2nd derivatives of MF CCS or in other words, taking the delta and the delta delta of MF CCS. What is this? Well,",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2736s",
        "start_time": "2736.449"
    },
    {
        "id": "b519b3a0",
        "text": "it's not necessarily gonna improve the quality of your algorithms like that much. But there's another strategy that's probably gonna like boost the accuracy of your ML uh machine learning algorithms quite a lot and it's taking the 1st and 2nd derivatives of MF CCS or in other words, taking the delta and the delta delta of MF CCS. What is this? Well, let's think about like MF CCS. So if you remember like the pipeline for extracting them, so multi step that one is used for each frame in a signal,",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2765s",
        "start_time": "2765.989"
    },
    {
        "id": "89db8a7f",
        "text": "What is this? Well, let's think about like MF CCS. So if you remember like the pipeline for extracting them, so multi step that one is used for each frame in a signal, which basically means if we have like a 12th long signal, we are gonna have like a ton like of frames. And at each frame you're gonna get a handful of MFCC. Now, if you want to take like the delta MFCC is what you do is you take all the values of MFCC values at one frame and you subtract the values from the previous frame so that you get the delta. Now, if you want to get like the",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2786s",
        "start_time": "2786.83"
    },
    {
        "id": "5a05190d",
        "text": "let's think about like MF CCS. So if you remember like the pipeline for extracting them, so multi step that one is used for each frame in a signal, which basically means if we have like a 12th long signal, we are gonna have like a ton like of frames. And at each frame you're gonna get a handful of MFCC. Now, if you want to take like the delta MFCC is what you do is you take all the values of MFCC values at one frame and you subtract the values from the previous frame so that you get the delta. Now, if you want to get like the delta delta, you do the same thing. But you start like with all the delta MF CCS and then you subtract the values for the delta MF CCS um for one frame and you subtract to that like the delta MF CCS from the previous uh frame. And in that way, you get the delta delta MF CCS if you think about this, this is equivalent to taking more or less the first and the second derivative of this MF CCS.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2788s",
        "start_time": "2788.969"
    },
    {
        "id": "7daca892",
        "text": "which basically means if we have like a 12th long signal, we are gonna have like a ton like of frames. And at each frame you're gonna get a handful of MFCC. Now, if you want to take like the delta MFCC is what you do is you take all the values of MFCC values at one frame and you subtract the values from the previous frame so that you get the delta. Now, if you want to get like the delta delta, you do the same thing. But you start like with all the delta MF CCS and then you subtract the values for the delta MF CCS um for one frame and you subtract to that like the delta MF CCS from the previous uh frame. And in that way, you get the delta delta MF CCS if you think about this, this is equivalent to taking more or less the first and the second derivative of this MF CCS. OK. So this is gonna, it's usually used in music and audio um speech processing really. And what this basically means like in terms of coefficients is that we start with 13 coefficients but then we take like the delta coefficient. So we, we add another 13",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2799s",
        "start_time": "2799.659"
    },
    {
        "id": "0d5f77c1",
        "text": "delta delta, you do the same thing. But you start like with all the delta MF CCS and then you subtract the values for the delta MF CCS um for one frame and you subtract to that like the delta MF CCS from the previous uh frame. And in that way, you get the delta delta MF CCS if you think about this, this is equivalent to taking more or less the first and the second derivative of this MF CCS. OK. So this is gonna, it's usually used in music and audio um speech processing really. And what this basically means like in terms of coefficients is that we start with 13 coefficients but then we take like the delta coefficient. So we, we add another 13 and then we take the delta delta which is another 13 on top of that. So the, the whole figure there is probably like something like 39 coefficients. And remember this is like for each single frame,",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2829s",
        "start_time": "2829.32"
    },
    {
        "id": "837e1b85",
        "text": "OK. So this is gonna, it's usually used in music and audio um speech processing really. And what this basically means like in terms of coefficients is that we start with 13 coefficients but then we take like the delta coefficient. So we, we add another 13 and then we take the delta delta which is another 13 on top of that. So the, the whole figure there is probably like something like 39 coefficients. And remember this is like for each single frame, OK. So now",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2859s",
        "start_time": "2859.399"
    },
    {
        "id": "4f654bb0",
        "text": "and then we take the delta delta which is another 13 on top of that. So the, the whole figure there is probably like something like 39 coefficients. And remember this is like for each single frame, OK. So now I think like it's we are at a point where we want to actually visualize like this MF CCS. And as you can see like when we visualize them like the the kind of impact that we have is very similar to a spectrogram that we saw in earlier videos, right? And that's because we have like MF CCS, like we can think of them like as a, as a matrix, right? In the",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2878s",
        "start_time": "2878.429"
    },
    {
        "id": "e50a18db",
        "text": "OK. So now I think like it's we are at a point where we want to actually visualize like this MF CCS. And as you can see like when we visualize them like the the kind of impact that we have is very similar to a spectrogram that we saw in earlier videos, right? And that's because we have like MF CCS, like we can think of them like as a, as a matrix, right? In the uh rows, we have like the different indexes, like the different MFC, the different coefficients. And on uh here, like on the columns, we have the different frames. OK. And so we can use a hit map like this and as the one that we use for visualizing spectrograms to visualize MF CCS. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2892s",
        "start_time": "2892.199"
    },
    {
        "id": "c4f19585",
        "text": "I think like it's we are at a point where we want to actually visualize like this MF CCS. And as you can see like when we visualize them like the the kind of impact that we have is very similar to a spectrogram that we saw in earlier videos, right? And that's because we have like MF CCS, like we can think of them like as a, as a matrix, right? In the uh rows, we have like the different indexes, like the different MFC, the different coefficients. And on uh here, like on the columns, we have the different frames. OK. And so we can use a hit map like this and as the one that we use for visualizing spectrograms to visualize MF CCS. So what happens here is that like the time is uh discrete and we have like as many uh like uh kind of like discrete section segments as the number of frames that we have in a given uh chunk of signal audio signal. And here we have like as many um",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2894s",
        "start_time": "2894.679"
    },
    {
        "id": "3ebcdb9f",
        "text": "uh rows, we have like the different indexes, like the different MFC, the different coefficients. And on uh here, like on the columns, we have the different frames. OK. And so we can use a hit map like this and as the one that we use for visualizing spectrograms to visualize MF CCS. So what happens here is that like the time is uh discrete and we have like as many uh like uh kind of like discrete section segments as the number of frames that we have in a given uh chunk of signal audio signal. And here we have like as many um discrete like segments here as the number of coefficients that we have. So for example, in this case, you can count them, I believe like we take 16 male frequency subs coefficients. And so we at each point",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2921s",
        "start_time": "2921.879"
    },
    {
        "id": "a4f2c7a8",
        "text": "what happens here is that like the time is uh discrete and we have like as many uh like uh kind of like discrete section segments as the number of frames that we have in a given uh chunk of signal audio signal. And here we have like as many um discrete like segments here as the number of coefficients that we have. So for example, in this case, you can count them, I believe like we take 16 male frequency subs coefficients. And so we at each point like in this metrics, we have the value for a given coefficient at a given point in time or at a given frame. And that value is expressed visually through some kind of like color coding here, right?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2945s",
        "start_time": "2945.229"
    },
    {
        "id": "f02c7baf",
        "text": "discrete like segments here as the number of coefficients that we have. So for example, in this case, you can count them, I believe like we take 16 male frequency subs coefficients. And so we at each point like in this metrics, we have the value for a given coefficient at a given point in time or at a given frame. And that value is expressed visually through some kind of like color coding here, right? And this is very similar to what we did with the spectrograms as well. Cool.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2967s",
        "start_time": "2967.79"
    },
    {
        "id": "e52c71f7",
        "text": "like in this metrics, we have the value for a given coefficient at a given point in time or at a given frame. And that value is expressed visually through some kind of like color coding here, right? And this is very similar to what we did with the spectrograms as well. Cool. OK. So now uh I want to talk about like a couple of like things just like to wrap up like this very long video. And I want to talk about some of the MF CCS advantages. So some of the benefits that come with MF CCS, some of the shortcomings and finally, the applications of MF CCS. So let's start, let's start with the good stuff, the MFCC advantages. So",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2983s",
        "start_time": "2983.169"
    },
    {
        "id": "b0055d59",
        "text": "And this is very similar to what we did with the spectrograms as well. Cool. OK. So now uh I want to talk about like a couple of like things just like to wrap up like this very long video. And I want to talk about some of the MF CCS advantages. So some of the benefits that come with MF CCS, some of the shortcomings and finally, the applications of MF CCS. So let's start, let's start with the good stuff, the MFCC advantages. So a great thing about MFCC is that they are able to describe the large structures of the spectrum. And we saw this and we, we, we, we said this like multiple times. So the moment like we take like the first MF CCS, we are mainly focusing on the spectral envelope on the forms. And so like the different coefficients MFCC coefficients are basically",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=2999s",
        "start_time": "2999.11"
    },
    {
        "id": "fbcc29d0",
        "text": "OK. So now uh I want to talk about like a couple of like things just like to wrap up like this very long video. And I want to talk about some of the MF CCS advantages. So some of the benefits that come with MF CCS, some of the shortcomings and finally, the applications of MF CCS. So let's start, let's start with the good stuff, the MFCC advantages. So a great thing about MFCC is that they are able to describe the large structures of the spectrum. And we saw this and we, we, we, we said this like multiple times. So the moment like we take like the first MF CCS, we are mainly focusing on the spectral envelope on the forms. And so like the different coefficients MFCC coefficients are basically uh providing us information about the performance about the phone names. And this is all we really need. We're just like cutting out the details and the noise that comes with the spec with the spectral details and focus on the main stuff on the phone names on the forms. OK. Uh And yeah, so the second point we ignore the fine spectral structures which we really don't care that much. So we don't care about pitch when we do",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3005s",
        "start_time": "3005.11"
    },
    {
        "id": "bc71db23",
        "text": "a great thing about MFCC is that they are able to describe the large structures of the spectrum. And we saw this and we, we, we, we said this like multiple times. So the moment like we take like the first MF CCS, we are mainly focusing on the spectral envelope on the forms. And so like the different coefficients MFCC coefficients are basically uh providing us information about the performance about the phone names. And this is all we really need. We're just like cutting out the details and the noise that comes with the spec with the spectral details and focus on the main stuff on the phone names on the forms. OK. Uh And yeah, so the second point we ignore the fine spectral structures which we really don't care that much. So we don't care about pitch when we do mostly when we do uh speech recognition. Uh for example, OK. So uh and the great thing about MFCC is is that like they've been tested for a long time both in speech and music processing and they work quite well. That's the point. And they've been like the, the audio uh feature of choice for speech and music processing for a long time, right.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3030s",
        "start_time": "3030.01"
    },
    {
        "id": "07398d5e",
        "text": "uh providing us information about the performance about the phone names. And this is all we really need. We're just like cutting out the details and the noise that comes with the spec with the spectral details and focus on the main stuff on the phone names on the forms. OK. Uh And yeah, so the second point we ignore the fine spectral structures which we really don't care that much. So we don't care about pitch when we do mostly when we do uh speech recognition. Uh for example, OK. So uh and the great thing about MFCC is is that like they've been tested for a long time both in speech and music processing and they work quite well. That's the point. And they've been like the, the audio uh feature of choice for speech and music processing for a long time, right. OK. Now, let's take a look at some of the disadvantages. So first of all, an esthesis are really not that robust with respect to additive noise. And then the second point I think like it's at the same time, a great thing and it's also like a shortcoming. So as we say to come up with MS CCS, we have to put a lot of like knowledge regarding like the way also we humans perceive speech or music. Um",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3059s",
        "start_time": "3059.83"
    },
    {
        "id": "5cd12db0",
        "text": "mostly when we do uh speech recognition. Uh for example, OK. So uh and the great thing about MFCC is is that like they've been tested for a long time both in speech and music processing and they work quite well. That's the point. And they've been like the, the audio uh feature of choice for speech and music processing for a long time, right. OK. Now, let's take a look at some of the disadvantages. So first of all, an esthesis are really not that robust with respect to additive noise. And then the second point I think like it's at the same time, a great thing and it's also like a shortcoming. So as we say to come up with MS CCS, we have to put a lot of like knowledge regarding like the way also we humans perceive speech or music. Um And that can be a shortcoming. And why is that? Well, that's because we are somewhat biasing this audio feature based on biases like that we have and we are not letting the machine decide for itself. What's the what are like the most relevant elements, for example, in a, in a rare audio file or in a spectrogram, right?",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3086s",
        "start_time": "3086.969"
    },
    {
        "id": "d0df03c1",
        "text": "OK. Now, let's take a look at some of the disadvantages. So first of all, an esthesis are really not that robust with respect to additive noise. And then the second point I think like it's at the same time, a great thing and it's also like a shortcoming. So as we say to come up with MS CCS, we have to put a lot of like knowledge regarding like the way also we humans perceive speech or music. Um And that can be a shortcoming. And why is that? Well, that's because we are somewhat biasing this audio feature based on biases like that we have and we are not letting the machine decide for itself. What's the what are like the most relevant elements, for example, in a, in a rare audio file or in a spectrogram, right? And some of these decisions, arbitrary decisions that we take is like the, for example, the the the male um the the the the male",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3113s",
        "start_time": "3113.229"
    },
    {
        "id": "d2ff74ff",
        "text": "And that can be a shortcoming. And why is that? Well, that's because we are somewhat biasing this audio feature based on biases like that we have and we are not letting the machine decide for itself. What's the what are like the most relevant elements, for example, in a, in a rare audio file or in a spectrogram, right? And some of these decisions, arbitrary decisions that we take is like the, for example, the the the male um the the the the male scaling like that we do on the spectrogram. So when we move from the spectrum to the male spectrogram. So for example, there we use the male scale, right? And the male scale is definitely like a valid like informed p perceptually informed pitch scale, but it's not the only one we could have used another one that's called bach, for example, right? Or even like when we are dealing with like the filters, the male filter bands, there we are using",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3143s",
        "start_time": "3143.76"
    },
    {
        "id": "f3947860",
        "text": "And some of these decisions, arbitrary decisions that we take is like the, for example, the the the male um the the the the male scaling like that we do on the spectrogram. So when we move from the spectrum to the male spectrogram. So for example, there we use the male scale, right? And the male scale is definitely like a valid like informed p perceptually informed pitch scale, but it's not the only one we could have used another one that's called bach, for example, right? Or even like when we are dealing with like the filters, the male filter bands, there we are using um triangular filters but that is like an arbitrary decision. So right, we are injecting some level of bias in there. So and the machine may not need that. It could just like use row um data like spectrograms for example, and figure out what it needs learning it directly from data without us biasing the data. OK.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3166s",
        "start_time": "3166.919"
    },
    {
        "id": "b2cfc93f",
        "text": "scaling like that we do on the spectrogram. So when we move from the spectrum to the male spectrogram. So for example, there we use the male scale, right? And the male scale is definitely like a valid like informed p perceptually informed pitch scale, but it's not the only one we could have used another one that's called bach, for example, right? Or even like when we are dealing with like the filters, the male filter bands, there we are using um triangular filters but that is like an arbitrary decision. So right, we are injecting some level of bias in there. So and the machine may not need that. It could just like use row um data like spectrograms for example, and figure out what it needs learning it directly from data without us biasing the data. OK. And finally, a a major disadvantage is that MS CCS are great for analysis. We can do like all types of like analytical stuff like speech recognition, music genre classification, but they're not great for synthesis because we don't really have a an inverse. So we're not capable of moving from MS CCS back to audio in in a perfect like manner. So we only have approximation of that. So you can't really use MF CCS for synthesizing Aio",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3177s",
        "start_time": "3177.989"
    },
    {
        "id": "107bfc4e",
        "text": "um triangular filters but that is like an arbitrary decision. So right, we are injecting some level of bias in there. So and the machine may not need that. It could just like use row um data like spectrograms for example, and figure out what it needs learning it directly from data without us biasing the data. OK. And finally, a a major disadvantage is that MS CCS are great for analysis. We can do like all types of like analytical stuff like speech recognition, music genre classification, but they're not great for synthesis because we don't really have a an inverse. So we're not capable of moving from MS CCS back to audio in in a perfect like manner. So we only have approximation of that. So you can't really use MF CCS for synthesizing Aio cool. OK. So now the final step is esthesis applications as I promised. And so",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3208s",
        "start_time": "3208.1"
    },
    {
        "id": "1f92818c",
        "text": "And finally, a a major disadvantage is that MS CCS are great for analysis. We can do like all types of like analytical stuff like speech recognition, music genre classification, but they're not great for synthesis because we don't really have a an inverse. So we're not capable of moving from MS CCS back to audio in in a perfect like manner. So we only have approximation of that. So you can't really use MF CCS for synthesizing Aio cool. OK. So now the final step is esthesis applications as I promised. And so as I said, MS CCS are an extremely, have been an extremely important audio feature now a little bit less. So because the trend with deep learning has been to move towards audio data that are like as rare as possible. So we are now using spectrograms or male spectrograms most of the time and",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3231s",
        "start_time": "3231.27"
    },
    {
        "id": "3a484ad4",
        "text": "cool. OK. So now the final step is esthesis applications as I promised. And so as I said, MS CCS are an extremely, have been an extremely important audio feature now a little bit less. So because the trend with deep learning has been to move towards audio data that are like as rare as possible. So we are now using spectrograms or male spectrograms most of the time and of MF CCS. But still it's very important to have like a deep understanding of MF CCS because like they provide us like with a clear, I don't know like a clear example of how you can create like something really cool in terms and relevant in terms of like audio features using a bunch of different transformations. And also to understand what",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3261s",
        "start_time": "3261.199"
    },
    {
        "id": "2503256f",
        "text": "as I said, MS CCS are an extremely, have been an extremely important audio feature now a little bit less. So because the trend with deep learning has been to move towards audio data that are like as rare as possible. So we are now using spectrograms or male spectrograms most of the time and of MF CCS. But still it's very important to have like a deep understanding of MF CCS because like they provide us like with a clear, I don't know like a clear example of how you can create like something really cool in terms and relevant in terms of like audio features using a bunch of different transformations. And also to understand what time before the deep learning wave like with all applications in speech and music processing. And of this, I want to talk so we can use MF CCS for speech processing. And here we can use them for uh as the main and only features for doing speech recognition, speaker recognition, speaker diar.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3269s",
        "start_time": "3269.34"
    },
    {
        "id": "d80b52c1",
        "text": "of MF CCS. But still it's very important to have like a deep understanding of MF CCS because like they provide us like with a clear, I don't know like a clear example of how you can create like something really cool in terms and relevant in terms of like audio features using a bunch of different transformations. And also to understand what time before the deep learning wave like with all applications in speech and music processing. And of this, I want to talk so we can use MF CCS for speech processing. And here we can use them for uh as the main and only features for doing speech recognition, speaker recognition, speaker diar. And also we can use them extensively for music processing. So you can use it for music genre classification, mood classification, automatic tagging because it turns out that a sis are very good describers for music as well and because they're capable of capturing the timer of instruments. So all of the um tasks in music processing that have to deal with timer features",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3290s",
        "start_time": "3290.853"
    },
    {
        "id": "67de019c",
        "text": "time before the deep learning wave like with all applications in speech and music processing. And of this, I want to talk so we can use MF CCS for speech processing. And here we can use them for uh as the main and only features for doing speech recognition, speaker recognition, speaker diar. And also we can use them extensively for music processing. So you can use it for music genre classification, mood classification, automatic tagging because it turns out that a sis are very good describers for music as well and because they're capable of capturing the timer of instruments. So all of the um tasks in music processing that have to deal with timer features are a good candidate for using MF CCS. And that's because MF CCS are mainly time centric and they are more or less like pitch in variant, right? But obviously you wouldn't use uh MF CCS for, for example, like for identifying codes, right? Because you're not really getting that much of a uh that much pitch information",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3312s",
        "start_time": "3312.365"
    },
    {
        "id": "e732eaf8",
        "text": "And also we can use them extensively for music processing. So you can use it for music genre classification, mood classification, automatic tagging because it turns out that a sis are very good describers for music as well and because they're capable of capturing the timer of instruments. So all of the um tasks in music processing that have to deal with timer features are a good candidate for using MF CCS. And that's because MF CCS are mainly time centric and they are more or less like pitch in variant, right? But obviously you wouldn't use uh MF CCS for, for example, like for identifying codes, right? Because you're not really getting that much of a uh that much pitch information good. So I think like you should really, really congratulate like yourself because uh we've done a lot of work like in this video. So we looked at the subs, we looked like at the math behind subst, we looked at the visualization of the subs how we can interpret subst in the context of speech. And then we moved on to male frequency subs coefficients.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3334s",
        "start_time": "3334.12"
    },
    {
        "id": "69524e5c",
        "text": "are a good candidate for using MF CCS. And that's because MF CCS are mainly time centric and they are more or less like pitch in variant, right? But obviously you wouldn't use uh MF CCS for, for example, like for identifying codes, right? Because you're not really getting that much of a uh that much pitch information good. So I think like you should really, really congratulate like yourself because uh we've done a lot of work like in this video. So we looked at the subs, we looked like at the math behind subst, we looked at the visualization of the subs how we can interpret subst in the context of speech. And then we moved on to male frequency subs coefficients. But up until up until now, we've just uh dealt with theory. So as it is usually the case in this series, uh the next step is gonna be that of implementation or just like playing around with Python.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3361s",
        "start_time": "3361.81"
    },
    {
        "id": "78ed7505",
        "text": "good. So I think like you should really, really congratulate like yourself because uh we've done a lot of work like in this video. So we looked at the subs, we looked like at the math behind subst, we looked at the visualization of the subs how we can interpret subst in the context of speech. And then we moved on to male frequency subs coefficients. But up until up until now, we've just uh dealt with theory. So as it is usually the case in this series, uh the next step is gonna be that of implementation or just like playing around with Python. So in the next video, we'll use all of the information that we've used here in this video to extract MF CCS with Python and Lisa. And we'll also visualize MF CCS for different audio files. OK. So I really hope that you enjoyed this video",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3384s",
        "start_time": "3384.419"
    },
    {
        "id": "d83d0f67",
        "text": "But up until up until now, we've just uh dealt with theory. So as it is usually the case in this series, uh the next step is gonna be that of implementation or just like playing around with Python. So in the next video, we'll use all of the information that we've used here in this video to extract MF CCS with Python and Lisa. And we'll also visualize MF CCS for different audio files. OK. So I really hope that you enjoyed this video and if you found it useful, yeah, please leave a like if you haven't subscribed and you want to have more um videos like this, please do remember to subscribe and I guess that's it for today. I'll see you next time. Cheers.",
        "video": "Mel-Frequency Cepstral Coefficients Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "4_SH2nfbQZ8",
        "youtube_link": "https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=3411s",
        "start_time": "3411.229"
    },
    {
        "id": "ba95eee1",
        "text": "Hi, everybody and welcome to a new exciting video in the audio signal processing for machine learning series. Last time we looked at the theory behind mel spectrograms. This time we'll be using that knowledge and we'll extract mel spectrograms using Python and li browser. So without any further ado, let's get started. So the first thing that we wanna do is just like import a bunch of like packages. So li browser, li browser doc display and map, put lip just like for review stuff. Next, we want to actually load uh an audio file with lib browser. So the first thing that we'll do is just um get a reference to uh the path to the file. And next we want to do is just like play you guys back like this audio file and you should be familiar with this because it's the one that we used also uh when we actually extracted uh vanilla spectrograms a couple of years ago. So let's listen to this scale and we have a second repetition of the same pattern.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "3ae79999",
        "text": "stuff. Next, we want to actually load uh an audio file with lib browser. So the first thing that we'll do is just um get a reference to uh the path to the file. And next we want to do is just like play you guys back like this audio file and you should be familiar with this because it's the one that we used also uh when we actually extracted uh vanilla spectrograms a couple of years ago. So let's listen to this scale and we have a second repetition of the same pattern. OK. Simple C major scale uh with a piano.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=28s",
        "start_time": "28.94"
    },
    {
        "id": "c563e8c8",
        "text": "and we have a second repetition of the same pattern. OK. Simple C major scale uh with a piano. OK. So next up what we wanna do is just like a uh yeah load uh this audio file in Li Brosa. And so for that, and you should be familiar with this right now. We do a Li Brosa dot load and we pass in the path to the file. What we get back is a signal uh which is just like an empire array and then ad sample rate and the default sample rate with libros is 22,050 Hertz. OK. So",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=60s",
        "start_time": "60.209"
    },
    {
        "id": "a1786af2",
        "text": "OK. Simple C major scale uh with a piano. OK. So next up what we wanna do is just like a uh yeah load uh this audio file in Li Brosa. And so for that, and you should be familiar with this right now. We do a Li Brosa dot load and we pass in the path to the file. What we get back is a signal uh which is just like an empire array and then ad sample rate and the default sample rate with libros is 22,050 Hertz. OK. So uh moving on, uh I want to show you the male filter banks before we extract the mel spectrogram from this audio file.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=63s",
        "start_time": "63.819"
    },
    {
        "id": "ce38ba2c",
        "text": "OK. So next up what we wanna do is just like a uh yeah load uh this audio file in Li Brosa. And so for that, and you should be familiar with this right now. We do a Li Brosa dot load and we pass in the path to the file. What we get back is a signal uh which is just like an empire array and then ad sample rate and the default sample rate with libros is 22,050 Hertz. OK. So uh moving on, uh I want to show you the male filter banks before we extract the mel spectrogram from this audio file. So if you remember from my previous video on mel spectrograms, male filter banks are really the key to getting to male spectrograms because what we do is we extract the spectrogram kind of like a vanilla spectrogram and then we apply a male filter bank to it. In other words, we do a matrix multiplication between the male filter banks and the vanilla spectrogram. And what we get is the",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=68s",
        "start_time": "68.339"
    },
    {
        "id": "35f1fc30",
        "text": "uh moving on, uh I want to show you the male filter banks before we extract the mel spectrogram from this audio file. So if you remember from my previous video on mel spectrograms, male filter banks are really the key to getting to male spectrograms because what we do is we extract the spectrogram kind of like a vanilla spectrogram and then we apply a male filter bank to it. In other words, we do a matrix multiplication between the male filter banks and the vanilla spectrogram. And what we get is the uh male spectrogram. OK. So now let's take a look at the at how we can extract this uh filter banks. Well, if you remember I gave you like all the the five different steps we we we should put into place to, to create a male filter bank. But the great thing about Libras is that we have a utility function that does all of that for us so that we",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=96s",
        "start_time": "96.919"
    },
    {
        "id": "ce21c400",
        "text": "So if you remember from my previous video on mel spectrograms, male filter banks are really the key to getting to male spectrograms because what we do is we extract the spectrogram kind of like a vanilla spectrogram and then we apply a male filter bank to it. In other words, we do a matrix multiplication between the male filter banks and the vanilla spectrogram. And what we get is the uh male spectrogram. OK. So now let's take a look at the at how we can extract this uh filter banks. Well, if you remember I gave you like all the the five different steps we we we should put into place to, to create a male filter bank. But the great thing about Libras is that we have a utility function that does all of that for us so that we can then just like get a, a whole filter bank. So we do uh a Li Breza dot filters dot mail and then we should pass some parameters. First of all, we pass the frame size which I've put to 2048 the simple rate and the number of malls that we want to use. And in other words, like this is like kind of like the number of like mel bands that we'll uh see. OK,",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=106s",
        "start_time": "106.139"
    },
    {
        "id": "ca404d21",
        "text": "uh male spectrogram. OK. So now let's take a look at the at how we can extract this uh filter banks. Well, if you remember I gave you like all the the five different steps we we we should put into place to, to create a male filter bank. But the great thing about Libras is that we have a utility function that does all of that for us so that we can then just like get a, a whole filter bank. So we do uh a Li Breza dot filters dot mail and then we should pass some parameters. First of all, we pass the frame size which I've put to 2048 the simple rate and the number of malls that we want to use. And in other words, like this is like kind of like the number of like mel bands that we'll uh see. OK, let's do that. So nothing really like showed up really. But in all actuality, what happened is that now we have the filter bands. So let's take a look at the shape here. So if you remember from my previous video, I said that this is a mesh",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=130s",
        "start_time": "130.326"
    },
    {
        "id": "9c8310f7",
        "text": "can then just like get a, a whole filter bank. So we do uh a Li Breza dot filters dot mail and then we should pass some parameters. First of all, we pass the frame size which I've put to 2048 the simple rate and the number of malls that we want to use. And in other words, like this is like kind of like the number of like mel bands that we'll uh see. OK, let's do that. So nothing really like showed up really. But in all actuality, what happened is that now we have the filter bands. So let's take a look at the shape here. So if you remember from my previous video, I said that this is a mesh and on uh the rows like the dimension uh like the first dimension is equal to uh has a size of like 10 in this case, which is equal to the number of uh meal bands. And the second dimension",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=154s",
        "start_time": "154.511"
    },
    {
        "id": "40c8efe9",
        "text": "let's do that. So nothing really like showed up really. But in all actuality, what happened is that now we have the filter bands. So let's take a look at the shape here. So if you remember from my previous video, I said that this is a mesh and on uh the rows like the dimension uh like the first dimension is equal to uh has a size of like 10 in this case, which is equal to the number of uh meal bands. And the second dimension has uh a size which is equal to the nus frequency uh of the frame size plus one. So in other words, it's this tt 22,048 divided by two plus one which is equal to 1025. So it checks out. OK. So now what I want to do next is try to visualize this male filter banks. Now, from",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=179s",
        "start_time": "179.759"
    },
    {
        "id": "c55f3ff0",
        "text": "and on uh the rows like the dimension uh like the first dimension is equal to uh has a size of like 10 in this case, which is equal to the number of uh meal bands. And the second dimension has uh a size which is equal to the nus frequency uh of the frame size plus one. So in other words, it's this tt 22,048 divided by two plus one which is equal to 1025. So it checks out. OK. So now what I want to do next is try to visualize this male filter banks. Now, from my previous video, you may remember, sorry, it wasn't that. It's this one. You may remember that this is a way that we can visualize the male filter band. So uh uh on the x axis, we have frequency on the Y axis over here, we have the weights and we have like all of the different male bands. And here like these points which weight equal one are the centers of the,",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=195s",
        "start_time": "195.389"
    },
    {
        "id": "637af11d",
        "text": "has uh a size which is equal to the nus frequency uh of the frame size plus one. So in other words, it's this tt 22,048 divided by two plus one which is equal to 1025. So it checks out. OK. So now what I want to do next is try to visualize this male filter banks. Now, from my previous video, you may remember, sorry, it wasn't that. It's this one. You may remember that this is a way that we can visualize the male filter band. So uh uh on the x axis, we have frequency on the Y axis over here, we have the weights and we have like all of the different male bands. And here like these points which weight equal one are the centers of the, of the filter of the, of the male bands, right? And then we have like this triangular filters like this. OK. So now let's try to visualize this in another way. So what we can do is we can use the Libras display dot spec show if you remember this spec function kind of like",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=211s",
        "start_time": "211.199"
    },
    {
        "id": "29fa5ce5",
        "text": "my previous video, you may remember, sorry, it wasn't that. It's this one. You may remember that this is a way that we can visualize the male filter band. So uh uh on the x axis, we have frequency on the Y axis over here, we have the weights and we have like all of the different male bands. And here like these points which weight equal one are the centers of the, of the filter of the, of the male bands, right? And then we have like this triangular filters like this. OK. So now let's try to visualize this in another way. So what we can do is we can use the Libras display dot spec show if you remember this spec function kind of like um displays of spectrogram like variables. But we can use it really for any type of um bi dimensional array or matrix. And so we know that filter banks like is an array so well is that a bi dimensional array or it is a matrix? And so we can just like plot it using spectra. OK. So now let's try to do this and let's see what's the results over here? Cool.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=236s",
        "start_time": "236.919"
    },
    {
        "id": "54587a5a",
        "text": "of the filter of the, of the male bands, right? And then we have like this triangular filters like this. OK. So now let's try to visualize this in another way. So what we can do is we can use the Libras display dot spec show if you remember this spec function kind of like um displays of spectrogram like variables. But we can use it really for any type of um bi dimensional array or matrix. And so we know that filter banks like is an array so well is that a bi dimensional array or it is a matrix? And so we can just like plot it using spectra. OK. So now let's try to do this and let's see what's the results over here? Cool. OK. So on the X axis here you have frequency expressed in Hertz on the Y axis, you have the 10 different uh uh mel bands, right? And you can see that we have like 10 Melbournes because you, you can see we can count like these blocks right there. So 123456789 and then uh 10 over here.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=263s",
        "start_time": "263.279"
    },
    {
        "id": "a7211ed3",
        "text": "um displays of spectrogram like variables. But we can use it really for any type of um bi dimensional array or matrix. And so we know that filter banks like is an array so well is that a bi dimensional array or it is a matrix? And so we can just like plot it using spectra. OK. So now let's try to do this and let's see what's the results over here? Cool. OK. So on the X axis here you have frequency expressed in Hertz on the Y axis, you have the 10 different uh uh mel bands, right? And you can see that we have like 10 Melbournes because you, you can see we can count like these blocks right there. So 123456789 and then uh 10 over here. And what, what also what we also see here is that we have like this kind of like bands over here and uh the color uh corresponds to the weight that we have uh for a male band at a certain frequency expressed in Hertz and the brighter the color and the closer we get to a which equal to.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=283s",
        "start_time": "283.625"
    },
    {
        "id": "b7f08d5f",
        "text": "OK. So on the X axis here you have frequency expressed in Hertz on the Y axis, you have the 10 different uh uh mel bands, right? And you can see that we have like 10 Melbournes because you, you can see we can count like these blocks right there. So 123456789 and then uh 10 over here. And what, what also what we also see here is that we have like this kind of like bands over here and uh the color uh corresponds to the weight that we have uh for a male band at a certain frequency expressed in Hertz and the brighter the color and the closer we get to a which equal to. So for example, let's analyze like this second uh mel band over here. So this is like the kind of like the the center point, the center frequency where we have which that's equal to one. And then the farther we move from the center frequency of this mel band and like the the lower the weights, which basically means like that the colors tend to fade out and then uh outside of the extremity",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=310s",
        "start_time": "310.17"
    },
    {
        "id": "ccd861d8",
        "text": "And what, what also what we also see here is that we have like this kind of like bands over here and uh the color uh corresponds to the weight that we have uh for a male band at a certain frequency expressed in Hertz and the brighter the color and the closer we get to a which equal to. So for example, let's analyze like this second uh mel band over here. So this is like the kind of like the the center point, the center frequency where we have which that's equal to one. And then the farther we move from the center frequency of this mel band and like the the lower the weights, which basically means like that the colors tend to fade out and then uh outside of the extremity we have like pitch black, which basically means that which is always like equal to zero. And then here you can actually see induction like this filter, um triangular filters, right? Uh for all of this uh different male bands. So yeah, um let's move on now to extracting mouse spectrograms now that we know, yeah, how to extract male filter banks and how to, yeah, just visualize them. OK.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=333s",
        "start_time": "333.739"
    },
    {
        "id": "92c04495",
        "text": "So for example, let's analyze like this second uh mel band over here. So this is like the kind of like the the center point, the center frequency where we have which that's equal to one. And then the farther we move from the center frequency of this mel band and like the the lower the weights, which basically means like that the colors tend to fade out and then uh outside of the extremity we have like pitch black, which basically means that which is always like equal to zero. And then here you can actually see induction like this filter, um triangular filters, right? Uh for all of this uh different male bands. So yeah, um let's move on now to extracting mouse spectrograms now that we know, yeah, how to extract male filter banks and how to, yeah, just visualize them. OK. So extracting mel spectrograms is as easy in Libres as running libros dot T dash mel spectrogram. Now, if you remember uh from my previous video, well, extracting or calculating me spectrograms, uh it's kind of like quite convoluted process because what you should do is actually extracting spectrogram first vanilla spectrograms then",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=358s",
        "start_time": "358.73"
    },
    {
        "id": "dcc249c7",
        "text": "we have like pitch black, which basically means that which is always like equal to zero. And then here you can actually see induction like this filter, um triangular filters, right? Uh for all of this uh different male bands. So yeah, um let's move on now to extracting mouse spectrograms now that we know, yeah, how to extract male filter banks and how to, yeah, just visualize them. OK. So extracting mel spectrograms is as easy in Libres as running libros dot T dash mel spectrogram. Now, if you remember uh from my previous video, well, extracting or calculating me spectrograms, uh it's kind of like quite convoluted process because what you should do is actually extracting spectrogram first vanilla spectrograms then uh uh creating a male filter bank and then apply mail filter banks to the spectrograms. And that's actually what these uh libre dot feature dot males spectrogram function does under the hood. And indeed what you should do here is pass information for both extracting um",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=383s",
        "start_time": "383.72"
    },
    {
        "id": "66a3468a",
        "text": "So extracting mel spectrograms is as easy in Libres as running libros dot T dash mel spectrogram. Now, if you remember uh from my previous video, well, extracting or calculating me spectrograms, uh it's kind of like quite convoluted process because what you should do is actually extracting spectrogram first vanilla spectrograms then uh uh creating a male filter bank and then apply mail filter banks to the spectrograms. And that's actually what these uh libre dot feature dot males spectrogram function does under the hood. And indeed what you should do here is pass information for both extracting um the uh like vanilla spectrogram as well as the co creating like a male uh filter back. So we pass in the the signal. So in this case, it's our scale signal. Then we should specify the some the frame size, the H length. And these are",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=409s",
        "start_time": "409.929"
    },
    {
        "id": "239fcd99",
        "text": "uh uh creating a male filter bank and then apply mail filter banks to the spectrograms. And that's actually what these uh libre dot feature dot males spectrogram function does under the hood. And indeed what you should do here is pass information for both extracting um the uh like vanilla spectrogram as well as the co creating like a male uh filter back. So we pass in the the signal. So in this case, it's our scale signal. Then we should specify the some the frame size, the H length. And these are uh this is all the information that we need for extracting the spectrogram. And then we also need to pass the number of male bands which yeah, let's put it just like equal to 90 for example, right?",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=434s",
        "start_time": "434.029"
    },
    {
        "id": "e17b93e4",
        "text": "the uh like vanilla spectrogram as well as the co creating like a male uh filter back. So we pass in the the signal. So in this case, it's our scale signal. Then we should specify the some the frame size, the H length. And these are uh this is all the information that we need for extracting the spectrogram. And then we also need to pass the number of male bands which yeah, let's put it just like equal to 90 for example, right? And as I said, what ma spectrogram does under the hood is calculating the uh vanilla spectrogram. It creates the male filter banks and apply the male filter banks uh to the spectrogram. OK. So let's run this. And next, what I want to show you guys is the shape of this um males spectrogram, right?",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=453s",
        "start_time": "453.299"
    },
    {
        "id": "36b8ce71",
        "text": "uh this is all the information that we need for extracting the spectrogram. And then we also need to pass the number of male bands which yeah, let's put it just like equal to 90 for example, right? And as I said, what ma spectrogram does under the hood is calculating the uh vanilla spectrogram. It creates the male filter banks and apply the male filter banks uh to the spectrogram. OK. So let's run this. And next, what I want to show you guys is the shape of this um males spectrogram, right? And so what we get here is a, a bi dimensional ray uh once again and the shape here is like, so the the, the first dimension has like size equal to 90. And this checks out because the, the number of rays that we have in the mount spectrogram hm uh should be equal to the number of um mel bands and in this case is equal to 90. So it checks out and this second dimension",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=471s",
        "start_time": "471.929"
    },
    {
        "id": "f6c9dd60",
        "text": "And as I said, what ma spectrogram does under the hood is calculating the uh vanilla spectrogram. It creates the male filter banks and apply the male filter banks uh to the spectrogram. OK. So let's run this. And next, what I want to show you guys is the shape of this um males spectrogram, right? And so what we get here is a, a bi dimensional ray uh once again and the shape here is like, so the the, the first dimension has like size equal to 90. And this checks out because the, the number of rays that we have in the mount spectrogram hm uh should be equal to the number of um mel bands and in this case is equal to 90. So it checks out and this second dimension as a size which is equal to the number of frames or temporal bins that we extract from the signal, which in this case is equal to 342. OK. So next up what we want to do is just like move from the power spectrogram, apply a logarithm logarithm to and then move to decibels. And so what we do by doing so is moving from mount spectrograms to log me spectrograms. And",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=483s",
        "start_time": "483.809"
    },
    {
        "id": "745dffa9",
        "text": "And so what we get here is a, a bi dimensional ray uh once again and the shape here is like, so the the, the first dimension has like size equal to 90. And this checks out because the, the number of rays that we have in the mount spectrogram hm uh should be equal to the number of um mel bands and in this case is equal to 90. So it checks out and this second dimension as a size which is equal to the number of frames or temporal bins that we extract from the signal, which in this case is equal to 342. OK. So next up what we want to do is just like move from the power spectrogram, apply a logarithm logarithm to and then move to decibels. And so what we do by doing so is moving from mount spectrograms to log me spectrograms. And uh this is like very important and we saw this also like a couple of years ago when we extracted uh by spectrograms. And that's because like the way we actually uh perceive amplitude is lori make is not linear. So this passage is quite important uh if you want to use like male spectrograms. OK. So let's do that,",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=505s",
        "start_time": "505.179"
    },
    {
        "id": "6fd9ef83",
        "text": "as a size which is equal to the number of frames or temporal bins that we extract from the signal, which in this case is equal to 342. OK. So next up what we want to do is just like move from the power spectrogram, apply a logarithm logarithm to and then move to decibels. And so what we do by doing so is moving from mount spectrograms to log me spectrograms. And uh this is like very important and we saw this also like a couple of years ago when we extracted uh by spectrograms. And that's because like the way we actually uh perceive amplitude is lori make is not linear. So this passage is quite important uh if you want to use like male spectrograms. OK. So let's do that, that obviously doesn't change at all the shape of the um male spectrogram. Uh Yeah, let me just like show you that. So I'll do a quick log male uh spectrogram like this and I'll do a uh sh",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=532s",
        "start_time": "532.26"
    },
    {
        "id": "6eee6432",
        "text": "uh this is like very important and we saw this also like a couple of years ago when we extracted uh by spectrograms. And that's because like the way we actually uh perceive amplitude is lori make is not linear. So this passage is quite important uh if you want to use like male spectrograms. OK. So let's do that, that obviously doesn't change at all the shape of the um male spectrogram. Uh Yeah, let me just like show you that. So I'll do a quick log male uh spectrogram like this and I'll do a uh sh shape. And so what we get over here is the same shape that we used to have to the mouse back. So the transformation happens at each, for each item in the matrix, but the overall shape of the matrix uh is not changed. OK.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=560s",
        "start_time": "560.869"
    },
    {
        "id": "8ff123a9",
        "text": "that obviously doesn't change at all the shape of the um male spectrogram. Uh Yeah, let me just like show you that. So I'll do a quick log male uh spectrogram like this and I'll do a uh sh shape. And so what we get over here is the same shape that we used to have to the mouse back. So the transformation happens at each, for each item in the matrix, but the overall shape of the matrix uh is not changed. OK. Good. So uh the final thing that we want to do here is just actually show the uh male spectrogram. And once again for doing that, we'll use this Li Breza dot display dot uh spec show. So I'm not gonna get into the details here, cos I've quoted them like multiple times in previous videos. So you guys should be aware of all of this. OK? And here we have like our nice little um mouse spectrogram. So",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=580s",
        "start_time": "580.07"
    },
    {
        "id": "3a687003",
        "text": "shape. And so what we get over here is the same shape that we used to have to the mouse back. So the transformation happens at each, for each item in the matrix, but the overall shape of the matrix uh is not changed. OK. Good. So uh the final thing that we want to do here is just actually show the uh male spectrogram. And once again for doing that, we'll use this Li Breza dot display dot uh spec show. So I'm not gonna get into the details here, cos I've quoted them like multiple times in previous videos. So you guys should be aware of all of this. OK? And here we have like our nice little um mouse spectrogram. So on the X axis, we have time and uh this is like this great time and uh each bin is a frame or yeah, it's a temporal bin. And on the Y axis, uh we have the frequency ex well, it's actually expressed in uh the different like mel bands, right? And in this case, we have like 90 bins. And as you can see here, we have like",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=595s",
        "start_time": "595.539"
    },
    {
        "id": "2c00c4a3",
        "text": "Good. So uh the final thing that we want to do here is just actually show the uh male spectrogram. And once again for doing that, we'll use this Li Breza dot display dot uh spec show. So I'm not gonna get into the details here, cos I've quoted them like multiple times in previous videos. So you guys should be aware of all of this. OK? And here we have like our nice little um mouse spectrogram. So on the X axis, we have time and uh this is like this great time and uh each bin is a frame or yeah, it's a temporal bin. And on the Y axis, uh we have the frequency ex well, it's actually expressed in uh the different like mel bands, right? And in this case, we have like 90 bins. And as you can see here, we have like an overall pattern that full is you can't like kind of like guess that we have like a scale here. So we start here with the melt bend which is like close to like a seed and we go up to d uh up until like we go to this point which is like the uh the uh octave uh above. And then you also have like the relative harmonics over here. OK.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=611s",
        "start_time": "611.359"
    },
    {
        "id": "c9d3fdf2",
        "text": "on the X axis, we have time and uh this is like this great time and uh each bin is a frame or yeah, it's a temporal bin. And on the Y axis, uh we have the frequency ex well, it's actually expressed in uh the different like mel bands, right? And in this case, we have like 90 bins. And as you can see here, we have like an overall pattern that full is you can't like kind of like guess that we have like a scale here. So we start here with the melt bend which is like close to like a seed and we go up to d uh up until like we go to this point which is like the uh the uh octave uh above. And then you also have like the relative harmonics over here. OK. But um it's kind of like a little bit difficult to understand that like the X axis is uh in this case, like it's divided uh like its discretion has like 90 mel bands because yeah, it's diff it's difficult to see like the discrete points there. So what we can do is just like move this number of mel bands over here from 90 to say 10 and just like rerun the whole thing. Uh So yeah, now, obviously like the mass spectrogram shape is equal to 10",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=637s",
        "start_time": "637.84"
    },
    {
        "id": "6dcdbf94",
        "text": "an overall pattern that full is you can't like kind of like guess that we have like a scale here. So we start here with the melt bend which is like close to like a seed and we go up to d uh up until like we go to this point which is like the uh the uh octave uh above. And then you also have like the relative harmonics over here. OK. But um it's kind of like a little bit difficult to understand that like the X axis is uh in this case, like it's divided uh like its discretion has like 90 mel bands because yeah, it's diff it's difficult to see like the discrete points there. So what we can do is just like move this number of mel bands over here from 90 to say 10 and just like rerun the whole thing. Uh So yeah, now, obviously like the mass spectrogram shape is equal to 10 uh and 342 as the second dimension because we have 10 mel bands. OK. So moving on. So yeah, and now we actually see that on the X axis, we have like 10 discrete blocks, right? So we have 123456789 and 10 over there.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=662s",
        "start_time": "662.83"
    },
    {
        "id": "c50bcd00",
        "text": "But um it's kind of like a little bit difficult to understand that like the X axis is uh in this case, like it's divided uh like its discretion has like 90 mel bands because yeah, it's diff it's difficult to see like the discrete points there. So what we can do is just like move this number of mel bands over here from 90 to say 10 and just like rerun the whole thing. Uh So yeah, now, obviously like the mass spectrogram shape is equal to 10 uh and 342 as the second dimension because we have 10 mel bands. OK. So moving on. So yeah, and now we actually see that on the X axis, we have like 10 discrete blocks, right? So we have 123456789 and 10 over there. Cool. OK. So by now you should be able to extract male spectrograms and visualize them as well as like extracting male Fitter Banks and visualize them with Python and specifically using the Li Brosa uh library.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=686s",
        "start_time": "686.299"
    },
    {
        "id": "5069b6ac",
        "text": "uh and 342 as the second dimension because we have 10 mel bands. OK. So moving on. So yeah, and now we actually see that on the X axis, we have like 10 discrete blocks, right? So we have 123456789 and 10 over there. Cool. OK. So by now you should be able to extract male spectrograms and visualize them as well as like extracting male Fitter Banks and visualize them with Python and specifically using the Li Brosa uh library. OK. So uh I hope you enjoyed this video and you found it uh useful. If that's the case, please leave a like to the video and if you want to watch more videos like this and you're not subscribed to the sound of the I channel, please remember to subscribe if you have any questions as always leave them in the comments section below. Uh That's all for today. I guess I'll see you next time. Cheers.",
        "video": "Extracting Mel Spectrograms with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "TdnVE5m3o_0",
        "youtube_link": "https://www.youtube.com/watch?v=TdnVE5m3o_0&t=715s",
        "start_time": "715.859"
    },
    {
        "id": "ccd05d29",
        "text": "Hi, everybody and welcome to a new exciting video and the audio signal processing for machine learning series. Last time we introduced the extraction pipelines for both time domain and frequency domain features. This time, I'm going to be introducing time domain audio features specifically, I'm going to focus on a few very important types them in audio features and explain the theory behind them. But before we get started, let me recall you once uh remind you once again about the sound of the I Slack community. This is a community of people interested in all things A I audio A I music audio signal processing. So if you want to improve your knowledge in the field and network with great people, I highly suggest you to go check out the sign up link that I've left in the description below now on to the good stuff. OK. So these are the 310 domain features that we'll be uh focusing on today. So these are amplitude envelope root mean square energy and zero crossing rate. So all of these features",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "75d85a84",
        "text": "them in audio features and explain the theory behind them. But before we get started, let me recall you once uh remind you once again about the sound of the I Slack community. This is a community of people interested in all things A I audio A I music audio signal processing. So if you want to improve your knowledge in the field and network with great people, I highly suggest you to go check out the sign up link that I've left in the description below now on to the good stuff. OK. So these are the 310 domain features that we'll be uh focusing on today. So these are amplitude envelope root mean square energy and zero crossing rate. So all of these features are low level acoustic features and they are instantaneous. In other words, we take these features at each frame in an audio signal, then obviously, we can just like take like all of these values that we have for each frame and then we can aggregate them with some statistical means like mean sum or things that are a little bit more sophisticated like Gaussian mixture models.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=20s",
        "start_time": "20.485"
    },
    {
        "id": "699bf384",
        "text": "So if you want to improve your knowledge in the field and network with great people, I highly suggest you to go check out the sign up link that I've left in the description below now on to the good stuff. OK. So these are the 310 domain features that we'll be uh focusing on today. So these are amplitude envelope root mean square energy and zero crossing rate. So all of these features are low level acoustic features and they are instantaneous. In other words, we take these features at each frame in an audio signal, then obviously, we can just like take like all of these values that we have for each frame and then we can aggregate them with some statistical means like mean sum or things that are a little bit more sophisticated like Gaussian mixture models. And but yeah, so if you don't remember like the definition of a frame or like the categorization of audio features, I highly suggest you go check out this video up here that will clarify all of those things for you. The first time the main feature that we look into",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=41s",
        "start_time": "41.169"
    },
    {
        "id": "7202d793",
        "text": "are low level acoustic features and they are instantaneous. In other words, we take these features at each frame in an audio signal, then obviously, we can just like take like all of these values that we have for each frame and then we can aggregate them with some statistical means like mean sum or things that are a little bit more sophisticated like Gaussian mixture models. And but yeah, so if you don't remember like the definition of a frame or like the categorization of audio features, I highly suggest you go check out this video up here that will clarify all of those things for you. The first time the main feature that we look into is the amplitude envelope. And that's the max amplitude value taken out of all the samples that we have in a frame. Now, I'll show you the math for calculating that don't be scared because I'll break it down uh piece by piece for you. What we want to find is the amplitude envelope at frame T. So it's, it's a specific frame.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=69s",
        "start_time": "69.58"
    },
    {
        "id": "b50c29da",
        "text": "And but yeah, so if you don't remember like the definition of a frame or like the categorization of audio features, I highly suggest you go check out this video up here that will clarify all of those things for you. The first time the main feature that we look into is the amplitude envelope. And that's the max amplitude value taken out of all the samples that we have in a frame. Now, I'll show you the math for calculating that don't be scared because I'll break it down uh piece by piece for you. What we want to find is the amplitude envelope at frame T. So it's, it's a specific frame. So now let's start with this SFK. So what's that? Well, that's just like the amplitude calculated at sample K. Now, another ingredient uh that we want to know about is this capital K, which is the frame size or in other words, the number of samples that we have in a given frame. OK.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=98s",
        "start_time": "98.239"
    },
    {
        "id": "f295c90a",
        "text": "is the amplitude envelope. And that's the max amplitude value taken out of all the samples that we have in a frame. Now, I'll show you the math for calculating that don't be scared because I'll break it down uh piece by piece for you. What we want to find is the amplitude envelope at frame T. So it's, it's a specific frame. So now let's start with this SFK. So what's that? Well, that's just like the amplitude calculated at sample K. Now, another ingredient uh that we want to know about is this capital K, which is the frame size or in other words, the number of samples that we have in a given frame. OK. So now we want to take the max amplitude value of all the samples in the frame. So we have to define this max between the first sample of frame T and we can access that by multiplying the uh frame we are at. So this,",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=116s",
        "start_time": "116.254"
    },
    {
        "id": "211bc070",
        "text": "So now let's start with this SFK. So what's that? Well, that's just like the amplitude calculated at sample K. Now, another ingredient uh that we want to know about is this capital K, which is the frame size or in other words, the number of samples that we have in a given frame. OK. So now we want to take the max amplitude value of all the samples in the frame. So we have to define this max between the first sample of frame T and we can access that by multiplying the uh frame we are at. So this, it could be like a frame 012, whatever you want. And capital K, which is the frame size. And this will give the first sample of frame T. And we want to calculate the max, as we said between first sample of frame T and obviously the last sample of frame T.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=141s",
        "start_time": "141.039"
    },
    {
        "id": "a27a2c98",
        "text": "So now we want to take the max amplitude value of all the samples in the frame. So we have to define this max between the first sample of frame T and we can access that by multiplying the uh frame we are at. So this, it could be like a frame 012, whatever you want. And capital K, which is the frame size. And this will give the first sample of frame T. And we want to calculate the max, as we said between first sample of frame T and obviously the last sample of frame T. And we can calculate this by first of all accessing the next frame. And that is like T plus one, we want to multiply that by the frame size. And these will give us the first sample of frame T plus one. So we want to go back to the last frame of T. And so we have to um",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=162s",
        "start_time": "162.75"
    },
    {
        "id": "fea088f5",
        "text": "it could be like a frame 012, whatever you want. And capital K, which is the frame size. And this will give the first sample of frame T. And we want to calculate the max, as we said between first sample of frame T and obviously the last sample of frame T. And we can calculate this by first of all accessing the next frame. And that is like T plus one, we want to multiply that by the frame size. And these will give us the first sample of frame T plus one. So we want to go back to the last frame of T. And so we have to um subtract one to, to T plus one multiplied by K, right. OK. So this is the amplitude envelope at frame T. But we know that we want, what we want to do is to calculate the amplitude envelope for all the frames. So for each frame in an audio signal, we are gonna use these uh formula here.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=181s",
        "start_time": "181.384"
    },
    {
        "id": "257f3cc3",
        "text": "And we can calculate this by first of all accessing the next frame. And that is like T plus one, we want to multiply that by the frame size. And these will give us the first sample of frame T plus one. So we want to go back to the last frame of T. And so we have to um subtract one to, to T plus one multiplied by K, right. OK. So this is the amplitude envelope at frame T. But we know that we want, what we want to do is to calculate the amplitude envelope for all the frames. So for each frame in an audio signal, we are gonna use these uh formula here. OK. I know this can feel a little bit abstract. Uh So let's try to visualize this to like unsend this like a process a little bit better. So here we have like our normal audio signal here. I'm gonna take like a few uh like I'm gonna be framing this audio signal. So this is the first frame, second frame, 3rd, 4th, 5th 6th, right? And I can continue",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=200s",
        "start_time": "200.55"
    },
    {
        "id": "a1fa57de",
        "text": "subtract one to, to T plus one multiplied by K, right. OK. So this is the amplitude envelope at frame T. But we know that we want, what we want to do is to calculate the amplitude envelope for all the frames. So for each frame in an audio signal, we are gonna use these uh formula here. OK. I know this can feel a little bit abstract. Uh So let's try to visualize this to like unsend this like a process a little bit better. So here we have like our normal audio signal here. I'm gonna take like a few uh like I'm gonna be framing this audio signal. So this is the first frame, second frame, 3rd, 4th, 5th 6th, right? And I can continue once again if you don't remember what framing is, what like all of these things concepts are. It's like frames just go back to",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=222s",
        "start_time": "222.08"
    },
    {
        "id": "e574ca41",
        "text": "OK. I know this can feel a little bit abstract. Uh So let's try to visualize this to like unsend this like a process a little bit better. So here we have like our normal audio signal here. I'm gonna take like a few uh like I'm gonna be framing this audio signal. So this is the first frame, second frame, 3rd, 4th, 5th 6th, right? And I can continue once again if you don't remember what framing is, what like all of these things concepts are. It's like frames just go back to uh a couple of videos ago in this series. Cos dare like I explained this like very in depth. OK. So now how do we get the amplitude envelope here? Well, what we do like what we did mathematically now we can do visually. In other words, we want to take for each frame, the highest value the max value of the amplitude. And it's this guy little guy here in green.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=249s",
        "start_time": "249.66"
    },
    {
        "id": "607c1687",
        "text": "once again if you don't remember what framing is, what like all of these things concepts are. It's like frames just go back to uh a couple of videos ago in this series. Cos dare like I explained this like very in depth. OK. So now how do we get the amplitude envelope here? Well, what we do like what we did mathematically now we can do visually. In other words, we want to take for each frame, the highest value the max value of the amplitude. And it's this guy little guy here in green. Now, if we move to the second um frame, we have like this guy here, that's the max value here. We have like this max value that's over here. Uh here for the fourth",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=276s",
        "start_time": "276.029"
    },
    {
        "id": "521ff448",
        "text": "uh a couple of videos ago in this series. Cos dare like I explained this like very in depth. OK. So now how do we get the amplitude envelope here? Well, what we do like what we did mathematically now we can do visually. In other words, we want to take for each frame, the highest value the max value of the amplitude. And it's this guy little guy here in green. Now, if we move to the second um frame, we have like this guy here, that's the max value here. We have like this max value that's over here. Uh here for the fourth fifth frame we'll get here. And I expect for the sixth frame to have the uh max amplitude uh over here. OK. So this is like visual what we are, what we did earlier like uh with a mathematical formula. OK. I hope like this is clear, it's quite like simple to grasp like once you understand the intuition behind it. OK. Now, um",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=285s",
        "start_time": "285.92"
    },
    {
        "id": "bcc88018",
        "text": "Now, if we move to the second um frame, we have like this guy here, that's the max value here. We have like this max value that's over here. Uh here for the fourth fifth frame we'll get here. And I expect for the sixth frame to have the uh max amplitude uh over here. OK. So this is like visual what we are, what we did earlier like uh with a mathematical formula. OK. I hope like this is clear, it's quite like simple to grasp like once you understand the intuition behind it. OK. Now, um the question is, so what does like amplitude envelope tell us about the audio signal? Well, it gives us like a rough idea of the loudness of the signal. And that's obviously because the amplitude is related with intensity and for uh loudness. OK. So",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=313s",
        "start_time": "313.38"
    },
    {
        "id": "b2fe2605",
        "text": "fifth frame we'll get here. And I expect for the sixth frame to have the uh max amplitude uh over here. OK. So this is like visual what we are, what we did earlier like uh with a mathematical formula. OK. I hope like this is clear, it's quite like simple to grasp like once you understand the intuition behind it. OK. Now, um the question is, so what does like amplitude envelope tell us about the audio signal? Well, it gives us like a rough idea of the loudness of the signal. And that's obviously because the amplitude is related with intensity and for uh loudness. OK. So uh the problem with the amplitude envelope is that it is like very sensitive to outliers. And that's because you're just like getting one value uh one sample, one the amplitude for one sample out of all the, the samples that you have in a frame. And so you may have like an outlier. So in other words, you may have like a a frame uh like that has like all, let's say almost like zero amplitude for all samples and then you have a spike",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=327s",
        "start_time": "327.309"
    },
    {
        "id": "bb9f693d",
        "text": "the question is, so what does like amplitude envelope tell us about the audio signal? Well, it gives us like a rough idea of the loudness of the signal. And that's obviously because the amplitude is related with intensity and for uh loudness. OK. So uh the problem with the amplitude envelope is that it is like very sensitive to outliers. And that's because you're just like getting one value uh one sample, one the amplitude for one sample out of all the, the samples that you have in a frame. And so you may have like an outlier. So in other words, you may have like a a frame uh like that has like all, let's say almost like zero amplitude for all samples and then you have a spike and then you get that spike. But that spike isn't really that representative of the whole frame, right? That's the basic idea why amplitude envelope is sensitive to our layers. Now, uh where do we use amplitude envelope? Well, it can be used in a bunch of different uh applications but uh a few like important ones are like onset detection.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=353s",
        "start_time": "353.51"
    },
    {
        "id": "d9f9930d",
        "text": "uh the problem with the amplitude envelope is that it is like very sensitive to outliers. And that's because you're just like getting one value uh one sample, one the amplitude for one sample out of all the, the samples that you have in a frame. And so you may have like an outlier. So in other words, you may have like a a frame uh like that has like all, let's say almost like zero amplitude for all samples and then you have a spike and then you get that spike. But that spike isn't really that representative of the whole frame, right? That's the basic idea why amplitude envelope is sensitive to our layers. Now, uh where do we use amplitude envelope? Well, it can be used in a bunch of different uh applications but uh a few like important ones are like onset detection. In other words, like answer detection is basically so if you have like a note, you want to just like uh understand when that note like starts, it could be a musical note, it could be like the utterance of a word or a phone. And so with amplitude envelope, you, you can kind of like ga guess where that like event acoustic event starts. And that's because amplitude envelope gives us information",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=373s",
        "start_time": "373.089"
    },
    {
        "id": "8ff734ee",
        "text": "and then you get that spike. But that spike isn't really that representative of the whole frame, right? That's the basic idea why amplitude envelope is sensitive to our layers. Now, uh where do we use amplitude envelope? Well, it can be used in a bunch of different uh applications but uh a few like important ones are like onset detection. In other words, like answer detection is basically so if you have like a note, you want to just like uh understand when that note like starts, it could be a musical note, it could be like the utterance of a word or a phone. And so with amplitude envelope, you, you can kind of like ga guess where that like event acoustic event starts. And that's because amplitude envelope gives us information uh about amplitude. And you would expect that at an onset you have like a burst of energy. So a spike in the amplitude and we can also use amplitude envelope for higher level classification problems like music genre classification. For example, now I'm not going to show you like graphs or visualizations like of amplitude envelope here like in this video. But in the next we'll be looking at uh the comparison like of",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=401s",
        "start_time": "401.64"
    },
    {
        "id": "a03a7fba",
        "text": "In other words, like answer detection is basically so if you have like a note, you want to just like uh understand when that note like starts, it could be a musical note, it could be like the utterance of a word or a phone. And so with amplitude envelope, you, you can kind of like ga guess where that like event acoustic event starts. And that's because amplitude envelope gives us information uh about amplitude. And you would expect that at an onset you have like a burst of energy. So a spike in the amplitude and we can also use amplitude envelope for higher level classification problems like music genre classification. For example, now I'm not going to show you like graphs or visualizations like of amplitude envelope here like in this video. But in the next we'll be looking at uh the comparison like of envelope taken for like different pieces of music from different genres. And we'll see if indeed there is a difference there. OK. So stay tuned for that. But now let's move on to the second time domain audio feature that's called root mean square energy. So what we do here is basically we took the root mean square of all the samples in a frame.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=425s",
        "start_time": "425.98"
    },
    {
        "id": "ac309739",
        "text": "uh about amplitude. And you would expect that at an onset you have like a burst of energy. So a spike in the amplitude and we can also use amplitude envelope for higher level classification problems like music genre classification. For example, now I'm not going to show you like graphs or visualizations like of amplitude envelope here like in this video. But in the next we'll be looking at uh the comparison like of envelope taken for like different pieces of music from different genres. And we'll see if indeed there is a difference there. OK. So stay tuned for that. But now let's move on to the second time domain audio feature that's called root mean square energy. So what we do here is basically we took the root mean square of all the samples in a frame. OK. So I know like if you're not familiar with like statistics and math like this once again can sound like very scary. And indeed this uh formula doesn't help much. But once again, I'm gonna break this down. So uh you can understand that. So here, what we are doing is we are taking the root M square energy at",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=454s",
        "start_time": "454.41"
    },
    {
        "id": "51e3707f",
        "text": "envelope taken for like different pieces of music from different genres. And we'll see if indeed there is a difference there. OK. So stay tuned for that. But now let's move on to the second time domain audio feature that's called root mean square energy. So what we do here is basically we took the root mean square of all the samples in a frame. OK. So I know like if you're not familiar with like statistics and math like this once again can sound like very scary. And indeed this uh formula doesn't help much. But once again, I'm gonna break this down. So uh you can understand that. So here, what we are doing is we are taking the root M square energy at frame T once again, as we said, all of these audio features are instantaneously, we take the values for each frame. OK. So here, the new thing which shouldn't be like too new for you is the as this uh s of K which is the amplitude uh at the sample K, but it's squared this night. So if we have like the square of the amplitude, basically we have the energy.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=483s",
        "start_time": "483.825"
    },
    {
        "id": "7aec5a30",
        "text": "OK. So I know like if you're not familiar with like statistics and math like this once again can sound like very scary. And indeed this uh formula doesn't help much. But once again, I'm gonna break this down. So uh you can understand that. So here, what we are doing is we are taking the root M square energy at frame T once again, as we said, all of these audio features are instantaneously, we take the values for each frame. OK. So here, the new thing which shouldn't be like too new for you is the as this uh s of K which is the amplitude uh at the sample K, but it's squared this night. So if we have like the square of the amplitude, basically we have the energy. Uh So this guy is the energy of the cave sample. So what we do here is we sum the energy for all the samples in frame T once again, like this is just like the sum symbol and we are uh like adding up like all the energy for",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=513s",
        "start_time": "513.549"
    },
    {
        "id": "e97dedd3",
        "text": "frame T once again, as we said, all of these audio features are instantaneously, we take the values for each frame. OK. So here, the new thing which shouldn't be like too new for you is the as this uh s of K which is the amplitude uh at the sample K, but it's squared this night. So if we have like the square of the amplitude, basically we have the energy. Uh So this guy is the energy of the cave sample. So what we do here is we sum the energy for all the samples in frame T once again, like this is just like the sum symbol and we are uh like adding up like all the energy for all the samples in frame T. And then once we have like this sum, what we want to do is we want to take the m of the sum of the energy. And we'll achieve that by dividing the sum of the energy by the frame size, which is the number of samples that we have in the given frame.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=535s",
        "start_time": "535.859"
    },
    {
        "id": "6e056672",
        "text": "Uh So this guy is the energy of the cave sample. So what we do here is we sum the energy for all the samples in frame T once again, like this is just like the sum symbol and we are uh like adding up like all the energy for all the samples in frame T. And then once we have like this sum, what we want to do is we want to take the m of the sum of the energy. And we'll achieve that by dividing the sum of the energy by the frame size, which is the number of samples that we have in the given frame. And after we've done that, we just apply the um the, the root over here. And that's it. So basically, we have the root min square energy. Now, uh once again, the root min square energy is an indicator of loudness because energy is strictly related to loudness.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=564s",
        "start_time": "564.479"
    },
    {
        "id": "5dd974f8",
        "text": "all the samples in frame T. And then once we have like this sum, what we want to do is we want to take the m of the sum of the energy. And we'll achieve that by dividing the sum of the energy by the frame size, which is the number of samples that we have in the given frame. And after we've done that, we just apply the um the, the root over here. And that's it. So basically, we have the root min square energy. Now, uh once again, the root min square energy is an indicator of loudness because energy is strictly related to loudness. And the great thing about Roitman Square energy. And one of the reasons why like it is like overwhelmingly used uh at least like in traditional like digital signal, audio, digital signal processing is that it is less sensitive to liers than the amplitude envelope. And this is because we're not just like sampling",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=586s",
        "start_time": "586.07"
    },
    {
        "id": "bb812eb0",
        "text": "And after we've done that, we just apply the um the, the root over here. And that's it. So basically, we have the root min square energy. Now, uh once again, the root min square energy is an indicator of loudness because energy is strictly related to loudness. And the great thing about Roitman Square energy. And one of the reasons why like it is like overwhelmingly used uh at least like in traditional like digital signal, audio, digital signal processing is that it is less sensitive to liers than the amplitude envelope. And this is because we're not just like sampling a single sample value uh for a frame, but rather we're just like getting information from all the samples. And then we take like the root M square for that's calculated across like all the samples.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=609s",
        "start_time": "609.919"
    },
    {
        "id": "2cf085ff",
        "text": "And the great thing about Roitman Square energy. And one of the reasons why like it is like overwhelmingly used uh at least like in traditional like digital signal, audio, digital signal processing is that it is less sensitive to liers than the amplitude envelope. And this is because we're not just like sampling a single sample value uh for a frame, but rather we're just like getting information from all the samples. And then we take like the root M square for that's calculated across like all the samples. And in this way, we are less sensitive to outliers like spikes in amplitude in one sample in all of the frames. OK. So now what are the applications for root M square energy? Once again, there are many, many applications here. I want to um talk about a couple of this. So",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=634s",
        "start_time": "634.559"
    },
    {
        "id": "3f6920c9",
        "text": "a single sample value uh for a frame, but rather we're just like getting information from all the samples. And then we take like the root M square for that's calculated across like all the samples. And in this way, we are less sensitive to outliers like spikes in amplitude in one sample in all of the frames. OK. So now what are the applications for root M square energy? Once again, there are many, many applications here. I want to um talk about a couple of this. So uh we can use ROM square energy for identifying new segments in an audio signal. And that's because the uh R MS tends to like change quite a lot when you have like new segments, new like events. And so we can use, we can leverage rhythm in square energy for all things like in audio segmentation, it could be done like for um for example, like for deciding",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=655s",
        "start_time": "655.45"
    },
    {
        "id": "45b6e39c",
        "text": "And in this way, we are less sensitive to outliers like spikes in amplitude in one sample in all of the frames. OK. So now what are the applications for root M square energy? Once again, there are many, many applications here. I want to um talk about a couple of this. So uh we can use ROM square energy for identifying new segments in an audio signal. And that's because the uh R MS tends to like change quite a lot when you have like new segments, new like events. And so we can use, we can leverage rhythm in square energy for all things like in audio segmentation, it could be done like for um for example, like for deciding uh like whether like someone is talking or it's not talking and you have like this change and you want to segment who's talking and things like that. And obviously, we can also use R MS for music genre classification. And we'll see some of this like over the next few videos in uh action.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=674s",
        "start_time": "674.844"
    },
    {
        "id": "defc2951",
        "text": "uh we can use ROM square energy for identifying new segments in an audio signal. And that's because the uh R MS tends to like change quite a lot when you have like new segments, new like events. And so we can use, we can leverage rhythm in square energy for all things like in audio segmentation, it could be done like for um for example, like for deciding uh like whether like someone is talking or it's not talking and you have like this change and you want to segment who's talking and things like that. And obviously, we can also use R MS for music genre classification. And we'll see some of this like over the next few videos in uh action. The third time domain audio feature that I want to introduce in this video is called zero crossing rate. This is a quite popular acoustic feature used both in speech recognition and in music information retrieval.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=698s",
        "start_time": "698.01"
    },
    {
        "id": "31b153a3",
        "text": "uh like whether like someone is talking or it's not talking and you have like this change and you want to segment who's talking and things like that. And obviously, we can also use R MS for music genre classification. And we'll see some of this like over the next few videos in uh action. The third time domain audio feature that I want to introduce in this video is called zero crossing rate. This is a quite popular acoustic feature used both in speech recognition and in music information retrieval. And it's a quite intuitive one because it provides us information about the number of times that a signal crosses the horizontal axis. So before getting into the math, let's visualize this. OK. So here we have uh a signal, a simple signal worth of a frame we can assume",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=727s",
        "start_time": "727.28"
    },
    {
        "id": "3313e6f5",
        "text": "The third time domain audio feature that I want to introduce in this video is called zero crossing rate. This is a quite popular acoustic feature used both in speech recognition and in music information retrieval. And it's a quite intuitive one because it provides us information about the number of times that a signal crosses the horizontal axis. So before getting into the math, let's visualize this. OK. So here we have uh a signal, a simple signal worth of a frame we can assume and then the zero crossing rate is equal to the count of like this green dots. OK. So uh for each of these green dots, we have a crossing of the horizontal axis. In other words, like for this signal, the zero crossing rate is equal to 123456. OK.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=746s",
        "start_time": "746.489"
    },
    {
        "id": "6eb2bca2",
        "text": "And it's a quite intuitive one because it provides us information about the number of times that a signal crosses the horizontal axis. So before getting into the math, let's visualize this. OK. So here we have uh a signal, a simple signal worth of a frame we can assume and then the zero crossing rate is equal to the count of like this green dots. OK. So uh for each of these green dots, we have a crossing of the horizontal axis. In other words, like for this signal, the zero crossing rate is equal to 123456. OK. Now the question is, how do we calculate the zero crossing rate mathematically? Well, here you have the formula for that. Once again, don't be scared about this. We'll just like break it down.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=760s",
        "start_time": "760.739"
    },
    {
        "id": "aeb93fa8",
        "text": "and then the zero crossing rate is equal to the count of like this green dots. OK. So uh for each of these green dots, we have a crossing of the horizontal axis. In other words, like for this signal, the zero crossing rate is equal to 123456. OK. Now the question is, how do we calculate the zero crossing rate mathematically? Well, here you have the formula for that. Once again, don't be scared about this. We'll just like break it down. OK. The basic intuition here is that we compare the um the amplitude of value for consecutive uh pairs of samples. And we look at whether there are differences",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=781s",
        "start_time": "781.14"
    },
    {
        "id": "da5bf6da",
        "text": "Now the question is, how do we calculate the zero crossing rate mathematically? Well, here you have the formula for that. Once again, don't be scared about this. We'll just like break it down. OK. The basic intuition here is that we compare the um the amplitude of value for consecutive uh pairs of samples. And we look at whether there are differences like in the signs of those amplitude for those consecutive samples. OK. So first of all, you should or we should familiarize with this sine function if you're not familiar with that already, basically what the sine function which is indicated by this SGN",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=802s",
        "start_time": "802.09"
    },
    {
        "id": "dac556fe",
        "text": "OK. The basic intuition here is that we compare the um the amplitude of value for consecutive uh pairs of samples. And we look at whether there are differences like in the signs of those amplitude for those consecutive samples. OK. So first of all, you should or we should familiarize with this sine function if you're not familiar with that already, basically what the sine function which is indicated by this SGN what it does, it gives us a back the sign of a given value. So in this case, for example, if the amplitude is greater than zero sine function gives us back plus one. That's because the amplitude is positive. If the amplitude is negative, we get back minus one. If the amplitude is equal to zero, we get, we get back zero.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=816s",
        "start_time": "816.0"
    },
    {
        "id": "3f439b4b",
        "text": "like in the signs of those amplitude for those consecutive samples. OK. So first of all, you should or we should familiarize with this sine function if you're not familiar with that already, basically what the sine function which is indicated by this SGN what it does, it gives us a back the sign of a given value. So in this case, for example, if the amplitude is greater than zero sine function gives us back plus one. That's because the amplitude is positive. If the amplitude is negative, we get back minus one. If the amplitude is equal to zero, we get, we get back zero. Now how do we calculate the uh the zero crossing rate like for each pair of samples? Well, what we do is we take the sign for uh the amplitude at sample K. And then we subtract to that the sign for the amplitude at sample K plus one. So we are basically comparing the two consecutive",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=833s",
        "start_time": "833.195"
    },
    {
        "id": "056c36fa",
        "text": "what it does, it gives us a back the sign of a given value. So in this case, for example, if the amplitude is greater than zero sine function gives us back plus one. That's because the amplitude is positive. If the amplitude is negative, we get back minus one. If the amplitude is equal to zero, we get, we get back zero. Now how do we calculate the uh the zero crossing rate like for each pair of samples? Well, what we do is we take the sign for uh the amplitude at sample K. And then we subtract to that the sign for the amplitude at sample K plus one. So we are basically comparing the two consecutive samples here. Now, the cool thing about this is that if we have um both um aptitude values that have like the, the same sign, so if they're both plus or minus, what happens here is that we get out of this",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=850s",
        "start_time": "850.719"
    },
    {
        "id": "1114b48d",
        "text": "Now how do we calculate the uh the zero crossing rate like for each pair of samples? Well, what we do is we take the sign for uh the amplitude at sample K. And then we subtract to that the sign for the amplitude at sample K plus one. So we are basically comparing the two consecutive samples here. Now, the cool thing about this is that if we have um both um aptitude values that have like the, the same sign, so if they're both plus or minus, what happens here is that we get out of this just like zero. So we don't get like any information regarding, well, we don't get any value that's gonna increase the zero crossing rate. But if we have alternate opposite signs, so say for example, like at sample K, we have like a negative amplitude and at, at sample K plus one, we have a positive sample.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=876s",
        "start_time": "876.789"
    },
    {
        "id": "f9b36d44",
        "text": "samples here. Now, the cool thing about this is that if we have um both um aptitude values that have like the, the same sign, so if they're both plus or minus, what happens here is that we get out of this just like zero. So we don't get like any information regarding, well, we don't get any value that's gonna increase the zero crossing rate. But if we have alternate opposite signs, so say for example, like at sample K, we have like a negative amplitude and at, at sample K plus one, we have a positive sample. So what happens is that we get a value of two here and that's because yeah, let's just like take a look at this. So if we have like minus here, it means like that we have like minus one minus one minus plus one gives us like minus two, we have to take the absolute value of that. So that's equal to two. But now we are,",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=905s",
        "start_time": "905.255"
    },
    {
        "id": "0e779e6a",
        "text": "just like zero. So we don't get like any information regarding, well, we don't get any value that's gonna increase the zero crossing rate. But if we have alternate opposite signs, so say for example, like at sample K, we have like a negative amplitude and at, at sample K plus one, we have a positive sample. So what happens is that we get a value of two here and that's because yeah, let's just like take a look at this. So if we have like minus here, it means like that we have like minus one minus one minus plus one gives us like minus two, we have to take the absolute value of that. So that's equal to two. But now we are, so this, this is gonna give us information only for 10 crossing, not two. So we have to divide this two by two and it's given over here so that we're gonna have just like a value of one here. Now, what we do is we uh kind of like uh sum over all the samples that we have in a",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=924s",
        "start_time": "924.14"
    },
    {
        "id": "d87f9919",
        "text": "So what happens is that we get a value of two here and that's because yeah, let's just like take a look at this. So if we have like minus here, it means like that we have like minus one minus one minus plus one gives us like minus two, we have to take the absolute value of that. So that's equal to two. But now we are, so this, this is gonna give us information only for 10 crossing, not two. So we have to divide this two by two and it's given over here so that we're gonna have just like a value of one here. Now, what we do is we uh kind of like uh sum over all the samples that we have in a um in a frame. And so that we are gonna get like the values for all these zero crossings that we have in a frame. OK. So that's like a simple, like mathematical formula that's very elegant for defining zero crossing rate. Now, let's take a look at some applications. As I said, this is like uh like zero crossing rate is extensively used in speech recognition as well as like in music processing.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=948s",
        "start_time": "948.78"
    },
    {
        "id": "e12e7393",
        "text": "so this, this is gonna give us information only for 10 crossing, not two. So we have to divide this two by two and it's given over here so that we're gonna have just like a value of one here. Now, what we do is we uh kind of like uh sum over all the samples that we have in a um in a frame. And so that we are gonna get like the values for all these zero crossings that we have in a frame. OK. So that's like a simple, like mathematical formula that's very elegant for defining zero crossing rate. Now, let's take a look at some applications. As I said, this is like uh like zero crossing rate is extensively used in speech recognition as well as like in music processing. So we can use zero crossing rate for recognizing percussive versus like pitched sounds. And that's because percussive sounds usually tend to have like quite random like zero crossing rate. So like they tend to change like the zero crossing rate like quite a lot, whereas like pitch sounds tend to be like way more stable in the zero crossing rates. And",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=971s",
        "start_time": "971.69"
    },
    {
        "id": "45de5f7e",
        "text": "um in a frame. And so that we are gonna get like the values for all these zero crossings that we have in a frame. OK. So that's like a simple, like mathematical formula that's very elegant for defining zero crossing rate. Now, let's take a look at some applications. As I said, this is like uh like zero crossing rate is extensively used in speech recognition as well as like in music processing. So we can use zero crossing rate for recognizing percussive versus like pitched sounds. And that's because percussive sounds usually tend to have like quite random like zero crossing rate. So like they tend to change like the zero crossing rate like quite a lot, whereas like pitch sounds tend to be like way more stable in the zero crossing rates. And uh we can also use zero crossing rate as a very regiment uh like monophonic pitch estimation algorithm. In other words, there's a um kind of like relationship between like the number of zero crossings like and the pitch. And if we have like a monophonic pitch, basically the idea is that that the higher",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=994s",
        "start_time": "994.859"
    },
    {
        "id": "45d33c6d",
        "text": "So we can use zero crossing rate for recognizing percussive versus like pitched sounds. And that's because percussive sounds usually tend to have like quite random like zero crossing rate. So like they tend to change like the zero crossing rate like quite a lot, whereas like pitch sounds tend to be like way more stable in the zero crossing rates. And uh we can also use zero crossing rate as a very regiment uh like monophonic pitch estimation algorithm. In other words, there's a um kind of like relationship between like the number of zero crossings like and the pitch. And if we have like a monophonic pitch, basically the idea is that that the higher zero, the number of zero crossings that we have and the higher the pitch is gonna be. Now this is not bulletproof. And now we have like way more sophisticated uh like uh monophonic pitch estimators, but zero crossing rate still like is a, is a basic idea that we can use like to get a monophonic pitch estimation.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=1023s",
        "start_time": "1023.65"
    },
    {
        "id": "f27b9ae2",
        "text": "uh we can also use zero crossing rate as a very regiment uh like monophonic pitch estimation algorithm. In other words, there's a um kind of like relationship between like the number of zero crossings like and the pitch. And if we have like a monophonic pitch, basically the idea is that that the higher zero, the number of zero crossings that we have and the higher the pitch is gonna be. Now this is not bulletproof. And now we have like way more sophisticated uh like uh monophonic pitch estimators, but zero crossing rate still like is a, is a basic idea that we can use like to get a monophonic pitch estimation. And finally, if we want to look at the speech recognition uh uh like space, we can use zero crossing rate for distinguishing between signals which contain like voice and signals that are like usually that are unvoiced. And that's because uh when we are like dealing with like voice,",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=1045s",
        "start_time": "1045.54"
    },
    {
        "id": "457d81d9",
        "text": "zero, the number of zero crossings that we have and the higher the pitch is gonna be. Now this is not bulletproof. And now we have like way more sophisticated uh like uh monophonic pitch estimators, but zero crossing rate still like is a, is a basic idea that we can use like to get a monophonic pitch estimation. And finally, if we want to look at the speech recognition uh uh like space, we can use zero crossing rate for distinguishing between signals which contain like voice and signals that are like usually that are unvoiced. And that's because uh when we are like dealing with like voice, um voice signal, we usually have like a zero crossing rate like that is like lower than what we have like in unvoiced like pieces of signals. And that probably has to do with the fact that like those invoiced parts are like noisier. And yeah, and that's the case in the next few videos, we'll get our hands dirty and",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=1067s",
        "start_time": "1067.91"
    },
    {
        "id": "70fa4cd4",
        "text": "And finally, if we want to look at the speech recognition uh uh like space, we can use zero crossing rate for distinguishing between signals which contain like voice and signals that are like usually that are unvoiced. And that's because uh when we are like dealing with like voice, um voice signal, we usually have like a zero crossing rate like that is like lower than what we have like in unvoiced like pieces of signals. And that probably has to do with the fact that like those invoiced parts are like noisier. And yeah, and that's the case in the next few videos, we'll get our hands dirty and implement this time domain audio features and use sometimes Al li browser for extracting them directly from audio signals. But specifically for the next video will implement the amplitude envelope from scratch and then we visualize the amplitude envelope for pieces of music which come from different music genres",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=1090s",
        "start_time": "1090.459"
    },
    {
        "id": "5732a82a",
        "text": "um voice signal, we usually have like a zero crossing rate like that is like lower than what we have like in unvoiced like pieces of signals. And that probably has to do with the fact that like those invoiced parts are like noisier. And yeah, and that's the case in the next few videos, we'll get our hands dirty and implement this time domain audio features and use sometimes Al li browser for extracting them directly from audio signals. But specifically for the next video will implement the amplitude envelope from scratch and then we visualize the amplitude envelope for pieces of music which come from different music genres so that we can appreciate if there is any difference uh of amplitude envelope for different genres. OK. That's all for today. I hope you enjoyed this video. If that's the case, please remember to leave a like if you have any questions as usual, feel free to uh leave a comment in the comment section below and I'll see you next time. Cheers.",
        "video": "Understanding Time Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "SRrQ_v-OOSg",
        "youtube_link": "https://www.youtube.com/watch?v=SRrQ_v-OOSg&t=1112s",
        "start_time": "1112.91"
    },
    {
        "id": "80406de2",
        "text": "Hey everybody and welcome to a new exciting video and the audio signal processing for machine learning series. This time we'll be looking into mel spectrograms. This is a flavor of spectrograms that is extensively used in A I audio research and A I audio real world applications. But before we get started into mel spectrograms, I want to give you an overview of what we did over the last couple of videos and that is the short time fourier transform. And we also looked into extracting and visualizing spectrograms. Now, if you're not familiar with this concept, I highly suggest you to go check out my previous two videos, cos what I'm going to say next in the video builds on top of this two notions extensively. Now without further ado let's move on to this visualization of a spectrogram. And as you can see here on the X axis, we have time on the Y axis. As usual we have uh frequency and each point which is represented by a color in this two D representation uh tells us how present a certain frequency is at each point in time.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "b45b3e64",
        "text": "and that is the short time fourier transform. And we also looked into extracting and visualizing spectrograms. Now, if you're not familiar with this concept, I highly suggest you to go check out my previous two videos, cos what I'm going to say next in the video builds on top of this two notions extensively. Now without further ado let's move on to this visualization of a spectrogram. And as you can see here on the X axis, we have time on the Y axis. As usual we have uh frequency and each point which is represented by a color in this two D representation uh tells us how present a certain frequency is at each point in time. The thing that I want you to understand here is that the frequency representation with a normal vanilla spectrum is leaner and it uses Hertz. And this is kind of problematic in terms of like the way we perceive frequency. Now, I just, I don't just want to tell you that I want to show you that and to show you that",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=24s",
        "start_time": "24.7"
    },
    {
        "id": "ee958890",
        "text": "And as you can see here on the X axis, we have time on the Y axis. As usual we have uh frequency and each point which is represented by a color in this two D representation uh tells us how present a certain frequency is at each point in time. The thing that I want you to understand here is that the frequency representation with a normal vanilla spectrum is leaner and it uses Hertz. And this is kind of problematic in terms of like the way we perceive frequency. Now, I just, I don't just want to tell you that I want to show you that and to show you that I, we're gonna do like a little psychotic experiment. So we have a couple of audio samples. So each of these has a pair of notes. So in the first one, we have C two that moves to C four, C four being the middle C note. So the C note at the center of a piano keyboard. And the second,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=48s",
        "start_time": "48.63"
    },
    {
        "id": "14d646eb",
        "text": "The thing that I want you to understand here is that the frequency representation with a normal vanilla spectrum is leaner and it uses Hertz. And this is kind of problematic in terms of like the way we perceive frequency. Now, I just, I don't just want to tell you that I want to show you that and to show you that I, we're gonna do like a little psychotic experiment. So we have a couple of audio samples. So each of these has a pair of notes. So in the first one, we have C two that moves to C four, C four being the middle C note. So the C note at the center of a piano keyboard. And the second, for example, we have G six that goes up to A six. Now if you take a look at the difference in the frequency mapping or in other words, in the uh frequency expressed in Hertz between the two NS in the two paths, you'll notice that more or less like these two differences are the same or almost the same. And that is around 200 Hertz.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=66s",
        "start_time": "66.61"
    },
    {
        "id": "2c4a7793",
        "text": "I, we're gonna do like a little psychotic experiment. So we have a couple of audio samples. So each of these has a pair of notes. So in the first one, we have C two that moves to C four, C four being the middle C note. So the C note at the center of a piano keyboard. And the second, for example, we have G six that goes up to A six. Now if you take a look at the difference in the frequency mapping or in other words, in the uh frequency expressed in Hertz between the two NS in the two paths, you'll notice that more or less like these two differences are the same or almost the same. And that is around 200 Hertz. But will these two pair of notes like sounds the same in terms of like their pitch distance? So this is my question to you. OK. So let's listen to these two pairs of notes. The first one, C two to C four.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=87s",
        "start_time": "87.019"
    },
    {
        "id": "de1ff7de",
        "text": "for example, we have G six that goes up to A six. Now if you take a look at the difference in the frequency mapping or in other words, in the uh frequency expressed in Hertz between the two NS in the two paths, you'll notice that more or less like these two differences are the same or almost the same. And that is around 200 Hertz. But will these two pair of notes like sounds the same in terms of like their pitch distance? So this is my question to you. OK. So let's listen to these two pairs of notes. The first one, C two to C four. OK. The second one G six J A six.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=106s",
        "start_time": "106.205"
    },
    {
        "id": "c41eb151",
        "text": "But will these two pair of notes like sounds the same in terms of like their pitch distance? So this is my question to you. OK. So let's listen to these two pairs of notes. The first one, C two to C four. OK. The second one G six J A six. OK. I'm sure you'll agree with me that the second sample has the two notes that are way closer in pitch than the first two notes. And this is a little bit weird, isn't it? Right. So the difference in frequency as expressed in Hertz is actually the same. But that isn't true from a perceptual perspective because like the the second pair of notes",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=129s",
        "start_time": "129.429"
    },
    {
        "id": "20d42833",
        "text": "OK. The second one G six J A six. OK. I'm sure you'll agree with me that the second sample has the two notes that are way closer in pitch than the first two notes. And this is a little bit weird, isn't it? Right. So the difference in frequency as expressed in Hertz is actually the same. But that isn't true from a perceptual perspective because like the the second pair of notes uh sound way closer in pitch than the first two. This basically means that the way we perceive pitch is non linear and indeed, we have way better resolution at lower frequencies than we have at higher frequencies specifically. What happens is that humans perceive frequency longer rhythmically. And this is a,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=148s",
        "start_time": "148.339"
    },
    {
        "id": "2697db2c",
        "text": "OK. I'm sure you'll agree with me that the second sample has the two notes that are way closer in pitch than the first two notes. And this is a little bit weird, isn't it? Right. So the difference in frequency as expressed in Hertz is actually the same. But that isn't true from a perceptual perspective because like the the second pair of notes uh sound way closer in pitch than the first two. This basically means that the way we perceive pitch is non linear and indeed, we have way better resolution at lower frequencies than we have at higher frequencies specifically. What happens is that humans perceive frequency longer rhythmically. And this is a, a little bit of a problem that we have with vanilla spectrograms. And that's because as I said, frequency is expressed linearly in uh row like or vanilla spectrograms. And so we don't account for the way we human beings perceive it.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=154s",
        "start_time": "154.289"
    },
    {
        "id": "be443d09",
        "text": "uh sound way closer in pitch than the first two. This basically means that the way we perceive pitch is non linear and indeed, we have way better resolution at lower frequencies than we have at higher frequencies specifically. What happens is that humans perceive frequency longer rhythmically. And this is a, a little bit of a problem that we have with vanilla spectrograms. And that's because as I said, frequency is expressed linearly in uh row like or vanilla spectrograms. And so we don't account for the way we human beings perceive it. Now, let's think about an ideal audio feature that we would like to have and would make the most sense uh to fit to our own like machine learning and deep learning algorithms. Well, first of all, we want some kind of a time frequency representation because we want to know how the different frequencies in a signal evolve over time.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=182s",
        "start_time": "182.6"
    },
    {
        "id": "00690c04",
        "text": "a little bit of a problem that we have with vanilla spectrograms. And that's because as I said, frequency is expressed linearly in uh row like or vanilla spectrograms. And so we don't account for the way we human beings perceive it. Now, let's think about an ideal audio feature that we would like to have and would make the most sense uh to fit to our own like machine learning and deep learning algorithms. Well, first of all, we want some kind of a time frequency representation because we want to know how the different frequencies in a signal evolve over time. Then we want for this F to F to have perceptually relevant amplitude representation. And this once again is because amplitude and the way we perceive it is logarithmic N it's not uh linear, right. And both of these things we can achieve with vanilla spectrograms. And indeed, we have like a lot amplitude spectrograms as we say in the",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=207s",
        "start_time": "207.8"
    },
    {
        "id": "49466eda",
        "text": "Now, let's think about an ideal audio feature that we would like to have and would make the most sense uh to fit to our own like machine learning and deep learning algorithms. Well, first of all, we want some kind of a time frequency representation because we want to know how the different frequencies in a signal evolve over time. Then we want for this F to F to have perceptually relevant amplitude representation. And this once again is because amplitude and the way we perceive it is logarithmic N it's not uh linear, right. And both of these things we can achieve with vanilla spectrograms. And indeed, we have like a lot amplitude spectrograms as we say in the uh previous video. But what we can't achieve with vanilla spectrograms is the third aspect which is perceptually relevant frequency representation. And this is what male spectrograms are all about, right? And they check out all of these three interesting um conditions",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=224s",
        "start_time": "224.86"
    },
    {
        "id": "b9d68a5f",
        "text": "Then we want for this F to F to have perceptually relevant amplitude representation. And this once again is because amplitude and the way we perceive it is logarithmic N it's not uh linear, right. And both of these things we can achieve with vanilla spectrograms. And indeed, we have like a lot amplitude spectrograms as we say in the uh previous video. But what we can't achieve with vanilla spectrograms is the third aspect which is perceptually relevant frequency representation. And this is what male spectrograms are all about, right? And they check out all of these three interesting um conditions but now male spectrograms. So it's a term, it's a concept with two words, one is spectrograms and by now, you should be more than familiar with it. The other one is male. So what is this thing? What's a male? Right?",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=246s",
        "start_time": "246.74"
    },
    {
        "id": "51121333",
        "text": "uh previous video. But what we can't achieve with vanilla spectrograms is the third aspect which is perceptually relevant frequency representation. And this is what male spectrograms are all about, right? And they check out all of these three interesting um conditions but now male spectrograms. So it's a term, it's a concept with two words, one is spectrograms and by now, you should be more than familiar with it. The other one is male. So what is this thing? What's a male? Right? This concept is, is connected with the idea or, or concept of a male scale and the male scale is a perceptually relevant or perceptually informed scale for pitch.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=267s",
        "start_time": "267.88"
    },
    {
        "id": "0220ae6d",
        "text": "but now male spectrograms. So it's a term, it's a concept with two words, one is spectrograms and by now, you should be more than familiar with it. The other one is male. So what is this thing? What's a male? Right? This concept is, is connected with the idea or, or concept of a male scale and the male scale is a perceptually relevant or perceptually informed scale for pitch. And here we have like a nice representation. So I suggest you like to focus on this graph right now. So on the X axis, we have frequency expressed in Hertz on the Y axis, we have frequency expressed in melts and this blue curve is just the mapping between Hertz and melts. And as you can see, it follows a logarithmic uh kind of like function curve.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=290s",
        "start_time": "290.079"
    },
    {
        "id": "12998aaf",
        "text": "This concept is, is connected with the idea or, or concept of a male scale and the male scale is a perceptually relevant or perceptually informed scale for pitch. And here we have like a nice representation. So I suggest you like to focus on this graph right now. So on the X axis, we have frequency expressed in Hertz on the Y axis, we have frequency expressed in melts and this blue curve is just the mapping between Hertz and melts. And as you can see, it follows a logarithmic uh kind of like function curve. And indeed the males scale is a logarithmic in nature. And by being logarithmic, what it does is it becomes perceptually uh relevant, perceptually informed. And what does this mean? Really? Well, it means that equal distances on the scale have the same perceptual distance. OK. Let's try to, to understand this with an example. So let's say we have a couple of like pairs of notes. So the first one,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=305s",
        "start_time": "305.744"
    },
    {
        "id": "c08ce85d",
        "text": "And here we have like a nice representation. So I suggest you like to focus on this graph right now. So on the X axis, we have frequency expressed in Hertz on the Y axis, we have frequency expressed in melts and this blue curve is just the mapping between Hertz and melts. And as you can see, it follows a logarithmic uh kind of like function curve. And indeed the males scale is a logarithmic in nature. And by being logarithmic, what it does is it becomes perceptually uh relevant, perceptually informed. And what does this mean? Really? Well, it means that equal distances on the scale have the same perceptual distance. OK. Let's try to, to understand this with an example. So let's say we have a couple of like pairs of notes. So the first one, uh the first part is 500 Hertz and five sorry, 500 melts and 510 melts. The second one is 1000 melts and 1010 melts. Now, the perceptual difference between these two, the two notes in the two pairs is the same. So the pitch distance is gonna be the same And that's obviously not true here for Hertz as we demonstrated with our little experiment earlier. OK.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=321s",
        "start_time": "321.779"
    },
    {
        "id": "fb443654",
        "text": "And indeed the males scale is a logarithmic in nature. And by being logarithmic, what it does is it becomes perceptually uh relevant, perceptually informed. And what does this mean? Really? Well, it means that equal distances on the scale have the same perceptual distance. OK. Let's try to, to understand this with an example. So let's say we have a couple of like pairs of notes. So the first one, uh the first part is 500 Hertz and five sorry, 500 melts and 510 melts. The second one is 1000 melts and 1010 melts. Now, the perceptual difference between these two, the two notes in the two pairs is the same. So the pitch distance is gonna be the same And that's obviously not true here for Hertz as we demonstrated with our little experiment earlier. OK. Now, one thing that we did that like researchers did with the males scale is that they standardize it. So that at 1000 Hertz, we have 1000 Hertz, we have 1000 males. Now you may be wondering but how did we arrive at such a perceptual scale? Well, we arrived it in an empirical way doing psychological experiment.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=345s",
        "start_time": "345.39"
    },
    {
        "id": "9e13be23",
        "text": "uh the first part is 500 Hertz and five sorry, 500 melts and 510 melts. The second one is 1000 melts and 1010 melts. Now, the perceptual difference between these two, the two notes in the two pairs is the same. So the pitch distance is gonna be the same And that's obviously not true here for Hertz as we demonstrated with our little experiment earlier. OK. Now, one thing that we did that like researchers did with the males scale is that they standardize it. So that at 1000 Hertz, we have 1000 Hertz, we have 1000 males. Now you may be wondering but how did we arrive at such a perceptual scale? Well, we arrived it in an empirical way doing psychological experiment. Now what? And the last question you may have regarding like the the mouse scale is, where does the term like mel comes from? Well, Mel is an abbreviation for melody and melody, I mean, is very connected with the concept of pitch because pitch, I mean, is the main thing that you have in a melody along with rhythm, I would say and say like melody, Mel pitch. So you get the idea, right?",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=372s",
        "start_time": "372.64"
    },
    {
        "id": "19183453",
        "text": "Now, one thing that we did that like researchers did with the males scale is that they standardize it. So that at 1000 Hertz, we have 1000 Hertz, we have 1000 males. Now you may be wondering but how did we arrive at such a perceptual scale? Well, we arrived it in an empirical way doing psychological experiment. Now what? And the last question you may have regarding like the the mouse scale is, where does the term like mel comes from? Well, Mel is an abbreviation for melody and melody, I mean, is very connected with the concept of pitch because pitch, I mean, is the main thing that you have in a melody along with rhythm, I would say and say like melody, Mel pitch. So you get the idea, right? OK. But uh now let's take a look at how we can move from uh the frequency expressed in Hertz to the frequency expressed in melt. And it's this like empirical uh like formula here as it said, like found by trial and error with experiments. And the important thing here, apart from all of this like um constants is like the logarithm that we use. And indeed, we have like this logarithm logarithmic",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=402s",
        "start_time": "402.0"
    },
    {
        "id": "ffedeb72",
        "text": "Now what? And the last question you may have regarding like the the mouse scale is, where does the term like mel comes from? Well, Mel is an abbreviation for melody and melody, I mean, is very connected with the concept of pitch because pitch, I mean, is the main thing that you have in a melody along with rhythm, I would say and say like melody, Mel pitch. So you get the idea, right? OK. But uh now let's take a look at how we can move from uh the frequency expressed in Hertz to the frequency expressed in melt. And it's this like empirical uh like formula here as it said, like found by trial and error with experiments. And the important thing here, apart from all of this like um constants is like the logarithm that we use. And indeed, we have like this logarithm logarithmic kind of relationship. Now, we can take the inverse of this function. And with this, we're gonna be able to move from melts back to Hertz. Now, I want to just let you rem just like, yeah, keep in mind these two formulas because we're gonna be needing them like moving forward. OK.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=424s",
        "start_time": "424.13"
    },
    {
        "id": "bb9d5888",
        "text": "OK. But uh now let's take a look at how we can move from uh the frequency expressed in Hertz to the frequency expressed in melt. And it's this like empirical uh like formula here as it said, like found by trial and error with experiments. And the important thing here, apart from all of this like um constants is like the logarithm that we use. And indeed, we have like this logarithm logarithmic kind of relationship. Now, we can take the inverse of this function. And with this, we're gonna be able to move from melts back to Hertz. Now, I want to just let you rem just like, yeah, keep in mind these two formulas because we're gonna be needing them like moving forward. OK. So now you should have like a basic idea of the male scale and why it is important in terms of like the way we perceive um music or not music, sorry we perceive pitch in this case. OK. But uh how does this relate to the male spectrograms? OK.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=450s",
        "start_time": "450.579"
    },
    {
        "id": "84856f3c",
        "text": "kind of relationship. Now, we can take the inverse of this function. And with this, we're gonna be able to move from melts back to Hertz. Now, I want to just let you rem just like, yeah, keep in mind these two formulas because we're gonna be needing them like moving forward. OK. So now you should have like a basic idea of the male scale and why it is important in terms of like the way we perceive um music or not music, sorry we perceive pitch in this case. OK. But uh how does this relate to the male spectrograms? OK. And here I'll give you a kind of recipe for extracting mel spectrograms and this recipe has three steps. So the first thing that we do is we extract the short time for transform. And the second thing is to convert the amplitude to decibels. In other words, uh like we take like some kind of like",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=475s",
        "start_time": "475.829"
    },
    {
        "id": "7de057e4",
        "text": "So now you should have like a basic idea of the male scale and why it is important in terms of like the way we perceive um music or not music, sorry we perceive pitch in this case. OK. But uh how does this relate to the male spectrograms? OK. And here I'll give you a kind of recipe for extracting mel spectrograms and this recipe has three steps. So the first thing that we do is we extract the short time for transform. And the second thing is to convert the amplitude to decibels. In other words, uh like we take like some kind of like logarithmic representation of amplitude. Now, as I said, these two steps are like the ones that we use for vanilla spectrogram. So nothing new here. The new thing is the third step here which is converting frequencies to the melt scale.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=495s",
        "start_time": "495.45"
    },
    {
        "id": "8f3aab06",
        "text": "And here I'll give you a kind of recipe for extracting mel spectrograms and this recipe has three steps. So the first thing that we do is we extract the short time for transform. And the second thing is to convert the amplitude to decibels. In other words, uh like we take like some kind of like logarithmic representation of amplitude. Now, as I said, these two steps are like the ones that we use for vanilla spectrogram. So nothing new here. The new thing is the third step here which is converting frequencies to the melt scale. So this is like the whole point of male spectrograms. So we take a spectrogram and then we convert the frequencies in that spectrogram to the male frequency uh representation. OK. So the next question is how do we do that?",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=516s",
        "start_time": "516.94"
    },
    {
        "id": "078ee032",
        "text": "logarithmic representation of amplitude. Now, as I said, these two steps are like the ones that we use for vanilla spectrogram. So nothing new here. The new thing is the third step here which is converting frequencies to the melt scale. So this is like the whole point of male spectrograms. So we take a spectrogram and then we convert the frequencies in that spectrogram to the male frequency uh representation. OK. So the next question is how do we do that? Well, uh we for converting like this frequent to the male scale, we have a bunch of like steps. Um It's like these three steps here. So we choose the number of male bands. First, we compute the male filter banks. Now don't be scared about like this scary word, male filter banks because I'll show you what this",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=535s",
        "start_time": "535.77"
    },
    {
        "id": "df69345a",
        "text": "So this is like the whole point of male spectrograms. So we take a spectrogram and then we convert the frequencies in that spectrogram to the male frequency uh representation. OK. So the next question is how do we do that? Well, uh we for converting like this frequent to the male scale, we have a bunch of like steps. Um It's like these three steps here. So we choose the number of male bands. First, we compute the male filter banks. Now don't be scared about like this scary word, male filter banks because I'll show you what this like in a few minutes. And finally, we want to apply the male filter banks to the spectrograms. OK? And now I'm going to break down each of these steps for you so that it becomes like super clear how to do this. OK? So let's start with the first thing. So choosing the number of male bands. Well,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=554s",
        "start_time": "554.88"
    },
    {
        "id": "7a64a5dd",
        "text": "Well, uh we for converting like this frequent to the male scale, we have a bunch of like steps. Um It's like these three steps here. So we choose the number of male bands. First, we compute the male filter banks. Now don't be scared about like this scary word, male filter banks because I'll show you what this like in a few minutes. And finally, we want to apply the male filter banks to the spectrograms. OK? And now I'm going to break down each of these steps for you so that it becomes like super clear how to do this. OK? So let's start with the first thing. So choosing the number of male bands. Well, the first of all, so male bands. So when you're just like reading some research, say in a audio or music information retrieval and researchers are using uh male spectrograms, one fundamental parameter that you have is the number of male bands, right? And uh that is like a parameter that can vary. So the question is like, how many male bands like should you pick? Well,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=573s",
        "start_time": "573.729"
    },
    {
        "id": "fc5f951e",
        "text": "like in a few minutes. And finally, we want to apply the male filter banks to the spectrograms. OK? And now I'm going to break down each of these steps for you so that it becomes like super clear how to do this. OK? So let's start with the first thing. So choosing the number of male bands. Well, the first of all, so male bands. So when you're just like reading some research, say in a audio or music information retrieval and researchers are using uh male spectrograms, one fundamental parameter that you have is the number of male bands, right? And uh that is like a parameter that can vary. So the question is like, how many male bands like should you pick? Well, 40 it's totally fine. 60 is fine. Sometimes you find numbers like uh close to 90 or even 100 and 28. The reality is it really depends on the problem. The reason like one single answer to this question, it's a little bit when you ask. OK. So what's learning rate should you use? Well, there isn't just like one answer.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=594s",
        "start_time": "594.234"
    },
    {
        "id": "eee90a43",
        "text": "the first of all, so male bands. So when you're just like reading some research, say in a audio or music information retrieval and researchers are using uh male spectrograms, one fundamental parameter that you have is the number of male bands, right? And uh that is like a parameter that can vary. So the question is like, how many male bands like should you pick? Well, 40 it's totally fine. 60 is fine. Sometimes you find numbers like uh close to 90 or even 100 and 28. The reality is it really depends on the problem. The reason like one single answer to this question, it's a little bit when you ask. OK. So what's learning rate should you use? Well, there isn't just like one answer. You have to try out different stuff and see what works best. And indeed, the number of male bands is a hyper parameter that you can vary and see the impact that it has on your algorithms and your deep learning algorithms. But all in all you see that usually like the the numbers that we have is between I would say like 40 100 and 20 100 and 30 like maximum. And",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=615s",
        "start_time": "615.7"
    },
    {
        "id": "22753ad7",
        "text": "40 it's totally fine. 60 is fine. Sometimes you find numbers like uh close to 90 or even 100 and 28. The reality is it really depends on the problem. The reason like one single answer to this question, it's a little bit when you ask. OK. So what's learning rate should you use? Well, there isn't just like one answer. You have to try out different stuff and see what works best. And indeed, the number of male bands is a hyper parameter that you can vary and see the impact that it has on your algorithms and your deep learning algorithms. But all in all you see that usually like the the numbers that we have is between I would say like 40 100 and 20 100 and 30 like maximum. And I mean, one thing like that's uh a little heuristic here is that if you think about the number of notes, for example, that we have on a piano keyboard, uh that is like 88 right? Um That is kind of like the way like we tend to like resolve like frequencies and we represent them. Now, the idea of a male band is like kind of",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=643s",
        "start_time": "643.669"
    },
    {
        "id": "c342ae8f",
        "text": "You have to try out different stuff and see what works best. And indeed, the number of male bands is a hyper parameter that you can vary and see the impact that it has on your algorithms and your deep learning algorithms. But all in all you see that usually like the the numbers that we have is between I would say like 40 100 and 20 100 and 30 like maximum. And I mean, one thing like that's uh a little heuristic here is that if you think about the number of notes, for example, that we have on a piano keyboard, uh that is like 88 right? Um That is kind of like the way like we tend to like resolve like frequencies and we represent them. Now, the idea of a male band is like kind of like a a range of frequencies like that are like perceptually relevant. So I would expect like that this number like makes sense because like they kind of look close to like the notes that we usually experience like for example, like on, on our like western uh music. So these types of numbers like make sense for that because they are comparable, they have like the same order of magnitude as the number of notes that we usually experience. OK,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=668s",
        "start_time": "668.489"
    },
    {
        "id": "1e555dbb",
        "text": "I mean, one thing like that's uh a little heuristic here is that if you think about the number of notes, for example, that we have on a piano keyboard, uh that is like 88 right? Um That is kind of like the way like we tend to like resolve like frequencies and we represent them. Now, the idea of a male band is like kind of like a a range of frequencies like that are like perceptually relevant. So I would expect like that this number like makes sense because like they kind of look close to like the notes that we usually experience like for example, like on, on our like western uh music. So these types of numbers like make sense for that because they are comparable, they have like the same order of magnitude as the number of notes that we usually experience. OK, good. So now we know uh like what numbers of male banks like to choose from? More or less. Now, the next step is to actually construct the male filter banks. And here we like,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=695s",
        "start_time": "695.01"
    },
    {
        "id": "ee22ebc4",
        "text": "like a a range of frequencies like that are like perceptually relevant. So I would expect like that this number like makes sense because like they kind of look close to like the notes that we usually experience like for example, like on, on our like western uh music. So these types of numbers like make sense for that because they are comparable, they have like the same order of magnitude as the number of notes that we usually experience. OK, good. So now we know uh like what numbers of male banks like to choose from? More or less. Now, the next step is to actually construct the male filter banks. And here we like, understand what these male bands are. OK. So how do we construct the male filter bands? Well, this is a kind of like a quite complex process or it's not really that complex. It's more like it's a multi step process. It has like five different steps. And so I'm gonna break down all of these five steps for you. I'm not gonna get too much into",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=715s",
        "start_time": "715.479"
    },
    {
        "id": "e486e568",
        "text": "good. So now we know uh like what numbers of male banks like to choose from? More or less. Now, the next step is to actually construct the male filter banks. And here we like, understand what these male bands are. OK. So how do we construct the male filter bands? Well, this is a kind of like a quite complex process or it's not really that complex. It's more like it's a multi step process. It has like five different steps. And so I'm gonna break down all of these five steps for you. I'm not gonna get too much into sorry the math here. Nor am I gonna like implement it. But I highly suggest you to try to implement all of these steps that I'm just gonna uh talk about in a theoretical way because that is a very good exercise to see whether like you understand all of these steps uh precisely. OK. So how do we build like this male filter banks? Well,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=743s",
        "start_time": "743.409"
    },
    {
        "id": "05a11e6a",
        "text": "understand what these male bands are. OK. So how do we construct the male filter bands? Well, this is a kind of like a quite complex process or it's not really that complex. It's more like it's a multi step process. It has like five different steps. And so I'm gonna break down all of these five steps for you. I'm not gonna get too much into sorry the math here. Nor am I gonna like implement it. But I highly suggest you to try to implement all of these steps that I'm just gonna uh talk about in a theoretical way because that is a very good exercise to see whether like you understand all of these steps uh precisely. OK. So how do we build like this male filter banks? Well, first of all, we take a lowest and highest frequency like in the frequency range and like in the short time four transform that we've just kind of like extracted and then we convert the lowest and highest frequency to a male representation. How do we do that? Well, we use like this little formula that we already encountered before when talking about the male scale.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=759s",
        "start_time": "759.914"
    },
    {
        "id": "7e05b6da",
        "text": "sorry the math here. Nor am I gonna like implement it. But I highly suggest you to try to implement all of these steps that I'm just gonna uh talk about in a theoretical way because that is a very good exercise to see whether like you understand all of these steps uh precisely. OK. So how do we build like this male filter banks? Well, first of all, we take a lowest and highest frequency like in the frequency range and like in the short time four transform that we've just kind of like extracted and then we convert the lowest and highest frequency to a male representation. How do we do that? Well, we use like this little formula that we already encountered before when talking about the male scale. OK. So now we have like the lowest and highest frequency that we want to consider expressed in melts next step. So now uh given like we've already chosen the number of mel bands that we want to use like for our mel spectrogram. The next step is to take like the um",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=781s",
        "start_time": "781.64"
    },
    {
        "id": "a6003027",
        "text": "first of all, we take a lowest and highest frequency like in the frequency range and like in the short time four transform that we've just kind of like extracted and then we convert the lowest and highest frequency to a male representation. How do we do that? Well, we use like this little formula that we already encountered before when talking about the male scale. OK. So now we have like the lowest and highest frequency that we want to consider expressed in melts next step. So now uh given like we've already chosen the number of mel bands that we want to use like for our mel spectrogram. The next step is to take like the um the frequency range in terms of like male, the lowest mail, like over here, the highest male here. And then we want to create as many uh points, equally spaced points as the number of male bands that we want to use.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=805s",
        "start_time": "805.969"
    },
    {
        "id": "57d71cd0",
        "text": "OK. So now we have like the lowest and highest frequency that we want to consider expressed in melts next step. So now uh given like we've already chosen the number of mel bands that we want to use like for our mel spectrogram. The next step is to take like the um the frequency range in terms of like male, the lowest mail, like over here, the highest male here. And then we want to create as many uh points, equally spaced points as the number of male bands that we want to use. In other words. So this is gonna be like very simple, like to visualize. Uh So let's say like we have like six mel bands, we want to use six mel bands. So what we'll do uh if this is like the lowest uh male frequency and this is the highest male frequency, we'll just like take this point here. So we'll just create this six points and they'll be equally spaced in this uh frequency uh range here. OK. Now,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=835s",
        "start_time": "835.21"
    },
    {
        "id": "9b80f6da",
        "text": "the frequency range in terms of like male, the lowest mail, like over here, the highest male here. And then we want to create as many uh points, equally spaced points as the number of male bands that we want to use. In other words. So this is gonna be like very simple, like to visualize. Uh So let's say like we have like six mel bands, we want to use six mel bands. So what we'll do uh if this is like the lowest uh male frequency and this is the highest male frequency, we'll just like take this point here. So we'll just create this six points and they'll be equally spaced in this uh frequency uh range here. OK. Now, what we are going to do next is quite simple really is basically just like converting this points back to Hertz by using this function here that we found when we were talking about the uh male scale. Now, if you're wondering about this",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=859s",
        "start_time": "859.82"
    },
    {
        "id": "5f380fd4",
        "text": "In other words. So this is gonna be like very simple, like to visualize. Uh So let's say like we have like six mel bands, we want to use six mel bands. So what we'll do uh if this is like the lowest uh male frequency and this is the highest male frequency, we'll just like take this point here. So we'll just create this six points and they'll be equally spaced in this uh frequency uh range here. OK. Now, what we are going to do next is quite simple really is basically just like converting this points back to Hertz by using this function here that we found when we were talking about the uh male scale. Now, if you're wondering about this melt point, this points, so what are they? Well, they are the center of the different mel bands. OK. They are the center frequency of the different mel bands that we are talking about here. OK. And we'll see this like uh with a visualization in a couple of moments. OK.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=876s",
        "start_time": "876.645"
    },
    {
        "id": "d3e2835f",
        "text": "what we are going to do next is quite simple really is basically just like converting this points back to Hertz by using this function here that we found when we were talking about the uh male scale. Now, if you're wondering about this melt point, this points, so what are they? Well, they are the center of the different mel bands. OK. They are the center frequency of the different mel bands that we are talking about here. OK. And we'll see this like uh with a visualization in a couple of moments. OK. Next step. So we've now converted back like this uh center frequency points where are like Melbournes back to Hertz. And now what we want to do is we want to round this",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=904s",
        "start_time": "904.46"
    },
    {
        "id": "490179c7",
        "text": "melt point, this points, so what are they? Well, they are the center of the different mel bands. OK. They are the center frequency of the different mel bands that we are talking about here. OK. And we'll see this like uh with a visualization in a couple of moments. OK. Next step. So we've now converted back like this uh center frequency points where are like Melbournes back to Hertz. And now what we want to do is we want to round this points to the nearest frequency bin. So why is this like important we say, why do we need to do that? Well, it's because we are dealing with uh discrete uh signals. And that means that we don't have like infinite perfect resolution like on the frequency and actually like a resolution is somewhat like constrained by the frame size,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=923s",
        "start_time": "923.76"
    },
    {
        "id": "b2eccedc",
        "text": "Next step. So we've now converted back like this uh center frequency points where are like Melbournes back to Hertz. And now what we want to do is we want to round this points to the nearest frequency bin. So why is this like important we say, why do we need to do that? Well, it's because we are dealing with uh discrete uh signals. And that means that we don't have like infinite perfect resolution like on the frequency and actually like a resolution is somewhat like constrained by the frame size, the short time fourier transform. So what that basically means is that when we'll get back to uh the this like center uh frequency uh points, we we can't just like take them like as they are, we need to like bin them and we need to just like round them to the nearest like frequency bin that we have available, right?",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=945s",
        "start_time": "945.119"
    },
    {
        "id": "842878e5",
        "text": "points to the nearest frequency bin. So why is this like important we say, why do we need to do that? Well, it's because we are dealing with uh discrete uh signals. And that means that we don't have like infinite perfect resolution like on the frequency and actually like a resolution is somewhat like constrained by the frame size, the short time fourier transform. So what that basically means is that when we'll get back to uh the this like center uh frequency uh points, we we can't just like take them like as they are, we need to like bin them and we need to just like round them to the nearest like frequency bin that we have available, right? And finally, we have the, the, the last step which probably is the most important, which is like creating triangular filters. And this triangular filters are basically are the the kind of like the building blocks of a male filter bank. And they are connected with this idea of the different like male bands. So I'll visual visualize",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=958s",
        "start_time": "958.919"
    },
    {
        "id": "86c3a890",
        "text": "the short time fourier transform. So what that basically means is that when we'll get back to uh the this like center uh frequency uh points, we we can't just like take them like as they are, we need to like bin them and we need to just like round them to the nearest like frequency bin that we have available, right? And finally, we have the, the, the last step which probably is the most important, which is like creating triangular filters. And this triangular filters are basically are the the kind of like the building blocks of a male filter bank. And they are connected with this idea of the different like male bands. So I'll visual visualize what these things are. And then I'll explain how we get like this visualization here. So don't be scared about like all of this complexity because it's way easier than it looks like. OK. So here on the X axis, we have the frequency in the bottom is expressed in huts in the top. Here is expressed in Mels. And here on the Y axis, we have weight and weight is between one and zero.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=980s",
        "start_time": "980.59"
    },
    {
        "id": "26fc0225",
        "text": "And finally, we have the, the, the last step which probably is the most important, which is like creating triangular filters. And this triangular filters are basically are the the kind of like the building blocks of a male filter bank. And they are connected with this idea of the different like male bands. So I'll visual visualize what these things are. And then I'll explain how we get like this visualization here. So don't be scared about like all of this complexity because it's way easier than it looks like. OK. So here on the X axis, we have the frequency in the bottom is expressed in huts in the top. Here is expressed in Mels. And here on the Y axis, we have weight and weight is between one and zero. Mm Why do we have a weight there? Well, because these are filters, basically what they do is they just like tend to filter sound and when you have a weight equal to one, you're not touching like that uh signal. But uh with a weight below one, what you are doing is just like ding down kind of like dumping the um the uh the signal, right? Because you're scaling it with a value that's below one, right?",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1002s",
        "start_time": "1002.559"
    },
    {
        "id": "202c8850",
        "text": "what these things are. And then I'll explain how we get like this visualization here. So don't be scared about like all of this complexity because it's way easier than it looks like. OK. So here on the X axis, we have the frequency in the bottom is expressed in huts in the top. Here is expressed in Mels. And here on the Y axis, we have weight and weight is between one and zero. Mm Why do we have a weight there? Well, because these are filters, basically what they do is they just like tend to filter sound and when you have a weight equal to one, you're not touching like that uh signal. But uh with a weight below one, what you are doing is just like ding down kind of like dumping the um the uh the signal, right? Because you're scaling it with a value that's below one, right? OK. So now, as you can see here, we, we have an example which like six mel bands. OK. And here indeed, you have like six",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1025s",
        "start_time": "1025.41"
    },
    {
        "id": "1ab0468f",
        "text": "Mm Why do we have a weight there? Well, because these are filters, basically what they do is they just like tend to filter sound and when you have a weight equal to one, you're not touching like that uh signal. But uh with a weight below one, what you are doing is just like ding down kind of like dumping the um the uh the signal, right? Because you're scaling it with a value that's below one, right? OK. So now, as you can see here, we, we have an example which like six mel bands. OK. And here indeed, you have like six points like overall and so these are the center frequencies for our six male bands. OK. And so uh let's take, for example, like this second point here. So this is, let's say like it's at 2000 Hertz which is 1526 males. OK. So what you can notice here is that's the difference between",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1051s",
        "start_time": "1051.239"
    },
    {
        "id": "cbc3f13b",
        "text": "OK. So now, as you can see here, we, we have an example which like six mel bands. OK. And here indeed, you have like six points like overall and so these are the center frequencies for our six male bands. OK. And so uh let's take, for example, like this second point here. So this is, let's say like it's at 2000 Hertz which is 1526 males. OK. So what you can notice here is that's the difference between uh all the, the center points for the male bands. I is the same uh when it's calculated in male. So we basically have always like the same difference between like two subsequent",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1081s",
        "start_time": "1081.4"
    },
    {
        "id": "a6af68c9",
        "text": "points like overall and so these are the center frequencies for our six male bands. OK. And so uh let's take, for example, like this second point here. So this is, let's say like it's at 2000 Hertz which is 1526 males. OK. So what you can notice here is that's the difference between uh all the, the center points for the male bands. I is the same uh when it's calculated in male. So we basically have always like the same difference between like two subsequent um two subsequent like center frequency uh points. But this is not true in the case of Hertz. And that's the whole point, right? That's the whole point because uh which are like frequency expressed in Hertz. When we go like towards higher frequencies,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1092s",
        "start_time": "1092.42"
    },
    {
        "id": "807f76cb",
        "text": "uh all the, the center points for the male bands. I is the same uh when it's calculated in male. So we basically have always like the same difference between like two subsequent um two subsequent like center frequency uh points. But this is not true in the case of Hertz. And that's the whole point, right? That's the whole point because uh which are like frequency expressed in Hertz. When we go like towards higher frequencies, we have to just like spread out the frequencies uh to have like the same perceptual uh difference in terms of like a frequency distances. OK. But now let's focus only like on constructing a single uh triangular filter for one male band. OK. So let's stick like this point. Uh um male bench number two here. So",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1122s",
        "start_time": "1122.64"
    },
    {
        "id": "20b9cf14",
        "text": "um two subsequent like center frequency uh points. But this is not true in the case of Hertz. And that's the whole point, right? That's the whole point because uh which are like frequency expressed in Hertz. When we go like towards higher frequencies, we have to just like spread out the frequencies uh to have like the same perceptual uh difference in terms of like a frequency distances. OK. But now let's focus only like on constructing a single uh triangular filter for one male band. OK. So let's stick like this point. Uh um male bench number two here. So the center of this male band, as we said is 1000 or 1526 mels and then the lower end and the higher end uh so are taken by respectively taking the center",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1137s",
        "start_time": "1137.91"
    },
    {
        "id": "2b08b6e0",
        "text": "we have to just like spread out the frequencies uh to have like the same perceptual uh difference in terms of like a frequency distances. OK. But now let's focus only like on constructing a single uh triangular filter for one male band. OK. So let's stick like this point. Uh um male bench number two here. So the center of this male band, as we said is 1000 or 1526 mels and then the lower end and the higher end uh so are taken by respectively taking the center of the previous male band and the center frequency of the subsequent male band. And for the lower end and the higher end of a male band, we have a weight which is like equal to zero. So in other words, like uh below this uh frequency, the lower end frequency",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1157s",
        "start_time": "1157.824"
    },
    {
        "id": "7c243f51",
        "text": "the center of this male band, as we said is 1000 or 1526 mels and then the lower end and the higher end uh so are taken by respectively taking the center of the previous male band and the center frequency of the subsequent male band. And for the lower end and the higher end of a male band, we have a weight which is like equal to zero. So in other words, like uh below this uh frequency, the lower end frequency uh like we are not gonna have so the, the signal is completely muted, it put to zero, same thing here for the frequencies above the higher range uh here. OK.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1185s",
        "start_time": "1185.0"
    },
    {
        "id": "2d9dcb16",
        "text": "of the previous male band and the center frequency of the subsequent male band. And for the lower end and the higher end of a male band, we have a weight which is like equal to zero. So in other words, like uh below this uh frequency, the lower end frequency uh like we are not gonna have so the, the signal is completely muted, it put to zero, same thing here for the frequencies above the higher range uh here. OK. And now what we do is we kind of like trace a line between the lower and the higher end and we connect that with the center frequency where the weight is equal to one. And so if we do that, we actually build a triangular",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1204s",
        "start_time": "1204.489"
    },
    {
        "id": "28f47121",
        "text": "uh like we are not gonna have so the, the signal is completely muted, it put to zero, same thing here for the frequencies above the higher range uh here. OK. And now what we do is we kind of like trace a line between the lower and the higher end and we connect that with the center frequency where the weight is equal to one. And so if we do that, we actually build a triangular filter like this. Now, if you do the same thing uh again, for the third Melba band, 4th, 5th and 6th, well, you just come up with the whole male filter bank, right? And these are triangular filters. OK? And so I hope lucky you now have like an understanding of like what a male filter bank is and the whole purpose of this beast is that then we can just like",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1225s",
        "start_time": "1225.42"
    },
    {
        "id": "9c285084",
        "text": "And now what we do is we kind of like trace a line between the lower and the higher end and we connect that with the center frequency where the weight is equal to one. And so if we do that, we actually build a triangular filter like this. Now, if you do the same thing uh again, for the third Melba band, 4th, 5th and 6th, well, you just come up with the whole male filter bank, right? And these are triangular filters. OK? And so I hope lucky you now have like an understanding of like what a male filter bank is and the whole purpose of this beast is that then we can just like apply this to uh like a normal spectrogram. And then we can filter out like the different frequencies and convert them like to normal frequencies in huts to frequencies expressed in males. OK? But here we have just like a visualization of a male filter bank but",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1238s",
        "start_time": "1238.55"
    },
    {
        "id": "8c18c8e9",
        "text": "filter like this. Now, if you do the same thing uh again, for the third Melba band, 4th, 5th and 6th, well, you just come up with the whole male filter bank, right? And these are triangular filters. OK? And so I hope lucky you now have like an understanding of like what a male filter bank is and the whole purpose of this beast is that then we can just like apply this to uh like a normal spectrogram. And then we can filter out like the different frequencies and convert them like to normal frequencies in huts to frequencies expressed in males. OK? But here we have just like a visualization of a male filter bank but uh obviously like in digital signal processing as well as like in machine learning. We we don't do operations with visualizations. We we actually use math and linear algebra uh like most of the time, right? And so what this means is that we can represent this male filter bank using metrics or a two dimensional array and the shape of these metrics",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1257s",
        "start_time": "1257.91"
    },
    {
        "id": "18e5eef6",
        "text": "apply this to uh like a normal spectrogram. And then we can filter out like the different frequencies and convert them like to normal frequencies in huts to frequencies expressed in males. OK? But here we have just like a visualization of a male filter bank but uh obviously like in digital signal processing as well as like in machine learning. We we don't do operations with visualizations. We we actually use math and linear algebra uh like most of the time, right? And so what this means is that we can represent this male filter bank using metrics or a two dimensional array and the shape of these metrics uh is gonna be like this. So like the number of rows that we're gonna have like in a male filter band is equal to the number of bands that we chose. In our example, it's gonna be equal to six, so six rows and then the uh number of columns that we have is equal to the ini frequency or in other words, frame size divided by two plus one.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1287s",
        "start_time": "1287.989"
    },
    {
        "id": "62861848",
        "text": "uh obviously like in digital signal processing as well as like in machine learning. We we don't do operations with visualizations. We we actually use math and linear algebra uh like most of the time, right? And so what this means is that we can represent this male filter bank using metrics or a two dimensional array and the shape of these metrics uh is gonna be like this. So like the number of rows that we're gonna have like in a male filter band is equal to the number of bands that we chose. In our example, it's gonna be equal to six, so six rows and then the uh number of columns that we have is equal to the ini frequency or in other words, frame size divided by two plus one. And so this is like the um metrics that we'll be using for representing a male filter bank. OK. So I hope you now have an understanding like of this male filter bank. And then now with all of this information, we need to go like to the final step in the conversion from frequencies to males scale for our spectrogram. In other words, we are actually moving from the spectrogram to the male spectrograms.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1312s",
        "start_time": "1312.63"
    },
    {
        "id": "a8f6813a",
        "text": "uh is gonna be like this. So like the number of rows that we're gonna have like in a male filter band is equal to the number of bands that we chose. In our example, it's gonna be equal to six, so six rows and then the uh number of columns that we have is equal to the ini frequency or in other words, frame size divided by two plus one. And so this is like the um metrics that we'll be using for representing a male filter bank. OK. So I hope you now have an understanding like of this male filter bank. And then now with all of this information, we need to go like to the final step in the conversion from frequencies to males scale for our spectrogram. In other words, we are actually moving from the spectrogram to the male spectrograms. And how do we do that? Well, we have to apply the male filter banks to the spectrogram.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1338s",
        "start_time": "1338.9"
    },
    {
        "id": "c536b267",
        "text": "And so this is like the um metrics that we'll be using for representing a male filter bank. OK. So I hope you now have an understanding like of this male filter bank. And then now with all of this information, we need to go like to the final step in the conversion from frequencies to males scale for our spectrogram. In other words, we are actually moving from the spectrogram to the male spectrograms. And how do we do that? Well, we have to apply the male filter banks to the spectrogram. And how do we do that? And here linear algebra can help us quite a lot. OK. So here we have like the, the shape of the male filter bank uh matrix. And now here we have the shape of another matrix and this is the matrix associated to a spectrogram. So if you follow it along like uh my last couple of videos, you know,",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1361s",
        "start_time": "1361.229"
    },
    {
        "id": "96881996",
        "text": "And how do we do that? Well, we have to apply the male filter banks to the spectrogram. And how do we do that? And here linear algebra can help us quite a lot. OK. So here we have like the, the shape of the male filter bank uh matrix. And now here we have the shape of another matrix and this is the matrix associated to a spectrogram. So if you follow it along like uh my last couple of videos, you know, now that the the shape of a spectrogram, uh the shape of a matrix that represents the spectrogram is given like by these numbers here. So the number of columns that we have sorry, the number of rows that we have in the spectrogram is equal to the unique frequency uh plus one and the number of columns is given by the number of frames or in other words like the temporal bins. Now.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1391s",
        "start_time": "1391.349"
    },
    {
        "id": "7e98b3c6",
        "text": "And how do we do that? And here linear algebra can help us quite a lot. OK. So here we have like the, the shape of the male filter bank uh matrix. And now here we have the shape of another matrix and this is the matrix associated to a spectrogram. So if you follow it along like uh my last couple of videos, you know, now that the the shape of a spectrogram, uh the shape of a matrix that represents the spectrogram is given like by these numbers here. So the number of columns that we have sorry, the number of rows that we have in the spectrogram is equal to the unique frequency uh plus one and the number of columns is given by the number of frames or in other words like the temporal bins. Now. So we, we said that the whole point here is to apply male filter banks to spectrum. What does that mean? So that must mean something in a mathematical way. Well, if you're familiar with linear algebra, you've probably noticed something interesting about these two matrices. And that's that the",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1398s",
        "start_time": "1398.91"
    },
    {
        "id": "0b6c559b",
        "text": "now that the the shape of a spectrogram, uh the shape of a matrix that represents the spectrogram is given like by these numbers here. So the number of columns that we have sorry, the number of rows that we have in the spectrogram is equal to the unique frequency uh plus one and the number of columns is given by the number of frames or in other words like the temporal bins. Now. So we, we said that the whole point here is to apply male filter banks to spectrum. What does that mean? So that must mean something in a mathematical way. Well, if you're familiar with linear algebra, you've probably noticed something interesting about these two matrices. And that's that the number of columns of the first matrix or the male filter bank uh matrix is equal to the number of rows of the um matrix that represents uh the spectrogram. So when this happens, it means that we can do apply like matrix multiplication. OK.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1422s",
        "start_time": "1422.964"
    },
    {
        "id": "5f527c9f",
        "text": "So we, we said that the whole point here is to apply male filter banks to spectrum. What does that mean? So that must mean something in a mathematical way. Well, if you're familiar with linear algebra, you've probably noticed something interesting about these two matrices. And that's that the number of columns of the first matrix or the male filter bank uh matrix is equal to the number of rows of the um matrix that represents uh the spectrogram. So when this happens, it means that we can do apply like matrix multiplication. OK. So in other words, applying male filter banks to spectrograms from mathematical standpoint means multiplying doing like matrix multiplication between the uh male filter banks and the spectrogram. The result of this multiplication",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1448s",
        "start_time": "1448.56"
    },
    {
        "id": "1c706b1a",
        "text": "number of columns of the first matrix or the male filter bank uh matrix is equal to the number of rows of the um matrix that represents uh the spectrogram. So when this happens, it means that we can do apply like matrix multiplication. OK. So in other words, applying male filter banks to spectrograms from mathematical standpoint means multiplying doing like matrix multiplication between the uh male filter banks and the spectrogram. The result of this multiplication is the male spectrogram. Now, I'm not gonna get into the details of this matrix multiplication because it's outside the scope of this series. But if you're interested and you don't know that much about linear algebra, I have a video over here that you can check out where I talk about uh basic operations with uh like matrices. And, and one of those is actually matrix multiplication.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1469s",
        "start_time": "1469.3"
    },
    {
        "id": "f91249cb",
        "text": "So in other words, applying male filter banks to spectrograms from mathematical standpoint means multiplying doing like matrix multiplication between the uh male filter banks and the spectrogram. The result of this multiplication is the male spectrogram. Now, I'm not gonna get into the details of this matrix multiplication because it's outside the scope of this series. But if you're interested and you don't know that much about linear algebra, I have a video over here that you can check out where I talk about uh basic operations with uh like matrices. And, and one of those is actually matrix multiplication. OK. So now uh let's take a look at the mel spectrogram. So the mel spectrogram is a metric, a metric itself and its shape is given by the number of bands. So we have like the as many number of as many rows as the number of bands that we chose and uh on, on the number of columns that we have is equal to the number of frames of the original spectrograms.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1493s",
        "start_time": "1493.199"
    },
    {
        "id": "ee37aa42",
        "text": "is the male spectrogram. Now, I'm not gonna get into the details of this matrix multiplication because it's outside the scope of this series. But if you're interested and you don't know that much about linear algebra, I have a video over here that you can check out where I talk about uh basic operations with uh like matrices. And, and one of those is actually matrix multiplication. OK. So now uh let's take a look at the mel spectrogram. So the mel spectrogram is a metric, a metric itself and its shape is given by the number of bands. So we have like the as many number of as many rows as the number of bands that we chose and uh on, on the number of columns that we have is equal to the number of frames of the original spectrograms. OK. So in other words, if you think about this like this uh multiplication here, so applying male filter banks to the spectrogram, what enabled us to do is basically to convert the frequencies just from Hertz to",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1513s",
        "start_time": "1513.53"
    },
    {
        "id": "ef41ab40",
        "text": "OK. So now uh let's take a look at the mel spectrogram. So the mel spectrogram is a metric, a metric itself and its shape is given by the number of bands. So we have like the as many number of as many rows as the number of bands that we chose and uh on, on the number of columns that we have is equal to the number of frames of the original spectrograms. OK. So in other words, if you think about this like this uh multiplication here, so applying male filter banks to the spectrogram, what enabled us to do is basically to convert the frequencies just from Hertz to male bands. OK. And that's the whole point of a male spectrogram. And the cool thing now is that we are, we are using uh male bands which are kind of psychologically uh relevant in terms of the way we perceive pitch. OK. But now let's take a look at what this means in terms of like a visual representation. And well, if you're familiar with spectrograms, well, mouse spectrograms are no different at all.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1539s",
        "start_time": "1539.89"
    },
    {
        "id": "0b178f5b",
        "text": "OK. So in other words, if you think about this like this uh multiplication here, so applying male filter banks to the spectrogram, what enabled us to do is basically to convert the frequencies just from Hertz to male bands. OK. And that's the whole point of a male spectrogram. And the cool thing now is that we are, we are using uh male bands which are kind of psychologically uh relevant in terms of the way we perceive pitch. OK. But now let's take a look at what this means in terms of like a visual representation. And well, if you're familiar with spectrograms, well, mouse spectrograms are no different at all. Uh So you still have a uh hit map. And that's because we have like a",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1567s",
        "start_time": "1567.989"
    },
    {
        "id": "2aedb8d8",
        "text": "male bands. OK. And that's the whole point of a male spectrogram. And the cool thing now is that we are, we are using uh male bands which are kind of psychologically uh relevant in terms of the way we perceive pitch. OK. But now let's take a look at what this means in terms of like a visual representation. And well, if you're familiar with spectrograms, well, mouse spectrograms are no different at all. Uh So you still have a uh hit map. And that's because we have like a uh a matrix, right? And on the X axis, we have like time on the Y axis. Once again, we have frequency. But this time like the different frequency bins are not uh like the linear bins that we use with the spectrum, but rather the",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1585s",
        "start_time": "1585.79"
    },
    {
        "id": "78b27272",
        "text": "Uh So you still have a uh hit map. And that's because we have like a uh a matrix, right? And on the X axis, we have like time on the Y axis. Once again, we have frequency. But this time like the different frequency bins are not uh like the linear bins that we use with the spectrum, but rather the uh male bands, each frequency B is a different male band which is perceptually relevant. And then each um point that we have here has a, an associated uh color that represents how present a certain Melba is at a certain point in time. So basically, the conceptual understanding or visualization",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1614s",
        "start_time": "1614.329"
    },
    {
        "id": "f1b65b50",
        "text": "uh a matrix, right? And on the X axis, we have like time on the Y axis. Once again, we have frequency. But this time like the different frequency bins are not uh like the linear bins that we use with the spectrum, but rather the uh male bands, each frequency B is a different male band which is perceptually relevant. And then each um point that we have here has a, an associated uh color that represents how present a certain Melba is at a certain point in time. So basically, the conceptual understanding or visualization uh of males spec spectrograms is the same that we had for spectrograms. The only thing that really differs here is the way we represent a frequency which is like pitch uh like psychologically relevant this time different differently from uh a spectrogram.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1620s",
        "start_time": "1620.619"
    },
    {
        "id": "8170bf8b",
        "text": "uh male bands, each frequency B is a different male band which is perceptually relevant. And then each um point that we have here has a, an associated uh color that represents how present a certain Melba is at a certain point in time. So basically, the conceptual understanding or visualization uh of males spec spectrograms is the same that we had for spectrograms. The only thing that really differs here is the way we represent a frequency which is like pitch uh like psychologically relevant this time different differently from uh a spectrogram. OK. So uh the last question that we might ask here is why should we both like learning about male spectrograms? So and the the answer to this question is that they are extensively used in a lot of A I audio and A I music research. So a lot of papers that you may be reading about like audio classification uses M spectrograms. This is very, very true also for a lot of like music information retrieval A I music",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1636s",
        "start_time": "1636.81"
    },
    {
        "id": "aa3d7b6f",
        "text": "uh of males spec spectrograms is the same that we had for spectrograms. The only thing that really differs here is the way we represent a frequency which is like pitch uh like psychologically relevant this time different differently from uh a spectrogram. OK. So uh the last question that we might ask here is why should we both like learning about male spectrograms? So and the the answer to this question is that they are extensively used in a lot of A I audio and A I music research. So a lot of papers that you may be reading about like audio classification uses M spectrograms. This is very, very true also for a lot of like music information retrieval A I music uh papers. So for example, math spectrograms are overwhelmingly used like in automatic minute recognition, music genre classification, music, instrument classification and way more applications we have for this type of uh spectrograms. OK. But uh this time",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1666s",
        "start_time": "1666.229"
    },
    {
        "id": "b03b8980",
        "text": "OK. So uh the last question that we might ask here is why should we both like learning about male spectrograms? So and the the answer to this question is that they are extensively used in a lot of A I audio and A I music research. So a lot of papers that you may be reading about like audio classification uses M spectrograms. This is very, very true also for a lot of like music information retrieval A I music uh papers. So for example, math spectrograms are overwhelmingly used like in automatic minute recognition, music genre classification, music, instrument classification and way more applications we have for this type of uh spectrograms. OK. But uh this time we only focused on the theory of male spectrograms. And I hope that by now you have like a clear understanding of like what male males are, the male representation and why like male spectrograms are different than spectrograms and why they are very convenient uh like to use in a lot of like the applications where like we're talking about audio and music.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1684s",
        "start_time": "1684.869"
    },
    {
        "id": "d184b40a",
        "text": "uh papers. So for example, math spectrograms are overwhelmingly used like in automatic minute recognition, music genre classification, music, instrument classification and way more applications we have for this type of uh spectrograms. OK. But uh this time we only focused on the theory of male spectrograms. And I hope that by now you have like a clear understanding of like what male males are, the male representation and why like male spectrograms are different than spectrograms and why they are very convenient uh like to use in a lot of like the applications where like we're talking about audio and music. Uh So the next time we'll be moving to actual some kind of like implementation. So we'll be trying, we'll be extracting mouse spectrograms uh from audio files using Python. And Lisa and Lisa has a lot of like very nice utility functions that we can use that will save us a lot of time.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1713s",
        "start_time": "1713.239"
    },
    {
        "id": "45ecfa1e",
        "text": "we only focused on the theory of male spectrograms. And I hope that by now you have like a clear understanding of like what male males are, the male representation and why like male spectrograms are different than spectrograms and why they are very convenient uh like to use in a lot of like the applications where like we're talking about audio and music. Uh So the next time we'll be moving to actual some kind of like implementation. So we'll be trying, we'll be extracting mouse spectrograms uh from audio files using Python. And Lisa and Lisa has a lot of like very nice utility functions that we can use that will save us a lot of time. And then we'll visualize male spectrograms and then we'll also try to extract and visualize male filter bands so that you can actually understand and see how we can. Yeah, just like we represent them and what they actually represent. OK. Before I dash off and I finish, uh I want to remind you about the Sound of the Ice Lack community. And there",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1730s",
        "start_time": "1730.39"
    },
    {
        "id": "3c9699d2",
        "text": "Uh So the next time we'll be moving to actual some kind of like implementation. So we'll be trying, we'll be extracting mouse spectrograms uh from audio files using Python. And Lisa and Lisa has a lot of like very nice utility functions that we can use that will save us a lot of time. And then we'll visualize male spectrograms and then we'll also try to extract and visualize male filter bands so that you can actually understand and see how we can. Yeah, just like we represent them and what they actually represent. OK. Before I dash off and I finish, uh I want to remind you about the Sound of the Ice Lack community. And there if you have like any questions about like this stuff, your projects and way more stuff regarding like A I audio in A I music, you can just ask and there you'll have like a community of people interested in A I audio A IUS audio processing that are really like to help you really like to share like their ideas, their projects. So I really suggest you to go check out this community and I'll give you the link to uh sign up to the Slack community in the description below.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1755s",
        "start_time": "1755.459"
    },
    {
        "id": "87a491a5",
        "text": "And then we'll visualize male spectrograms and then we'll also try to extract and visualize male filter bands so that you can actually understand and see how we can. Yeah, just like we represent them and what they actually represent. OK. Before I dash off and I finish, uh I want to remind you about the Sound of the Ice Lack community. And there if you have like any questions about like this stuff, your projects and way more stuff regarding like A I audio in A I music, you can just ask and there you'll have like a community of people interested in A I audio A IUS audio processing that are really like to help you really like to share like their ideas, their projects. So I really suggest you to go check out this community and I'll give you the link to uh sign up to the Slack community in the description below. Uh It's all for today. I hope you really enjoyed the video and I guess I'll see you next time. Cheers.",
        "video": "Mel Spectrograms Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "9GHCiiDLHQ4",
        "youtube_link": "https://www.youtube.com/watch?v=9GHCiiDLHQ4&t=1773s",
        "start_time": "1773.369"
    },
    {
        "id": "7070b367",
        "text": "Hi, everybody and welcome to a new exciting video in the audio signal processing for machine learning series. Last time, we looked at the theory behind a few frequency domain audio features. This time, I'm gonna implement one of those from scratch in Python. And the one that I'll be implementing is called band energy ratio. I'm not gonna get too much into the the theoretical details of this because I've done this like in the previous video. So if you find yourself not understanding what I'm talking about, just go back to my previous video which should be over here. OK? So let's take a look at this notebook. Uh And as you can see, I've already implemented some code and that's because this is like stuff that we've seen multiple times throughout this series and I didn't want to spend my time like coding, typing this down.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=0s",
        "start_time": "0.31"
    },
    {
        "id": "23a22f10",
        "text": "I'll be implementing is called band energy ratio. I'm not gonna get too much into the the theoretical details of this because I've done this like in the previous video. So if you find yourself not understanding what I'm talking about, just go back to my previous video which should be over here. OK? So let's take a look at this notebook. Uh And as you can see, I've already implemented some code and that's because this is like stuff that we've seen multiple times throughout this series and I didn't want to spend my time like coding, typing this down. OK? So the first thing that it is I uh import some libraries that we need, then I load the audio file. So I have a couple of audio files that uh I'll be working with today. So one is a red, red hot chili peppers song and we've heard this time and again throughout the series. And the other one is a classical music piece by uh Claude uh De Bey. So let's quickly listen to this too.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=16s",
        "start_time": "16.704"
    },
    {
        "id": "8f95bbaa",
        "text": "OK? So let's take a look at this notebook. Uh And as you can see, I've already implemented some code and that's because this is like stuff that we've seen multiple times throughout this series and I didn't want to spend my time like coding, typing this down. OK? So the first thing that it is I uh import some libraries that we need, then I load the audio file. So I have a couple of audio files that uh I'll be working with today. So one is a red, red hot chili peppers song and we've heard this time and again throughout the series. And the other one is a classical music piece by uh Claude uh De Bey. So let's quickly listen to this too. So, yeah, this is a lush string",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=33s",
        "start_time": "33.34"
    },
    {
        "id": "97a203b5",
        "text": "OK? So the first thing that it is I uh import some libraries that we need, then I load the audio file. So I have a couple of audio files that uh I'll be working with today. So one is a red, red hot chili peppers song and we've heard this time and again throughout the series. And the other one is a classical music piece by uh Claude uh De Bey. So let's quickly listen to this too. So, yeah, this is a lush string driven orchestra piece by uh the PC.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=48s",
        "start_time": "48.79"
    },
    {
        "id": "8528de7d",
        "text": "So, yeah, this is a lush string driven orchestra piece by uh the PC. And here we have the song by the red hot chili peppers.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=77s",
        "start_time": "77.279"
    },
    {
        "id": "3b6bcef0",
        "text": "driven orchestra piece by uh the PC. And here we have the song by the red hot chili peppers. OK? You get the idea, right? And so what I do next is just I load the audio files which are Li Brosa using the this Libros do load and I get the um the waveform really. So like an Empire Ray and then the the sample rate which is the 42 22,050 Hertz. And then given we are working with uh",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=81s",
        "start_time": "81.599"
    },
    {
        "id": "7ee73437",
        "text": "And here we have the song by the red hot chili peppers. OK? You get the idea, right? And so what I do next is just I load the audio files which are Li Brosa using the this Libros do load and I get the um the waveform really. So like an Empire Ray and then the the sample rate which is the 42 22,050 Hertz. And then given we are working with uh frequency domain features and bands energy ratio is a frequency domain uh feature. What I want to do is just extract the spectrogram. And so how do I do that? Well, uh I just here once again, uh a simple function from Libros that's called Libros dot Stft and I pass in a couple of constants. So the frame size and the hot size and I obtain the spectrogram for the BC and red hot chili peppers. Now, if you don't know what a spectrogram is",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=86s",
        "start_time": "86.51"
    },
    {
        "id": "bff27011",
        "text": "OK? You get the idea, right? And so what I do next is just I load the audio files which are Li Brosa using the this Libros do load and I get the um the waveform really. So like an Empire Ray and then the the sample rate which is the 42 22,050 Hertz. And then given we are working with uh frequency domain features and bands energy ratio is a frequency domain uh feature. What I want to do is just extract the spectrogram. And so how do I do that? Well, uh I just here once again, uh a simple function from Libros that's called Libros dot Stft and I pass in a couple of constants. So the frame size and the hot size and I obtain the spectrogram for the BC and red hot chili peppers. Now, if you don't know what a spectrogram is I or like short time fourier transform is I have a bunch of videos on this uh topics uh in this series. So I highly suggest you to go check out before like moving along with this video. Let's now move on to the cool stuff and start calculating the band energy ratio. So let me add a few",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=93s",
        "start_time": "93.18"
    },
    {
        "id": "f8860f7f",
        "text": "frequency domain features and bands energy ratio is a frequency domain uh feature. What I want to do is just extract the spectrogram. And so how do I do that? Well, uh I just here once again, uh a simple function from Libros that's called Libros dot Stft and I pass in a couple of constants. So the frame size and the hot size and I obtain the spectrogram for the BC and red hot chili peppers. Now, if you don't know what a spectrogram is I or like short time fourier transform is I have a bunch of videos on this uh topics uh in this series. So I highly suggest you to go check out before like moving along with this video. Let's now move on to the cool stuff and start calculating the band energy ratio. So let me add a few boxes down here and I'll use some mark down to say yeah, coate a band energy uh ratio. Ok.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=117s",
        "start_time": "117.029"
    },
    {
        "id": "e807d632",
        "text": "I or like short time fourier transform is I have a bunch of videos on this uh topics uh in this series. So I highly suggest you to go check out before like moving along with this video. Let's now move on to the cool stuff and start calculating the band energy ratio. So let me add a few boxes down here and I'll use some mark down to say yeah, coate a band energy uh ratio. Ok. So let's take a quick look at the definition of the band energy ratio. So here we have the definition from yeah uh the the previous video basically. So and here you can see that the band energy ratio at time two. So at a given frame is given by this formula here. So the, the key thing that we want to uh get of as the first point is the split frequency F capture",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=145s",
        "start_time": "145.964"
    },
    {
        "id": "f009f4b9",
        "text": "boxes down here and I'll use some mark down to say yeah, coate a band energy uh ratio. Ok. So let's take a quick look at the definition of the band energy ratio. So here we have the definition from yeah uh the the previous video basically. So and here you can see that the band energy ratio at time two. So at a given frame is given by this formula here. So the, the key thing that we want to uh get of as the first point is the split frequency F capture F because this is the one if you recall, that's gonna tell us uh that's gonna give us like the threshold that we'll use to say. OK. So all the frequencies above this frequency are uh belong to the higher frequencies and all the frequencies below the split frequency below to the lower frequencies. But now, obviously, uh we can pass in a continuous frequency like in Hertz like 2000 Hertz or 3000 Hertz. But",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=167s",
        "start_time": "167.679"
    },
    {
        "id": "b2125bda",
        "text": "So let's take a quick look at the definition of the band energy ratio. So here we have the definition from yeah uh the the previous video basically. So and here you can see that the band energy ratio at time two. So at a given frame is given by this formula here. So the, the key thing that we want to uh get of as the first point is the split frequency F capture F because this is the one if you recall, that's gonna tell us uh that's gonna give us like the threshold that we'll use to say. OK. So all the frequencies above this frequency are uh belong to the higher frequencies and all the frequencies below the split frequency below to the lower frequencies. But now, obviously, uh we can pass in a continuous frequency like in Hertz like 2000 Hertz or 3000 Hertz. But the spectrogram has discrete values, right? So it has discrete frequency bins. So we need to create a function that maps a continuous frequency onto the discrete frequency bins that we have. And so that's what we want to build now. So we'll uh define a function and we'll cal we'll call it, calculate uh split",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=180s",
        "start_time": "180.96"
    },
    {
        "id": "727043d2",
        "text": "F because this is the one if you recall, that's gonna tell us uh that's gonna give us like the threshold that we'll use to say. OK. So all the frequencies above this frequency are uh belong to the higher frequencies and all the frequencies below the split frequency below to the lower frequencies. But now, obviously, uh we can pass in a continuous frequency like in Hertz like 2000 Hertz or 3000 Hertz. But the spectrogram has discrete values, right? So it has discrete frequency bins. So we need to create a function that maps a continuous frequency onto the discrete frequency bins that we have. And so that's what we want to build now. So we'll uh define a function and we'll cal we'll call it, calculate uh split frequency bin like this and this uh function accepts a few parameters. So first of all, it, it accepts a spectrogram, then the split frequency and finally the sample rate. OK.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=205s",
        "start_time": "205.949"
    },
    {
        "id": "896da3c2",
        "text": "the spectrogram has discrete values, right? So it has discrete frequency bins. So we need to create a function that maps a continuous frequency onto the discrete frequency bins that we have. And so that's what we want to build now. So we'll uh define a function and we'll cal we'll call it, calculate uh split frequency bin like this and this uh function accepts a few parameters. So first of all, it, it accepts a spectrogram, then the split frequency and finally the sample rate. OK. So now let's uh see what we should do here like all the different steps. So as the first thing, what we want to calculate is the frequency range that we have in the spectrogram. In other words, so we want to ask ourselves. So how uh what's the frequency range that we are capturing in the spectrogram? And now to show you what I mean here, let me just go back and take a look at the, for example, at the, the BC spectrogram. So",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=231s",
        "start_time": "231.22"
    },
    {
        "id": "83da2d27",
        "text": "frequency bin like this and this uh function accepts a few parameters. So first of all, it, it accepts a spectrogram, then the split frequency and finally the sample rate. OK. So now let's uh see what we should do here like all the different steps. So as the first thing, what we want to calculate is the frequency range that we have in the spectrogram. In other words, so we want to ask ourselves. So how uh what's the frequency range that we are capturing in the spectrogram? And now to show you what I mean here, let me just go back and take a look at the, for example, at the, the BC spectrogram. So uh I want to just like take the shape here of this. And as you can see like in the uh the, the shape of this spectrogram is given, it's, it's kind of like a two dimensional array, right? And so on the first dimension we have 1025 and that is the number of frequency bins that we have, right? And so what we want to really understand is like",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=256s",
        "start_time": "256.239"
    },
    {
        "id": "48cccda4",
        "text": "So now let's uh see what we should do here like all the different steps. So as the first thing, what we want to calculate is the frequency range that we have in the spectrogram. In other words, so we want to ask ourselves. So how uh what's the frequency range that we are capturing in the spectrogram? And now to show you what I mean here, let me just go back and take a look at the, for example, at the, the BC spectrogram. So uh I want to just like take the shape here of this. And as you can see like in the uh the, the shape of this spectrogram is given, it's, it's kind of like a two dimensional array, right? And so on the first dimension we have 1025 and that is the number of frequency bins that we have, right? And so what we want to really understand is like what do does 1025 frequency bins like uh correspond to in terms of like frequency range? OK. So, and how do we do that? Well, this is like uh something that we can easily do because uh we know that the spectrogram",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=277s",
        "start_time": "277.6"
    },
    {
        "id": "8929da5d",
        "text": "uh I want to just like take the shape here of this. And as you can see like in the uh the, the shape of this spectrogram is given, it's, it's kind of like a two dimensional array, right? And so on the first dimension we have 1025 and that is the number of frequency bins that we have, right? And so what we want to really understand is like what do does 1025 frequency bins like uh correspond to in terms of like frequency range? OK. So, and how do we do that? Well, this is like uh something that we can easily do because uh we know that the spectrogram um reduces the its like frequency range from the sample range, it moves on to the NICS frequency. And so we can obtain that by",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=307s",
        "start_time": "307.279"
    },
    {
        "id": "da71f468",
        "text": "what do does 1025 frequency bins like uh correspond to in terms of like frequency range? OK. So, and how do we do that? Well, this is like uh something that we can easily do because uh we know that the spectrogram um reduces the its like frequency range from the sample range, it moves on to the NICS frequency. And so we can obtain that by uh doing the sample rate divided by two, the next logical step here is to calculate the delta uh frequency between two adjacent uh bins. So in other words, how much, so when we move, say from frequency bin to number three to frequency bin four, so how much do we actually move in the continuous frequency? Right. And so we can do that by saying that the frequency",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=329s",
        "start_time": "329.739"
    },
    {
        "id": "717310b7",
        "text": "um reduces the its like frequency range from the sample range, it moves on to the NICS frequency. And so we can obtain that by uh doing the sample rate divided by two, the next logical step here is to calculate the delta uh frequency between two adjacent uh bins. So in other words, how much, so when we move, say from frequency bin to number three to frequency bin four, so how much do we actually move in the continuous frequency? Right. And so we can do that by saying that the frequency delta per bin",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=348s",
        "start_time": "348.269"
    },
    {
        "id": "fc9b8932",
        "text": "uh doing the sample rate divided by two, the next logical step here is to calculate the delta uh frequency between two adjacent uh bins. So in other words, how much, so when we move, say from frequency bin to number three to frequency bin four, so how much do we actually move in the continuous frequency? Right. And so we can do that by saying that the frequency delta per bin is equal to the frequency range divided by the total number of frequency bins that we have. And we can easily get that from the spectrogram. So we do a spectrogram dot a shape",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=359s",
        "start_time": "359.049"
    },
    {
        "id": "768fa042",
        "text": "delta per bin is equal to the frequency range divided by the total number of frequency bins that we have. And we can easily get that from the spectrogram. So we do a spectrogram dot a shape and we take the first dimension and the first dimension as we sow is equal to the number of bins that we have in the spectrogram. OK. So now we know the frequency delta for each bin. And so we are now ready to uh calculate the um split frequency bin. So the split frequency bin",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=389s",
        "start_time": "389.44"
    },
    {
        "id": "93ab76a4",
        "text": "is equal to the frequency range divided by the total number of frequency bins that we have. And we can easily get that from the spectrogram. So we do a spectrogram dot a shape and we take the first dimension and the first dimension as we sow is equal to the number of bins that we have in the spectrogram. OK. So now we know the frequency delta for each bin. And so we are now ready to uh calculate the um split frequency bin. So the split frequency bin uh is equal to the uh split frequency. And this is given in Hertz and that's divided by the frequency delta per bin. In other words, what we are doing is we are mapping this continuous um frequency, that's the split frequency onto the, the closest frequency uh bin available.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=393s",
        "start_time": "393.839"
    },
    {
        "id": "4838dd57",
        "text": "and we take the first dimension and the first dimension as we sow is equal to the number of bins that we have in the spectrogram. OK. So now we know the frequency delta for each bin. And so we are now ready to uh calculate the um split frequency bin. So the split frequency bin uh is equal to the uh split frequency. And this is given in Hertz and that's divided by the frequency delta per bin. In other words, what we are doing is we are mapping this continuous um frequency, that's the split frequency onto the, the closest frequency uh bin available. And uh but uh you, you may see here that uh given like we have two numbers. So the frequency to frequency delta per bin, it's likely that we're gonna get like some kind of a float number. And obviously the the frequency bins that we have are",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=409s",
        "start_time": "409.489"
    },
    {
        "id": "a5b85193",
        "text": "uh is equal to the uh split frequency. And this is given in Hertz and that's divided by the frequency delta per bin. In other words, what we are doing is we are mapping this continuous um frequency, that's the split frequency onto the, the closest frequency uh bin available. And uh but uh you, you may see here that uh given like we have two numbers. So the frequency to frequency delta per bin, it's likely that we're gonna get like some kind of a float number. And obviously the the frequency bins that we have are the script and that, that's the whole point of having like this critic here. So we can't take a frequency bin that's equal to 10.4. It doesn't make sense, right? So we have to move to uh an integer number. So we have to round up the numbers. So what, what can we do to round this round this number? So we can use the NP dot uh floor function",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=434s",
        "start_time": "434.359"
    },
    {
        "id": "cff68ab4",
        "text": "And uh but uh you, you may see here that uh given like we have two numbers. So the frequency to frequency delta per bin, it's likely that we're gonna get like some kind of a float number. And obviously the the frequency bins that we have are the script and that, that's the whole point of having like this critic here. So we can't take a frequency bin that's equal to 10.4. It doesn't make sense, right? So we have to move to uh an integer number. So we have to round up the numbers. So what, what can we do to round this round this number? So we can use the NP dot uh floor function and what this function does, it's a form of rounding and it always round uh rounds uh numbers down. So for example, say I have 10.4 and then if I apply the floor uh when it gets back a stem",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=461s",
        "start_time": "461.839"
    },
    {
        "id": "67895251",
        "text": "the script and that, that's the whole point of having like this critic here. So we can't take a frequency bin that's equal to 10.4. It doesn't make sense, right? So we have to move to uh an integer number. So we have to round up the numbers. So what, what can we do to round this round this number? So we can use the NP dot uh floor function and what this function does, it's a form of rounding and it always round uh rounds uh numbers down. So for example, say I have 10.4 and then if I apply the floor uh when it gets back a stem and I get it as a, as a float number. Uh If I have like 10.9 again, I'm gonna get 10",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=477s",
        "start_time": "477.195"
    },
    {
        "id": "81213833",
        "text": "and what this function does, it's a form of rounding and it always round uh rounds uh numbers down. So for example, say I have 10.4 and then if I apply the floor uh when it gets back a stem and I get it as a, as a float number. Uh If I have like 10.9 again, I'm gonna get 10 0.0 like this. OK? So here we have the split frequency bin. So",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=505s",
        "start_time": "505.079"
    },
    {
        "id": "6c5cf927",
        "text": "and I get it as a, as a float number. Uh If I have like 10.9 again, I'm gonna get 10 0.0 like this. OK? So here we have the split frequency bin. So now we can return it. But as I mentioned, this is a float number. So we want to cast it to an IND.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=523s",
        "start_time": "523.59"
    },
    {
        "id": "417672e0",
        "text": "0.0 like this. OK? So here we have the split frequency bin. So now we can return it. But as I mentioned, this is a float number. So we want to cast it to an IND. OK. So",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=532s",
        "start_time": "532.809"
    },
    {
        "id": "afe5dc6e",
        "text": "now we can return it. But as I mentioned, this is a float number. So we want to cast it to an IND. OK. So this should be ready. So now let's try it. So we can say split frequency uh bin and then we use this one, calculate split frequency bin. So as the spectrogra M, we just pass the, the PC spectrogram. And then uh as the split frequency, we, we're gonna be using 2000 Hertz which is a totally fine",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=540s",
        "start_time": "540.01"
    },
    {
        "id": "9a519e38",
        "text": "OK. So this should be ready. So now let's try it. So we can say split frequency uh bin and then we use this one, calculate split frequency bin. So as the spectrogra M, we just pass the, the PC spectrogram. And then uh as the split frequency, we, we're gonna be using 2000 Hertz which is a totally fine uh split frequency. And as the sample reach, I'm gonna use the default rere one which is 22,050.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=549s",
        "start_time": "549.349"
    },
    {
        "id": "d237a2f2",
        "text": "this should be ready. So now let's try it. So we can say split frequency uh bin and then we use this one, calculate split frequency bin. So as the spectrogra M, we just pass the, the PC spectrogram. And then uh as the split frequency, we, we're gonna be using 2000 Hertz which is a totally fine uh split frequency. And as the sample reach, I'm gonna use the default rere one which is 22,050. And now let's print this",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=552s",
        "start_time": "552.52"
    },
    {
        "id": "b8152116",
        "text": "uh split frequency. And as the sample reach, I'm gonna use the default rere one which is 22,050. And now let's print this split frequency bin like this. And as you can see, OK. So",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=579s",
        "start_time": "579.63"
    },
    {
        "id": "b8a36464",
        "text": "And now let's print this split frequency bin like this. And as you can see, OK. So uh the 2000 Hertz split frequency has been uh mapped onto this frequency bin which is 100 and 85 which is 100 and 85 out of uh 1025 frequency bins that we have in the spectrogram. OK?",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=587s",
        "start_time": "587.109"
    },
    {
        "id": "238027b2",
        "text": "split frequency bin like this. And as you can see, OK. So uh the 2000 Hertz split frequency has been uh mapped onto this frequency bin which is 100 and 85 which is 100 and 85 out of uh 1025 frequency bins that we have in the spectrogram. OK? So with this knowledge, we can now move on and actually calculate the um band energy ratio. So let's write the function here. So we'll do that definition with defined kate band energy uh ratio. And this function gets three parameters once again. So it gets the spectrogram,",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=591s",
        "start_time": "591.409"
    },
    {
        "id": "4edff969",
        "text": "uh the 2000 Hertz split frequency has been uh mapped onto this frequency bin which is 100 and 85 which is 100 and 85 out of uh 1025 frequency bins that we have in the spectrogram. OK? So with this knowledge, we can now move on and actually calculate the um band energy ratio. So let's write the function here. So we'll do that definition with defined kate band energy uh ratio. And this function gets three parameters once again. So it gets the spectrogram, it gets the split frequency and the sample rate C and obviously these are the same that we are using in calculated frequency bin. OK. So the uh first step is to actually get the split frequency bin, right? And so we can easily",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=597s",
        "start_time": "597.429"
    },
    {
        "id": "78674084",
        "text": "So with this knowledge, we can now move on and actually calculate the um band energy ratio. So let's write the function here. So we'll do that definition with defined kate band energy uh ratio. And this function gets three parameters once again. So it gets the spectrogram, it gets the split frequency and the sample rate C and obviously these are the same that we are using in calculated frequency bin. OK. So the uh first step is to actually get the split frequency bin, right? And so we can easily do this by using our newly implemented function. So we'll call calculator splits frequency bin and we'll pass in all of these arguments. And that function",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=614s",
        "start_time": "614.76"
    },
    {
        "id": "aaf4f70c",
        "text": "it gets the split frequency and the sample rate C and obviously these are the same that we are using in calculated frequency bin. OK. So the uh first step is to actually get the split frequency bin, right? And so we can easily do this by using our newly implemented function. So we'll call calculator splits frequency bin and we'll pass in all of these arguments. And that function will be so nice as to give us back the expected split frequency uh bin. OK. So now the next thing that we wanna do is to move to the power spectrogram. OK?",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=643s",
        "start_time": "643.669"
    },
    {
        "id": "b506afd3",
        "text": "do this by using our newly implemented function. So we'll call calculator splits frequency bin and we'll pass in all of these arguments. And that function will be so nice as to give us back the expected split frequency uh bin. OK. So now the next thing that we wanna do is to move to the power spectrogram. OK? And why do we do that? Well, we do that because as you can recall, probably like from this formula from last B or just like looking at it. Now, we are interested in the uh power spectrograms here. So this MT of N squared is basically like the magnitude spectrogram.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=665s",
        "start_time": "665.789"
    },
    {
        "id": "5f000de9",
        "text": "will be so nice as to give us back the expected split frequency uh bin. OK. So now the next thing that we wanna do is to move to the power spectrogram. OK? And why do we do that? Well, we do that because as you can recall, probably like from this formula from last B or just like looking at it. Now, we are interested in the uh power spectrograms here. So this MT of N squared is basically like the magnitude spectrogram. Uh So it's kind of like the power spectrogram, right? Because we are taking the magnitude spectrogram here uh for a given frame at a groan frequency bin. And we are squaring it, which basically means we are moving from the magnitude spectrogram to to",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=679s",
        "start_time": "679.75"
    },
    {
        "id": "51a0affd",
        "text": "And why do we do that? Well, we do that because as you can recall, probably like from this formula from last B or just like looking at it. Now, we are interested in the uh power spectrograms here. So this MT of N squared is basically like the magnitude spectrogram. Uh So it's kind of like the power spectrogram, right? Because we are taking the magnitude spectrogram here uh for a given frame at a groan frequency bin. And we are squaring it, which basically means we are moving from the magnitude spectrogram to to from the magnitude to the, to the power. OK. And so rather than calculating this uh power uh at each point in time and frequency bin will just like calculate it once initially. So what I want to do, as I said is like move to, to the power spectrogram.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=696s",
        "start_time": "696.429"
    },
    {
        "id": "4661c60e",
        "text": "Uh So it's kind of like the power spectrogram, right? Because we are taking the magnitude spectrogram here uh for a given frame at a groan frequency bin. And we are squaring it, which basically means we are moving from the magnitude spectrogram to to from the magnitude to the, to the power. OK. And so rather than calculating this uh power uh at each point in time and frequency bin will just like calculate it once initially. So what I want to do, as I said is like move to, to the power spectrogram. And so how can I do that? Well, this is quite simple because uh we start from the spectrogram which has complex values and we apply the uh absolute value to Dutch. And so we'll do an NPI dot Arbs and I'll pass in the uh spectrogram.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=717s",
        "start_time": "717.309"
    },
    {
        "id": "72fd7850",
        "text": "from the magnitude to the, to the power. OK. And so rather than calculating this uh power uh at each point in time and frequency bin will just like calculate it once initially. So what I want to do, as I said is like move to, to the power spectrogram. And so how can I do that? Well, this is quite simple because uh we start from the spectrogram which has complex values and we apply the uh absolute value to Dutch. And so we'll do an NPI dot Arbs and I'll pass in the uh spectrogram. And if we did only this, we would have the magnitude um spectrogram. But now we are going to be squaring this so that we can get the power spectrogram. Now, there's another trick that we need to apply here. And that's because we're going to be calculating the band energy ratio at each frame.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=732s",
        "start_time": "732.734"
    },
    {
        "id": "6b3da11e",
        "text": "And so how can I do that? Well, this is quite simple because uh we start from the spectrogram which has complex values and we apply the uh absolute value to Dutch. And so we'll do an NPI dot Arbs and I'll pass in the uh spectrogram. And if we did only this, we would have the magnitude um spectrogram. But now we are going to be squaring this so that we can get the power spectrogram. Now, there's another trick that we need to apply here. And that's because we're going to be calculating the band energy ratio at each frame. So we want to iterate through the power spectrogram and, and be able to iterate like frame by frame. But now if you take a look at the spectrogram like this, here, you'll see that the shape is given by. Yeah, it's a two by this, it's a two dimensional, right? But the first dimension is the the frequency dimension and the second one is the time dimension.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=752s",
        "start_time": "752.53"
    },
    {
        "id": "86410546",
        "text": "And if we did only this, we would have the magnitude um spectrogram. But now we are going to be squaring this so that we can get the power spectrogram. Now, there's another trick that we need to apply here. And that's because we're going to be calculating the band energy ratio at each frame. So we want to iterate through the power spectrogram and, and be able to iterate like frame by frame. But now if you take a look at the spectrogram like this, here, you'll see that the shape is given by. Yeah, it's a two by this, it's a two dimensional, right? But the first dimension is the the frequency dimension and the second one is the time dimension. So in other words, here we have like the number of frequency bins and here we have the number of frames. Now if we want to iterate uh through this uh yeah, the spectrogram or the power spectrogram",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=773s",
        "start_time": "773.9"
    },
    {
        "id": "11d7fc14",
        "text": "So we want to iterate through the power spectrogram and, and be able to iterate like frame by frame. But now if you take a look at the spectrogram like this, here, you'll see that the shape is given by. Yeah, it's a two by this, it's a two dimensional, right? But the first dimension is the the frequency dimension and the second one is the time dimension. So in other words, here we have like the number of frequency bins and here we have the number of frames. Now if we want to iterate uh through this uh yeah, the spectrogram or the power spectrogram uh based off time. So what we need to do is just like invert, get the so called transpose of this um thing here of the spectrogram. And so how can we do that? And that's very, very simple. So let me just like get this and say W spec transpose and this is equal to the BC spec dot capital T",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=796s",
        "start_time": "796.549"
    },
    {
        "id": "6058a31e",
        "text": "So in other words, here we have like the number of frequency bins and here we have the number of frames. Now if we want to iterate uh through this uh yeah, the spectrogram or the power spectrogram uh based off time. So what we need to do is just like invert, get the so called transpose of this um thing here of the spectrogram. And so how can we do that? And that's very, very simple. So let me just like get this and say W spec transpose and this is equal to the BC spec dot capital T and this will give us the so called transpose of the spectrogram of, of a matrix or NR A. So now if we take a look at the shape here,",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=823s",
        "start_time": "823.02"
    },
    {
        "id": "2cdd1046",
        "text": "uh based off time. So what we need to do is just like invert, get the so called transpose of this um thing here of the spectrogram. And so how can we do that? And that's very, very simple. So let me just like get this and say W spec transpose and this is equal to the BC spec dot capital T and this will give us the so called transpose of the spectrogram of, of a matrix or NR A. So now if we take a look at the shape here, you'll see that's what happened was kind of like the inversion of these two dimensions. So now I have the um the time dimension first, so the number of frames and then I have the frequency as the second dimension. And this is what we want to do on our power spectrogram.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=835s",
        "start_time": "835.84"
    },
    {
        "id": "827e486b",
        "text": "and this will give us the so called transpose of the spectrogram of, of a matrix or NR A. So now if we take a look at the shape here, you'll see that's what happened was kind of like the inversion of these two dimensions. So now I have the um the time dimension first, so the number of frames and then I have the frequency as the second dimension. And this is what we want to do on our power spectrogram. So we'll do like this. A power spectrum is equal to power",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=864s",
        "start_time": "864.349"
    },
    {
        "id": "99e8feb2",
        "text": "you'll see that's what happened was kind of like the inversion of these two dimensions. So now I have the um the time dimension first, so the number of frames and then I have the frequency as the second dimension. And this is what we want to do on our power spectrogram. So we'll do like this. A power spectrum is equal to power spec",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=877s",
        "start_time": "877.539"
    },
    {
        "id": "bfdcb1f6",
        "text": "So we'll do like this. A power spectrum is equal to power spec dot capital T. And so this way we just get the transfer of the power uh spectrum. OK? So now let's create a",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=894s",
        "start_time": "894.44"
    },
    {
        "id": "7834aa27",
        "text": "spec dot capital T. And so this way we just get the transfer of the power uh spectrum. OK? So now let's create a variable like this. So an empty list like this and this is where we'll cash the band energy ratio for each frame. And remember the band energy ratio as as a feature is is a frame best frame based feature. So we're going to have a value",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=902s",
        "start_time": "902.479"
    },
    {
        "id": "a1209a8d",
        "text": "dot capital T. And so this way we just get the transfer of the power uh spectrum. OK? So now let's create a variable like this. So an empty list like this and this is where we'll cash the band energy ratio for each frame. And remember the band energy ratio as as a feature is is a frame best frame based feature. So we're going to have a value a band energy ratio for each frame. So and we'll be cashing it for each frame here like in this variable called band energy ratio here. OK. So we'll do now I will iterate through all the frames in the power spectrogram.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=904s",
        "start_time": "904.78"
    },
    {
        "id": "dc42cc2f",
        "text": "variable like this. So an empty list like this and this is where we'll cash the band energy ratio for each frame. And remember the band energy ratio as as a feature is is a frame best frame based feature. So we're going to have a value a band energy ratio for each frame. So and we'll be cashing it for each frame here like in this variable called band energy ratio here. OK. So we'll do now I will iterate through all the frames in the power spectrogram. I'd say we'll do four frequencies",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=917s",
        "start_time": "917.95"
    },
    {
        "id": "da61be91",
        "text": "a band energy ratio for each frame. So and we'll be cashing it for each frame here like in this variable called band energy ratio here. OK. So we'll do now I will iterate through all the frames in the power spectrogram. I'd say we'll do four frequencies and frame",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=936s",
        "start_time": "936.5"
    },
    {
        "id": "537c395f",
        "text": "I'd say we'll do four frequencies and frame uh in power",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=955s",
        "start_time": "955.69"
    },
    {
        "id": "9bb1b860",
        "text": "and frame uh in power spectrogram.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=962s",
        "start_time": "962.26"
    },
    {
        "id": "2cbd4bb9",
        "text": "uh in power spectrogram. So what should we do here? Well, so here we are iterating through all the frames and getting like the the the the value like of the of the uh for each frequencies, right? Like uh for each frame. In other words, what we want to do here is calculate the band energy ratio for each",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=966s",
        "start_time": "966.63"
    },
    {
        "id": "8c7e31a2",
        "text": "spectrogram. So what should we do here? Well, so here we are iterating through all the frames and getting like the the the the value like of the of the uh for each frequencies, right? Like uh for each frame. In other words, what we want to do here is calculate the band energy ratio for each frame. So let me put a password time being. And so what should we do here? Well, it's quite straightforward. So what we want to uh calculate uh is like this two items. So the numerator and the denominator. So the numerator is basically the the sum of the power in the lower energies",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=970s",
        "start_time": "970.14"
    },
    {
        "id": "05224e7a",
        "text": "So what should we do here? Well, so here we are iterating through all the frames and getting like the the the the value like of the of the uh for each frequencies, right? Like uh for each frame. In other words, what we want to do here is calculate the band energy ratio for each frame. So let me put a password time being. And so what should we do here? Well, it's quite straightforward. So what we want to uh calculate uh is like this two items. So the numerator and the denominator. So the numerator is basically the the sum of the power in the lower energies and the denominator corresponds to the sum of the power in the higher frequencies.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=973s",
        "start_time": "973.07"
    },
    {
        "id": "706627a7",
        "text": "frame. So let me put a password time being. And so what should we do here? Well, it's quite straightforward. So what we want to uh calculate uh is like this two items. So the numerator and the denominator. So the numerator is basically the the sum of the power in the lower energies and the denominator corresponds to the sum of the power in the higher frequencies. So",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=996s",
        "start_time": "996.5"
    },
    {
        "id": "b8ed26ee",
        "text": "and the denominator corresponds to the sum of the power in the higher frequencies. So let's do that. So we'll do some power low uh frequencies frequencies. And how do we calculate this? Well, uh we can use NP dot Sun and then we'll pass in the frequencies infra",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1022s",
        "start_time": "1022.169"
    },
    {
        "id": "9f337a08",
        "text": "So let's do that. So we'll do some power low uh frequencies frequencies. And how do we calculate this? Well, uh we can use NP dot Sun and then we'll pass in the frequencies infra here.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1029s",
        "start_time": "1029.77"
    },
    {
        "id": "2c378d03",
        "text": "let's do that. So we'll do some power low uh frequencies frequencies. And how do we calculate this? Well, uh we can use NP dot Sun and then we'll pass in the frequencies infra here. But we want",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1031s",
        "start_time": "1031.708"
    },
    {
        "id": "2e85fa0b",
        "text": "here. But we want some across all the values or across all the frequencies. But rather",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1053s",
        "start_time": "1053.4"
    },
    {
        "id": "73f60f10",
        "text": "But we want some across all the values or across all the frequencies. But rather just take the first uh frequency bins up to the split frequency",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1055s",
        "start_time": "1055.13"
    },
    {
        "id": "62a751c2",
        "text": "some across all the values or across all the frequencies. But rather just take the first uh frequency bins up to the split frequency then. OK. So, and uh it's gonna be easy to do the same thing for the high frequency. But so let me just like rewrite this as high. So some power high frequencies and so here will sum the values uh uh all the, the, the values for the power at all the different frequency bins in the higher",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1057s",
        "start_time": "1057.609"
    },
    {
        "id": "0a35d651",
        "text": "just take the first uh frequency bins up to the split frequency then. OK. So, and uh it's gonna be easy to do the same thing for the high frequency. But so let me just like rewrite this as high. So some power high frequencies and so here will sum the values uh uh all the, the, the values for the power at all the different frequency bins in the higher um part of the spectrum. And so what this actually means is that we'll slice this starting from the split frequency bin and then go up to like the, the the last frequency bin, the highest frequency bin.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1064s",
        "start_time": "1064.51"
    },
    {
        "id": "390fbfd4",
        "text": "then. OK. So, and uh it's gonna be easy to do the same thing for the high frequency. But so let me just like rewrite this as high. So some power high frequencies and so here will sum the values uh uh all the, the, the values for the power at all the different frequency bins in the higher um part of the spectrum. And so what this actually means is that we'll slice this starting from the split frequency bin and then go up to like the, the the last frequency bin, the highest frequency bin. OK. So now with this, we have both of the elements. So the numerator and the denominator here in this um formula. So what remains to you to do is just like to divide this. OK. So we'll do that the uh band energy uh ratio uh current frame is equal to the sum power",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1074s",
        "start_time": "1074.0"
    },
    {
        "id": "d3c916d8",
        "text": "um part of the spectrum. And so what this actually means is that we'll slice this starting from the split frequency bin and then go up to like the, the the last frequency bin, the highest frequency bin. OK. So now with this, we have both of the elements. So the numerator and the denominator here in this um formula. So what remains to you to do is just like to divide this. OK. So we'll do that the uh band energy uh ratio uh current frame is equal to the sum power of the uh low frequencies divided by the sum of the power for the high frequencies like this. OK. So",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1098s",
        "start_time": "1098.18"
    },
    {
        "id": "f3bbb576",
        "text": "OK. So now with this, we have both of the elements. So the numerator and the denominator here in this um formula. So what remains to you to do is just like to divide this. OK. So we'll do that the uh band energy uh ratio uh current frame is equal to the sum power of the uh low frequencies divided by the sum of the power for the high frequencies like this. OK. So the next step here is to just uh append",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1115s",
        "start_time": "1115.579"
    },
    {
        "id": "9c526dd0",
        "text": "of the uh low frequencies divided by the sum of the power for the high frequencies like this. OK. So the next step here is to just uh append the band energy ratio for the current frame to the band energy ratio. List here so that we can catch the value of the uh band uh energy ratio",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1143s",
        "start_time": "1143.099"
    },
    {
        "id": "dbdf11f7",
        "text": "the next step here is to just uh append the band energy ratio for the current frame to the band energy ratio. List here so that we can catch the value of the uh band uh energy ratio uh for the current frame. So we'll do a band energy ratio dot uh append and we'll pass in the band energy ratio of the current frame like this. Ok? So now we are ready to return and I'll just cast this list to a NPI array. So I'll do an NPI dot array and we'll pass the band",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1154s",
        "start_time": "1154.949"
    },
    {
        "id": "841eb3ad",
        "text": "the band energy ratio for the current frame to the band energy ratio. List here so that we can catch the value of the uh band uh energy ratio uh for the current frame. So we'll do a band energy ratio dot uh append and we'll pass in the band energy ratio of the current frame like this. Ok? So now we are ready to return and I'll just cast this list to a NPI array. So I'll do an NPI dot array and we'll pass the band energy ratio like this, ok? So now let's see if this works. So what I want to do is to get the band energy ratio for uh the BC. And so the way I'll do this is I'll call the calculator band energy ratio and I'll pass in the spectrogram and the spectrogram here is gonna be this, the PC spec",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1161s",
        "start_time": "1161.349"
    },
    {
        "id": "1e6c6c91",
        "text": "uh for the current frame. So we'll do a band energy ratio dot uh append and we'll pass in the band energy ratio of the current frame like this. Ok? So now we are ready to return and I'll just cast this list to a NPI array. So I'll do an NPI dot array and we'll pass the band energy ratio like this, ok? So now let's see if this works. So what I want to do is to get the band energy ratio for uh the BC. And so the way I'll do this is I'll call the calculator band energy ratio and I'll pass in the spectrogram and the spectrogram here is gonna be this, the PC spec then I'll need to specify the split frequency. So the split frequency will be using 2000. That as I said is a totally fine uh split frequency. And then for the Sa Mle rate, we, if you remember guys like when we load it,",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1174s",
        "start_time": "1174.719"
    },
    {
        "id": "466cf60b",
        "text": "energy ratio like this, ok? So now let's see if this works. So what I want to do is to get the band energy ratio for uh the BC. And so the way I'll do this is I'll call the calculator band energy ratio and I'll pass in the spectrogram and the spectrogram here is gonna be this, the PC spec then I'll need to specify the split frequency. So the split frequency will be using 2000. That as I said is a totally fine uh split frequency. And then for the Sa Mle rate, we, if you remember guys like when we load it, uh the waveform here, we got back like the simple rates that's defaulted to 22 2050. But I'll get like this sr",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1202s",
        "start_time": "1202.28"
    },
    {
        "id": "d85724ed",
        "text": "then I'll need to specify the split frequency. So the split frequency will be using 2000. That as I said is a totally fine uh split frequency. And then for the Sa Mle rate, we, if you remember guys like when we load it, uh the waveform here, we got back like the simple rates that's defaulted to 22 2050. But I'll get like this sr over here",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1230s",
        "start_time": "1230.839"
    },
    {
        "id": "165e6311",
        "text": "uh the waveform here, we got back like the simple rates that's defaulted to 22 2050. But I'll get like this sr over here like this,",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1248s",
        "start_time": "1248.39"
    },
    {
        "id": "4a1aa4f7",
        "text": "over here like this, ok? So now let's take a look at the band's energy ratio",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1258s",
        "start_time": "1258.27"
    },
    {
        "id": "608fd546",
        "text": "like this, ok? So now let's take a look at the band's energy ratio um shape.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1261s",
        "start_time": "1261.189"
    },
    {
        "id": "159a37df",
        "text": "ok? So now let's take a look at the band's energy ratio um shape. Let's take a look at this, ok? So this is a numpy array uh which only like one dimension and it has 1292 items in it. And is it correct? Well, it is correct because if you guys remember",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1263s",
        "start_time": "1263.17"
    },
    {
        "id": "2cba34af",
        "text": "um shape. Let's take a look at this, ok? So this is a numpy array uh which only like one dimension and it has 1292 items in it. And is it correct? Well, it is correct because if you guys remember here, like in the spectrogram, we had uh well, this is the transpo spectrogram. So here like in the first dimension, we have the, the, the time dimension which is equal to 1292 which is the same number for spent energy ratio array.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1270s",
        "start_time": "1270.109"
    },
    {
        "id": "afd3a596",
        "text": "Let's take a look at this, ok? So this is a numpy array uh which only like one dimension and it has 1292 items in it. And is it correct? Well, it is correct because if you guys remember here, like in the spectrogram, we had uh well, this is the transpo spectrogram. So here like in the first dimension, we have the, the, the time dimension which is equal to 1292 which is the same number for spent energy ratio array. And that basically means that we're getting like values for all the uh different frames that we have in the spectrum, which is what we were looking for. Ok. So now let me do the same thing. So let me also get the band energy ratio for the red hot",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1274s",
        "start_time": "1274.68"
    },
    {
        "id": "a5a67343",
        "text": "here, like in the spectrogram, we had uh well, this is the transpo spectrogram. So here like in the first dimension, we have the, the, the time dimension which is equal to 1292 which is the same number for spent energy ratio array. And that basically means that we're getting like values for all the uh different frames that we have in the spectrum, which is what we were looking for. Ok. So now let me do the same thing. So let me also get the band energy ratio for the red hot chili pepper,",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1293s",
        "start_time": "1293.25"
    },
    {
        "id": "13b40b45",
        "text": "And that basically means that we're getting like values for all the uh different frames that we have in the spectrum, which is what we were looking for. Ok. So now let me do the same thing. So let me also get the band energy ratio for the red hot chili pepper, red hots",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1309s",
        "start_time": "1309.869"
    },
    {
        "id": "88787148",
        "text": "chili pepper, red hots song. And so here obviously I need to pass the red hot spectrogram. OK? Good. OK. So the last thing that now remains to do is they actually visualize this uh bent energy ratio curves. And I also want to compare this two so that we can draw some conclusion about sp energy ratio curves in different genres like in this case,",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1328s",
        "start_time": "1328.859"
    },
    {
        "id": "4aeba0fc",
        "text": "red hots song. And so here obviously I need to pass the red hot spectrogram. OK? Good. OK. So the last thing that now remains to do is they actually visualize this uh bent energy ratio curves. And I also want to compare this two so that we can draw some conclusion about sp energy ratio curves in different genres like in this case, music and classical music. OK. So now, first of all, let me add uh a few blocks like this and then I'll Yeah, well, not that I want the mark mark down and I'll say V URL",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1330s",
        "start_time": "1330.65"
    },
    {
        "id": "bbaa31f1",
        "text": "song. And so here obviously I need to pass the red hot spectrogram. OK? Good. OK. So the last thing that now remains to do is they actually visualize this uh bent energy ratio curves. And I also want to compare this two so that we can draw some conclusion about sp energy ratio curves in different genres like in this case, music and classical music. OK. So now, first of all, let me add uh a few blocks like this and then I'll Yeah, well, not that I want the mark mark down and I'll say V URL bands and A G ratio curves. First thing we wanna do is create a figure. So we'll do plot dot uh figure and I'll pass a fig",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1333s",
        "start_time": "1333.839"
    },
    {
        "id": "8bf2f227",
        "text": "music and classical music. OK. So now, first of all, let me add uh a few blocks like this and then I'll Yeah, well, not that I want the mark mark down and I'll say V URL bands and A G ratio curves. First thing we wanna do is create a figure. So we'll do plot dot uh figure and I'll pass a fig size and I'll set this to 25 by 10 like this. OK? So next we want to create a plot, so we'll do plot dot plot and here we should pass three things. So the value for the X axis, which is gonna be time for us and then the value for the Y axis, which is the actual",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1358s",
        "start_time": "1358.959"
    },
    {
        "id": "e02b5da0",
        "text": "bands and A G ratio curves. First thing we wanna do is create a figure. So we'll do plot dot uh figure and I'll pass a fig size and I'll set this to 25 by 10 like this. OK? So next we want to create a plot, so we'll do plot dot plot and here we should pass three things. So the value for the X axis, which is gonna be time for us and then the value for the Y axis, which is the actual um band energy ratio array and then an optional color so that we can identify the two different curves for the BC uh stuff and for red hot chili pepper salt.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1377s",
        "start_time": "1377.01"
    },
    {
        "id": "edb6f1b9",
        "text": "size and I'll set this to 25 by 10 like this. OK? So next we want to create a plot, so we'll do plot dot plot and here we should pass three things. So the value for the X axis, which is gonna be time for us and then the value for the Y axis, which is the actual um band energy ratio array and then an optional color so that we can identify the two different curves for the BC uh stuff and for red hot chili pepper salt. OK? So let's do the first thing. So we'll pass t now t doesn't exist yet. So, uh but for the time being, I'll put it there as a placeholder, then I'll pass the band energy ratio for the BC as the Y axis and then for uh the color outside blue for the BC. OK?",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1391s",
        "start_time": "1391.959"
    },
    {
        "id": "3c343585",
        "text": "um band energy ratio array and then an optional color so that we can identify the two different curves for the BC uh stuff and for red hot chili pepper salt. OK? So let's do the first thing. So we'll pass t now t doesn't exist yet. So, uh but for the time being, I'll put it there as a placeholder, then I'll pass the band energy ratio for the BC as the Y axis and then for uh the color outside blue for the BC. OK? So we'll do the same thing for the uh red hot chili pepper song. So here I'll pass the relative uh band to energy ratio",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1413s",
        "start_time": "1413.81"
    },
    {
        "id": "6774e9e0",
        "text": "OK? So let's do the first thing. So we'll pass t now t doesn't exist yet. So, uh but for the time being, I'll put it there as a placeholder, then I'll pass the band energy ratio for the BC as the Y axis and then for uh the color outside blue for the BC. OK? So we'll do the same thing for the uh red hot chili pepper song. So here I'll pass the relative uh band to energy ratio uh array. And here we'll set this color equal to uh red. And finally, we'll do a plot dot show. Now, obviously, if I get on and press enter, I get uh an error because the T is not defined. We don't have a value for that. So I should implement that first thing I want to just like bring up one of these boxes here so that I can find T.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1426s",
        "start_time": "1426.01"
    },
    {
        "id": "64485587",
        "text": "So we'll do the same thing for the uh red hot chili pepper song. So here I'll pass the relative uh band to energy ratio uh array. And here we'll set this color equal to uh red. And finally, we'll do a plot dot show. Now, obviously, if I get on and press enter, I get uh an error because the T is not defined. We don't have a value for that. So I should implement that first thing I want to just like bring up one of these boxes here so that I can find T. So 1st, 1st off, I need to define uh frames and this is gonna be equal to a range and I'll get the uh length of the say bear",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1446s",
        "start_time": "1446.16"
    },
    {
        "id": "6a4dcdee",
        "text": "uh array. And here we'll set this color equal to uh red. And finally, we'll do a plot dot show. Now, obviously, if I get on and press enter, I get uh an error because the T is not defined. We don't have a value for that. So I should implement that first thing I want to just like bring up one of these boxes here so that I can find T. So 1st, 1st off, I need to define uh frames and this is gonna be equal to a range and I'll get the uh length of the say bear the BC band energy ratio uh array for the BC. And the, the length is going to be equal for the BC and the red hot chili pepper. So I can use just one. OK. So here, now I have like this value for this variable frames and now I can move on and implement T and here what I can do is use uh Libres.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1456s",
        "start_time": "1456.52"
    },
    {
        "id": "cd79a377",
        "text": "So 1st, 1st off, I need to define uh frames and this is gonna be equal to a range and I'll get the uh length of the say bear the BC band energy ratio uh array for the BC. And the, the length is going to be equal for the BC and the red hot chili pepper. So I can use just one. OK. So here, now I have like this value for this variable frames and now I can move on and implement T and here what I can do is use uh Libres. And uh here Libres has a nice utility function called frames to uh time. And what it it needs as arguments is the frames. And then I should",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1484s",
        "start_time": "1484.589"
    },
    {
        "id": "9ffe69ce",
        "text": "the BC band energy ratio uh array for the BC. And the, the length is going to be equal for the BC and the red hot chili pepper. So I can use just one. OK. So here, now I have like this value for this variable frames and now I can move on and implement T and here what I can do is use uh Libres. And uh here Libres has a nice utility function called frames to uh time. And what it it needs as arguments is the frames. And then I should um specify the hop length and the hop length I think we specified up here as a constant called hop size, which is equal to 512. So totally uh yeah, traditional number for that. OK. So we'll do this. OK? And so here what it gets is basically like the, the value of time at each frame. So let, let me show you what I mean by that. So",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1499s",
        "start_time": "1499.38"
    },
    {
        "id": "7fcb1617",
        "text": "And uh here Libres has a nice utility function called frames to uh time. And what it it needs as arguments is the frames. And then I should um specify the hop length and the hop length I think we specified up here as a constant called hop size, which is equal to 512. So totally uh yeah, traditional number for that. OK. So we'll do this. OK? And so here what it gets is basically like the, the value of time at each frame. So let, let me show you what I mean by that. So uh here T",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1522s",
        "start_time": "1522.979"
    },
    {
        "id": "bc502b82",
        "text": "um specify the hop length and the hop length I think we specified up here as a constant called hop size, which is equal to 512. So totally uh yeah, traditional number for that. OK. So we'll do this. OK? And so here what it gets is basically like the, the value of time at each frame. So let, let me show you what I mean by that. So uh here T is an array and the uh length uh of T is equal to 1292. So these are all like the, the frames and for at each frame, uh we're just like converting uh from just like in to like the relative time given the hop length that we are using now that we have T we can move on and plot our band energy ratio for the two songs.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1540s",
        "start_time": "1540.569"
    },
    {
        "id": "402fbba8",
        "text": "uh here T is an array and the uh length uh of T is equal to 1292. So these are all like the, the frames and for at each frame, uh we're just like converting uh from just like in to like the relative time given the hop length that we are using now that we have T we can move on and plot our band energy ratio for the two songs. So, and as you can see in blue, we have the band energy ratio for the BC piece. And in red, we have the one for the red hot chili pepper song. And there's a stark difference between the two and basically the B energy ratio is way higher for the BC PS than the red hot chili peppers ones. And that's usually the case, it's something that you find uh quite often when you compare classical music",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1568s",
        "start_time": "1568.17"
    },
    {
        "id": "0be6df52",
        "text": "is an array and the uh length uh of T is equal to 1292. So these are all like the, the frames and for at each frame, uh we're just like converting uh from just like in to like the relative time given the hop length that we are using now that we have T we can move on and plot our band energy ratio for the two songs. So, and as you can see in blue, we have the band energy ratio for the BC piece. And in red, we have the one for the red hot chili pepper song. And there's a stark difference between the two and basically the B energy ratio is way higher for the BC PS than the red hot chili peppers ones. And that's usually the case, it's something that you find uh quite often when you compare classical music with rock music. And that's because in classical music, most of the energy is concentrated in the lower end of the spectrum. Whereas in rock music, you have a more balanced distribution and that's probably has to do with the fact that there are a lot of like noisy uh sounds like snares or like stuff like this that just like provides energy or offers energy in the higher end of the spectrum in rock music.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1570s",
        "start_time": "1570.88"
    },
    {
        "id": "d7bfbf2a",
        "text": "So, and as you can see in blue, we have the band energy ratio for the BC piece. And in red, we have the one for the red hot chili pepper song. And there's a stark difference between the two and basically the B energy ratio is way higher for the BC PS than the red hot chili peppers ones. And that's usually the case, it's something that you find uh quite often when you compare classical music with rock music. And that's because in classical music, most of the energy is concentrated in the lower end of the spectrum. Whereas in rock music, you have a more balanced distribution and that's probably has to do with the fact that there are a lot of like noisy uh sounds like snares or like stuff like this that just like provides energy or offers energy in the higher end of the spectrum in rock music. OK. So uh by now, you should be able to have, well, you should have like a very deep understanding of band energy ratio, how to calculate it from scratch with Python. And you probably also have an idea of how bent energy ratio like works in different genres, which is great. So that's all really for today. So next time we'll be uh looking into the remaining two frequency to all your features and how to like implement,",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1598s",
        "start_time": "1598.81"
    },
    {
        "id": "aa029723",
        "text": "with rock music. And that's because in classical music, most of the energy is concentrated in the lower end of the spectrum. Whereas in rock music, you have a more balanced distribution and that's probably has to do with the fact that there are a lot of like noisy uh sounds like snares or like stuff like this that just like provides energy or offers energy in the higher end of the spectrum in rock music. OK. So uh by now, you should be able to have, well, you should have like a very deep understanding of band energy ratio, how to calculate it from scratch with Python. And you probably also have an idea of how bent energy ratio like works in different genres, which is great. So that's all really for today. So next time we'll be uh looking into the remaining two frequency to all your features and how to like implement, but we won't implement them from scratch, but rather we'll be using uh Li Breza for uh implementing them. So if you've enjoyed the video, please leave a like if you haven't subscribed and want more content like this, please uh subscribe to the channel and I guess I'll see you next time. Cheers.",
        "video": "Implementing Band Energy Ratio in Python from Scratch",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8UJ8ZDR7yUs",
        "youtube_link": "https://www.youtube.com/watch?v=8UJ8ZDR7yUs&t=1625s",
        "start_time": "1625.27"
    },
    {
        "id": "a1a3f65b",
        "text": "Hi, everybody and welcome to a new video in the audio signal processing for machine learning series. Last time we introduced complex numbers and we said that we would need this to introduce many aspects of audio signal processing. And so this time we'll just build on top of that knowledge and we'll use complex numbers to define the fourier transform in complex terms. And as you'll see, this is gonna be super compact and super elegant. But before we get started, I just want to remind you about the sound of the I Slack community here. You can talk with a lot of like cool people who are interested in A IOU and A I music and you can also get feedback. So I really suggest you to check this out. I'll leave you the sign up link in the description below now onto the cool stuff. So I want to remind you about like a couple of things that I introduced some math stuff that I introduced over the last couple of meters. And the first couple of formulas are these two and this is the formula on top over here for um getting like the face as a parameter of the fourier transform. And then we have the mag",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "edcfdab0",
        "text": "the fourier transform in complex terms. And as you'll see, this is gonna be super compact and super elegant. But before we get started, I just want to remind you about the sound of the I Slack community here. You can talk with a lot of like cool people who are interested in A IOU and A I music and you can also get feedback. So I really suggest you to check this out. I'll leave you the sign up link in the description below now onto the cool stuff. So I want to remind you about like a couple of things that I introduced some math stuff that I introduced over the last couple of meters. And the first couple of formulas are these two and this is the formula on top over here for um getting like the face as a parameter of the fourier transform. And then we have the mag and as you can see here, we have just like the way we can extract that. Now, I'm not going to get you into the details here because I covered this in a couple of videos ago when I gave you an introduction and intuition of the four year transform. So I definitely suggest you to check that out if you haven't. And so you should have the video over here cool. But now what are like the face and magnitude on a very high level where these are like the",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=20s",
        "start_time": "20.61"
    },
    {
        "id": "778f9445",
        "text": "stuff. So I want to remind you about like a couple of things that I introduced some math stuff that I introduced over the last couple of meters. And the first couple of formulas are these two and this is the formula on top over here for um getting like the face as a parameter of the fourier transform. And then we have the mag and as you can see here, we have just like the way we can extract that. Now, I'm not going to get you into the details here because I covered this in a couple of videos ago when I gave you an introduction and intuition of the four year transform. So I definitely suggest you to check that out if you haven't. And so you should have the video over here cool. But now what are like the face and magnitude on a very high level where these are like the two parameters that we extract when we decompose a complex sound using the fourier transform. So we have a decompose uh sine wave frequency. And for that frequency, we extract a face and a magnitude and the magnitude tells us how much of that pure turn we have in the original signal",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=48s",
        "start_time": "48.082"
    },
    {
        "id": "20204bf2",
        "text": "and as you can see here, we have just like the way we can extract that. Now, I'm not going to get you into the details here because I covered this in a couple of videos ago when I gave you an introduction and intuition of the four year transform. So I definitely suggest you to check that out if you haven't. And so you should have the video over here cool. But now what are like the face and magnitude on a very high level where these are like the two parameters that we extract when we decompose a complex sound using the fourier transform. So we have a decompose uh sine wave frequency. And for that frequency, we extract a face and a magnitude and the magnitude tells us how much of that pure turn we have in the original signal good. So the next thing that I want to remind you about is this nice formula for, for a complex number. So here we have a complex number that's, that's defined as the absolute value of the com complex number multiplied by E to the I times gamma where I is the imaginary unit. So this is the polar definition of a complex number using the exponential here, once again,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=75s",
        "start_time": "75.554"
    },
    {
        "id": "352c8668",
        "text": "two parameters that we extract when we decompose a complex sound using the fourier transform. So we have a decompose uh sine wave frequency. And for that frequency, we extract a face and a magnitude and the magnitude tells us how much of that pure turn we have in the original signal good. So the next thing that I want to remind you about is this nice formula for, for a complex number. So here we have a complex number that's, that's defined as the absolute value of the com complex number multiplied by E to the I times gamma where I is the imaginary unit. So this is the polar definition of a complex number using the exponential here, once again, if you don't know uh this or if you haven't checked out my previous video on CNA numbers, definitely do check this out because all of this video will build on top of my previous two videos. OK.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=103s",
        "start_time": "103.026"
    },
    {
        "id": "60b4d62e",
        "text": "good. So the next thing that I want to remind you about is this nice formula for, for a complex number. So here we have a complex number that's, that's defined as the absolute value of the com complex number multiplied by E to the I times gamma where I is the imaginary unit. So this is the polar definition of a complex number using the exponential here, once again, if you don't know uh this or if you haven't checked out my previous video on CNA numbers, definitely do check this out because all of this video will build on top of my previous two videos. OK. So now let's move on. So what's the intuition that we use to define the fourier transform using complex numbers? Well, the idea here is that whenever we um apply a fourier transform, what we end up with for each frequency for each li is",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=130s",
        "start_time": "130.75"
    },
    {
        "id": "088e90ce",
        "text": "if you don't know uh this or if you haven't checked out my previous video on CNA numbers, definitely do check this out because all of this video will build on top of my previous two videos. OK. So now let's move on. So what's the intuition that we use to define the fourier transform using complex numbers? Well, the idea here is that whenever we um apply a fourier transform, what we end up with for each frequency for each li is a pair of parameters. So one is magnitude as we saw it and the other one is phase. And the idea is that we can use this magnitude and phase as polar coordinates of a complex number. In other words, we can encode both of these coefficients. So the magnitude and phase in a single complex number. So yeah, but let's see how that looks like",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=158s",
        "start_time": "158.619"
    },
    {
        "id": "e10393c0",
        "text": "So now let's move on. So what's the intuition that we use to define the fourier transform using complex numbers? Well, the idea here is that whenever we um apply a fourier transform, what we end up with for each frequency for each li is a pair of parameters. So one is magnitude as we saw it and the other one is phase. And the idea is that we can use this magnitude and phase as polar coordinates of a complex number. In other words, we can encode both of these coefficients. So the magnitude and phase in a single complex number. So yeah, but let's see how that looks like uh in in in in empirically like applied. So let's start like with the um mathematical equations that I introduced earlier. So here we have like our phase once again the magnitude and here we have the definition of a complex number in polar coordinates. OK. So how do we get the fourier transform coefficients? Well, this is the formula, right.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=172s",
        "start_time": "172.479"
    },
    {
        "id": "5ab9bf74",
        "text": "a pair of parameters. So one is magnitude as we saw it and the other one is phase. And the idea is that we can use this magnitude and phase as polar coordinates of a complex number. In other words, we can encode both of these coefficients. So the magnitude and phase in a single complex number. So yeah, but let's see how that looks like uh in in in in empirically like applied. So let's start like with the um mathematical equations that I introduced earlier. So here we have like our phase once again the magnitude and here we have the definition of a complex number in polar coordinates. OK. So how do we get the fourier transform coefficients? Well, this is the formula, right. So basically we can encode both the um magnitude as well as the phase in a single",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=196s",
        "start_time": "196.485"
    },
    {
        "id": "d9c05395",
        "text": "uh in in in in empirically like applied. So let's start like with the um mathematical equations that I introduced earlier. So here we have like our phase once again the magnitude and here we have the definition of a complex number in polar coordinates. OK. So how do we get the fourier transform coefficients? Well, this is the formula, right. So basically we can encode both the um magnitude as well as the phase in a single complex number. And this is called the complex fourier transform coefficient. And we have one of these for each of the frequencies we decompose our original signal intake. OK. So now let's take a look a a closer look at this. So",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=225s",
        "start_time": "225.25"
    },
    {
        "id": "42771463",
        "text": "So basically we can encode both the um magnitude as well as the phase in a single complex number. And this is called the complex fourier transform coefficient. And we have one of these for each of the frequencies we decompose our original signal intake. OK. So now let's take a look a a closer look at this. So basically, as you can see this is um we can just like map this um complex coefficient to this complex definition of a well polar representation of a complex number. So the absolute value of C gets mapped to the magnitude divided by square root of two.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=253s",
        "start_time": "253.839"
    },
    {
        "id": "a97d54df",
        "text": "complex number. And this is called the complex fourier transform coefficient. And we have one of these for each of the frequencies we decompose our original signal intake. OK. So now let's take a look a a closer look at this. So basically, as you can see this is um we can just like map this um complex coefficient to this complex definition of a well polar representation of a complex number. So the absolute value of C gets mapped to the magnitude divided by square root of two. Now, if you're wondering why we use the square root of two here is just because of like some normalization reasons. I'm not going to get into the details there, but it really doesn't matter that much because it's just divided by a concept. It's all it is right",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=265s",
        "start_time": "265.41"
    },
    {
        "id": "fbc72235",
        "text": "basically, as you can see this is um we can just like map this um complex coefficient to this complex definition of a well polar representation of a complex number. So the absolute value of C gets mapped to the magnitude divided by square root of two. Now, if you're wondering why we use the square root of two here is just because of like some normalization reasons. I'm not going to get into the details there, but it really doesn't matter that much because it's just divided by a concept. It's all it is right then the other thing that we have is gamma, the angle over here. And as you can see, gamma over here is equal to two pi phi and where Phi is basically the face and we also take like the minus here. And so in other words, like gamma here uh has this minus as well in it over here.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=283s",
        "start_time": "283.149"
    },
    {
        "id": "0c6c21a0",
        "text": "Now, if you're wondering why we use the square root of two here is just because of like some normalization reasons. I'm not going to get into the details there, but it really doesn't matter that much because it's just divided by a concept. It's all it is right then the other thing that we have is gamma, the angle over here. And as you can see, gamma over here is equal to two pi phi and where Phi is basically the face and we also take like the minus here. And so in other words, like gamma here uh has this minus as well in it over here. Cool. OK. So now let's go back to kind of like a visual representation of like this um fourier transform coefficients. So you should be familiar with this already from my previous video. But if you're not, I'll just like give you like an overview here about how we can interpret a complex number visually. And in this case, the complex number is our fourier transform coefficient for frequency F.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=306s",
        "start_time": "306.75"
    },
    {
        "id": "50d512be",
        "text": "then the other thing that we have is gamma, the angle over here. And as you can see, gamma over here is equal to two pi phi and where Phi is basically the face and we also take like the minus here. And so in other words, like gamma here uh has this minus as well in it over here. Cool. OK. So now let's go back to kind of like a visual representation of like this um fourier transform coefficients. So you should be familiar with this already from my previous video. But if you're not, I'll just like give you like an overview here about how we can interpret a complex number visually. And in this case, the complex number is our fourier transform coefficient for frequency F. So this guy here is nothing more than a point in the complex plane",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=322s",
        "start_time": "322.869"
    },
    {
        "id": "7b7f510f",
        "text": "Cool. OK. So now let's go back to kind of like a visual representation of like this um fourier transform coefficients. So you should be familiar with this already from my previous video. But if you're not, I'll just like give you like an overview here about how we can interpret a complex number visually. And in this case, the complex number is our fourier transform coefficient for frequency F. So this guy here is nothing more than a point in the complex plane in the complex plane. Obviously, we have the horizontal axis where we have like the real part of the complex number and the vertical axis where we have the imaginary parts of the complex number. So cf are free transform coefficient for frequency F is equal to like a point in this um a complex plane. And now if we want to map like the different parts to like this number, so we can see that",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=350s",
        "start_time": "350.79"
    },
    {
        "id": "bb1dfa47",
        "text": "So this guy here is nothing more than a point in the complex plane in the complex plane. Obviously, we have the horizontal axis where we have like the real part of the complex number and the vertical axis where we have the imaginary parts of the complex number. So cf are free transform coefficient for frequency F is equal to like a point in this um a complex plane. And now if we want to map like the different parts to like this number, so we can see that the distance of our complex number from the origin is given by the absolute value of the coefficient itself. So this guy over here, right?",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=380s",
        "start_time": "380.82"
    },
    {
        "id": "04ad85d2",
        "text": "in the complex plane. Obviously, we have the horizontal axis where we have like the real part of the complex number and the vertical axis where we have the imaginary parts of the complex number. So cf are free transform coefficient for frequency F is equal to like a point in this um a complex plane. And now if we want to map like the different parts to like this number, so we can see that the distance of our complex number from the origin is given by the absolute value of the coefficient itself. So this guy over here, right? So it's the magnitude divided by the square root of two. And then we have gamma and gamma is the angle between the positive real axis and the line that connects the uh our coefficient complex number with the origin and gamma once again is given by this two pi multiplied by the face. OK.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=388s",
        "start_time": "388.559"
    },
    {
        "id": "b0ad5762",
        "text": "the distance of our complex number from the origin is given by the absolute value of the coefficient itself. So this guy over here, right? So it's the magnitude divided by the square root of two. And then we have gamma and gamma is the angle between the positive real axis and the line that connects the uh our coefficient complex number with the origin and gamma once again is given by this two pi multiplied by the face. OK. And the face here, I should remind you is between zero and one and being B and B between zero and one. It means that we can go and just like design or just like trace the whole circle here. Cool. OK. So what does this minus",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=417s",
        "start_time": "417.85"
    },
    {
        "id": "b4fb5433",
        "text": "So it's the magnitude divided by the square root of two. And then we have gamma and gamma is the angle between the positive real axis and the line that connects the uh our coefficient complex number with the origin and gamma once again is given by this two pi multiplied by the face. OK. And the face here, I should remind you is between zero and one and being B and B between zero and one. It means that we can go and just like design or just like trace the whole circle here. Cool. OK. So what does this minus uh do? Right. So the cool thing about this minus is that when we increase the, the phase, what happens is that we are rotating clockwise? So if we increase the phase here, we just go down here, then we go down here. OK. So usually we are the complex number definition.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=431s",
        "start_time": "431.13"
    },
    {
        "id": "5d52e6fe",
        "text": "And the face here, I should remind you is between zero and one and being B and B between zero and one. It means that we can go and just like design or just like trace the whole circle here. Cool. OK. So what does this minus uh do? Right. So the cool thing about this minus is that when we increase the, the phase, what happens is that we are rotating clockwise? So if we increase the phase here, we just go down here, then we go down here. OK. So usually we are the complex number definition. Uh you just like rotate counterclockwise as we saw in the previous video but here we add a minus in this definition so that we can rotate clockwise when the phase increases cool.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=455s",
        "start_time": "455.829"
    },
    {
        "id": "fd77252a",
        "text": "uh do? Right. So the cool thing about this minus is that when we increase the, the phase, what happens is that we are rotating clockwise? So if we increase the phase here, we just go down here, then we go down here. OK. So usually we are the complex number definition. Uh you just like rotate counterclockwise as we saw in the previous video but here we add a minus in this definition so that we can rotate clockwise when the phase increases cool. OK. So now I want to uh like show you how we actually get to the fourier transform. And what's the uh definition of the fourier transform mathematically? And but for doing that, I just want to start from a continuous audio signal.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=477s",
        "start_time": "477.959"
    },
    {
        "id": "7343104f",
        "text": "Uh you just like rotate counterclockwise as we saw in the previous video but here we add a minus in this definition so that we can rotate clockwise when the phase increases cool. OK. So now I want to uh like show you how we actually get to the fourier transform. And what's the uh definition of the fourier transform mathematically? And but for doing that, I just want to start from a continuous audio signal. So our continuous audio signal is GFT and this is a function and mathematically this function G uh does this thing. So it gets a real number as an input and it outputs another real number. And this is as simple as a as having like this plot here, right? It's, we should be familiar by now with this. It's a, it's a, it's a wave form. OK.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=504s",
        "start_time": "504.57"
    },
    {
        "id": "05d915bf",
        "text": "OK. So now I want to uh like show you how we actually get to the fourier transform. And what's the uh definition of the fourier transform mathematically? And but for doing that, I just want to start from a continuous audio signal. So our continuous audio signal is GFT and this is a function and mathematically this function G uh does this thing. So it gets a real number as an input and it outputs another real number. And this is as simple as a as having like this plot here, right? It's, we should be familiar by now with this. It's a, it's a, it's a wave form. OK. So, and on the X axis, we have time and for each time we get back a an amplitude or intensity value. And this is why we, this function G which is our continuous audio signal gets a real number time as input and it provides us uh it outputs a uh another real number, sorry about that,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=522s",
        "start_time": "522.58"
    },
    {
        "id": "cfb8598e",
        "text": "So our continuous audio signal is GFT and this is a function and mathematically this function G uh does this thing. So it gets a real number as an input and it outputs another real number. And this is as simple as a as having like this plot here, right? It's, we should be familiar by now with this. It's a, it's a, it's a wave form. OK. So, and on the X axis, we have time and for each time we get back a an amplitude or intensity value. And this is why we, this function G which is our continuous audio signal gets a real number time as input and it provides us uh it outputs a uh another real number, sorry about that, another real number which is the actual sound pressure, intensity or amplitude.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=544s",
        "start_time": "544.369"
    },
    {
        "id": "f85a3475",
        "text": "So, and on the X axis, we have time and for each time we get back a an amplitude or intensity value. And this is why we, this function G which is our continuous audio signal gets a real number time as input and it provides us uh it outputs a uh another real number, sorry about that, another real number which is the actual sound pressure, intensity or amplitude. OK. Cool. This is in stark contrast with the actual complex fourier transform. So the complex fourier transform we can indicate with GH of F and the output of this is our coefficient, the fourier transform coefficients that we saw a couple of minutes ago, right? And we have one of those coefficients.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=571s",
        "start_time": "571.599"
    },
    {
        "id": "fec44e26",
        "text": "another real number which is the actual sound pressure, intensity or amplitude. OK. Cool. This is in stark contrast with the actual complex fourier transform. So the complex fourier transform we can indicate with GH of F and the output of this is our coefficient, the fourier transform coefficients that we saw a couple of minutes ago, right? And we have one of those coefficients. In other words, one of those complex numbers for each frequency cool. OK. But how does like these fourier transform like look like? So this this function, well, this is slightly different from the original signal. And that's because this function takes as input a real number which is the frequency right and the frequency is expressed in Hertz. But then",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=596s",
        "start_time": "596.89"
    },
    {
        "id": "d349060d",
        "text": "OK. Cool. This is in stark contrast with the actual complex fourier transform. So the complex fourier transform we can indicate with GH of F and the output of this is our coefficient, the fourier transform coefficients that we saw a couple of minutes ago, right? And we have one of those coefficients. In other words, one of those complex numbers for each frequency cool. OK. But how does like these fourier transform like look like? So this this function, well, this is slightly different from the original signal. And that's because this function takes as input a real number which is the frequency right and the frequency is expressed in Hertz. But then it doesn't output a real number but rather it outputs a complex number and the complex number that it outputs is the fourier transform coefficient cool.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=602s",
        "start_time": "602.38"
    },
    {
        "id": "ef198074",
        "text": "In other words, one of those complex numbers for each frequency cool. OK. But how does like these fourier transform like look like? So this this function, well, this is slightly different from the original signal. And that's because this function takes as input a real number which is the frequency right and the frequency is expressed in Hertz. But then it doesn't output a real number but rather it outputs a complex number and the complex number that it outputs is the fourier transform coefficient cool. So let's take a look at this in the um complex plane. So we already see this like so for frequency F one, for example, the output of the complex fourier transform is a point can be visualized as a point in the complex plane",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=632s",
        "start_time": "632.409"
    },
    {
        "id": "91cd7567",
        "text": "it doesn't output a real number but rather it outputs a complex number and the complex number that it outputs is the fourier transform coefficient cool. So let's take a look at this in the um complex plane. So we already see this like so for frequency F one, for example, the output of the complex fourier transform is a point can be visualized as a point in the complex plane cool. This is really, really interesting. So now probably you start to understand why we are using a complex representation for the fourier transform. The great thing about this is that given the fourier transform comes out with two",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=662s",
        "start_time": "662.63"
    },
    {
        "id": "02af62e2",
        "text": "So let's take a look at this in the um complex plane. So we already see this like so for frequency F one, for example, the output of the complex fourier transform is a point can be visualized as a point in the complex plane cool. This is really, really interesting. So now probably you start to understand why we are using a complex representation for the fourier transform. The great thing about this is that given the fourier transform comes out with two uh different parameters. So magnitude and phase we can just like package them up into a handy tool which is a complex number and then easily visualize that on a complex plane. Isn't that like fascinating how like math works so well when we describe like natural physical things in nature, right? Cool. OK. So this is another example of a",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=675s",
        "start_time": "675.89"
    },
    {
        "id": "936f7664",
        "text": "cool. This is really, really interesting. So now probably you start to understand why we are using a complex representation for the fourier transform. The great thing about this is that given the fourier transform comes out with two uh different parameters. So magnitude and phase we can just like package them up into a handy tool which is a complex number and then easily visualize that on a complex plane. Isn't that like fascinating how like math works so well when we describe like natural physical things in nature, right? Cool. OK. So this is another example of a uh of a of an output that we get from the complex fourier transform. And perhaps this is at F two, right? And then if we go to F three, we may have still another coefficients that's over here. Cool.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=701s",
        "start_time": "701.349"
    },
    {
        "id": "8d35cdad",
        "text": "uh different parameters. So magnitude and phase we can just like package them up into a handy tool which is a complex number and then easily visualize that on a complex plane. Isn't that like fascinating how like math works so well when we describe like natural physical things in nature, right? Cool. OK. So this is another example of a uh of a of an output that we get from the complex fourier transform. And perhaps this is at F two, right? And then if we go to F three, we may have still another coefficients that's over here. Cool. OK. Now, you have a little bit of an understanding of how like this uh complex fourier transform like works. But what about its mathematical definition? So how do we get to this cof to the uh fourier transform coefficient? Well, once again, let's start from the original uh stuff that we looked at uh a couple of videos ago. So here we have the formula for both the",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=716s",
        "start_time": "716.07"
    },
    {
        "id": "885e16e7",
        "text": "uh of a of an output that we get from the complex fourier transform. And perhaps this is at F two, right? And then if we go to F three, we may have still another coefficients that's over here. Cool. OK. Now, you have a little bit of an understanding of how like this uh complex fourier transform like works. But what about its mathematical definition? So how do we get to this cof to the uh fourier transform coefficient? Well, once again, let's start from the original uh stuff that we looked at uh a couple of videos ago. So here we have the formula for both the um magnitude and the face that come out from a fourier transform. And here we have the definition of the complex fourier transform cool. So I know this can look a little bit scary here but bear with me because we are going to look into a visual interpretation of this and you'll see that this is actually quite simple to grasp.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=745s",
        "start_time": "745.27"
    },
    {
        "id": "ac821a98",
        "text": "OK. Now, you have a little bit of an understanding of how like this uh complex fourier transform like works. But what about its mathematical definition? So how do we get to this cof to the uh fourier transform coefficient? Well, once again, let's start from the original uh stuff that we looked at uh a couple of videos ago. So here we have the formula for both the um magnitude and the face that come out from a fourier transform. And here we have the definition of the complex fourier transform cool. So I know this can look a little bit scary here but bear with me because we are going to look into a visual interpretation of this and you'll see that this is actually quite simple to grasp. OK. So as we said before, so the output of this G hatch at a specific fee is nothing more than uh a complex number on the complex plane. And it's our uh for a transform coefficient, right?",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=760s",
        "start_time": "760.65"
    },
    {
        "id": "ea86896b",
        "text": "um magnitude and the face that come out from a fourier transform. And here we have the definition of the complex fourier transform cool. So I know this can look a little bit scary here but bear with me because we are going to look into a visual interpretation of this and you'll see that this is actually quite simple to grasp. OK. So as we said before, so the output of this G hatch at a specific fee is nothing more than uh a complex number on the complex plane. And it's our uh for a transform coefficient, right? But now how do we actually get this guy starting from this beast of an integral here that has this exponential way, like some kind of weird like complex values and things like that. OK. So the first thing like we, we we will do here is to look into the different parts of this uh formula.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=790s",
        "start_time": "790.09"
    },
    {
        "id": "bfb3180a",
        "text": "OK. So as we said before, so the output of this G hatch at a specific fee is nothing more than uh a complex number on the complex plane. And it's our uh for a transform coefficient, right? But now how do we actually get this guy starting from this beast of an integral here that has this exponential way, like some kind of weird like complex values and things like that. OK. So the first thing like we, we we will do here is to look into the different parts of this uh formula. So the, the, the first one that we'll look into. And by now you should be familiar with this is this exponential over here. So the exponential here",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=820s",
        "start_time": "820.419"
    },
    {
        "id": "8f2f46f6",
        "text": "But now how do we actually get this guy starting from this beast of an integral here that has this exponential way, like some kind of weird like complex values and things like that. OK. So the first thing like we, we we will do here is to look into the different parts of this uh formula. So the, the, the first one that we'll look into. And by now you should be familiar with this is this exponential over here. So the exponential here um basically traces the unit circle in the complex plane. So ST increases, we just go through, we just like trace the whole unit circle. But now we are tracing the unit circle going clockwise. And why is that? Because we have this minus symbol over here.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=837s",
        "start_time": "837.625"
    },
    {
        "id": "c3e6ed75",
        "text": "So the, the, the first one that we'll look into. And by now you should be familiar with this is this exponential over here. So the exponential here um basically traces the unit circle in the complex plane. So ST increases, we just go through, we just like trace the whole unit circle. But now we are tracing the unit circle going clockwise. And why is that? Because we have this minus symbol over here. Now, the other thing to keep in mind here is that the speed at which we can complete a circle depends on this F value, which is the frequency. So for example, uh let's assume that frequency is equal to one is one Hertz, then it'll take us one second to go through to just complete one full circle. Now, if the frequency is equals to two,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=862s",
        "start_time": "862.989"
    },
    {
        "id": "e67f4982",
        "text": "um basically traces the unit circle in the complex plane. So ST increases, we just go through, we just like trace the whole unit circle. But now we are tracing the unit circle going clockwise. And why is that? Because we have this minus symbol over here. Now, the other thing to keep in mind here is that the speed at which we can complete a circle depends on this F value, which is the frequency. So for example, uh let's assume that frequency is equal to one is one Hertz, then it'll take us one second to go through to just complete one full circle. Now, if the frequency is equals to two, it'll take us half a second.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=874s",
        "start_time": "874.059"
    },
    {
        "id": "571723cc",
        "text": "Now, the other thing to keep in mind here is that the speed at which we can complete a circle depends on this F value, which is the frequency. So for example, uh let's assume that frequency is equal to one is one Hertz, then it'll take us one second to go through to just complete one full circle. Now, if the frequency is equals to two, it'll take us half a second. And that's because the period, if you remember is always like the inverse of frequency. So it will be like one divided by two, which is uh no 0.5 seconds. OK. So here you get the idea of how we can kind of like represent a sinusoidal, a sine wave um",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=902s",
        "start_time": "902.46"
    },
    {
        "id": "4641ed15",
        "text": "it'll take us half a second. And that's because the period, if you remember is always like the inverse of frequency. So it will be like one divided by two, which is uh no 0.5 seconds. OK. So here you get the idea of how we can kind of like represent a sinusoidal, a sine wave um using like complex numbers here. And this is going to be like the pure term that we use for decomposing a uh the original signal, right? And basically the idea is that we are going to be using many different frequencies for decomposing the original complex uh signal which is like this uh GFT",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=931s",
        "start_time": "931.239"
    },
    {
        "id": "09dcdb18",
        "text": "And that's because the period, if you remember is always like the inverse of frequency. So it will be like one divided by two, which is uh no 0.5 seconds. OK. So here you get the idea of how we can kind of like represent a sinusoidal, a sine wave um using like complex numbers here. And this is going to be like the pure term that we use for decomposing a uh the original signal, right? And basically the idea is that we are going to be using many different frequencies for decomposing the original complex uh signal which is like this uh GFT and into its different frequency components. The next component that we need to check out in our complex fourier transform formula is the signal GFT. Now I've created a custom signal because obviously I want to show you how the complex fourier transform works vis and how we can interpret that visually. So I've created a Jupiter notebook and here I have a bunch of",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=934s",
        "start_time": "934.84"
    },
    {
        "id": "6eaf5025",
        "text": "using like complex numbers here. And this is going to be like the pure term that we use for decomposing a uh the original signal, right? And basically the idea is that we are going to be using many different frequencies for decomposing the original complex uh signal which is like this uh GFT and into its different frequency components. The next component that we need to check out in our complex fourier transform formula is the signal GFT. Now I've created a custom signal because obviously I want to show you how the complex fourier transform works vis and how we can interpret that visually. So I've created a Jupiter notebook and here I have a bunch of functions. I'm not going to go through them line by line just because yeah, that's not like what like this video is about. But if you want to just dig into like this notebook, feel free to do it. I'll just leave you the link uh to the github where the core is. Uh the code is stored in my description below. But now let's take a look at this signal.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=955s",
        "start_time": "955.559"
    },
    {
        "id": "0fa1fd5e",
        "text": "and into its different frequency components. The next component that we need to check out in our complex fourier transform formula is the signal GFT. Now I've created a custom signal because obviously I want to show you how the complex fourier transform works vis and how we can interpret that visually. So I've created a Jupiter notebook and here I have a bunch of functions. I'm not going to go through them line by line just because yeah, that's not like what like this video is about. But if you want to just dig into like this notebook, feel free to do it. I'll just leave you the link uh to the github where the core is. Uh the code is stored in my description below. But now let's take a look at this signal. So what I've done here, you can quickly take a look at this. So I take in like a frequency and I use that like as a fundamental frequency. And so I have like a sine wave and I it's superimposed in a couple of other sine waves, the sine to and sine three and these have double the original the fundamental frequency and triple the fundamental uh frequency",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=982s",
        "start_time": "982.169"
    },
    {
        "id": "ecaa2eba",
        "text": "functions. I'm not going to go through them line by line just because yeah, that's not like what like this video is about. But if you want to just dig into like this notebook, feel free to do it. I'll just leave you the link uh to the github where the core is. Uh the code is stored in my description below. But now let's take a look at this signal. So what I've done here, you can quickly take a look at this. So I take in like a frequency and I use that like as a fundamental frequency. And so I have like a sine wave and I it's superimposed in a couple of other sine waves, the sine to and sine three and these have double the original the fundamental frequency and triple the fundamental uh frequency cool. OK. So now let's uh take a look at the results for that. And so I have like this nice util utility function that I defined. So plot signal. So I'm passing in the time and the time is between zero and 10 seconds and I have like 10,000 steps and then passing the signal. And as you can see, we have our signal which is time",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1008s",
        "start_time": "1008.4"
    },
    {
        "id": "e7986c17",
        "text": "So what I've done here, you can quickly take a look at this. So I take in like a frequency and I use that like as a fundamental frequency. And so I have like a sine wave and I it's superimposed in a couple of other sine waves, the sine to and sine three and these have double the original the fundamental frequency and triple the fundamental uh frequency cool. OK. So now let's uh take a look at the results for that. And so I have like this nice util utility function that I defined. So plot signal. So I'm passing in the time and the time is between zero and 10 seconds and I have like 10,000 steps and then passing the signal. And as you can see, we have our signal which is time uh against uh intensity over here. The next thing we want to check in our complex fourier transform is the multiplication between our original signal and the puritan. What does that look like visually? So let's check that out. OK. So we are back in the um Jupiter notebook. And so here I have a function that's called plot fourier transform. So here I can pass a pure turn frequency and I'll put this equal to one Hertz for the time being.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1030s",
        "start_time": "1030.68"
    },
    {
        "id": "04a5d609",
        "text": "cool. OK. So now let's uh take a look at the results for that. And so I have like this nice util utility function that I defined. So plot signal. So I'm passing in the time and the time is between zero and 10 seconds and I have like 10,000 steps and then passing the signal. And as you can see, we have our signal which is time uh against uh intensity over here. The next thing we want to check in our complex fourier transform is the multiplication between our original signal and the puritan. What does that look like visually? So let's check that out. OK. So we are back in the um Jupiter notebook. And so here I have a function that's called plot fourier transform. So here I can pass a pure turn frequency and I'll put this equal to one Hertz for the time being. And this is basically, it's gonna build a uh a sine wave and that's gonna be like our exponential over here. And then we're gonna have the signal frequency and we need to pass that in and I'll pass in that equal to one. And what that will do behind the scenes is that it will create this signal.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1055s",
        "start_time": "1055.689"
    },
    {
        "id": "f65543fe",
        "text": "uh against uh intensity over here. The next thing we want to check in our complex fourier transform is the multiplication between our original signal and the puritan. What does that look like visually? So let's check that out. OK. So we are back in the um Jupiter notebook. And so here I have a function that's called plot fourier transform. So here I can pass a pure turn frequency and I'll put this equal to one Hertz for the time being. And this is basically, it's gonna build a uh a sine wave and that's gonna be like our exponential over here. And then we're gonna have the signal frequency and we need to pass that in and I'll pass in that equal to one. And what that will do behind the scenes is that it will create this signal. And now when we plot the fourier transform, what we are doing is basically multiplying our uh signal by the sine wave of frequency one Hertz. OK. So now the next thing that once you pass in obviously like is time and so we will take once again like 10 seconds and I'll have like 1000 steps between those 10 seconds. OK. So here we go.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1083s",
        "start_time": "1083.67"
    },
    {
        "id": "5587d54e",
        "text": "And this is basically, it's gonna build a uh a sine wave and that's gonna be like our exponential over here. And then we're gonna have the signal frequency and we need to pass that in and I'll pass in that equal to one. And what that will do behind the scenes is that it will create this signal. And now when we plot the fourier transform, what we are doing is basically multiplying our uh signal by the sine wave of frequency one Hertz. OK. So now the next thing that once you pass in obviously like is time and so we will take once again like 10 seconds and I'll have like 1000 steps between those 10 seconds. OK. So here we go. Isn't this beautiful? So this is the complex plane and this is uh the horizontal axis is the real axis and the vertical axis is the imaginary axis. And here what we see this shape is the multiplication of the original signal which the sine wave frequency one. So the Pewter cool. OK. But",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1113s",
        "start_time": "1113.069"
    },
    {
        "id": "23fe1ca6",
        "text": "And now when we plot the fourier transform, what we are doing is basically multiplying our uh signal by the sine wave of frequency one Hertz. OK. So now the next thing that once you pass in obviously like is time and so we will take once again like 10 seconds and I'll have like 1000 steps between those 10 seconds. OK. So here we go. Isn't this beautiful? So this is the complex plane and this is uh the horizontal axis is the real axis and the vertical axis is the imaginary axis. And here what we see this shape is the multiplication of the original signal which the sine wave frequency one. So the Pewter cool. OK. But how can we interpret this? So what what's happening? So what's happening is that we are taking the original signal and we are wrapping that around the complex plane.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1137s",
        "start_time": "1137.3"
    },
    {
        "id": "57a208d4",
        "text": "Isn't this beautiful? So this is the complex plane and this is uh the horizontal axis is the real axis and the vertical axis is the imaginary axis. And here what we see this shape is the multiplication of the original signal which the sine wave frequency one. So the Pewter cool. OK. But how can we interpret this? So what what's happening? So what's happening is that we are taking the original signal and we are wrapping that around the complex plane. Isn't that really, really cool? OK. So now if I change the pent frequency slightly, say instead of one Hertz, I have 1.1 Hertz, what happens here is this crazy thing? Wow.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1166s",
        "start_time": "1166.29"
    },
    {
        "id": "20c96b62",
        "text": "how can we interpret this? So what what's happening? So what's happening is that we are taking the original signal and we are wrapping that around the complex plane. Isn't that really, really cool? OK. So now if I change the pent frequency slightly, say instead of one Hertz, I have 1.1 Hertz, what happens here is this crazy thing? Wow. So that tiny difference and I mean that tiny difference like in frequency caused all of this difference in it in its output. So how can we interpret this? Well, to see how to interpret this, we need to just like zoom into like the, the time domain. So I'll just like take a look at one second. So we'll, we'll just consider time between zero and one and see what happens here. And as you can see, we have like this",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1189s",
        "start_time": "1189.89"
    },
    {
        "id": "9890fe36",
        "text": "Isn't that really, really cool? OK. So now if I change the pent frequency slightly, say instead of one Hertz, I have 1.1 Hertz, what happens here is this crazy thing? Wow. So that tiny difference and I mean that tiny difference like in frequency caused all of this difference in it in its output. So how can we interpret this? Well, to see how to interpret this, we need to just like zoom into like the, the time domain. So I'll just like take a look at one second. So we'll, we'll just consider time between zero and one and see what happens here. And as you can see, we have like this shape over here. Now if we move to two seconds,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1205s",
        "start_time": "1205.829"
    },
    {
        "id": "e204fc6f",
        "text": "So that tiny difference and I mean that tiny difference like in frequency caused all of this difference in it in its output. So how can we interpret this? Well, to see how to interpret this, we need to just like zoom into like the, the time domain. So I'll just like take a look at one second. So we'll, we'll just consider time between zero and one and see what happens here. And as you can see, we have like this shape over here. Now if we move to two seconds, the next step is like drawing this other shape. Now if I continue at each step, you'll see that we continue drawing more stuff until we get to 10 where we basically like rotated",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1221s",
        "start_time": "1221.319"
    },
    {
        "id": "cc171fd8",
        "text": "shape over here. Now if we move to two seconds, the next step is like drawing this other shape. Now if I continue at each step, you'll see that we continue drawing more stuff until we get to 10 where we basically like rotated oops, we rotated all the way around cool. OK. So now another thing that I want to show you is that if we go to say two,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1248s",
        "start_time": "1248.77"
    },
    {
        "id": "769ccdd4",
        "text": "the next step is like drawing this other shape. Now if I continue at each step, you'll see that we continue drawing more stuff until we get to 10 where we basically like rotated oops, we rotated all the way around cool. OK. So now another thing that I want to show you is that if we go to say two, you can see that once again we have like quite stable shape, right? And the same thing also happens with free.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1253s",
        "start_time": "1253.599"
    },
    {
        "id": "f173bbcc",
        "text": "oops, we rotated all the way around cool. OK. So now another thing that I want to show you is that if we go to say two, you can see that once again we have like quite stable shape, right? And the same thing also happens with free. OK?",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1272s",
        "start_time": "1272.77"
    },
    {
        "id": "d2cc04cb",
        "text": "you can see that once again we have like quite stable shape, right? And the same thing also happens with free. OK? But if I do 3.1 for example, we have like once again, this crazy thing. And why is that the case?",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1289s",
        "start_time": "1289.02"
    },
    {
        "id": "5875503a",
        "text": "OK? But if I do 3.1 for example, we have like once again, this crazy thing. And why is that the case? Well, it's not the case because if you take a look at our signal, so we have like the fundamental frequency is one and then we have like the harmonics that are like a frequency equal to equals to Hertz and three Hertz. And so in those points, so it means on an intuitive level. What happen",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1301s",
        "start_time": "1301.75"
    },
    {
        "id": "1895dd60",
        "text": "But if I do 3.1 for example, we have like once again, this crazy thing. And why is that the case? Well, it's not the case because if you take a look at our signal, so we have like the fundamental frequency is one and then we have like the harmonics that are like a frequency equal to equals to Hertz and three Hertz. And so in those points, so it means on an intuitive level. What happen there? It basically means that whenever we have like frequencies that are represented that are present in the original sound, then we have a more stable output like this",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1303s",
        "start_time": "1303.239"
    },
    {
        "id": "a0391852",
        "text": "Well, it's not the case because if you take a look at our signal, so we have like the fundamental frequency is one and then we have like the harmonics that are like a frequency equal to equals to Hertz and three Hertz. And so in those points, so it means on an intuitive level. What happen there? It basically means that whenever we have like frequencies that are represented that are present in the original sound, then we have a more stable output like this isn't that cool? OK. But now we have like this kind of like interpretation of the um multiplication between our signal and the pure sound. But what about this beast? What about the integral?",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1313s",
        "start_time": "1313.339"
    },
    {
        "id": "48e989a7",
        "text": "there? It basically means that whenever we have like frequencies that are represented that are present in the original sound, then we have a more stable output like this isn't that cool? OK. But now we have like this kind of like interpretation of the um multiplication between our signal and the pure sound. But what about this beast? What about the integral? What does that look like? So I'm gonna show you that.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1340s",
        "start_time": "1340.525"
    },
    {
        "id": "7d0bdda7",
        "text": "isn't that cool? OK. But now we have like this kind of like interpretation of the um multiplication between our signal and the pure sound. But what about this beast? What about the integral? What does that look like? So I'm gonna show you that. So, and for showing, showing you dash I need another flag in here that's called plots center of gravity. And I'll put this equal to true. Now, intuitively what I want to do here",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1356s",
        "start_time": "1356.81"
    },
    {
        "id": "1695e53c",
        "text": "What does that look like? So I'm gonna show you that. So, and for showing, showing you dash I need another flag in here that's called plots center of gravity. And I'll put this equal to true. Now, intuitively what I want to do here is basically",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1371s",
        "start_time": "1371.989"
    },
    {
        "id": "b7fd385e",
        "text": "So, and for showing, showing you dash I need another flag in here that's called plots center of gravity. And I'll put this equal to true. Now, intuitively what I want to do here is basically take all of the different points and average them all. And if we do that, what happens is basically, so if, if we were dealing with some kind of like physical system. This would be the same as taking the center of gravity of that system, right? So we take all the points and then we average them.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1377s",
        "start_time": "1377.949"
    },
    {
        "id": "73f456da",
        "text": "is basically take all of the different points and average them all. And if we do that, what happens is basically, so if, if we were dealing with some kind of like physical system. This would be the same as taking the center of gravity of that system, right? So we take all the points and then we average them. And now I want to plot this center of gravity here and as you can see is this red dot over here cool. Now we are passing in the, we are decomposing uh using the a puritan frequency of three Hertz. Now let's go back to the fundamental. So one Hertz",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1396s",
        "start_time": "1396.91"
    },
    {
        "id": "d9b211f8",
        "text": "take all of the different points and average them all. And if we do that, what happens is basically, so if, if we were dealing with some kind of like physical system. This would be the same as taking the center of gravity of that system, right? So we take all the points and then we average them. And now I want to plot this center of gravity here and as you can see is this red dot over here cool. Now we are passing in the, we are decomposing uh using the a puritan frequency of three Hertz. Now let's go back to the fundamental. So one Hertz over here and it's the same, right? So we have this center of gravity that's here. OK. Now what happens if we take the center of gravity of 1 to 1, for example, right. So we want to decompose our signal with a sine wave that has frequency 1.1 and the result is the origin",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1400s",
        "start_time": "1400.68"
    },
    {
        "id": "2250d8ef",
        "text": "And now I want to plot this center of gravity here and as you can see is this red dot over here cool. Now we are passing in the, we are decomposing uh using the a puritan frequency of three Hertz. Now let's go back to the fundamental. So one Hertz over here and it's the same, right? So we have this center of gravity that's here. OK. Now what happens if we take the center of gravity of 1 to 1, for example, right. So we want to decompose our signal with a sine wave that has frequency 1.1 and the result is the origin is that somewhat unexpected? Well, I'd say no, why is that, why is not, why is not that thing unexpected? Well, that's because we are averaging across all the different points. And here, as you can see, we have like full symmetry and when we average everything, we just end up with zero because all the different points are canceling out. OK. Cool.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1423s",
        "start_time": "1423.39"
    },
    {
        "id": "b980f828",
        "text": "over here and it's the same, right? So we have this center of gravity that's here. OK. Now what happens if we take the center of gravity of 1 to 1, for example, right. So we want to decompose our signal with a sine wave that has frequency 1.1 and the result is the origin is that somewhat unexpected? Well, I'd say no, why is that, why is not, why is not that thing unexpected? Well, that's because we are averaging across all the different points. And here, as you can see, we have like full symmetry and when we average everything, we just end up with zero because all the different points are canceling out. OK. Cool. So this seems to like resonate with, with the intuitive like uh understanding that we had of the free transform, right? So whenever we have a frequency component that's represented, we have a magnitude that's different from zero necessarily. OK. Where, whereas when we are",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1444s",
        "start_time": "1444.189"
    },
    {
        "id": "b073c916",
        "text": "is that somewhat unexpected? Well, I'd say no, why is that, why is not, why is not that thing unexpected? Well, that's because we are averaging across all the different points. And here, as you can see, we have like full symmetry and when we average everything, we just end up with zero because all the different points are canceling out. OK. Cool. So this seems to like resonate with, with the intuitive like uh understanding that we had of the free transform, right? So whenever we have a frequency component that's represented, we have a magnitude that's different from zero necessarily. OK. Where, whereas when we are dealing with frequency components that are not present in the original signal, then we, we end up with magnitudes that are equal to zero, right? Because we don't have any traces of that in the original signal. Now,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1471s",
        "start_time": "1471.26"
    },
    {
        "id": "cad3ebd5",
        "text": "So this seems to like resonate with, with the intuitive like uh understanding that we had of the free transform, right? So whenever we have a frequency component that's represented, we have a magnitude that's different from zero necessarily. OK. Where, whereas when we are dealing with frequency components that are not present in the original signal, then we, we end up with magnitudes that are equal to zero, right? Because we don't have any traces of that in the original signal. Now, the thing here is that, so now let's go back to one here.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1499s",
        "start_time": "1499.52"
    },
    {
        "id": "17ae575b",
        "text": "dealing with frequency components that are not present in the original signal, then we, we end up with magnitudes that are equal to zero, right? Because we don't have any traces of that in the original signal. Now, the thing here is that, so now let's go back to one here. Uh The cool thing here is that we can think of the complex fourier transform coefficient as this point in the complex plane, right? And so now you get a",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1525s",
        "start_time": "1525.305"
    },
    {
        "id": "122a5a73",
        "text": "the thing here is that, so now let's go back to one here. Uh The cool thing here is that we can think of the complex fourier transform coefficient as this point in the complex plane, right? And so now you get a kind of like idea of how we get to that single point uh taking the foreign transform and basically going through the hassle of calculating all of this best, right.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1544s",
        "start_time": "1544.27"
    },
    {
        "id": "78c21e3f",
        "text": "Uh The cool thing here is that we can think of the complex fourier transform coefficient as this point in the complex plane, right? And so now you get a kind of like idea of how we get to that single point uh taking the foreign transform and basically going through the hassle of calculating all of this best, right. So what we do is we wrap around as the first step everything uh the the signal around the complex plane. And then we can we take the average of the center of gravity of this and that is going to be a complex number and that is the uh coefficient the full transform transform coefficients that has information about the face as well as about the",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1549s",
        "start_time": "1549.839"
    },
    {
        "id": "06e64a94",
        "text": "kind of like idea of how we get to that single point uh taking the foreign transform and basically going through the hassle of calculating all of this best, right. So what we do is we wrap around as the first step everything uh the the signal around the complex plane. And then we can we take the average of the center of gravity of this and that is going to be a complex number and that is the uh coefficient the full transform transform coefficients that has information about the face as well as about the magnitude. And the magnitude is given by the distance of this dot from the center of gravity dot from the origin and the face is given by the angle from the positive real axis.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1566s",
        "start_time": "1566.9"
    },
    {
        "id": "d3ab28cb",
        "text": "So what we do is we wrap around as the first step everything uh the the signal around the complex plane. And then we can we take the average of the center of gravity of this and that is going to be a complex number and that is the uh coefficient the full transform transform coefficients that has information about the face as well as about the magnitude. And the magnitude is given by the distance of this dot from the center of gravity dot from the origin and the face is given by the angle from the positive real axis. OK.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1582s",
        "start_time": "1582.849"
    },
    {
        "id": "50edde88",
        "text": "magnitude. And the magnitude is given by the distance of this dot from the center of gravity dot from the origin and the face is given by the angle from the positive real axis. OK. But I was just cheating a little bit because this is not really what's happening with the fourier transform. There's still another tiny thing that we are missing here here. I said that we are taking the um",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1612s",
        "start_time": "1612.209"
    },
    {
        "id": "8bdf2632",
        "text": "OK. But I was just cheating a little bit because this is not really what's happening with the fourier transform. There's still another tiny thing that we are missing here here. I said that we are taking the um the center of gravity. So the average, but this is not really what we are doing here. So we're not averaging all these values across time, right? But rather what we are doing is summing them up",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1627s",
        "start_time": "1627.569"
    },
    {
        "id": "50791eb3",
        "text": "But I was just cheating a little bit because this is not really what's happening with the fourier transform. There's still another tiny thing that we are missing here here. I said that we are taking the um the center of gravity. So the average, but this is not really what we are doing here. So we're not averaging all these values across time, right? But rather what we are doing is summing them up and the integral basically tells us to su to sum them up.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1629s",
        "start_time": "1629.199"
    },
    {
        "id": "d6d96777",
        "text": "the center of gravity. So the average, but this is not really what we are doing here. So we're not averaging all these values across time, right? But rather what we are doing is summing them up and the integral basically tells us to su to sum them up. So now let's see what happens if I plot the sum of all of these guys. So once again, I have another flag here that I can easily call and I'll say plot sum is equal to true. Now, let's do this and see what happens.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1645s",
        "start_time": "1645.109"
    },
    {
        "id": "dfdeeeb8",
        "text": "and the integral basically tells us to su to sum them up. So now let's see what happens if I plot the sum of all of these guys. So once again, I have another flag here that I can easily call and I'll say plot sum is equal to true. Now, let's do this and see what happens. Whoa That's weird.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1662s",
        "start_time": "1662.93"
    },
    {
        "id": "fd0204f2",
        "text": "So now let's see what happens if I plot the sum of all of these guys. So once again, I have another flag here that I can easily call and I'll say plot sum is equal to true. Now, let's do this and see what happens. Whoa That's weird. What's going on there, right? OK. So why do we have minus 500 for this green dot which is our sum and this is actually the coefficient um the fourier transform coefficient. So why is it minus 500? Well, that's because we are actually summing all of the values here. And now",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1668s",
        "start_time": "1668.819"
    },
    {
        "id": "2ddc1e0b",
        "text": "Whoa That's weird. What's going on there, right? OK. So why do we have minus 500 for this green dot which is our sum and this is actually the coefficient um the fourier transform coefficient. So why is it minus 500? Well, that's because we are actually summing all of the values here. And now uh across time now we have like 1000 steps here that we are summing. Now, if instead of like um 1000 steps here, we only took 100 steps. What we would end up with",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1689s",
        "start_time": "1689.829"
    },
    {
        "id": "202d606e",
        "text": "What's going on there, right? OK. So why do we have minus 500 for this green dot which is our sum and this is actually the coefficient um the fourier transform coefficient. So why is it minus 500? Well, that's because we are actually summing all of the values here. And now uh across time now we have like 1000 steps here that we are summing. Now, if instead of like um 1000 steps here, we only took 100 steps. What we would end up with is this guy here? OK, around more or less minus 50 on the imaginary axis, right? And why is that the case? Well, because here we are only summing 100 steps, not 1000. And do you see any sort of connection with the uh center of gravity here? So probably this is going to show you this",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1692s",
        "start_time": "1692.39"
    },
    {
        "id": "15b20bec",
        "text": "uh across time now we have like 1000 steps here that we are summing. Now, if instead of like um 1000 steps here, we only took 100 steps. What we would end up with is this guy here? OK, around more or less minus 50 on the imaginary axis, right? And why is that the case? Well, because here we are only summing 100 steps, not 1000. And do you see any sort of connection with the uh center of gravity here? So probably this is going to show you this easier.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1721s",
        "start_time": "1721.839"
    },
    {
        "id": "206ea859",
        "text": "is this guy here? OK, around more or less minus 50 on the imaginary axis, right? And why is that the case? Well, because here we are only summing 100 steps, not 1000. And do you see any sort of connection with the uh center of gravity here? So probably this is going to show you this easier. So the center of gravity is around like minus",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1741s",
        "start_time": "1741.589"
    },
    {
        "id": "7c14d84c",
        "text": "easier. So the center of gravity is around like minus 0.5 on the imaginary axis, whereas the sum in this case is equal to five. So what we are doing here is we are taking the center of gravity and then we are multiplying that by the numbers of time steps that we have in this case, it's equal to 10. So it's minus 0.5 times 10 and we get minus five. OK. So you, you get the idea here. So",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1771s",
        "start_time": "1771.31"
    },
    {
        "id": "6201156a",
        "text": "So the center of gravity is around like minus 0.5 on the imaginary axis, whereas the sum in this case is equal to five. So what we are doing here is we are taking the center of gravity and then we are multiplying that by the numbers of time steps that we have in this case, it's equal to 10. So it's minus 0.5 times 10 and we get minus five. OK. So you, you get the idea here. So in other words, uh like what we do is basically conceptually is we take the center of gravity and then we multiply that by the number of steps that we have that we are considering. And so the higher the number of steps that we are considering and then the, the higher the the the number of times we are multiplying the center of gravity for cool but",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1773s",
        "start_time": "1773.13"
    },
    {
        "id": "7d8b8a14",
        "text": "0.5 on the imaginary axis, whereas the sum in this case is equal to five. So what we are doing here is we are taking the center of gravity and then we are multiplying that by the numbers of time steps that we have in this case, it's equal to 10. So it's minus 0.5 times 10 and we get minus five. OK. So you, you get the idea here. So in other words, uh like what we do is basically conceptually is we take the center of gravity and then we multiply that by the number of steps that we have that we are considering. And so the higher the number of steps that we are considering and then the, the higher the the the number of times we are multiplying the center of gravity for cool but good. But what happens if we, we have like that 1.1 frequency instead which like the whole like symmetrical, really cool shape. So now let's go back to say between zero and 10 and here we'll do uh 1000 steps.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1777s",
        "start_time": "1777.689"
    },
    {
        "id": "4678d601",
        "text": "in other words, uh like what we do is basically conceptually is we take the center of gravity and then we multiply that by the number of steps that we have that we are considering. And so the higher the number of steps that we are considering and then the, the higher the the the number of times we are multiplying the center of gravity for cool but good. But what happens if we, we have like that 1.1 frequency instead which like the whole like symmetrical, really cool shape. So now let's go back to say between zero and 10 and here we'll do uh 1000 steps. So what's your guess? What are we going to see? Where is the green, where does like the green dot land?",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1807s",
        "start_time": "1807.3"
    },
    {
        "id": "f239907a",
        "text": "good. But what happens if we, we have like that 1.1 frequency instead which like the whole like symmetrical, really cool shape. So now let's go back to say between zero and 10 and here we'll do uh 1000 steps. So what's your guess? What are we going to see? Where is the green, where does like the green dot land? And as you can see it lands just right in the center. And why is that the case? Well, because once again, we are summing all the different points at different times, but they are just like canceling out",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1833s",
        "start_time": "1833.88"
    },
    {
        "id": "3de690de",
        "text": "So what's your guess? What are we going to see? Where is the green, where does like the green dot land? And as you can see it lands just right in the center. And why is that the case? Well, because once again, we are summing all the different points at different times, but they are just like canceling out And so this is how we end up with those peaks on the frequencies that are actually are present in the original sound. Now, you should have a visual understanding of how we get to the fourier transform coefficient. So",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1851s",
        "start_time": "1851.93"
    },
    {
        "id": "b3f8b7ab",
        "text": "And as you can see it lands just right in the center. And why is that the case? Well, because once again, we are summing all the different points at different times, but they are just like canceling out And so this is how we end up with those peaks on the frequencies that are actually are present in the original sound. Now, you should have a visual understanding of how we get to the fourier transform coefficient. So just like to wrap this up, so we start with the signal, then we wrap uh the signal around the complex plane by multiplying the signal by a pure turn.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1861s",
        "start_time": "1861.699"
    },
    {
        "id": "a3d9fbb2",
        "text": "And so this is how we end up with those peaks on the frequencies that are actually are present in the original sound. Now, you should have a visual understanding of how we get to the fourier transform coefficient. So just like to wrap this up, so we start with the signal, then we wrap uh the signal around the complex plane by multiplying the signal by a pure turn. And then what we do, we sum all the results uh for each time step. OK. Cool. But why do we have a, an integral sign here? Shouldn't we use like a sum kind of thing? Well, this really depends on the fact that we are using like um a continuous representation. So let me show you what it meant here. So first of all, let me,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1877s",
        "start_time": "1877.06"
    },
    {
        "id": "04dc7193",
        "text": "just like to wrap this up, so we start with the signal, then we wrap uh the signal around the complex plane by multiplying the signal by a pure turn. And then what we do, we sum all the results uh for each time step. OK. Cool. But why do we have a, an integral sign here? Shouldn't we use like a sum kind of thing? Well, this really depends on the fact that we are using like um a continuous representation. So let me show you what it meant here. So first of all, let me, I don't want to see all of these guys. I'll put falls here and falls here.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1900s",
        "start_time": "1900.27"
    },
    {
        "id": "078a70d3",
        "text": "And then what we do, we sum all the results uh for each time step. OK. Cool. But why do we have a, an integral sign here? Shouldn't we use like a sum kind of thing? Well, this really depends on the fact that we are using like um a continuous representation. So let me show you what it meant here. So first of all, let me, I don't want to see all of these guys. I'll put falls here and falls here. So now uh let's uh I don't want the lines to be plot. I just want uh points. So I'll do this.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1915s",
        "start_time": "1915.189"
    },
    {
        "id": "31ce0a2d",
        "text": "I don't want to see all of these guys. I'll put falls here and falls here. So now uh let's uh I don't want the lines to be plot. I just want uh points. So I'll do this. OK?",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1941s",
        "start_time": "1941.05"
    },
    {
        "id": "efd0e5e3",
        "text": "So now uh let's uh I don't want the lines to be plot. I just want uh points. So I'll do this. OK? So now let me start with only say",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1948s",
        "start_time": "1948.609"
    },
    {
        "id": "d2463ac5",
        "text": "OK? So now let me start with only say between zero and one second and we'll only take 10 steps.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1960s",
        "start_time": "1960.199"
    },
    {
        "id": "e14a03c4",
        "text": "So now let me start with only say between zero and one second and we'll only take 10 steps. And so here we have like 10 sets. Oh By the way, I have to rerun this, otherwise I'm gonna get these lines. So let me rerun this and now I have like all of these points. OK? So now I'm just like taking 10 points uh",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1961s",
        "start_time": "1961.54"
    },
    {
        "id": "e0a7dbaf",
        "text": "between zero and one second and we'll only take 10 steps. And so here we have like 10 sets. Oh By the way, I have to rerun this, otherwise I'm gonna get these lines. So let me rerun this and now I have like all of these points. OK? So now I'm just like taking 10 points uh for uh in, in this like one second. Interval. Now, if I take 100 points and this is gonna look a little bit more uh yeah, continuous. Like in a sense now, if I continue like this, uh I'll get to 1000 then I can get 10,000. Uh And we can uh continue like however we want. But basically, the idea is that",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1968s",
        "start_time": "1968.0"
    },
    {
        "id": "4c456bfa",
        "text": "And so here we have like 10 sets. Oh By the way, I have to rerun this, otherwise I'm gonna get these lines. So let me rerun this and now I have like all of these points. OK? So now I'm just like taking 10 points uh for uh in, in this like one second. Interval. Now, if I take 100 points and this is gonna look a little bit more uh yeah, continuous. Like in a sense now, if I continue like this, uh I'll get to 1000 then I can get 10,000. Uh And we can uh continue like however we want. But basically, the idea is that uh if we end up like with infinity here, so we have like uh as many steps as we want, then we are lacking continuous. Uh We are like dealing with a continuous variable because time is continuous and then the sum uh symbol becomes the integral. So yeah, this is the mystery of the integral explained for you. OK.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1973s",
        "start_time": "1973.589"
    },
    {
        "id": "8893e1a6",
        "text": "for uh in, in this like one second. Interval. Now, if I take 100 points and this is gonna look a little bit more uh yeah, continuous. Like in a sense now, if I continue like this, uh I'll get to 1000 then I can get 10,000. Uh And we can uh continue like however we want. But basically, the idea is that uh if we end up like with infinity here, so we have like uh as many steps as we want, then we are lacking continuous. Uh We are like dealing with a continuous variable because time is continuous and then the sum uh symbol becomes the integral. So yeah, this is the mystery of the integral explained for you. OK. So I hope that he really got the visual understanding of the complex fourier transform because I know you can feel like a lot. But once you uh think of this in visual terms, it all becomes like easier. OK. So now going back to this definition, if you remember from the previous video, we had this Euler formula. And so what we can do here is just like plug",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=1989s",
        "start_time": "1989.729"
    },
    {
        "id": "35c79846",
        "text": "uh if we end up like with infinity here, so we have like uh as many steps as we want, then we are lacking continuous. Uh We are like dealing with a continuous variable because time is continuous and then the sum uh symbol becomes the integral. So yeah, this is the mystery of the integral explained for you. OK. So I hope that he really got the visual understanding of the complex fourier transform because I know you can feel like a lot. But once you uh think of this in visual terms, it all becomes like easier. OK. So now going back to this definition, if you remember from the previous video, we had this Euler formula. And so what we can do here is just like plug this guy inside of here. And so what happens is that we can rewrite this initial um complex fourier transform like this. And this has the advantage of having like a real part in an imaginary part because as we know, uh G uh hat of F is gonna be a complex uh number and that is gonna be the complex. Um four transform coefficient cool.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2018s",
        "start_time": "2018.92"
    },
    {
        "id": "108b830a",
        "text": "So I hope that he really got the visual understanding of the complex fourier transform because I know you can feel like a lot. But once you uh think of this in visual terms, it all becomes like easier. OK. So now going back to this definition, if you remember from the previous video, we had this Euler formula. And so what we can do here is just like plug this guy inside of here. And so what happens is that we can rewrite this initial um complex fourier transform like this. And this has the advantage of having like a real part in an imaginary part because as we know, uh G uh hat of F is gonna be a complex uh number and that is gonna be the complex. Um four transform coefficient cool. OK. So moving on, so we can take the magnitude of the fourier transform. And for doing that, what we do is basically we just like take the absolute value of this function.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2044s",
        "start_time": "2044.88"
    },
    {
        "id": "5943361b",
        "text": "this guy inside of here. And so what happens is that we can rewrite this initial um complex fourier transform like this. And this has the advantage of having like a real part in an imaginary part because as we know, uh G uh hat of F is gonna be a complex uh number and that is gonna be the complex. Um four transform coefficient cool. OK. So moving on, so we can take the magnitude of the fourier transform. And for doing that, what we do is basically we just like take the absolute value of this function. And now if we go back to the complex fourier transform coefficient, we see that we, we can take like this coefficient or the magnitude D of F it's basically equal to the absolute value of the fourier transform of F multiplied by",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2074s",
        "start_time": "2074.35"
    },
    {
        "id": "84d95976",
        "text": "OK. So moving on, so we can take the magnitude of the fourier transform. And for doing that, what we do is basically we just like take the absolute value of this function. And now if we go back to the complex fourier transform coefficient, we see that we, we can take like this coefficient or the magnitude D of F it's basically equal to the absolute value of the fourier transform of F multiplied by um square root of two. And this is because we know that uh like this guy here, so the absolute value of the fourier transform is equal to this guy. Once again, this square root of two is a normalization uh constant. So yeah, let's not bother about that because it's not really that important at all. Now, in terms of the phase, the phase we can define by taking",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2104s",
        "start_time": "2104.459"
    },
    {
        "id": "c1d1c304",
        "text": "And now if we go back to the complex fourier transform coefficient, we see that we, we can take like this coefficient or the magnitude D of F it's basically equal to the absolute value of the fourier transform of F multiplied by um square root of two. And this is because we know that uh like this guy here, so the absolute value of the fourier transform is equal to this guy. Once again, this square root of two is a normalization uh constant. So yeah, let's not bother about that because it's not really that important at all. Now, in terms of the phase, the phase we can define by taking um the",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2118s",
        "start_time": "2118.06"
    },
    {
        "id": "cc6ff844",
        "text": "um square root of two. And this is because we know that uh like this guy here, so the absolute value of the fourier transform is equal to this guy. Once again, this square root of two is a normalization uh constant. So yeah, let's not bother about that because it's not really that important at all. Now, in terms of the phase, the phase we can define by taking um the A gamma. So the angle of uh the frequency F divided by two pi and take the minus there. And now we have like that gamma is equal to this guy here. So two pi multiplied by times the uh phase cool.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2140s",
        "start_time": "2140.679"
    },
    {
        "id": "ebcf948f",
        "text": "um the A gamma. So the angle of uh the frequency F divided by two pi and take the minus there. And now we have like that gamma is equal to this guy here. So two pi multiplied by times the uh phase cool. That's great. So now you should have like a real good deep understanding of the fourier transform. Now, you know what the magnitude is, what the phase is, how we can encode the magnet",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2169s",
        "start_time": "2169.899"
    },
    {
        "id": "7656a646",
        "text": "A gamma. So the angle of uh the frequency F divided by two pi and take the minus there. And now we have like that gamma is equal to this guy here. So two pi multiplied by times the uh phase cool. That's great. So now you should have like a real good deep understanding of the fourier transform. Now, you know what the magnitude is, what the phase is, how we can encode the magnet and the phase into a complex number which we call the fourier transform um coefficient. And now we also know the mathematical formula for getting the fourier transform. And we definitely understand the visual uh interpretation of that, which is great. So what's missing there? Well,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2172s",
        "start_time": "2172.969"
    },
    {
        "id": "eb3386b6",
        "text": "That's great. So now you should have like a real good deep understanding of the fourier transform. Now, you know what the magnitude is, what the phase is, how we can encode the magnet and the phase into a complex number which we call the fourier transform um coefficient. And now we also know the mathematical formula for getting the fourier transform. And we definitely understand the visual uh interpretation of that, which is great. So what's missing there? Well, what's missing is the inverse transform, but I promise this is going to be like super quick compared to what we've gone through so far. So we saw this a couple of videos ago already. But basically, we know that it's possible to go back from the frequency domain to the time domain using the",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2192s",
        "start_time": "2192.09"
    },
    {
        "id": "ef73ca49",
        "text": "and the phase into a complex number which we call the fourier transform um coefficient. And now we also know the mathematical formula for getting the fourier transform. And we definitely understand the visual uh interpretation of that, which is great. So what's missing there? Well, what's missing is the inverse transform, but I promise this is going to be like super quick compared to what we've gone through so far. So we saw this a couple of videos ago already. But basically, we know that it's possible to go back from the frequency domain to the time domain using the inverse fourier transform. And we saw this uh also in practice doing some a simplified version of this doing some adjective synthesis a couple of videos ago where I took different uh Syo sine waves and I just like added them up and I could recreate a create a complex sound. And basically, that's what we actually do with the inverse fourier transform. But there's a",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2207s",
        "start_time": "2207.56"
    },
    {
        "id": "175ef0d7",
        "text": "what's missing is the inverse transform, but I promise this is going to be like super quick compared to what we've gone through so far. So we saw this a couple of videos ago already. But basically, we know that it's possible to go back from the frequency domain to the time domain using the inverse fourier transform. And we saw this uh also in practice doing some a simplified version of this doing some adjective synthesis a couple of videos ago where I took different uh Syo sine waves and I just like added them up and I could recreate a create a complex sound. And basically, that's what we actually do with the inverse fourier transform. But there's a representation for this. So there's an actual mathematical representation for this and it's given by this other integral over here. And now the, the kind of like high level intuition here is that we just take all the different uh frequency components, we just multiply them by the magnitude and we add the phase and then we add them all up and we recon",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2234s",
        "start_time": "2234.04"
    },
    {
        "id": "b509d3c6",
        "text": "inverse fourier transform. And we saw this uh also in practice doing some a simplified version of this doing some adjective synthesis a couple of videos ago where I took different uh Syo sine waves and I just like added them up and I could recreate a create a complex sound. And basically, that's what we actually do with the inverse fourier transform. But there's a representation for this. So there's an actual mathematical representation for this and it's given by this other integral over here. And now the, the kind of like high level intuition here is that we just take all the different uh frequency components, we just multiply them by the magnitude and we add the phase and then we add them all up and we recon construct the original signal GFT. And the point here is that the original signal and the fourier transform do have the same data. So we can go from one to the other back and forth. However, how many times we want and then we'll never lose any information. Now, I know once again, this can feel a little bit frightening, but let's take a look at this visually very quick. So",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2253s",
        "start_time": "2253.334"
    },
    {
        "id": "77cea87f",
        "text": "representation for this. So there's an actual mathematical representation for this and it's given by this other integral over here. And now the, the kind of like high level intuition here is that we just take all the different uh frequency components, we just multiply them by the magnitude and we add the phase and then we add them all up and we recon construct the original signal GFT. And the point here is that the original signal and the fourier transform do have the same data. So we can go from one to the other back and forth. However, how many times we want and then we'll never lose any information. Now, I know once again, this can feel a little bit frightening, but let's take a look at this visually very quick. So here this guy, once again, we should be familiar with this by now, this is the pure tone of a frequency F. So in other words, we can visualize this uh like a simple like sine wave like this.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2282s",
        "start_time": "2282.159"
    },
    {
        "id": "c1c758fe",
        "text": "construct the original signal GFT. And the point here is that the original signal and the fourier transform do have the same data. So we can go from one to the other back and forth. However, how many times we want and then we'll never lose any information. Now, I know once again, this can feel a little bit frightening, but let's take a look at this visually very quick. So here this guy, once again, we should be familiar with this by now, this is the pure tone of a frequency F. So in other words, we can visualize this uh like a simple like sine wave like this. And then when the next step is to actually multiply the fourier transform coefficient. So the complex fourier transform coefficient by the pin. And when we do this, what we,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2309s",
        "start_time": "2309.635"
    },
    {
        "id": "7929d72f",
        "text": "here this guy, once again, we should be familiar with this by now, this is the pure tone of a frequency F. So in other words, we can visualize this uh like a simple like sine wave like this. And then when the next step is to actually multiply the fourier transform coefficient. So the complex fourier transform coefficient by the pin. And when we do this, what we, we actually do is we wait the puritan with the magnitude and add the phase. And so we basically move from this original puritan signal to this new signal that has potentially a different phase and a different amplitude because of the magnitude cool. So now what's the last step? Well, we have to integrate by now, we should know that integrating basically means adding up",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2336s",
        "start_time": "2336.26"
    },
    {
        "id": "ba06dc1b",
        "text": "And then when the next step is to actually multiply the fourier transform coefficient. So the complex fourier transform coefficient by the pin. And when we do this, what we, we actually do is we wait the puritan with the magnitude and add the phase. And so we basically move from this original puritan signal to this new signal that has potentially a different phase and a different amplitude because of the magnitude cool. So now what's the last step? Well, we have to integrate by now, we should know that integrating basically means adding up OK, a series of like infinite terms. And in this case, we're not integrating across time but rather across frequency. And in other words, what we are doing here is we add up all the weighted sinusoids and we get to the original",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2352s",
        "start_time": "2352.439"
    },
    {
        "id": "a5f1fba0",
        "text": "we actually do is we wait the puritan with the magnitude and add the phase. And so we basically move from this original puritan signal to this new signal that has potentially a different phase and a different amplitude because of the magnitude cool. So now what's the last step? Well, we have to integrate by now, we should know that integrating basically means adding up OK, a series of like infinite terms. And in this case, we're not integrating across time but rather across frequency. And in other words, what we are doing here is we add up all the weighted sinusoids and we get to the original sound. So we take for example, this first sine wave, the second sine wave, the third sine wave, we reconstruct them uh waiting them with the magnitude and not in the face, then we add them all up and we get this original signal",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2369s",
        "start_time": "2369.405"
    },
    {
        "id": "18961a98",
        "text": "OK, a series of like infinite terms. And in this case, we're not integrating across time but rather across frequency. And in other words, what we are doing here is we add up all the weighted sinusoids and we get to the original sound. So we take for example, this first sine wave, the second sine wave, the third sine wave, we reconstruct them uh waiting them with the magnitude and not in the face, then we add them all up and we get this original signal cool. So this is uh really, really cool stuff and now we are ready to look at the fourier round trip. So here we have like this couple of uh formulas here. The top one now we know is the fourier transform. The bottom one is the inverse fourier transform. Let's take a look at them and wonder at them",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2396s",
        "start_time": "2396.729"
    },
    {
        "id": "f697b974",
        "text": "sound. So we take for example, this first sine wave, the second sine wave, the third sine wave, we reconstruct them uh waiting them with the magnitude and not in the face, then we add them all up and we get this original signal cool. So this is uh really, really cool stuff and now we are ready to look at the fourier round trip. So here we have like this couple of uh formulas here. The top one now we know is the fourier transform. The bottom one is the inverse fourier transform. Let's take a look at them and wonder at them and they are fantastic. They are basically the same more or less function. The only thing that changes really is that here in the case of the fourier transform, we are integrating over time. Whereas in the case of the inverse fourier transform, we are integrating over frequency.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2414s",
        "start_time": "2414.945"
    },
    {
        "id": "63a7bc45",
        "text": "cool. So this is uh really, really cool stuff and now we are ready to look at the fourier round trip. So here we have like this couple of uh formulas here. The top one now we know is the fourier transform. The bottom one is the inverse fourier transform. Let's take a look at them and wonder at them and they are fantastic. They are basically the same more or less function. The only thing that changes really is that here in the case of the fourier transform, we are integrating over time. Whereas in the case of the inverse fourier transform, we are integrating over frequency. So isn't this beautiful? So all of that complexity captured in an elegant way with a very straightforward and minimal formula.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2433s",
        "start_time": "2433.85"
    },
    {
        "id": "dc10d75f",
        "text": "and they are fantastic. They are basically the same more or less function. The only thing that changes really is that here in the case of the fourier transform, we are integrating over time. Whereas in the case of the inverse fourier transform, we are integrating over frequency. So isn't this beautiful? So all of that complexity captured in an elegant way with a very straightforward and minimal formula. And also this idea of like getting back and forth from the time domain to the frequency domain and from the frequency of the time domain is just wonderful. And now I hope you understand why we went through the hustle of introducing compact numbers because now we have a super compact definition of the fourier transform and its inverse just by relying on this mathematical contract.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2459s",
        "start_time": "2459.439"
    },
    {
        "id": "32995ac6",
        "text": "So isn't this beautiful? So all of that complexity captured in an elegant way with a very straightforward and minimal formula. And also this idea of like getting back and forth from the time domain to the frequency domain and from the frequency of the time domain is just wonderful. And now I hope you understand why we went through the hustle of introducing compact numbers because now we have a super compact definition of the fourier transform and its inverse just by relying on this mathematical contract. OK. So this was a long one, wasn't it? But I I hope like it was like really work for you. Now I can finally go back to this uh pink Floyd iconic cover for the dark side of the moon.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2482s",
        "start_time": "2482.909"
    },
    {
        "id": "dc557e52",
        "text": "And also this idea of like getting back and forth from the time domain to the frequency domain and from the frequency of the time domain is just wonderful. And now I hope you understand why we went through the hustle of introducing compact numbers because now we have a super compact definition of the fourier transform and its inverse just by relying on this mathematical contract. OK. So this was a long one, wasn't it? But I I hope like it was like really work for you. Now I can finally go back to this uh pink Floyd iconic cover for the dark side of the moon. So now we really cracked the magic that happens in this algorithm down here. So we are now know how we can get a complex sound and divide that into its frequency components and vice versa, get all of the frequency components and put them back into a complex sound. We now know about the four transform.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2495s",
        "start_time": "2495.33"
    },
    {
        "id": "c638ddf8",
        "text": "OK. So this was a long one, wasn't it? But I I hope like it was like really work for you. Now I can finally go back to this uh pink Floyd iconic cover for the dark side of the moon. So now we really cracked the magic that happens in this algorithm down here. So we are now know how we can get a complex sound and divide that into its frequency components and vice versa, get all of the frequency components and put them back into a complex sound. We now know about the four transform. I hope like this uh was like a worth journey for you. For me, it's been really, really cool just like to cover all of these things in detail now. So what should we do next? Well, we have a problem",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2521s",
        "start_time": "2521.979"
    },
    {
        "id": "2284ccf5",
        "text": "So now we really cracked the magic that happens in this algorithm down here. So we are now know how we can get a complex sound and divide that into its frequency components and vice versa, get all of the frequency components and put them back into a complex sound. We now know about the four transform. I hope like this uh was like a worth journey for you. For me, it's been really, really cool just like to cover all of these things in detail now. So what should we do next? Well, we have a problem and the problem is that so far, we've treated everything in all the theory for the fourier transform. Uh in which a continuous mathematics, basically, we assumed that we were dealing with analog or continuous signals. But in reality,",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2537s",
        "start_time": "2537.879"
    },
    {
        "id": "db225378",
        "text": "I hope like this uh was like a worth journey for you. For me, it's been really, really cool just like to cover all of these things in detail now. So what should we do next? Well, we have a problem and the problem is that so far, we've treated everything in all the theory for the fourier transform. Uh in which a continuous mathematics, basically, we assumed that we were dealing with analog or continuous signals. But in reality, whenever we are actually applying a free transform, we are dealing with a digital signals, discrete signals. And so what we do is we start from a analog signal and then we sample that we digitalize that. And if you want to know how to do that, you can check out this video. But",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2564s",
        "start_time": "2564.979"
    },
    {
        "id": "10e821c2",
        "text": "and the problem is that so far, we've treated everything in all the theory for the fourier transform. Uh in which a continuous mathematics, basically, we assumed that we were dealing with analog or continuous signals. But in reality, whenever we are actually applying a free transform, we are dealing with a digital signals, discrete signals. And so what we do is we start from a analog signal and then we sample that we digitalize that. And if you want to know how to do that, you can check out this video. But so what's remaining to do regarding the fourier transform is understanding how we can adapt the theory that we've studied so far to the discrete uh world. And that's the world of our digital computers and that's the world that we'll use for actually extracting information from a uh signal. Cool. So next time we'll look into the discrete fourier transform",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2580s",
        "start_time": "2580.84"
    },
    {
        "id": "243dd0ee",
        "text": "whenever we are actually applying a free transform, we are dealing with a digital signals, discrete signals. And so what we do is we start from a analog signal and then we sample that we digitalize that. And if you want to know how to do that, you can check out this video. But so what's remaining to do regarding the fourier transform is understanding how we can adapt the theory that we've studied so far to the discrete uh world. And that's the world of our digital computers and that's the world that we'll use for actually extracting information from a uh signal. Cool. So next time we'll look into the discrete fourier transform cool. That's really it for today, I know it was a long one. I hope like it was worth for you going through all of this. But now you should have a very deep understanding of the fourier transform. So if you have any questions and I know that there's a lot to suggest for you today. Uh Just like, feel free to leave a comment in the comment section below and I hope I'll see you next time. Cheers.",
        "video": "Defining the Fourier Transform with Complex Numbers",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "KxRmbtJWUzI",
        "youtube_link": "https://www.youtube.com/watch?v=KxRmbtJWUzI&t=2600s",
        "start_time": "2600.004"
    },
    {
        "id": "70a34c7c",
        "text": "Hi, everybody and welcome to a new exciting VND audio signal processing for machine learning series. This time we'll continue our quest into the time domain audio features and specifically we'll be extracting the root main squared energy and zero crossing rate features using uh Li Brosa. OK. So let's get started. But before that, I, yeah, I just wanted to tell you that we are going to be rehashing a lot of the code that I used in my previous video where I was extracting amplitude envelope. So yeah, let's just like grab some of this code from here. OK. So the first thing that I want to grab is just like this imports here. So as you can see, I'm importing my plot noon P Li Brosa Li Brosa display for just displaying the waveforms and then I Python display for uh playing audio directly in the Jupiter Notebook. OK. So the next thing that I want to do is just load uh some audio and today just like in the previous video, we're gonna be using three audio files that are like three",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=0s",
        "start_time": "0.129"
    },
    {
        "id": "2fde0fb5",
        "text": "OK. So let's get started. But before that, I, yeah, I just wanted to tell you that we are going to be rehashing a lot of the code that I used in my previous video where I was extracting amplitude envelope. So yeah, let's just like grab some of this code from here. OK. So the first thing that I want to grab is just like this imports here. So as you can see, I'm importing my plot noon P Li Brosa Li Brosa display for just displaying the waveforms and then I Python display for uh playing audio directly in the Jupiter Notebook. OK. So the next thing that I want to do is just load uh some audio and today just like in the previous video, we're gonna be using three audio files that are like three music passages. One is by uh the Bey an orchestral piece. Another one is jazz piece by J Allington. And the third one is a rock song from the red hot chili peppers. OK. So let me just grab the path to these files and then as the next thing I just want to load them and again, I'm just gonna be reusing this code",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=21s",
        "start_time": "21.12"
    },
    {
        "id": "adf61340",
        "text": "So as you can see, I'm importing my plot noon P Li Brosa Li Brosa display for just displaying the waveforms and then I Python display for uh playing audio directly in the Jupiter Notebook. OK. So the next thing that I want to do is just load uh some audio and today just like in the previous video, we're gonna be using three audio files that are like three music passages. One is by uh the Bey an orchestral piece. Another one is jazz piece by J Allington. And the third one is a rock song from the red hot chili peppers. OK. So let me just grab the path to these files and then as the next thing I just want to load them and again, I'm just gonna be reusing this code just to keep things quicker. OK. So here, the only thing that I want to change is I'm like replacing that SR sampling rate, which oncore because we don't really need the sampling rate this time around, I believe. OK.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=43s",
        "start_time": "43.529"
    },
    {
        "id": "65100006",
        "text": "music passages. One is by uh the Bey an orchestral piece. Another one is jazz piece by J Allington. And the third one is a rock song from the red hot chili peppers. OK. So let me just grab the path to these files and then as the next thing I just want to load them and again, I'm just gonna be reusing this code just to keep things quicker. OK. So here, the only thing that I want to change is I'm like replacing that SR sampling rate, which oncore because we don't really need the sampling rate this time around, I believe. OK. So now we should have the signals loaded for the BC, red hot and red hot peppers and Duke Ellington as a first thing, we want to extract the rin squared energy using libros. So let me write that down. So extract R MS E with Li Brosa. So how do we do that? Well, we can use,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=70s",
        "start_time": "70.43"
    },
    {
        "id": "90928cdb",
        "text": "just to keep things quicker. OK. So here, the only thing that I want to change is I'm like replacing that SR sampling rate, which oncore because we don't really need the sampling rate this time around, I believe. OK. So now we should have the signals loaded for the BC, red hot and red hot peppers and Duke Ellington as a first thing, we want to extract the rin squared energy using libros. So let me write that down. So extract R MS E with Li Brosa. So how do we do that? Well, we can use, use a simple function coming from the browser in the featured module. But before we do that, let's just create a couple of constants or variables. So the first one is frame length and I'll put this equal to 1000 24 samples. So I'll use the same one that we used in the previous video and same thing for hop length.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=98s",
        "start_time": "98.069"
    },
    {
        "id": "a1398e28",
        "text": "So now we should have the signals loaded for the BC, red hot and red hot peppers and Duke Ellington as a first thing, we want to extract the rin squared energy using libros. So let me write that down. So extract R MS E with Li Brosa. So how do we do that? Well, we can use, use a simple function coming from the browser in the featured module. But before we do that, let's just create a couple of constants or variables. So the first one is frame length and I'll put this equal to 1000 24 samples. So I'll use the same one that we used in the previous video and same thing for hop length. Uh Yep. And this should be equal to 512. OK. So now let's extract the R MS for the PC. And for doing this, we'll do a Lisa dot feature",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=114s",
        "start_time": "114.25"
    },
    {
        "id": "7c17d8f3",
        "text": "use a simple function coming from the browser in the featured module. But before we do that, let's just create a couple of constants or variables. So the first one is frame length and I'll put this equal to 1000 24 samples. So I'll use the same one that we used in the previous video and same thing for hop length. Uh Yep. And this should be equal to 512. OK. So now let's extract the R MS for the PC. And for doing this, we'll do a Lisa dot feature dot uh R MS and we should pass in the signal for the BC. And then we should pass a couple of uh parameters. The first one being the frame length. And we'll set this uh equal to our frame length uh variable.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=139s",
        "start_time": "139.559"
    },
    {
        "id": "59aac0a0",
        "text": "Uh Yep. And this should be equal to 512. OK. So now let's extract the R MS for the PC. And for doing this, we'll do a Lisa dot feature dot uh R MS and we should pass in the signal for the BC. And then we should pass a couple of uh parameters. The first one being the frame length. And we'll set this uh equal to our frame length uh variable. And then we have the hop length and we'll set this equal to hop length. Once again, let me remind you that we can uh alternatively use both like hub length and H size or frame length and frame size. We're referring to the very same concept. OK. So now let me do the same thing for um",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=165s",
        "start_time": "165.899"
    },
    {
        "id": "6d7a77d0",
        "text": "dot uh R MS and we should pass in the signal for the BC. And then we should pass a couple of uh parameters. The first one being the frame length. And we'll set this uh equal to our frame length uh variable. And then we have the hop length and we'll set this equal to hop length. Once again, let me remind you that we can uh alternatively use both like hub length and H size or frame length and frame size. We're referring to the very same concept. OK. So now let me do the same thing for um uh the red hot sun, red hot chili pepper song. So I'll do A R MS red hot and I should pass in the signal for red hot in here. And here let me pass the signal for Duke Allington. Yeah, this was in the way of the cursor and here I should put in Duke. OK? So let me",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=181s",
        "start_time": "181.139"
    },
    {
        "id": "69aca98b",
        "text": "And then we have the hop length and we'll set this equal to hop length. Once again, let me remind you that we can uh alternatively use both like hub length and H size or frame length and frame size. We're referring to the very same concept. OK. So now let me do the same thing for um uh the red hot sun, red hot chili pepper song. So I'll do A R MS red hot and I should pass in the signal for red hot in here. And here let me pass the signal for Duke Allington. Yeah, this was in the way of the cursor and here I should put in Duke. OK? So let me uh yeah, run dutch. It seems like it works fine. So now what do we get back from the R MS uh function? Well, you can guess it, we get a non pi array but let's take a look at the shape of this NPI array. So we'll do a R MS DC dot shape.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=197s",
        "start_time": "197.339"
    },
    {
        "id": "ea7eaaf1",
        "text": "uh the red hot sun, red hot chili pepper song. So I'll do A R MS red hot and I should pass in the signal for red hot in here. And here let me pass the signal for Duke Allington. Yeah, this was in the way of the cursor and here I should put in Duke. OK? So let me uh yeah, run dutch. It seems like it works fine. So now what do we get back from the R MS uh function? Well, you can guess it, we get a non pi array but let's take a look at the shape of this NPI array. So we'll do a R MS DC dot shape. OK? And so as you can see, we have a couple of dimensions. So the first one is just like one and then we have like 1000 292 values, right? And so if you remember And yeah, this might be like a little bit like hard to remember because it just a number, but this is the very same number that we got when we extracted the amplitude envelope in the previous video. And that's because we were",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=219s",
        "start_time": "219.32"
    },
    {
        "id": "2bdc7789",
        "text": "uh yeah, run dutch. It seems like it works fine. So now what do we get back from the R MS uh function? Well, you can guess it, we get a non pi array but let's take a look at the shape of this NPI array. So we'll do a R MS DC dot shape. OK? And so as you can see, we have a couple of dimensions. So the first one is just like one and then we have like 1000 292 values, right? And so if you remember And yeah, this might be like a little bit like hard to remember because it just a number, but this is the very same number that we got when we extracted the amplitude envelope in the previous video. And that's because we were using the very same H length 512 samples and we had the same duration for the audio signal. Now, what we want is this guy here. So in order to get just this, so what we can do here is just take the",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=244s",
        "start_time": "244.039"
    },
    {
        "id": "f5d922a9",
        "text": "OK? And so as you can see, we have a couple of dimensions. So the first one is just like one and then we have like 1000 292 values, right? And so if you remember And yeah, this might be like a little bit like hard to remember because it just a number, but this is the very same number that we got when we extracted the amplitude envelope in the previous video. And that's because we were using the very same H length 512 samples and we had the same duration for the audio signal. Now, what we want is this guy here. So in order to get just this, so what we can do here is just take the index. So we, we'll just like take this guy here at index zero.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=264s",
        "start_time": "264.709"
    },
    {
        "id": "f2f224c3",
        "text": "using the very same H length 512 samples and we had the same duration for the audio signal. Now, what we want is this guy here. So in order to get just this, so what we can do here is just take the index. So we, we'll just like take this guy here at index zero. And now if we rerun this, this is just like equal to this, which is like what we wanted. Now, why did, why did I want that? Well, because now we are gonna plot.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=294s",
        "start_time": "294.415"
    },
    {
        "id": "90ca703d",
        "text": "index. So we, we'll just like take this guy here at index zero. And now if we rerun this, this is just like equal to this, which is like what we wanted. Now, why did, why did I want that? Well, because now we are gonna plot. So we want to uh plot the R MS E for all the music pieces. OK? So now I'm not gonna rewrite the code like for like the plot, but I'm just gonna steal it from my previous video. So not here. I'm gonna steal the one down here. OK?",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=314s",
        "start_time": "314.739"
    },
    {
        "id": "798d1244",
        "text": "And now if we rerun this, this is just like equal to this, which is like what we wanted. Now, why did, why did I want that? Well, because now we are gonna plot. So we want to uh plot the R MS E for all the music pieces. OK? So now I'm not gonna rewrite the code like for like the plot, but I'm just gonna steal it from my previous video. So not here. I'm gonna steal the one down here. OK? So let me go back here.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=320s",
        "start_time": "320.959"
    },
    {
        "id": "4241fa6b",
        "text": "So we want to uh plot the R MS E for all the music pieces. OK? So now I'm not gonna rewrite the code like for like the plot, but I'm just gonna steal it from my previous video. So not here. I'm gonna steal the one down here. OK? So let me go back here. And so let's do this. OK? So let's see what we should change our way here. So we have like our figure. Uh we create a subplot, we create like three stats, vertically stacked subplots and we want to display the wave plots for the beauty for Rich Hart and for Duke.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=334s",
        "start_time": "334.7"
    },
    {
        "id": "9a03c78b",
        "text": "So let me go back here. And so let's do this. OK? So let's see what we should change our way here. So we have like our figure. Uh we create a subplot, we create like three stats, vertically stacked subplots and we want to display the wave plots for the beauty for Rich Hart and for Duke. And here we want to uh display not the amplitude envelope for the BC, but rather the R MS for the BC. Here, we want to uh",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=365s",
        "start_time": "365.459"
    },
    {
        "id": "f90f0f7b",
        "text": "And so let's do this. OK? So let's see what we should change our way here. So we have like our figure. Uh we create a subplot, we create like three stats, vertically stacked subplots and we want to display the wave plots for the beauty for Rich Hart and for Duke. And here we want to uh display not the amplitude envelope for the BC, but rather the R MS for the BC. Here, we want to uh yeah, visualize the R MS for red hot chili peppers and down here for uh jake Allinson. So now there's one thing missing. So it is just like this tea value. And again, I'm gonna just grab it over here.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=368s",
        "start_time": "368.73"
    },
    {
        "id": "b3bf44bc",
        "text": "And here we want to uh display not the amplitude envelope for the BC, but rather the R MS for the BC. Here, we want to uh yeah, visualize the R MS for red hot chili peppers and down here for uh jake Allinson. So now there's one thing missing. So it is just like this tea value. And again, I'm gonna just grab it over here. So basically like this is the X",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=387s",
        "start_time": "387.7"
    },
    {
        "id": "06176fa7",
        "text": "yeah, visualize the R MS for red hot chili peppers and down here for uh jake Allinson. So now there's one thing missing. So it is just like this tea value. And again, I'm gonna just grab it over here. So basically like this is the X the value for all the values like on the X axis that you have like down here and here this uh the R MS red hot, for example, is the values for the Y axis for our um uh plots. And obviously what we want to do is just like move from frames like to, to time and we can do this like using the frames to time uh function that I introduced in my previous video.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=404s",
        "start_time": "404.44"
    },
    {
        "id": "5c4ead4a",
        "text": "So basically like this is the X the value for all the values like on the X axis that you have like down here and here this uh the R MS red hot, for example, is the values for the Y axis for our um uh plots. And obviously what we want to do is just like move from frames like to, to time and we can do this like using the frames to time uh function that I introduced in my previous video. OK? So now this should do the trick unless I messed up something like in the code. So let's see if it works and indeed I messed up something.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=419s",
        "start_time": "419.029"
    },
    {
        "id": "a5d50041",
        "text": "the value for all the values like on the X axis that you have like down here and here this uh the R MS red hot, for example, is the values for the Y axis for our um uh plots. And obviously what we want to do is just like move from frames like to, to time and we can do this like using the frames to time uh function that I introduced in my previous video. OK? So now this should do the trick unless I messed up something like in the code. So let's see if it works and indeed I messed up something. And that's Oh yeah. Right. That's because it was passing the amplitude envelope for Debussy, but we don't have that. We have that in the previous video. So let me just grab R MS for the bey. Now, uh the length for R MS DC R MS red hot and R MS D is going to be the same because we have the, these ones that have the very same duration, 30 seconds. OK. So now let's rerun this and it's",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=424s",
        "start_time": "424.609"
    },
    {
        "id": "5665180e",
        "text": "OK? So now this should do the trick unless I messed up something like in the code. So let's see if it works and indeed I messed up something. And that's Oh yeah. Right. That's because it was passing the amplitude envelope for Debussy, but we don't have that. We have that in the previous video. So let me just grab R MS for the bey. Now, uh the length for R MS DC R MS red hot and R MS D is going to be the same because we have the, these ones that have the very same duration, 30 seconds. OK. So now let's rerun this and it's here, we have it. So in red, you can see the R MS energy in this case for the busy here, for the red hot chili peppers and here for uh J Allington. OK.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=452s",
        "start_time": "452.26"
    },
    {
        "id": "c7772ae8",
        "text": "And that's Oh yeah. Right. That's because it was passing the amplitude envelope for Debussy, but we don't have that. We have that in the previous video. So let me just grab R MS for the bey. Now, uh the length for R MS DC R MS red hot and R MS D is going to be the same because we have the, these ones that have the very same duration, 30 seconds. OK. So now let's rerun this and it's here, we have it. So in red, you can see the R MS energy in this case for the busy here, for the red hot chili peppers and here for uh J Allington. OK. So as you can see, uh we still have like ideas that are like more or less comparable to what like we discovered in the previous video, which the amplitude envelope. But the cool thing here is that we have like way less like variability and outliers. And that's because for the R MS, we are considering all the samples in a frame, we're not just like taking the max uh value of the amplitude for a given sample inside like the the A frame, right?",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=464s",
        "start_time": "464.94"
    },
    {
        "id": "c0022fd7",
        "text": "here, we have it. So in red, you can see the R MS energy in this case for the busy here, for the red hot chili peppers and here for uh J Allington. OK. So as you can see, uh we still have like ideas that are like more or less comparable to what like we discovered in the previous video, which the amplitude envelope. But the cool thing here is that we have like way less like variability and outliers. And that's because for the R MS, we are considering all the samples in a frame, we're not just like taking the max uh value of the amplitude for a given sample inside like the the A frame, right? And uh now uh just like for the sake of comparison, let's just compare the amplitude envelope. So this is the amplitude envelope for the BC for red hot peppers and Duke Kington as we extracted like last time and compare that against this, the R MS. And as you can see as I was mentioning, so here like this, this just like go",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=493s",
        "start_time": "493.339"
    },
    {
        "id": "75d4dd80",
        "text": "So as you can see, uh we still have like ideas that are like more or less comparable to what like we discovered in the previous video, which the amplitude envelope. But the cool thing here is that we have like way less like variability and outliers. And that's because for the R MS, we are considering all the samples in a frame, we're not just like taking the max uh value of the amplitude for a given sample inside like the the A frame, right? And uh now uh just like for the sake of comparison, let's just compare the amplitude envelope. So this is the amplitude envelope for the BC for red hot peppers and Duke Kington as we extracted like last time and compare that against this, the R MS. And as you can see as I was mentioning, so here like this, this just like go it goes crazy, right? We have like a lot of spikes and it's just like follows like the upper part of the envelope obviously. Uh But here everything like is toned down. And again, this is like because we are uh considering",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=511s",
        "start_time": "511.0"
    },
    {
        "id": "b3518bac",
        "text": "And uh now uh just like for the sake of comparison, let's just compare the amplitude envelope. So this is the amplitude envelope for the BC for red hot peppers and Duke Kington as we extracted like last time and compare that against this, the R MS. And as you can see as I was mentioning, so here like this, this just like go it goes crazy, right? We have like a lot of spikes and it's just like follows like the upper part of the envelope obviously. Uh But here everything like is toned down. And again, this is like because we are uh considering aggregating the results for all the samples in a frame. OK. But um OK. So here, now you know how you can extract the R MS energy which Lisa. But wouldn't it be nice if we try to extract the R MS energy from scratch? Yes, it will be nice. So let's do that. OK. So let's create a function",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=541s",
        "start_time": "541.08"
    },
    {
        "id": "8c25bd93",
        "text": "it goes crazy, right? We have like a lot of spikes and it's just like follows like the upper part of the envelope obviously. Uh But here everything like is toned down. And again, this is like because we are uh considering aggregating the results for all the samples in a frame. OK. But um OK. So here, now you know how you can extract the R MS energy which Lisa. But wouldn't it be nice if we try to extract the R MS energy from scratch? Yes, it will be nice. So let's do that. OK. So let's create a function that we call R MS. So what does this function get as an input? It gets the signal,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=569s",
        "start_time": "569.0"
    },
    {
        "id": "89957941",
        "text": "aggregating the results for all the samples in a frame. OK. But um OK. So here, now you know how you can extract the R MS energy which Lisa. But wouldn't it be nice if we try to extract the R MS energy from scratch? Yes, it will be nice. So let's do that. OK. So let's create a function that we call R MS. So what does this function get as an input? It gets the signal, it also gets the frame length and it gets the hop length. Again, this function is gonna be like quite uh similar to the function that we built. Uh uh we built last time for the amplitude envelope. So,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=584s",
        "start_time": "584.609"
    },
    {
        "id": "e9a4f8d8",
        "text": "that we call R MS. So what does this function get as an input? It gets the signal, it also gets the frame length and it gets the hop length. Again, this function is gonna be like quite uh similar to the function that we built. Uh uh we built last time for the amplitude envelope. So, but before we get in here, so what I want to do is just like show you the R MS. OK. Let me show you do. So this is like the formula that we use for getting the root mean square energy. And so we do like we, we take the R MS of all the uh samples uh like in a frame.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=613s",
        "start_time": "613.619"
    },
    {
        "id": "84be30f3",
        "text": "it also gets the frame length and it gets the hop length. Again, this function is gonna be like quite uh similar to the function that we built. Uh uh we built last time for the amplitude envelope. So, but before we get in here, so what I want to do is just like show you the R MS. OK. Let me show you do. So this is like the formula that we use for getting the root mean square energy. And so we do like we, we take the R MS of all the uh samples uh like in a frame. And uh so here you have like the R MS calculated at frame T. And as you can see what we should see here is basically just calculating the energy first, then we should like sum the energy of all the, all the samples in a frame.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=622s",
        "start_time": "622.169"
    },
    {
        "id": "cda97f13",
        "text": "but before we get in here, so what I want to do is just like show you the R MS. OK. Let me show you do. So this is like the formula that we use for getting the root mean square energy. And so we do like we, we take the R MS of all the uh samples uh like in a frame. And uh so here you have like the R MS calculated at frame T. And as you can see what we should see here is basically just calculating the energy first, then we should like sum the energy of all the, all the samples in a frame. And then uh once we've done that, we want to divide by the number of samples that we have in a frame or the frame size. And then we just take the square root out of that. So we should convert this into code,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=645s",
        "start_time": "645.989"
    },
    {
        "id": "6e30486a",
        "text": "And uh so here you have like the R MS calculated at frame T. And as you can see what we should see here is basically just calculating the energy first, then we should like sum the energy of all the, all the samples in a frame. And then uh once we've done that, we want to divide by the number of samples that we have in a frame or the frame size. And then we just take the square root out of that. So we should convert this into code, let's do that. OK? So the first thing that we want to do is uh just create an empty",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=667s",
        "start_time": "667.989"
    },
    {
        "id": "6a0cc9f0",
        "text": "And then uh once we've done that, we want to divide by the number of samples that we have in a frame or the frame size. And then we just take the square root out of that. So we should convert this into code, let's do that. OK? So the first thing that we want to do is uh just create an empty um a list and, and this is gonna contain the R MS for each frame.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=685s",
        "start_time": "685.479"
    },
    {
        "id": "cababaf1",
        "text": "let's do that. OK? So the first thing that we want to do is uh just create an empty um a list and, and this is gonna contain the R MS for each frame. OK? And now we want to just look through all the frames in the audio signal. And so if we're doing that, we can say for I in a range and here we start from zero, we should stop at the length of the signal. And the step that we should use is the",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=704s",
        "start_time": "704.799"
    },
    {
        "id": "6d0cc3e3",
        "text": "um a list and, and this is gonna contain the R MS for each frame. OK? And now we want to just look through all the frames in the audio signal. And so if we're doing that, we can say for I in a range and here we start from zero, we should stop at the length of the signal. And the step that we should use is the HL right.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=714s",
        "start_time": "714.179"
    },
    {
        "id": "2b68e27e",
        "text": "OK? And now we want to just look through all the frames in the audio signal. And so if we're doing that, we can say for I in a range and here we start from zero, we should stop at the length of the signal. And the step that we should use is the HL right. So at every step we are moving the number of samples in the hop length like to the right to start like a new frame. Now, if you want to get like the details about this, I suggest you just like go check out my previous video in the amplitude envelope where I explained this like in detail. OK.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=724s",
        "start_time": "724.09"
    },
    {
        "id": "d22067d2",
        "text": "HL right. So at every step we are moving the number of samples in the hop length like to the right to start like a new frame. Now, if you want to get like the details about this, I suggest you just like go check out my previous video in the amplitude envelope where I explained this like in detail. OK. Now what we should do here is basically calculating the R MS for the current frame and the RM SS for the current frame is given by this formula here. So yeah, let's try like to, to put this like in, in place. So what should we do here? Well, first of all, let's take the non pi sum",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=751s",
        "start_time": "751.729"
    },
    {
        "id": "7a4fe83b",
        "text": "So at every step we are moving the number of samples in the hop length like to the right to start like a new frame. Now, if you want to get like the details about this, I suggest you just like go check out my previous video in the amplitude envelope where I explained this like in detail. OK. Now what we should do here is basically calculating the R MS for the current frame and the RM SS for the current frame is given by this formula here. So yeah, let's try like to, to put this like in, in place. So what should we do here? Well, first of all, let's take the non pi sum and we should take the sum of the signal",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=754s",
        "start_time": "754.52"
    },
    {
        "id": "bc06c116",
        "text": "Now what we should do here is basically calculating the R MS for the current frame and the RM SS for the current frame is given by this formula here. So yeah, let's try like to, to put this like in, in place. So what should we do here? Well, first of all, let's take the non pi sum and we should take the sum of the signal and the out of the signal, we want to slice only the samples in the current frame. And for doing that, we should do, we should like slice it between I and I plus uh frame length. And I've just discovered I have a typo if you let me",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=778s",
        "start_time": "778.599"
    },
    {
        "id": "6f6fca96",
        "text": "and we should take the sum of the signal and the out of the signal, we want to slice only the samples in the current frame. And for doing that, we should do, we should like slice it between I and I plus uh frame length. And I've just discovered I have a typo if you let me correct that OK. Frame life over here, which is cool. And uh OK, so once we've done this, we should",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=805s",
        "start_time": "805.45"
    },
    {
        "id": "799eb79f",
        "text": "and the out of the signal, we want to slice only the samples in the current frame. And for doing that, we should do, we should like slice it between I and I plus uh frame length. And I've just discovered I have a typo if you let me correct that OK. Frame life over here, which is cool. And uh OK, so once we've done this, we should square this volume.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=812s",
        "start_time": "812.409"
    },
    {
        "id": "27cb6abc",
        "text": "correct that OK. Frame life over here, which is cool. And uh OK, so once we've done this, we should square this volume. So basically, I'm just doing this, I'm taking this guy here and I'm taking like the, the sum",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=834s",
        "start_time": "834.539"
    },
    {
        "id": "44deced9",
        "text": "square this volume. So basically, I'm just doing this, I'm taking this guy here and I'm taking like the, the sum uh of all of this, of the energy. And uh once we've done that, the next step that we want to do is divide by the uh frame length. And finally,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=846s",
        "start_time": "846.07"
    },
    {
        "id": "ee8e3fba",
        "text": "So basically, I'm just doing this, I'm taking this guy here and I'm taking like the, the sum uh of all of this, of the energy. And uh once we've done that, the next step that we want to do is divide by the uh frame length. And finally, we should",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=849s",
        "start_time": "849.469"
    },
    {
        "id": "5d1a4ae0",
        "text": "uh of all of this, of the energy. And uh once we've done that, the next step that we want to do is divide by the uh frame length. And finally, we should do I then pi dot square root, OK. So this should work. And out of like all of this guy, we should get the R MS for the current frame and we want to append this to the R MS list. So we'll do an R MS dot uh uh pinned and we'll pass in the R MS current frame.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=858s",
        "start_time": "858.46"
    },
    {
        "id": "defc08bc",
        "text": "we should do I then pi dot square root, OK. So this should work. And out of like all of this guy, we should get the R MS for the current frame and we want to append this to the R MS list. So we'll do an R MS dot uh uh pinned and we'll pass in the R MS current frame. Great. So now we can return R MS. But before doing that, we'll convert this bad boy to be a NPI array. So",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=872s",
        "start_time": "872.63"
    },
    {
        "id": "2eae5969",
        "text": "do I then pi dot square root, OK. So this should work. And out of like all of this guy, we should get the R MS for the current frame and we want to append this to the R MS list. So we'll do an R MS dot uh uh pinned and we'll pass in the R MS current frame. Great. So now we can return R MS. But before doing that, we'll convert this bad boy to be a NPI array. So done this, OK? So this should work. So now let's see if it actually works.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=874s",
        "start_time": "874.469"
    },
    {
        "id": "b06ae274",
        "text": "Great. So now we can return R MS. But before doing that, we'll convert this bad boy to be a NPI array. So done this, OK? So this should work. So now let's see if it actually works. So we'll do, I, yeah, let me just like grab this",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=898s",
        "start_time": "898.739"
    },
    {
        "id": "f5acabbe",
        "text": "done this, OK? So this should work. So now let's see if it actually works. So we'll do, I, yeah, let me just like grab this is this guy down here.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=913s",
        "start_time": "913.369"
    },
    {
        "id": "b9fca239",
        "text": "So we'll do, I, yeah, let me just like grab this is this guy down here. We're here. So let's say R MS, one R MS, one red heart and R MS or one Duke. So now, obviously, we're not using the liberals implementation of R MS, but our own the homemade,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=920s",
        "start_time": "920.219"
    },
    {
        "id": "1e5df319",
        "text": "is this guy down here. We're here. So let's say R MS, one R MS, one red heart and R MS or one Duke. So now, obviously, we're not using the liberals implementation of R MS, but our own the homemade, the sound of a Ir MS implementation and I think like the, the arguments are the same. The only thing is we don't want to take like the, the element zero here.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=927s",
        "start_time": "927.69"
    },
    {
        "id": "177baafb",
        "text": "We're here. So let's say R MS, one R MS, one red heart and R MS or one Duke. So now, obviously, we're not using the liberals implementation of R MS, but our own the homemade, the sound of a Ir MS implementation and I think like the, the arguments are the same. The only thing is we don't want to take like the, the element zero here. And yeah, this seems to work. But now let's see if this actually works by comparing like our implementation to the, the bras one. So how do we do that? Well, we just like get this guy here",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=930s",
        "start_time": "930.46"
    },
    {
        "id": "90ab0e1e",
        "text": "the sound of a Ir MS implementation and I think like the, the arguments are the same. The only thing is we don't want to take like the, the element zero here. And yeah, this seems to work. But now let's see if this actually works by comparing like our implementation to the, the bras one. So how do we do that? Well, we just like get this guy here and we'll just like re plot everything but adding our implementation",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=946s",
        "start_time": "946.08"
    },
    {
        "id": "98233f24",
        "text": "And yeah, this seems to work. But now let's see if this actually works by comparing like our implementation to the, the bras one. So how do we do that? Well, we just like get this guy here and we'll just like re plot everything but adding our implementation as well. So we'll do a plot dot uh plot and we'll pass in T then R MS one to BC and but we want to change the color and yeah, let's put it to yellow, for example here.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=961s",
        "start_time": "961.179"
    },
    {
        "id": "9fe056a3",
        "text": "and we'll just like re plot everything but adding our implementation as well. So we'll do a plot dot uh plot and we'll pass in T then R MS one to BC and but we want to change the color and yeah, let's put it to yellow, for example here. Yeah, we'll do the same thing with the red hots.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=977s",
        "start_time": "977.63"
    },
    {
        "id": "96f90ed3",
        "text": "as well. So we'll do a plot dot uh plot and we'll pass in T then R MS one to BC and but we want to change the color and yeah, let's put it to yellow, for example here. Yeah, we'll do the same thing with the red hots. Peace",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=985s",
        "start_time": "985.01"
    },
    {
        "id": "a48063b1",
        "text": "Yeah, we'll do the same thing with the red hots. Peace and then we'll do the same thing for Duke Ellington. OK. So now let's plot this and obviously I have an issue here and yeah,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1005s",
        "start_time": "1005.08"
    },
    {
        "id": "298f5995",
        "text": "Peace and then we'll do the same thing for Duke Ellington. OK. So now let's plot this and obviously I have an issue here and yeah, yeah, this typos. So I was just missing this equal sign over here. Ok.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1012s",
        "start_time": "1012.169"
    },
    {
        "id": "6b54906b",
        "text": "and then we'll do the same thing for Duke Ellington. OK. So now let's plot this and obviously I have an issue here and yeah, yeah, this typos. So I was just missing this equal sign over here. Ok. Let's go and see. Yeah. And as you can see, basically like you can kind of like see the red.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1014s",
        "start_time": "1014.719"
    },
    {
        "id": "a83608fe",
        "text": "yeah, this typos. So I was just missing this equal sign over here. Ok. Let's go and see. Yeah. And as you can see, basically like you can kind of like see the red. Um Hello, like just like below the yellow, like R MS uh like RR MS. So basically this means that the two implementations like gave us the very same result, which is nice and it seems like Dutch like our implementation is working great. OK. So now we want to move on to another thing which is the zero crossing",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1029s",
        "start_time": "1029.119"
    },
    {
        "id": "002dbae2",
        "text": "Let's go and see. Yeah. And as you can see, basically like you can kind of like see the red. Um Hello, like just like below the yellow, like R MS uh like RR MS. So basically this means that the two implementations like gave us the very same result, which is nice and it seems like Dutch like our implementation is working great. OK. So now we want to move on to another thing which is the zero crossing crossing",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1037s",
        "start_time": "1037.41"
    },
    {
        "id": "55939c97",
        "text": "Um Hello, like just like below the yellow, like R MS uh like RR MS. So basically this means that the two implementations like gave us the very same result, which is nice and it seems like Dutch like our implementation is working great. OK. So now we want to move on to another thing which is the zero crossing crossing rates. OK. So how do we extract the zero crossing rate. Well, again, this is really a piece of cake because of uh li Brosa. So now I'm just gonna grab this",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1048s",
        "start_time": "1048.109"
    },
    {
        "id": "bbda8799",
        "text": "crossing rates. OK. So how do we extract the zero crossing rate. Well, again, this is really a piece of cake because of uh li Brosa. So now I'm just gonna grab this and instead of having like R MS, we'll say ZRC. So ZRC, this is the or well, I should say ZCR, so zero crossing rate",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1072s",
        "start_time": "1072.89"
    },
    {
        "id": "fcbb88eb",
        "text": "rates. OK. So how do we extract the zero crossing rate. Well, again, this is really a piece of cake because of uh li Brosa. So now I'm just gonna grab this and instead of having like R MS, we'll say ZRC. So ZRC, this is the or well, I should say ZCR, so zero crossing rate you OK.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1075s",
        "start_time": "1075.339"
    },
    {
        "id": "b6688d5a",
        "text": "and instead of having like R MS, we'll say ZRC. So ZRC, this is the or well, I should say ZCR, so zero crossing rate you OK. So we are getting in the zero crossing range. Obviously, what we should change here is we don't want to take the R MS from the feature module, but rather the zero crossing range.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1094s",
        "start_time": "1094.04"
    },
    {
        "id": "b98c32a2",
        "text": "you OK. So we are getting in the zero crossing range. Obviously, what we should change here is we don't want to take the R MS from the feature module, but rather the zero crossing range. OK. Let me replace this here. And then the",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1108s",
        "start_time": "1108.869"
    },
    {
        "id": "5699717b",
        "text": "So we are getting in the zero crossing range. Obviously, what we should change here is we don't want to take the R MS from the feature module, but rather the zero crossing range. OK. Let me replace this here. And then the uh the arguments are the very, very same arguments. OK.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1111s",
        "start_time": "1111.3"
    },
    {
        "id": "c7e1c05e",
        "text": "OK. Let me replace this here. And then the uh the arguments are the very, very same arguments. OK. So now we can take this. OK. So now what I want to do is just plot these guys. So let's visualize",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1122s",
        "start_time": "1122.65"
    },
    {
        "id": "1ca0dace",
        "text": "uh the arguments are the very, very same arguments. OK. So now we can take this. OK. So now what I want to do is just plot these guys. So let's visualize the zero crossing rate for all music pieces.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1129s",
        "start_time": "1129.4"
    },
    {
        "id": "2547ba3d",
        "text": "So now we can take this. OK. So now what I want to do is just plot these guys. So let's visualize the zero crossing rate for all music pieces. OK?",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1135s",
        "start_time": "1135.439"
    },
    {
        "id": "7f1d86d2",
        "text": "the zero crossing rate for all music pieces. OK? So how do we do that? Well, we can do, we can always use like the, the same",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1148s",
        "start_time": "1148.15"
    },
    {
        "id": "b2c60236",
        "text": "OK? So how do we do that? Well, we can do, we can always use like the, the same kind of like templates that we've been using so far.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1155s",
        "start_time": "1155.51"
    },
    {
        "id": "cae82e25",
        "text": "So how do we do that? Well, we can do, we can always use like the, the same kind of like templates that we've been using so far. Well, yeah, but perhaps I, yeah, I want to change it like this time. So yeah, so what I want to do here is only have a single",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1158s",
        "start_time": "1158.25"
    },
    {
        "id": "787eeae6",
        "text": "kind of like templates that we've been using so far. Well, yeah, but perhaps I, yeah, I want to change it like this time. So yeah, so what I want to do here is only have a single uh like a figure and no subplots and then have all the different uh yeah, the graphs for the different like zero crossing rates for like the different pieces inside the same thing so that we can compare it. OK. So let me do that. So I'll do I",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1166s",
        "start_time": "1166.079"
    },
    {
        "id": "bd10367b",
        "text": "Well, yeah, but perhaps I, yeah, I want to change it like this time. So yeah, so what I want to do here is only have a single uh like a figure and no subplots and then have all the different uh yeah, the graphs for the different like zero crossing rates for like the different pieces inside the same thing so that we can compare it. OK. So let me do that. So I'll do I plots dot plot T here. It's not R MS W BC, it's ZCRW BC. So we want this then we'll, yeah, I'll just copy this down here and I'll say ZCR and here we want the red hot and here ZCR for Duke Ellington. Ok.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1170s",
        "start_time": "1170.979"
    },
    {
        "id": "24fe09a0",
        "text": "uh like a figure and no subplots and then have all the different uh yeah, the graphs for the different like zero crossing rates for like the different pieces inside the same thing so that we can compare it. OK. So let me do that. So I'll do I plots dot plot T here. It's not R MS W BC, it's ZCRW BC. So we want this then we'll, yeah, I'll just copy this down here and I'll say ZCR and here we want the red hot and here ZCR for Duke Ellington. Ok. So now,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1180s",
        "start_time": "1180.719"
    },
    {
        "id": "d006246d",
        "text": "plots dot plot T here. It's not R MS W BC, it's ZCRW BC. So we want this then we'll, yeah, I'll just copy this down here and I'll say ZCR and here we want the red hot and here ZCR for Duke Ellington. Ok. So now, uh I know because I already tried it",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1206s",
        "start_time": "1206.67"
    },
    {
        "id": "33eca5c1",
        "text": "So now, uh I know because I already tried it that we can.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1234s",
        "start_time": "1234.03"
    },
    {
        "id": "13dd1d7a",
        "text": "uh I know because I already tried it that we can. So we are gonna get uh basically like the, the values for the zero crossing rate are gonna be normalized by the number of samples that we have in a frame. So they're normalized by the frame size so that we can get as a maximum, a value of one which would be like changing like sign at every uh value. OK.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1238s",
        "start_time": "1238.319"
    },
    {
        "id": "e54d62d2",
        "text": "that we can. So we are gonna get uh basically like the, the values for the zero crossing rate are gonna be normalized by the number of samples that we have in a frame. So they're normalized by the frame size so that we can get as a maximum, a value of one which would be like changing like sign at every uh value. OK. But uh so we'll,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1243s",
        "start_time": "1243.829"
    },
    {
        "id": "84136439",
        "text": "So we are gonna get uh basically like the, the values for the zero crossing rate are gonna be normalized by the number of samples that we have in a frame. So they're normalized by the frame size so that we can get as a maximum, a value of one which would be like changing like sign at every uh value. OK. But uh so we'll, so I, I because of that, I'll put the range on the Y axis between zero and one.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1247s",
        "start_time": "1247.489"
    },
    {
        "id": "71a2b661",
        "text": "But uh so we'll, so I, I because of that, I'll put the range on the Y axis between zero and one. OK? So I don't want a title here. I just want to do a plot show down here. So let's see if this works.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1272s",
        "start_time": "1272.569"
    },
    {
        "id": "3dea0c33",
        "text": "so I, I because of that, I'll put the range on the Y axis between zero and one. OK? So I don't want a title here. I just want to do a plot show down here. So let's see if this works. Here we go. Yes, but there's a problem, right? So now all of them are in red. So now we could put the WC one in yellow, red, hot chili peppers.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1276s",
        "start_time": "1276.369"
    },
    {
        "id": "e89ad48e",
        "text": "OK? So I don't want a title here. I just want to do a plot show down here. So let's see if this works. Here we go. Yes, but there's a problem, right? So now all of them are in red. So now we could put the WC one in yellow, red, hot chili peppers. You guess what color I'm gonna use that. I'm just gonna keep red there here and here for Duke Ellington and I'm gonna put blue. OK. So let's see.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1284s",
        "start_time": "1284.27"
    },
    {
        "id": "9b4625ac",
        "text": "Here we go. Yes, but there's a problem, right? So now all of them are in red. So now we could put the WC one in yellow, red, hot chili peppers. You guess what color I'm gonna use that. I'm just gonna keep red there here and here for Duke Ellington and I'm gonna put blue. OK. So let's see. And here you have it. So",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1293s",
        "start_time": "1293.42"
    },
    {
        "id": "23808b81",
        "text": "You guess what color I'm gonna use that. I'm just gonna keep red there here and here for Duke Ellington and I'm gonna put blue. OK. So let's see. And here you have it. So as we said in yellow, we have the bey, the zero crossing rate for the Bey in blue. We have the one for uh Duke Ellington and red the one for the red hot chili peppers. Now, is it a surprise that the red hot chili peppers have kind of like the highest zero crossing rat? No. And that's because like, they are using a lot of like, percussive instruments and usually rock music has a ZCR that's kind of like higher than classical music.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1304s",
        "start_time": "1304.829"
    },
    {
        "id": "d911053c",
        "text": "And here you have it. So as we said in yellow, we have the bey, the zero crossing rate for the Bey in blue. We have the one for uh Duke Ellington and red the one for the red hot chili peppers. Now, is it a surprise that the red hot chili peppers have kind of like the highest zero crossing rat? No. And that's because like, they are using a lot of like, percussive instruments and usually rock music has a ZCR that's kind of like higher than classical music. And here indeed, you have, like for the, the BC zero crossing rate over time, that's like quite low. It goes up probably when we have like the climates in the music.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1315s",
        "start_time": "1315.0"
    },
    {
        "id": "6f371c1a",
        "text": "as we said in yellow, we have the bey, the zero crossing rate for the Bey in blue. We have the one for uh Duke Ellington and red the one for the red hot chili peppers. Now, is it a surprise that the red hot chili peppers have kind of like the highest zero crossing rat? No. And that's because like, they are using a lot of like, percussive instruments and usually rock music has a ZCR that's kind of like higher than classical music. And here indeed, you have, like for the, the BC zero crossing rate over time, that's like quite low. It goes up probably when we have like the climates in the music. And perhaps because we are getting also like higher like with peach and that determines a higher like zero crossing reach. And the blue uh zero crossing rate for Duke Ellington is a little bit like all over the place and it seems to be a little bit like in between",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1320s",
        "start_time": "1320.64"
    },
    {
        "id": "023b8dfd",
        "text": "And here indeed, you have, like for the, the BC zero crossing rate over time, that's like quite low. It goes up probably when we have like the climates in the music. And perhaps because we are getting also like higher like with peach and that determines a higher like zero crossing reach. And the blue uh zero crossing rate for Duke Ellington is a little bit like all over the place and it seems to be a little bit like in between the one from the uh for, for the classical music and for the uh rock music. Now, if you want the actual values of the zero crossing rate and not just the normalized ones. So what you could do is take this guy and multiply it by the uh frame length. OK.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1350s",
        "start_time": "1350.92"
    },
    {
        "id": "668b4d07",
        "text": "And perhaps because we are getting also like higher like with peach and that determines a higher like zero crossing reach. And the blue uh zero crossing rate for Duke Ellington is a little bit like all over the place and it seems to be a little bit like in between the one from the uh for, for the classical music and for the uh rock music. Now, if you want the actual values of the zero crossing rate and not just the normalized ones. So what you could do is take this guy and multiply it by the uh frame length. OK. So we are gonna",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1364s",
        "start_time": "1364.359"
    },
    {
        "id": "c56fa402",
        "text": "the one from the uh for, for the classical music and for the uh rock music. Now, if you want the actual values of the zero crossing rate and not just the normalized ones. So what you could do is take this guy and multiply it by the uh frame length. OK. So we are gonna multiply the vector by the frame length. And now if I remember correctly, like I, we could put like between zero and 300 the range uh and it should be fine like this. Yes.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1380s",
        "start_time": "1380.4"
    },
    {
        "id": "e07f6d52",
        "text": "So we are gonna multiply the vector by the frame length. And now if I remember correctly, like I, we could put like between zero and 300 the range uh and it should be fine like this. Yes. Yeah. Let me just put a little bit more. Oops,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1403s",
        "start_time": "1403.949"
    },
    {
        "id": "8431beca",
        "text": "multiply the vector by the frame length. And now if I remember correctly, like I, we could put like between zero and 300 the range uh and it should be fine like this. Yes. Yeah. Let me just put a little bit more. Oops, I mean here, let's try 500. Yes. Looks good. Ok. So, and this is the actual number of zero crossing rates, the absolute number of 00 of, of zero crossings that you have like in these different signals.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1406s",
        "start_time": "1406.91"
    },
    {
        "id": "93da96c9",
        "text": "Yeah. Let me just put a little bit more. Oops, I mean here, let's try 500. Yes. Looks good. Ok. So, and this is the actual number of zero crossing rates, the absolute number of 00 of, of zero crossings that you have like in these different signals. Ok. So now I'm not done yet because another thing that I want to try is to see the zero crossing rates on some voice and compare that against an audio signal that only has uh white noise. Ok. So let's try that. So for train that we should just like a load",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1420s",
        "start_time": "1420.709"
    },
    {
        "id": "c9e3a998",
        "text": "I mean here, let's try 500. Yes. Looks good. Ok. So, and this is the actual number of zero crossing rates, the absolute number of 00 of, of zero crossings that you have like in these different signals. Ok. So now I'm not done yet because another thing that I want to try is to see the zero crossing rates on some voice and compare that against an audio signal that only has uh white noise. Ok. So let's try that. So for train that we should just like a load uh a couple of things. And so we'll do a voice file",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1426s",
        "start_time": "1426.369"
    },
    {
        "id": "2d9e67b7",
        "text": "Ok. So now I'm not done yet because another thing that I want to try is to see the zero crossing rates on some voice and compare that against an audio signal that only has uh white noise. Ok. So let's try that. So for train that we should just like a load uh a couple of things. And so we'll do a voice file and this, if I remember correctly, I should have stored it in audio inside the current uh like folder for like video nine. And it's called voice dot W OK. And then we have the noise file and this is called sorry, it's in the audio folder and then it's noise dot web.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1443s",
        "start_time": "1443.699"
    },
    {
        "id": "654cacaa",
        "text": "uh a couple of things. And so we'll do a voice file and this, if I remember correctly, I should have stored it in audio inside the current uh like folder for like video nine. And it's called voice dot W OK. And then we have the noise file and this is called sorry, it's in the audio folder and then it's noise dot web. OK. So now let's try to",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1468s",
        "start_time": "1468.01"
    },
    {
        "id": "9466725e",
        "text": "and this, if I remember correctly, I should have stored it in audio inside the current uh like folder for like video nine. And it's called voice dot W OK. And then we have the noise file and this is called sorry, it's in the audio folder and then it's noise dot web. OK. So now let's try to actually",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1474s",
        "start_time": "1474.79"
    },
    {
        "id": "0f291ef5",
        "text": "OK. So now let's try to actually uh load these guys first. So I'll do a voice and then for underscore and we'll use again li browser dot uh load and I'll pass in the uh voice file.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1502s",
        "start_time": "1502.38"
    },
    {
        "id": "f2eb7919",
        "text": "actually uh load these guys first. So I'll do a voice and then for underscore and we'll use again li browser dot uh load and I'll pass in the uh voice file. And here I'm gonna be using another really cool feature or argument that you can pass to um Libres alert, which is the duration. So you can't by passing like this argument, you, you actually tell Libres how much in terms of seconds you want of the audio uh yeah, file. OK. So we'll say, yeah, 15 seconds. OK? And I'll do the same for a noise",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1507s",
        "start_time": "1507.68"
    },
    {
        "id": "41a9305f",
        "text": "uh load these guys first. So I'll do a voice and then for underscore and we'll use again li browser dot uh load and I'll pass in the uh voice file. And here I'm gonna be using another really cool feature or argument that you can pass to um Libres alert, which is the duration. So you can't by passing like this argument, you, you actually tell Libres how much in terms of seconds you want of the audio uh yeah, file. OK. So we'll say, yeah, 15 seconds. OK? And I'll do the same for a noise and here it's not the voice file but it's the noise file. Next thing we want to listen to this audio files directly in the Jupiter notebook. So for doing that, we can easily use IP D dot Audio. And then we'll pass in the voice file.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1510s",
        "start_time": "1510.359"
    },
    {
        "id": "a566def0",
        "text": "And here I'm gonna be using another really cool feature or argument that you can pass to um Libres alert, which is the duration. So you can't by passing like this argument, you, you actually tell Libres how much in terms of seconds you want of the audio uh yeah, file. OK. So we'll say, yeah, 15 seconds. OK? And I'll do the same for a noise and here it's not the voice file but it's the noise file. Next thing we want to listen to this audio files directly in the Jupiter notebook. So for doing that, we can easily use IP D dot Audio. And then we'll pass in the voice file. Here we go. And here we have like our nice little player and I'll pass in the noise file.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1530s",
        "start_time": "1530.4"
    },
    {
        "id": "95a52bf8",
        "text": "and here it's not the voice file but it's the noise file. Next thing we want to listen to this audio files directly in the Jupiter notebook. So for doing that, we can easily use IP D dot Audio. And then we'll pass in the voice file. Here we go. And here we have like our nice little player and I'll pass in the noise file. Oh, here. Ok. It should go. OK. So let's listen to the 1st 15 seconds of the voice, Jack Pepsi CD. What do you think about that? I like the TD, Pepsi logo. It's like a Pepsi logo but it says Tad instead of Pepsi. Yeah, it's pretty. Yeah. So you get it a highly philosophical discussion about Pepsi Loger and here we have some white noise.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1560s",
        "start_time": "1560.849"
    },
    {
        "id": "c87d2d40",
        "text": "Here we go. And here we have like our nice little player and I'll pass in the noise file. Oh, here. Ok. It should go. OK. So let's listen to the 1st 15 seconds of the voice, Jack Pepsi CD. What do you think about that? I like the TD, Pepsi logo. It's like a Pepsi logo but it says Tad instead of Pepsi. Yeah, it's pretty. Yeah. So you get it a highly philosophical discussion about Pepsi Loger and here we have some white noise. Delightful, isn't it? OK.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1578s",
        "start_time": "1578.53"
    },
    {
        "id": "303ebb12",
        "text": "Oh, here. Ok. It should go. OK. So let's listen to the 1st 15 seconds of the voice, Jack Pepsi CD. What do you think about that? I like the TD, Pepsi logo. It's like a Pepsi logo but it says Tad instead of Pepsi. Yeah, it's pretty. Yeah. So you get it a highly philosophical discussion about Pepsi Loger and here we have some white noise. Delightful, isn't it? OK. Let's get the zero crossing rate for this too. So we'll do a ZCR voice and once again, we'll do a Li Brosa dot uh feature dot A zero crossing rate. And here",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1589s",
        "start_time": "1589.06"
    },
    {
        "id": "fe668b14",
        "text": "Delightful, isn't it? OK. Let's get the zero crossing rate for this too. So we'll do a ZCR voice and once again, we'll do a Li Brosa dot uh feature dot A zero crossing rate. And here we pass in uh the signal. So it's voice and then the frame length and opacity, our frame length and the uh hop length",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1621s",
        "start_time": "1621.5"
    },
    {
        "id": "29487e2a",
        "text": "Let's get the zero crossing rate for this too. So we'll do a ZCR voice and once again, we'll do a Li Brosa dot uh feature dot A zero crossing rate. And here we pass in uh the signal. So it's voice and then the frame length and opacity, our frame length and the uh hop length and let me pass the H length over here and now I'll just copy, paste this because I'm too lazy to rewrite dots the ZCR noise. And here we want to pass noise. OK? Looks good. Now, let's grab this guy over here",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1625s",
        "start_time": "1625.55"
    },
    {
        "id": "80222f20",
        "text": "we pass in uh the signal. So it's voice and then the frame length and opacity, our frame length and the uh hop length and let me pass the H length over here and now I'll just copy, paste this because I'm too lazy to rewrite dots the ZCR noise. And here we want to pass noise. OK? Looks good. Now, let's grab this guy over here just to visualize the voice and the noise in the same um",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1643s",
        "start_time": "1643.39"
    },
    {
        "id": "7c54a6de",
        "text": "and let me pass the H length over here and now I'll just copy, paste this because I'm too lazy to rewrite dots the ZCR noise. And here we want to pass noise. OK? Looks good. Now, let's grab this guy over here just to visualize the voice and the noise in the same um uh graph. OK?",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1657s",
        "start_time": "1657.31"
    },
    {
        "id": "cad91a20",
        "text": "just to visualize the voice and the noise in the same um uh graph. OK? To let me just",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1680s",
        "start_time": "1680.239"
    },
    {
        "id": "cd165ebf",
        "text": "uh graph. OK? To let me just copy this voice and paste it here and then",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1688s",
        "start_time": "1688.17"
    },
    {
        "id": "d62f6790",
        "text": "To let me just copy this voice and paste it here and then do the same thing for no. And now we'll just like keep the normalized uh version of those. So I'm just gonna, yeah, get rid of these guys and these should be in range 01. So now yeah, let me just like",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1691s",
        "start_time": "1691.3"
    },
    {
        "id": "b1874922",
        "text": "copy this voice and paste it here and then do the same thing for no. And now we'll just like keep the normalized uh version of those. So I'm just gonna, yeah, get rid of these guys and these should be in range 01. So now yeah, let me just like put this like to 12 because before like it was like too, too high, wasn't it? OK. Yeah, we have an issue here.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1694s",
        "start_time": "1694.43"
    },
    {
        "id": "cc9c5bce",
        "text": "do the same thing for no. And now we'll just like keep the normalized uh version of those. So I'm just gonna, yeah, get rid of these guys and these should be in range 01. So now yeah, let me just like put this like to 12 because before like it was like too, too high, wasn't it? OK. Yeah, we have an issue here. Yes,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1700s",
        "start_time": "1700.06"
    },
    {
        "id": "cffaa6cf",
        "text": "put this like to 12 because before like it was like too, too high, wasn't it? OK. Yeah, we have an issue here. Yes, that's a good one, right? So like the mistake here is basically I passed uh like this T right. But this t was calculated on the number of frames that we had for the uh the previous, for the music passages and that was like 30 seconds worth worth of audio here, we only have like 15 seconds worth of audio. So we'll have to",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1719s",
        "start_time": "1719.109"
    },
    {
        "id": "95d69635",
        "text": "Yes, that's a good one, right? So like the mistake here is basically I passed uh like this T right. But this t was calculated on the number of frames that we had for the uh the previous, for the music passages and that was like 30 seconds worth worth of audio here, we only have like 15 seconds worth of audio. So we'll have to uh recalculate this of T value. OK? So for doing that, we'll do I frames and here uh we should do a range between zero and the length of ZCR",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1727s",
        "start_time": "1727.54"
    },
    {
        "id": "43541bcd",
        "text": "that's a good one, right? So like the mistake here is basically I passed uh like this T right. But this t was calculated on the number of frames that we had for the uh the previous, for the music passages and that was like 30 seconds worth worth of audio here, we only have like 15 seconds worth of audio. So we'll have to uh recalculate this of T value. OK? So for doing that, we'll do I frames and here uh we should do a range between zero and the length of ZCR voice. OK?",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1729s",
        "start_time": "1729.119"
    },
    {
        "id": "72e69e12",
        "text": "uh recalculate this of T value. OK? So for doing that, we'll do I frames and here uh we should do a range between zero and the length of ZCR voice. OK? And then the next thing that we need to do is, yeah, just like get uh this value here. And for that we should",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1752s",
        "start_time": "1752.989"
    },
    {
        "id": "4d8be932",
        "text": "voice. OK? And then the next thing that we need to do is, yeah, just like get uh this value here. And for that we should use,",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1773s",
        "start_time": "1773.119"
    },
    {
        "id": "df1a69c6",
        "text": "And then the next thing that we need to do is, yeah, just like get uh this value here. And for that we should use, where is it over here? Yes, here it goes",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1776s",
        "start_time": "1776.13"
    },
    {
        "id": "d0b66823",
        "text": "use, where is it over here? Yes, here it goes is function from Libras.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1787s",
        "start_time": "1787.51"
    },
    {
        "id": "7c485961",
        "text": "where is it over here? Yes, here it goes is function from Libras. OK. So frames to time and we should pass in frames. Now, there's another mistake that I noticed. So here we should take uh like the, the value at index uh zero. Ok. So now let me redo this and hopefully this time it should work and indeed it does. Ok.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1790s",
        "start_time": "1790.329"
    },
    {
        "id": "7f4d7d0e",
        "text": "is function from Libras. OK. So frames to time and we should pass in frames. Now, there's another mistake that I noticed. So here we should take uh like the, the value at index uh zero. Ok. So now let me redo this and hopefully this time it should work and indeed it does. Ok. So in a yellow, you should have the voice and in a red the noise. And as expected, usually the white noise has a zero, a number of crossing rates that's higher than voice and",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1794s",
        "start_time": "1794.91"
    },
    {
        "id": "33cd76b1",
        "text": "OK. So frames to time and we should pass in frames. Now, there's another mistake that I noticed. So here we should take uh like the, the value at index uh zero. Ok. So now let me redo this and hopefully this time it should work and indeed it does. Ok. So in a yellow, you should have the voice and in a red the noise. And as expected, usually the white noise has a zero, a number of crossing rates that's higher than voice and that's it. Ok. So you now get the idea of how to extract zero crossing rate using Li Brosa. And we've also seen how like zero crossing rates like changes in different music styles as well when we compare it for voice and noise um audio files. So yeah, that's it. So we are basically done with the temporal",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1799s",
        "start_time": "1799.15"
    },
    {
        "id": "234d470f",
        "text": "So in a yellow, you should have the voice and in a red the noise. And as expected, usually the white noise has a zero, a number of crossing rates that's higher than voice and that's it. Ok. So you now get the idea of how to extract zero crossing rate using Li Brosa. And we've also seen how like zero crossing rates like changes in different music styles as well when we compare it for voice and noise um audio files. So yeah, that's it. So we are basically done with the temporal audio features. So next time we'll start looking into uh frequency domain uh features and specifically, we are going to be starting looking into the fourier transfer of transform, which is going to be like our kind of like very important tool that we need for getting any information in the frequency domain.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1818s",
        "start_time": "1818.699"
    },
    {
        "id": "31b8e2b5",
        "text": "that's it. Ok. So you now get the idea of how to extract zero crossing rate using Li Brosa. And we've also seen how like zero crossing rates like changes in different music styles as well when we compare it for voice and noise um audio files. So yeah, that's it. So we are basically done with the temporal audio features. So next time we'll start looking into uh frequency domain uh features and specifically, we are going to be starting looking into the fourier transfer of transform, which is going to be like our kind of like very important tool that we need for getting any information in the frequency domain. OK. So I hope you've really enjoyed uh this video. If that's the case, please give us, give us a thumb up. Then if you have any questions as usual, just post them like in the comments section below. One thing that I want to remind you of is the sound of the eye slack community, which is a community where you'll find",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1837s",
        "start_time": "1837.65"
    },
    {
        "id": "d7f34892",
        "text": "audio features. So next time we'll start looking into uh frequency domain uh features and specifically, we are going to be starting looking into the fourier transfer of transform, which is going to be like our kind of like very important tool that we need for getting any information in the frequency domain. OK. So I hope you've really enjoyed uh this video. If that's the case, please give us, give us a thumb up. Then if you have any questions as usual, just post them like in the comments section below. One thing that I want to remind you of is the sound of the eye slack community, which is a community where you'll find very interesting people uh who are like interested in all things A I Audio A I music digital signal processing. So if you want to grow your skills, grow your network as well, I suggest you to go and join this community. I'll leave you the link to the community in this description section. OK? It's all for today. I hope I'll see you next time. Cheers.",
        "video": "How to Extract Root-Mean Square Energy and Zero-Crossing Rate from Audio",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "EycaSbIRx-0",
        "youtube_link": "https://www.youtube.com/watch?v=EycaSbIRx-0&t=1863s",
        "start_time": "1863.739"
    },
    {
        "id": "10d22016",
        "text": "Hi, everybody and welcome to another video in the audio processing for machine learning series. Last time we talked about basic features of sounds. So what sound is waveforms and introduce the concept of frequency. This time we continue delving into the features of sound, talking about intensity, power, loudness and timbre. So first of all, I want to talk about two, a couple of concepts. So intensity and power and these are like connected together. So let's start from the power of sound. Well, this is we all know how the power of sound, right? But we don't want to talk about the power of sound here. We actually want to talk about the reverse of that, the sound power. So what's the sound power? Well, this is a physical that we can express like this. It's basically the rate at which the energy is transferred. This is basically the idea of the concept of a power in general. But if we talk about sound power specifically, this is the energy per unit of time emitted by a sound source in all directions across, I mean like the air across like the the medium",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=0s",
        "start_time": "0.219"
    },
    {
        "id": "5937ebbe",
        "text": "a couple of concepts. So intensity and power and these are like connected together. So let's start from the power of sound. Well, this is we all know how the power of sound, right? But we don't want to talk about the power of sound here. We actually want to talk about the reverse of that, the sound power. So what's the sound power? Well, this is a physical that we can express like this. It's basically the rate at which the energy is transferred. This is basically the idea of the concept of a power in general. But if we talk about sound power specifically, this is the energy per unit of time emitted by a sound source in all directions across, I mean like the air across like the the medium uh that the sound is troubling in and we measure power in watts, OK. And we indicate that with a capital W OK. So this is like the idea of sound uh power. Now connected with sound power, we have another uh thing that's called sound intensity. And sound",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=26s",
        "start_time": "26.79"
    },
    {
        "id": "4b89f9b0",
        "text": "that we can express like this. It's basically the rate at which the energy is transferred. This is basically the idea of the concept of a power in general. But if we talk about sound power specifically, this is the energy per unit of time emitted by a sound source in all directions across, I mean like the air across like the the medium uh that the sound is troubling in and we measure power in watts, OK. And we indicate that with a capital W OK. So this is like the idea of sound uh power. Now connected with sound power, we have another uh thing that's called sound intensity. And sound intensity is simply sound power per unit area, right? And we can measure that as what's divided by squared meters. OK. So the higher like the intensity and obviously the higher the kind of like perception like of loudness of that sound that we we're gonna have.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=53s",
        "start_time": "53.36"
    },
    {
        "id": "f296cdfe",
        "text": "uh that the sound is troubling in and we measure power in watts, OK. And we indicate that with a capital W OK. So this is like the idea of sound uh power. Now connected with sound power, we have another uh thing that's called sound intensity. And sound intensity is simply sound power per unit area, right? And we can measure that as what's divided by squared meters. OK. So the higher like the intensity and obviously the higher the kind of like perception like of loudness of that sound that we we're gonna have. OK. So now I'm going to ask you like a question. So how much power do you think I thunder like a super heavy thunder has or how much power do you think I a concert orchestra has? Well, you may think like quite a lot, right? Because like we perceive it as very loud, definitely like the the heavy thunder, right? And it gave us like a few scares like, I guess in the past when we were a child,",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=80s",
        "start_time": "80.169"
    },
    {
        "id": "94a82244",
        "text": "intensity is simply sound power per unit area, right? And we can measure that as what's divided by squared meters. OK. So the higher like the intensity and obviously the higher the kind of like perception like of loudness of that sound that we we're gonna have. OK. So now I'm going to ask you like a question. So how much power do you think I thunder like a super heavy thunder has or how much power do you think I a concert orchestra has? Well, you may think like quite a lot, right? Because like we perceive it as very loud, definitely like the the heavy thunder, right? And it gave us like a few scares like, I guess in the past when we were a child, OK. But",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=105s",
        "start_time": "105.834"
    },
    {
        "id": "998685e1",
        "text": "OK. So now I'm going to ask you like a question. So how much power do you think I thunder like a super heavy thunder has or how much power do you think I a concert orchestra has? Well, you may think like quite a lot, right? Because like we perceive it as very loud, definitely like the the heavy thunder, right? And it gave us like a few scares like, I guess in the past when we were a child, OK. But uh that's not really the case.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=129s",
        "start_time": "129.25"
    },
    {
        "id": "5bb4827a",
        "text": "OK. But uh that's not really the case. So both a concert orchestra and a heavy thunder usually have a sound power of one watt. Now, if you are not familiar with like any reference like with water, I'm gonna give you one. So your usual like a light bulb uh used to be 100 watts. OK. So like a whole orchestra is uh 1/100",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=158s",
        "start_time": "158.169"
    },
    {
        "id": "13df1f76",
        "text": "uh that's not really the case. So both a concert orchestra and a heavy thunder usually have a sound power of one watt. Now, if you are not familiar with like any reference like with water, I'm gonna give you one. So your usual like a light bulb uh used to be 100 watts. OK. So like a whole orchestra is uh 1/100 basically of a light bulb in terms of like power, same thing for this heavy thunder. So, yeah, this is like quite mesmerizing, isn't it? Ok. But basically this tells us another very interesting thing, uh which is connected with a concept called the threshold of hearing. So humans are capable of perceiving sounds which have extremely small uh intensities and the threshold of hearing is at",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=160s",
        "start_time": "160.009"
    },
    {
        "id": "7c757162",
        "text": "So both a concert orchestra and a heavy thunder usually have a sound power of one watt. Now, if you are not familiar with like any reference like with water, I'm gonna give you one. So your usual like a light bulb uh used to be 100 watts. OK. So like a whole orchestra is uh 1/100 basically of a light bulb in terms of like power, same thing for this heavy thunder. So, yeah, this is like quite mesmerizing, isn't it? Ok. But basically this tells us another very interesting thing, uh which is connected with a concept called the threshold of hearing. So humans are capable of perceiving sounds which have extremely small uh intensities and the threshold of hearing is at 10 to the minus 12 watt uh over squared meters, right? So this is like an extremely small number, but still we're capable like of hearing that. So obviously like the threshold of hearing like is the minimum intensity of sound that we can appreciate. OK.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=162s",
        "start_time": "162.71"
    },
    {
        "id": "b36443e3",
        "text": "basically of a light bulb in terms of like power, same thing for this heavy thunder. So, yeah, this is like quite mesmerizing, isn't it? Ok. But basically this tells us another very interesting thing, uh which is connected with a concept called the threshold of hearing. So humans are capable of perceiving sounds which have extremely small uh intensities and the threshold of hearing is at 10 to the minus 12 watt uh over squared meters, right? So this is like an extremely small number, but still we're capable like of hearing that. So obviously like the threshold of hearing like is the minimum intensity of sound that we can appreciate. OK. So now uh another uh kind of threshold which is like very important for us is called the threshold of pain. So which is like the the threshold beyond which we start to have like a hearing pain, right? Because like the sound like a uh is like too intense. OK. And that's it at 10",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=190s",
        "start_time": "190.309"
    },
    {
        "id": "dbd235d7",
        "text": "10 to the minus 12 watt uh over squared meters, right? So this is like an extremely small number, but still we're capable like of hearing that. So obviously like the threshold of hearing like is the minimum intensity of sound that we can appreciate. OK. So now uh another uh kind of threshold which is like very important for us is called the threshold of pain. So which is like the the threshold beyond which we start to have like a hearing pain, right? Because like the sound like a uh is like too intense. OK. And that's it at 10 um the watts over squared meters. So if you just like uh calculate the difference between 10 to the minus 12 and 10, you appreciate that like the amount the range of intensity that we can perceive is enormous. It's basically like 13 orders of magnitude, which is incredible.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=219s",
        "start_time": "219.809"
    },
    {
        "id": "36bcd026",
        "text": "So now uh another uh kind of threshold which is like very important for us is called the threshold of pain. So which is like the the threshold beyond which we start to have like a hearing pain, right? Because like the sound like a uh is like too intense. OK. And that's it at 10 um the watts over squared meters. So if you just like uh calculate the difference between 10 to the minus 12 and 10, you appreciate that like the amount the range of intensity that we can perceive is enormous. It's basically like 13 orders of magnitude, which is incredible. And that's why we use uh the so called intensity level, like for describing the um the intensity of the sound. And that's why we put the intensity level on a logarithmic scale because like the range like it's really, really enormous and the intensity level is connected",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=239s",
        "start_time": "239.619"
    },
    {
        "id": "b1c7dbed",
        "text": "um the watts over squared meters. So if you just like uh calculate the difference between 10 to the minus 12 and 10, you appreciate that like the amount the range of intensity that we can perceive is enormous. It's basically like 13 orders of magnitude, which is incredible. And that's why we use uh the so called intensity level, like for describing the um the intensity of the sound. And that's why we put the intensity level on a logarithmic scale because like the range like it's really, really enormous and the intensity level is connected with this um measure which we call with this unit of measure, which we call decibels. I'm sure like you may be familiar with this, but you may be wondering, uh you may have wondered, ok, but what's the decibel? Well, it's a measure of intensity level and it's a ratio between two in intensity values.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=259s",
        "start_time": "259.92"
    },
    {
        "id": "e9b22ff4",
        "text": "And that's why we use uh the so called intensity level, like for describing the um the intensity of the sound. And that's why we put the intensity level on a logarithmic scale because like the range like it's really, really enormous and the intensity level is connected with this um measure which we call with this unit of measure, which we call decibels. I'm sure like you may be familiar with this, but you may be wondering, uh you may have wondered, ok, but what's the decibel? Well, it's a measure of intensity level and it's a ratio between two in intensity values. I just like check found that I have like a small like typo here. It's not ration because I don't know what that is, but it's a ratio between two intensity values, right? And so we use an intensity of reference which sometimes is the threshold of hearing. And then we compare that against um a current uh like intensity and we applied that ratio uh a logarithm.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=285s",
        "start_time": "285.95"
    },
    {
        "id": "aadb0979",
        "text": "with this um measure which we call with this unit of measure, which we call decibels. I'm sure like you may be familiar with this, but you may be wondering, uh you may have wondered, ok, but what's the decibel? Well, it's a measure of intensity level and it's a ratio between two in intensity values. I just like check found that I have like a small like typo here. It's not ration because I don't know what that is, but it's a ratio between two intensity values, right? And so we use an intensity of reference which sometimes is the threshold of hearing. And then we compare that against um a current uh like intensity and we applied that ratio uh a logarithm. And so if we want to take a look at the um function that describe decibels as a function of intensity is this one right? So we have 10 by log of a intensity divided by the intensity of the threshold of hearing. OK. Cool. OK. So now you may be wondering but what's zero decibels?",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=307s",
        "start_time": "307.632"
    },
    {
        "id": "eff88098",
        "text": "I just like check found that I have like a small like typo here. It's not ration because I don't know what that is, but it's a ratio between two intensity values, right? And so we use an intensity of reference which sometimes is the threshold of hearing. And then we compare that against um a current uh like intensity and we applied that ratio uh a logarithm. And so if we want to take a look at the um function that describe decibels as a function of intensity is this one right? So we have 10 by log of a intensity divided by the intensity of the threshold of hearing. OK. Cool. OK. So now you may be wondering but what's zero decibels? And so for that we need to pass in",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=329s",
        "start_time": "329.316"
    },
    {
        "id": "4da4d042",
        "text": "And so if we want to take a look at the um function that describe decibels as a function of intensity is this one right? So we have 10 by log of a intensity divided by the intensity of the threshold of hearing. OK. Cool. OK. So now you may be wondering but what's zero decibels? And so for that we need to pass in uh the intensity of the threshold of hearing. And so if we do that this ratio uh basically like goes down to uh to one and logarithm or one is equal to zero. So we multiply 10 by zero which is zero. OK. So a zero at zero decibel, we are basically appreciating the uh",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=356s",
        "start_time": "356.32"
    },
    {
        "id": "4cec4fd0",
        "text": "And so for that we need to pass in uh the intensity of the threshold of hearing. And so if we do that this ratio uh basically like goes down to uh to one and logarithm or one is equal to zero. So we multiply 10 by zero which is zero. OK. So a zero at zero decibel, we are basically appreciating the uh density of the threshold of hearing. Now, this is not a universal because it really depends on the uh kind of like reference intensity that you use in the intensity level. But usually like, you'll find a lot of people like using the threshold of hearing like as a, as an intensity, um a level of reference.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=385s",
        "start_time": "385.359"
    },
    {
        "id": "cb8e4188",
        "text": "uh the intensity of the threshold of hearing. And so if we do that this ratio uh basically like goes down to uh to one and logarithm or one is equal to zero. So we multiply 10 by zero which is zero. OK. So a zero at zero decibel, we are basically appreciating the uh density of the threshold of hearing. Now, this is not a universal because it really depends on the uh kind of like reference intensity that you use in the intensity level. But usually like, you'll find a lot of people like using the threshold of hearing like as a, as an intensity, um a level of reference. OK. So another thing uh to notice about the intensity level is that every time we go up by three decibels, the intensity tends to uh double. So if we are at zero decibel, we are at the um uh intensity of like the threshold of hearing. But if we go up by three, we are just like doubling uh that uh intensity",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=390s",
        "start_time": "390.5"
    },
    {
        "id": "e49a4981",
        "text": "density of the threshold of hearing. Now, this is not a universal because it really depends on the uh kind of like reference intensity that you use in the intensity level. But usually like, you'll find a lot of people like using the threshold of hearing like as a, as an intensity, um a level of reference. OK. So another thing uh to notice about the intensity level is that every time we go up by three decibels, the intensity tends to uh double. So if we are at zero decibel, we are at the um uh intensity of like the threshold of hearing. But if we go up by three, we are just like doubling uh that uh intensity cool. OK. So now I want to show you a cool table which by the way is, but uh I, like I took it from a book which is an amazing book like on audio music processing that's called Fundamentals of Music pro Assessing it.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=416s",
        "start_time": "416.984"
    },
    {
        "id": "b02878a0",
        "text": "OK. So another thing uh to notice about the intensity level is that every time we go up by three decibels, the intensity tends to uh double. So if we are at zero decibel, we are at the um uh intensity of like the threshold of hearing. But if we go up by three, we are just like doubling uh that uh intensity cool. OK. So now I want to show you a cool table which by the way is, but uh I, like I took it from a book which is an amazing book like on audio music processing that's called Fundamentals of Music pro Assessing it. It's by Mueller. So many of the uh like uh pictures like graphs and tables I'm gonna use here are just like taken from that book. And I highly suggest you to go check that out because it's great and it will give you like way more context that what I'm covering here. OK. But here you have a table where we have a lot of like different sound sources",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=436s",
        "start_time": "436.14"
    },
    {
        "id": "29a0c615",
        "text": "cool. OK. So now I want to show you a cool table which by the way is, but uh I, like I took it from a book which is an amazing book like on audio music processing that's called Fundamentals of Music pro Assessing it. It's by Mueller. So many of the uh like uh pictures like graphs and tables I'm gonna use here are just like taken from that book. And I highly suggest you to go check that out because it's great and it will give you like way more context that what I'm covering here. OK. But here you have a table where we have a lot of like different sound sources and we have like the intensity measured in watts over a squared meters. The and the relative intensity uh level and the uh re that like",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=462s",
        "start_time": "462.299"
    },
    {
        "id": "91b1d1aa",
        "text": "It's by Mueller. So many of the uh like uh pictures like graphs and tables I'm gonna use here are just like taken from that book. And I highly suggest you to go check that out because it's great and it will give you like way more context that what I'm covering here. OK. But here you have a table where we have a lot of like different sound sources and we have like the intensity measured in watts over a squared meters. The and the relative intensity uh level and the uh re that like compared in, in terms of like the threshold of hearing, OK. So like when we whisper, we have an intensity which is at 10 to the minus uh 10, which is basically a 20 decibels. So a normal conversation happens at an intensity of 10 to the minus six which is 60 decibels",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=479s",
        "start_time": "479.149"
    },
    {
        "id": "dc151df8",
        "text": "and we have like the intensity measured in watts over a squared meters. The and the relative intensity uh level and the uh re that like compared in, in terms of like the threshold of hearing, OK. So like when we whisper, we have an intensity which is at 10 to the minus uh 10, which is basically a 20 decibels. So a normal conversation happens at an intensity of 10 to the minus six which is 60 decibels and the uh threshold of pain, which is, we know is already uh intensity 10, it's 100 and 30 decibels and the jet. So when you have like a jet engine, a take off, this is at intensity 10 to 2",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=502s",
        "start_time": "502.179"
    },
    {
        "id": "d612b369",
        "text": "compared in, in terms of like the threshold of hearing, OK. So like when we whisper, we have an intensity which is at 10 to the minus uh 10, which is basically a 20 decibels. So a normal conversation happens at an intensity of 10 to the minus six which is 60 decibels and the uh threshold of pain, which is, we know is already uh intensity 10, it's 100 and 30 decibels and the jet. So when you have like a jet engine, a take off, this is at intensity 10 to 2 and it's 100 and 40 decibels. OK. So this gives you like more or less like an idea of like the different sounds and the relative intensity. But now let's move on to another aspect that I think like it's extremely fascinating, which is loudness now, while in",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=515s",
        "start_time": "515.289"
    },
    {
        "id": "2d8c8ae6",
        "text": "and the uh threshold of pain, which is, we know is already uh intensity 10, it's 100 and 30 decibels and the jet. So when you have like a jet engine, a take off, this is at intensity 10 to 2 and it's 100 and 40 decibels. OK. So this gives you like more or less like an idea of like the different sounds and the relative intensity. But now let's move on to another aspect that I think like it's extremely fascinating, which is loudness now, while in intensity and power are to objective measures, loudness is a subjective measure of intensity. It is really like how loud we perceive a sound. And as we've seen for um a frequency pitch which is like the relative like um",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=537s",
        "start_time": "537.474"
    },
    {
        "id": "8e17fec5",
        "text": "and it's 100 and 40 decibels. OK. So this gives you like more or less like an idea of like the different sounds and the relative intensity. But now let's move on to another aspect that I think like it's extremely fascinating, which is loudness now, while in intensity and power are to objective measures, loudness is a subjective measure of intensity. It is really like how loud we perceive a sound. And as we've seen for um a frequency pitch which is like the relative like um kind of like subjective measure of frequency, obviously, it it correlates with frequency, but like there's some level of like uh subjectivity and some level of like mapping that's not like linear, not the direct and the same thing happens here between intensity and loudness. OK. So um",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=556s",
        "start_time": "556.46"
    },
    {
        "id": "be956030",
        "text": "intensity and power are to objective measures, loudness is a subjective measure of intensity. It is really like how loud we perceive a sound. And as we've seen for um a frequency pitch which is like the relative like um kind of like subjective measure of frequency, obviously, it it correlates with frequency, but like there's some level of like uh subjectivity and some level of like mapping that's not like linear, not the direct and the same thing happens here between intensity and loudness. OK. So um loudness. So first of all, uh depends on duration and the frequency of the sound in terms of duration. So we usually hear as at, at equal intensity, we hear shorter sounds as less loud than longer sounds. So for example, if I have a sound which is at uh say three decibels and it only lasts for 100 milliseconds. I'll hear that",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=574s",
        "start_time": "574.755"
    },
    {
        "id": "83d66799",
        "text": "kind of like subjective measure of frequency, obviously, it it correlates with frequency, but like there's some level of like uh subjectivity and some level of like mapping that's not like linear, not the direct and the same thing happens here between intensity and loudness. OK. So um loudness. So first of all, uh depends on duration and the frequency of the sound in terms of duration. So we usually hear as at, at equal intensity, we hear shorter sounds as less loud than longer sounds. So for example, if I have a sound which is at uh say three decibels and it only lasts for 100 milliseconds. I'll hear that as less loud than a similar sound with the same three decibel intensity level, but which lasts for 600 milliseconds. So this is like kind of like very fascinating because at the end of the day, the intensity is the same, but the perception of loudness is different",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=597s",
        "start_time": "597.409"
    },
    {
        "id": "89a1c828",
        "text": "loudness. So first of all, uh depends on duration and the frequency of the sound in terms of duration. So we usually hear as at, at equal intensity, we hear shorter sounds as less loud than longer sounds. So for example, if I have a sound which is at uh say three decibels and it only lasts for 100 milliseconds. I'll hear that as less loud than a similar sound with the same three decibel intensity level, but which lasts for 600 milliseconds. So this is like kind of like very fascinating because at the end of the day, the intensity is the same, but the perception of loudness is different and a similar thing happens also with frequency. So frequency, depending on which frequency the sound is, we're gonna hear that sound at a different loudness level and obviously loudness is also correlated with age. So people of different age are gonna hear uh like sound with same intensity",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=619s",
        "start_time": "619.429"
    },
    {
        "id": "ff2a9c61",
        "text": "as less loud than a similar sound with the same three decibel intensity level, but which lasts for 600 milliseconds. So this is like kind of like very fascinating because at the end of the day, the intensity is the same, but the perception of loudness is different and a similar thing happens also with frequency. So frequency, depending on which frequency the sound is, we're gonna hear that sound at a different loudness level and obviously loudness is also correlated with age. So people of different age are gonna hear uh like sound with same intensity but like with different loudness, right? And for loudness. So what um researchers have done is basically they went out there and they did a lot of experiments to understand how we perceive uh sound and measured that with psychological experiments. And they",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=649s",
        "start_time": "649.429"
    },
    {
        "id": "d6ec58bb",
        "text": "and a similar thing happens also with frequency. So frequency, depending on which frequency the sound is, we're gonna hear that sound at a different loudness level and obviously loudness is also correlated with age. So people of different age are gonna hear uh like sound with same intensity but like with different loudness, right? And for loudness. So what um researchers have done is basically they went out there and they did a lot of experiments to understand how we perceive uh sound and measured that with psychological experiments. And they came out with this measure UNICEF measure called fonts. So now let's take a look at fonts. So here you have a very interesting chart that's called like equal, equal, equal loudness content. So now on the y axis, you have the sound pressure level measured in decibels.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=668s",
        "start_time": "668.64"
    },
    {
        "id": "67c1f8ef",
        "text": "but like with different loudness, right? And for loudness. So what um researchers have done is basically they went out there and they did a lot of experiments to understand how we perceive uh sound and measured that with psychological experiments. And they came out with this measure UNICEF measure called fonts. So now let's take a look at fonts. So here you have a very interesting chart that's called like equal, equal, equal loudness content. So now on the y axis, you have the sound pressure level measured in decibels. So that's the intensity level here on the X axis, you have um frequency in a logarithmic scale. And it's measured obviously in Hertz, you should know that by now. And here like this curves that you see like in a ranch. So these are called equal loudness contours. So",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=689s",
        "start_time": "689.166"
    },
    {
        "id": "4f9ebca0",
        "text": "came out with this measure UNICEF measure called fonts. So now let's take a look at fonts. So here you have a very interesting chart that's called like equal, equal, equal loudness content. So now on the y axis, you have the sound pressure level measured in decibels. So that's the intensity level here on the X axis, you have um frequency in a logarithmic scale. And it's measured obviously in Hertz, you should know that by now. And here like this curves that you see like in a ranch. So these are called equal loudness contours. So along these curves, we hear a sound to at a same intensity. So for example, on this curve here, we hear sound at 80 fonts, which basically is like we, we hear it at the same perceived loudness. However, what changes",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=709s",
        "start_time": "709.692"
    },
    {
        "id": "36b8d351",
        "text": "So that's the intensity level here on the X axis, you have um frequency in a logarithmic scale. And it's measured obviously in Hertz, you should know that by now. And here like this curves that you see like in a ranch. So these are called equal loudness contours. So along these curves, we hear a sound to at a same intensity. So for example, on this curve here, we hear sound at 80 fonts, which basically is like we, we hear it at the same perceived loudness. However, what changes is the intensity level across the frequencies? So for example, when we are like at a very low frequency, say 20 Hertz, we need a lot of intensity. So 100 and 20 decibels to perceive that sound at 80 forms. But then when we go up say to 1000",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=730s",
        "start_time": "730.669"
    },
    {
        "id": "d6e03884",
        "text": "along these curves, we hear a sound to at a same intensity. So for example, on this curve here, we hear sound at 80 fonts, which basically is like we, we hear it at the same perceived loudness. However, what changes is the intensity level across the frequencies? So for example, when we are like at a very low frequency, say 20 Hertz, we need a lot of intensity. So 100 and 20 decibels to perceive that sound at 80 forms. But then when we go up say to 1000 uh Hertz, then to perceive that sound at 80 forms, we need way uh less intense of sound, which is around 80 decibels. And so which basically means that uh like at lower frequencies, we perceive sounds uh like as less loud if they are equal in intensity.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=751s",
        "start_time": "751.929"
    },
    {
        "id": "eaabd3b3",
        "text": "is the intensity level across the frequencies? So for example, when we are like at a very low frequency, say 20 Hertz, we need a lot of intensity. So 100 and 20 decibels to perceive that sound at 80 forms. But then when we go up say to 1000 uh Hertz, then to perceive that sound at 80 forms, we need way uh less intense of sound, which is around 80 decibels. And so which basically means that uh like at lower frequencies, we perceive sounds uh like as less loud if they are equal in intensity. And this is like interesting. And if you look at like where we are the most efficient with loudness here, uh it's around like from a few 100 Hertz up until like 5000, I would say 7000 Hertz, which is like more or less like the",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=773s",
        "start_time": "773.19"
    },
    {
        "id": "6c1cffb1",
        "text": "uh Hertz, then to perceive that sound at 80 forms, we need way uh less intense of sound, which is around 80 decibels. And so which basically means that uh like at lower frequencies, we perceive sounds uh like as less loud if they are equal in intensity. And this is like interesting. And if you look at like where we are the most efficient with loudness here, uh it's around like from a few 100 Hertz up until like 5000, I would say 7000 Hertz, which is like more or less like the the range of like human speaking and human like singing, which may not be like a case at all. Right. So we are kind of like tailored to be the most efficient at that frequency range. And then like when the frequency goes up",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=795s",
        "start_time": "795.08"
    },
    {
        "id": "13b4ef5d",
        "text": "And this is like interesting. And if you look at like where we are the most efficient with loudness here, uh it's around like from a few 100 Hertz up until like 5000, I would say 7000 Hertz, which is like more or less like the the range of like human speaking and human like singing, which may not be like a case at all. Right. So we are kind of like tailored to be the most efficient at that frequency range. And then like when the frequency goes up again. So we still uh yeah, we we are less efficient like at hearing sound at perceiving like the the loudness. And so we need more intensity to perceive like the same uh level of loudness. OK. So this is like extremely, extremely uh interesting and I am always like, fascinated about like the the correlation between like physical measures and",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=818s",
        "start_time": "818.619"
    },
    {
        "id": "3aaefa41",
        "text": "the range of like human speaking and human like singing, which may not be like a case at all. Right. So we are kind of like tailored to be the most efficient at that frequency range. And then like when the frequency goes up again. So we still uh yeah, we we are less efficient like at hearing sound at perceiving like the the loudness. And so we need more intensity to perceive like the same uh level of loudness. OK. So this is like extremely, extremely uh interesting and I am always like, fascinated about like the the correlation between like physical measures and um kind of like perceptual measures like and the way that these two things uh yeah, are connected together. And now we enter like yet another uh aspect of sound which is kind of like highly subjective and difficult to grasp and that's timer. OK. So what's Tre? Well,",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=834s",
        "start_time": "834.95"
    },
    {
        "id": "ea92394e",
        "text": "again. So we still uh yeah, we we are less efficient like at hearing sound at perceiving like the the loudness. And so we need more intensity to perceive like the same uh level of loudness. OK. So this is like extremely, extremely uh interesting and I am always like, fascinated about like the the correlation between like physical measures and um kind of like perceptual measures like and the way that these two things uh yeah, are connected together. And now we enter like yet another uh aspect of sound which is kind of like highly subjective and difficult to grasp and that's timer. OK. So what's Tre? Well, I wish we knew because like researchers interested in sound uh music and all of these kind of things like I've studied timbre like for a long time. Uh The problem is that we don't have like a real and comprehensive definition of timbre, but we have like a few hints. So",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=851s",
        "start_time": "851.489"
    },
    {
        "id": "9579751e",
        "text": "um kind of like perceptual measures like and the way that these two things uh yeah, are connected together. And now we enter like yet another uh aspect of sound which is kind of like highly subjective and difficult to grasp and that's timer. OK. So what's Tre? Well, I wish we knew because like researchers interested in sound uh music and all of these kind of things like I've studied timbre like for a long time. Uh The problem is that we don't have like a real and comprehensive definition of timbre, but we have like a few hints. So uh if you uh ask a musician what Tre is like, they would probably tell you like it's the color of sound. But that's a weird definition like for some, something like which we hope like could be like a measurable thing, right?",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=876s",
        "start_time": "876.809"
    },
    {
        "id": "34c4d84a",
        "text": "I wish we knew because like researchers interested in sound uh music and all of these kind of things like I've studied timbre like for a long time. Uh The problem is that we don't have like a real and comprehensive definition of timbre, but we have like a few hints. So uh if you uh ask a musician what Tre is like, they would probably tell you like it's the color of sound. But that's a weird definition like for some, something like which we hope like could be like a measurable thing, right? And but if we talk about this, like from a kind of like more like programmatic, like logical perspective, we could think of like a timer as the de between two sounds which have like all aspects equal, like in terms of like intensity, frequency and duration. So you do a diff and what remains like as uh sound there?",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=899s",
        "start_time": "899.07"
    },
    {
        "id": "61064569",
        "text": "uh if you uh ask a musician what Tre is like, they would probably tell you like it's the color of sound. But that's a weird definition like for some, something like which we hope like could be like a measurable thing, right? And but if we talk about this, like from a kind of like more like programmatic, like logical perspective, we could think of like a timer as the de between two sounds which have like all aspects equal, like in terms of like intensity, frequency and duration. So you do a diff and what remains like as uh sound there? Uh time bra sorry, it remains like time. Well, uh the point is like if you have like the, the same uh like notes like C five, for example, uh played on a trumpet, a trumpet and on a violin with the same intensity you still hear that there is a difference there. Uh But it's not intensity, it's not uh frequency, it's not duration, it's time breath. OK. So",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=918s",
        "start_time": "918.34"
    },
    {
        "id": "d66e48a5",
        "text": "And but if we talk about this, like from a kind of like more like programmatic, like logical perspective, we could think of like a timer as the de between two sounds which have like all aspects equal, like in terms of like intensity, frequency and duration. So you do a diff and what remains like as uh sound there? Uh time bra sorry, it remains like time. Well, uh the point is like if you have like the, the same uh like notes like C five, for example, uh played on a trumpet, a trumpet and on a violin with the same intensity you still hear that there is a difference there. Uh But it's not intensity, it's not uh frequency, it's not duration, it's time breath. OK. So um and timbre is usually described with words like bright, dark, dull, harsh word. So all of this like sounds a little bit fuzzy, right? And that's because it is fuzzy,",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=932s",
        "start_time": "932.13"
    },
    {
        "id": "2a2795d2",
        "text": "Uh time bra sorry, it remains like time. Well, uh the point is like if you have like the, the same uh like notes like C five, for example, uh played on a trumpet, a trumpet and on a violin with the same intensity you still hear that there is a difference there. Uh But it's not intensity, it's not uh frequency, it's not duration, it's time breath. OK. So um and timbre is usually described with words like bright, dark, dull, harsh word. So all of this like sounds a little bit fuzzy, right? And that's because it is fuzzy, but now we don't know what a timer is perfectly, but we have a clear idea that Tre is somewhat multidimensional. So many features coming into place to define tre, this is different from other aspects of sounds like uh frequency or intensity.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=954s",
        "start_time": "954.7"
    },
    {
        "id": "d8483606",
        "text": "um and timbre is usually described with words like bright, dark, dull, harsh word. So all of this like sounds a little bit fuzzy, right? And that's because it is fuzzy, but now we don't know what a timer is perfectly, but we have a clear idea that Tre is somewhat multidimensional. So many features coming into place to define tre, this is different from other aspects of sounds like uh frequency or intensity. We just have like one like one measurable, one observable there. OK. So what are like some of these features that come into place which are timer? So here I've listed three which are like the most important ones. So one is called the sound envelope. Then we have the harmonic concepts and then amplitude and frequency modulation. Now,",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=980s",
        "start_time": "980.849"
    },
    {
        "id": "d22c1a83",
        "text": "but now we don't know what a timer is perfectly, but we have a clear idea that Tre is somewhat multidimensional. So many features coming into place to define tre, this is different from other aspects of sounds like uh frequency or intensity. We just have like one like one measurable, one observable there. OK. So what are like some of these features that come into place which are timer? So here I've listed three which are like the most important ones. So one is called the sound envelope. Then we have the harmonic concepts and then amplitude and frequency modulation. Now, I'm quite sure that most of you are completely oblivious about like what these things are. But don't worry because I'm going to cover like all of these things uh uh in depth and we're gonna have like quite a lot of fun going through all of these aspects of sound. OK. So let's start with sound envelope, which is called also amplitude envelope. So what's that? Well,",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=994s",
        "start_time": "994.53"
    },
    {
        "id": "d824c998",
        "text": "We just have like one like one measurable, one observable there. OK. So what are like some of these features that come into place which are timer? So here I've listed three which are like the most important ones. So one is called the sound envelope. Then we have the harmonic concepts and then amplitude and frequency modulation. Now, I'm quite sure that most of you are completely oblivious about like what these things are. But don't worry because I'm going to cover like all of these things uh uh in depth and we're gonna have like quite a lot of fun going through all of these aspects of sound. OK. So let's start with sound envelope, which is called also amplitude envelope. So what's that? Well, when we think of a sound definitely like this is true for like um musical sounds like notes and things like that on different instruments. So the sound usually has like a an envelope, right? And so this envelope can be divided with a model that's called the A DS R model which stands for attack decay sustain release",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1014s",
        "start_time": "1014.669"
    },
    {
        "id": "9b68dc63",
        "text": "I'm quite sure that most of you are completely oblivious about like what these things are. But don't worry because I'm going to cover like all of these things uh uh in depth and we're gonna have like quite a lot of fun going through all of these aspects of sound. OK. So let's start with sound envelope, which is called also amplitude envelope. So what's that? Well, when we think of a sound definitely like this is true for like um musical sounds like notes and things like that on different instruments. So the sound usually has like a an envelope, right? And so this envelope can be divided with a model that's called the A DS R model which stands for attack decay sustain release model. So basically, the idea is that uh the amplitude of these sounds has an initial spike and that's like the attack. So for example, is when you strike a key on a piano, you have like a spike in amplitude which also has uh some kind of like noisy sounds due to the hummers like of the of the piano. And that's like a type of like transient sound.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1036s",
        "start_time": "1036.3"
    },
    {
        "id": "ded8ec05",
        "text": "when we think of a sound definitely like this is true for like um musical sounds like notes and things like that on different instruments. So the sound usually has like a an envelope, right? And so this envelope can be divided with a model that's called the A DS R model which stands for attack decay sustain release model. So basically, the idea is that uh the amplitude of these sounds has an initial spike and that's like the attack. So for example, is when you strike a key on a piano, you have like a spike in amplitude which also has uh some kind of like noisy sounds due to the hummers like of the of the piano. And that's like a type of like transient sound. Then you have like a decay which is like where the sound like stabilizes a sustain a period which is like where the sounds like uh remains like more or less like constant in amplitude. And then you have a release which is like just like the fading out phase of a sound.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1059s",
        "start_time": "1059.975"
    },
    {
        "id": "b2135a08",
        "text": "model. So basically, the idea is that uh the amplitude of these sounds has an initial spike and that's like the attack. So for example, is when you strike a key on a piano, you have like a spike in amplitude which also has uh some kind of like noisy sounds due to the hummers like of the of the piano. And that's like a type of like transient sound. Then you have like a decay which is like where the sound like stabilizes a sustain a period which is like where the sounds like uh remains like more or less like constant in amplitude. And then you have a release which is like just like the fading out phase of a sound. Now, the interesting thing is that different types of like sounds have different envelopes and this is like definitely true for musical instruments. So let's take a look at this once again, I'm using like this uh figure from the Mueller's like book of fundamentals of music processing.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1083s",
        "start_time": "1083.651"
    },
    {
        "id": "b8b86f42",
        "text": "Then you have like a decay which is like where the sound like stabilizes a sustain a period which is like where the sounds like uh remains like more or less like constant in amplitude. And then you have a release which is like just like the fading out phase of a sound. Now, the interesting thing is that different types of like sounds have different envelopes and this is like definitely true for musical instruments. So let's take a look at this once again, I'm using like this uh figure from the Mueller's like book of fundamentals of music processing. OK. So here we can uh like um compare two different envelopes for a piano sound. So here, like you just like press a key and you wait for it like to, to just like decay and here you have violin sound, OK? And you see that the A DS R and the envelope of this key",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1107s",
        "start_time": "1107.5"
    },
    {
        "id": "a4f05967",
        "text": "Now, the interesting thing is that different types of like sounds have different envelopes and this is like definitely true for musical instruments. So let's take a look at this once again, I'm using like this uh figure from the Mueller's like book of fundamentals of music processing. OK. So here we can uh like um compare two different envelopes for a piano sound. So here, like you just like press a key and you wait for it like to, to just like decay and here you have violin sound, OK? And you see that the A DS R and the envelope of this key um sounds, it's like very, very different. So with the piano, you have like a short attack, a transient here, then you have like a little bit of a period of decay and then you have a sustain and finally you have a release down here, right?",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1127s",
        "start_time": "1127.18"
    },
    {
        "id": "4c23b2b6",
        "text": "OK. So here we can uh like um compare two different envelopes for a piano sound. So here, like you just like press a key and you wait for it like to, to just like decay and here you have violin sound, OK? And you see that the A DS R and the envelope of this key um sounds, it's like very, very different. So with the piano, you have like a short attack, a transient here, then you have like a little bit of a period of decay and then you have a sustain and finally you have a release down here, right? And uh this is different for, for violin as you can appreciate here, the attack is way longer in terms of time and that's because like the attack is less sharp. OK? Then you really don't have like a period of decay here. You just have like some kind of like sustain",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1145s",
        "start_time": "1145.41"
    },
    {
        "id": "8b5fc289",
        "text": "um sounds, it's like very, very different. So with the piano, you have like a short attack, a transient here, then you have like a little bit of a period of decay and then you have a sustain and finally you have a release down here, right? And uh this is different for, for violin as you can appreciate here, the attack is way longer in terms of time and that's because like the attack is less sharp. OK? Then you really don't have like a period of decay here. You just have like some kind of like sustain and then you have the release when uh yeah, just like end the sound. OK? So sound envelope is an important feature that determines the um the sound uh OK. So one thing that I, that I want to tell you here is that for example, if you remove the attack part from a key sound uh from a, from a piano uh sound.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1164s",
        "start_time": "1164.88"
    },
    {
        "id": "6e1d132f",
        "text": "And uh this is different for, for violin as you can appreciate here, the attack is way longer in terms of time and that's because like the attack is less sharp. OK? Then you really don't have like a period of decay here. You just have like some kind of like sustain and then you have the release when uh yeah, just like end the sound. OK? So sound envelope is an important feature that determines the um the sound uh OK. So one thing that I, that I want to tell you here is that for example, if you remove the attack part from a key sound uh from a, from a piano uh sound. Um what happens is that it's difficult to recognize that that is like a, a piano that's uh like playing, right? That's because like we, we associate like a piano sound with its like sharp transient quite a lot.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1178s",
        "start_time": "1178.949"
    },
    {
        "id": "42ee3422",
        "text": "and then you have the release when uh yeah, just like end the sound. OK? So sound envelope is an important feature that determines the um the sound uh OK. So one thing that I, that I want to tell you here is that for example, if you remove the attack part from a key sound uh from a, from a piano uh sound. Um what happens is that it's difficult to recognize that that is like a, a piano that's uh like playing, right? That's because like we, we associate like a piano sound with its like sharp transient quite a lot. OK. So now let's move on to the second aspect which is harmonic content. But in order to understand how, what harmonic content is, we need to understand what a complex sound is made up of. So a complex sound is made up of a superposition of uh many S sinusoids or like fundamental uh like simple like sounds, right? So",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1196s",
        "start_time": "1196.56"
    },
    {
        "id": "a11b2799",
        "text": "Um what happens is that it's difficult to recognize that that is like a, a piano that's uh like playing, right? That's because like we, we associate like a piano sound with its like sharp transient quite a lot. OK. So now let's move on to the second aspect which is harmonic content. But in order to understand how, what harmonic content is, we need to understand what a complex sound is made up of. So a complex sound is made up of a superposition of uh many S sinusoids or like fundamental uh like simple like sounds, right? So we can think of like a partial as a sinusoid, which is used to describe a sound. So uh the many different sinusoids that we have uh can, which are superimposed together to create a complex sound are called partials or harmonic",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1222s",
        "start_time": "1222.979"
    },
    {
        "id": "75f425ab",
        "text": "OK. So now let's move on to the second aspect which is harmonic content. But in order to understand how, what harmonic content is, we need to understand what a complex sound is made up of. So a complex sound is made up of a superposition of uh many S sinusoids or like fundamental uh like simple like sounds, right? So we can think of like a partial as a sinusoid, which is used to describe a sound. So uh the many different sinusoids that we have uh can, which are superimposed together to create a complex sound are called partials or harmonic partials. So the lowest partial is called the fundamental uh frequency. And this is it, this is usually like the one that gives the, the pitch name to a note. We are, we, we are talking about like a note, for example, musical note and now the harmonic partial. Um",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1239s",
        "start_time": "1239.329"
    },
    {
        "id": "d9be4383",
        "text": "we can think of like a partial as a sinusoid, which is used to describe a sound. So uh the many different sinusoids that we have uh can, which are superimposed together to create a complex sound are called partials or harmonic partials. So the lowest partial is called the fundamental uh frequency. And this is it, this is usually like the one that gives the, the pitch name to a note. We are, we, we are talking about like a note, for example, musical note and now the harmonic partial. Um uh so, so the harmonic partials are frequency frequencies that are like a an integer multiple of the fundamental frequency. So let's say we have a fundamental frequency which is at 440 Hertz which is a uh four.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1265s",
        "start_time": "1265.959"
    },
    {
        "id": "9321036e",
        "text": "partials. So the lowest partial is called the fundamental uh frequency. And this is it, this is usually like the one that gives the, the pitch name to a note. We are, we, we are talking about like a note, for example, musical note and now the harmonic partial. Um uh so, so the harmonic partials are frequency frequencies that are like a an integer multiple of the fundamental frequency. So let's say we have a fundamental frequency which is at 440 Hertz which is a uh four. Now, if we, if we uh take a look at the second partial there. So harmonic partial, what happens is that we have to multiply that 440 by two. And this gives us like 880 Hertz. Now the third partial will be three multiplied by 440 which is",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1282s",
        "start_time": "1282.329"
    },
    {
        "id": "fa5cfc80",
        "text": "uh so, so the harmonic partials are frequency frequencies that are like a an integer multiple of the fundamental frequency. So let's say we have a fundamental frequency which is at 440 Hertz which is a uh four. Now, if we, if we uh take a look at the second partial there. So harmonic partial, what happens is that we have to multiply that 440 by two. And this gives us like 880 Hertz. Now the third partial will be three multiplied by 440 which is um 1320 Hertz. Now, you can move on.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1303s",
        "start_time": "1303.39"
    },
    {
        "id": "e8d6be4c",
        "text": "Now, if we, if we uh take a look at the second partial there. So harmonic partial, what happens is that we have to multiply that 440 by two. And this gives us like 880 Hertz. Now the third partial will be three multiplied by 440 which is um 1320 Hertz. Now, you can move on. And basically, the idea is that like the complex sound is gonna made up of all of these like partials and the harmonic content tells us how much energy we have in each of these uh partials. And that determines somehow the temper of the sound. Now, not all sounds are perfectly harmonic. So there's a lot of in harmonic in some sounds and that uh we determine in harmonic, we indicate at the",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1323s",
        "start_time": "1323.4"
    },
    {
        "id": "491b590f",
        "text": "um 1320 Hertz. Now, you can move on. And basically, the idea is that like the complex sound is gonna made up of all of these like partials and the harmonic content tells us how much energy we have in each of these uh partials. And that determines somehow the temper of the sound. Now, not all sounds are perfectly harmonic. So there's a lot of in harmonic in some sounds and that uh we determine in harmonic, we indicate at the cation from a harmonic partial. So that's for example, if uh we had like some frequencies that are not perfect harmonic partials of the fundamental frequency. Now, if we take a look at music instruments, for example, we know that um pitched, pitched instruments tend to be like harmonic, whereas like percussive instruments tend to have a lot of in harmonic, right?",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1349s",
        "start_time": "1349.839"
    },
    {
        "id": "41978b00",
        "text": "And basically, the idea is that like the complex sound is gonna made up of all of these like partials and the harmonic content tells us how much energy we have in each of these uh partials. And that determines somehow the temper of the sound. Now, not all sounds are perfectly harmonic. So there's a lot of in harmonic in some sounds and that uh we determine in harmonic, we indicate at the cation from a harmonic partial. So that's for example, if uh we had like some frequencies that are not perfect harmonic partials of the fundamental frequency. Now, if we take a look at music instruments, for example, we know that um pitched, pitched instruments tend to be like harmonic, whereas like percussive instruments tend to have a lot of in harmonic, right? So, and usually obviously like noise are, are highly completely like in harmonic, right?",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1355s",
        "start_time": "1355.079"
    },
    {
        "id": "ec3e61e6",
        "text": "cation from a harmonic partial. So that's for example, if uh we had like some frequencies that are not perfect harmonic partials of the fundamental frequency. Now, if we take a look at music instruments, for example, we know that um pitched, pitched instruments tend to be like harmonic, whereas like percussive instruments tend to have a lot of in harmonic, right? So, and usually obviously like noise are, are highly completely like in harmonic, right? Cool. So this gives you like a an idea. But now let's try to like listen to some of these things. So for showing uh the harmonic content, we should go and check out like a Jupiter Notebook that I just like uh OK.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1382s",
        "start_time": "1382.93"
    },
    {
        "id": "9fe6ebe4",
        "text": "So, and usually obviously like noise are, are highly completely like in harmonic, right? Cool. So this gives you like a an idea. But now let's try to like listen to some of these things. So for showing uh the harmonic content, we should go and check out like a Jupiter Notebook that I just like uh OK. So here I'm not gonna go through like the code because like the the point here is not like to show you like some of these things because we're gonna cover them like in future videos anyways. But I want to, to, to show you like some sounds and then the relative spectrograms. OK. So",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1410s",
        "start_time": "1410.969"
    },
    {
        "id": "a075bc58",
        "text": "Cool. So this gives you like a an idea. But now let's try to like listen to some of these things. So for showing uh the harmonic content, we should go and check out like a Jupiter Notebook that I just like uh OK. So here I'm not gonna go through like the code because like the the point here is not like to show you like some of these things because we're gonna cover them like in future videos anyways. But I want to, to, to show you like some sounds and then the relative spectrograms. OK. So yeah, so here just like I learn some sounds and here like I have like a nice function that I can use to plot a spectrogram. Now, what's the spectrogram? So you'll hear about this like a lot moving forward. But basically, the idea is that with a spectrogram, we get a snapshot of,",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1417s",
        "start_time": "1417.25"
    },
    {
        "id": "ecd80c48",
        "text": "So here I'm not gonna go through like the code because like the the point here is not like to show you like some of these things because we're gonna cover them like in future videos anyways. But I want to, to, to show you like some sounds and then the relative spectrograms. OK. So yeah, so here just like I learn some sounds and here like I have like a nice function that I can use to plot a spectrogram. Now, what's the spectrogram? So you'll hear about this like a lot moving forward. But basically, the idea is that with a spectrogram, we get a snapshot of, of like the different energy in different frequencies of a sound across the duration of that sound. And we can visualize that with a nice um plot, which is this one. OK. So now let's try to um uh listen to a sound. And so here, what I want to show you like is two sounds and we'll start with a violin sound uh which is at",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1433s",
        "start_time": "1433.78"
    },
    {
        "id": "187f8462",
        "text": "yeah, so here just like I learn some sounds and here like I have like a nice function that I can use to plot a spectrogram. Now, what's the spectrogram? So you'll hear about this like a lot moving forward. But basically, the idea is that with a spectrogram, we get a snapshot of, of like the different energy in different frequencies of a sound across the duration of that sound. And we can visualize that with a nice um plot, which is this one. OK. So now let's try to um uh listen to a sound. And so here, what I want to show you like is two sounds and we'll start with a violin sound uh which is at uh I believe at uh C four. OK. So that's middle C and so let's listen to this sound.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1450s",
        "start_time": "1450.839"
    },
    {
        "id": "9130d14a",
        "text": "of like the different energy in different frequencies of a sound across the duration of that sound. And we can visualize that with a nice um plot, which is this one. OK. So now let's try to um uh listen to a sound. And so here, what I want to show you like is two sounds and we'll start with a violin sound uh which is at uh I believe at uh C four. OK. So that's middle C and so let's listen to this sound. OK. Yeah, let's",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1468s",
        "start_time": "1468.464"
    },
    {
        "id": "9a9ad4de",
        "text": "uh I believe at uh C four. OK. So that's middle C and so let's listen to this sound. OK. Yeah, let's OK.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1497s",
        "start_time": "1497.88"
    },
    {
        "id": "71098efd",
        "text": "OK. Yeah, let's OK. Good. So that's C four on a um violin. OK. So now let's plot the spectrogram for this and here you have it OK? In all of its glory. So basically the idea here is that on the X axis you have time measured usually in seconds on the y axis, you have Hertz and this is like a logarithmic scale.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1508s",
        "start_time": "1508.43"
    },
    {
        "id": "f7fa6c7c",
        "text": "OK. Good. So that's C four on a um violin. OK. So now let's plot the spectrogram for this and here you have it OK? In all of its glory. So basically the idea here is that on the X axis you have time measured usually in seconds on the y axis, you have Hertz and this is like a logarithmic scale. And here, like each point here has a color and the color tells you, uh, like the uh intensity of a, um, yeah, of that frequency at that like specific time. And so here, like you have this color bar that tells you like how to read like the intensity based on color, like the redder and the more intense like the that frequency is OK.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1511s",
        "start_time": "1511.689"
    },
    {
        "id": "114e3ce9",
        "text": "Good. So that's C four on a um violin. OK. So now let's plot the spectrogram for this and here you have it OK? In all of its glory. So basically the idea here is that on the X axis you have time measured usually in seconds on the y axis, you have Hertz and this is like a logarithmic scale. And here, like each point here has a color and the color tells you, uh, like the uh intensity of a, um, yeah, of that frequency at that like specific time. And so here, like you have this color bar that tells you like how to read like the intensity based on color, like the redder and the more intense like the that frequency is OK. So as you can see here, so we, we have uh a lot of like energy here in, at days like",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1512s",
        "start_time": "1512.8"
    },
    {
        "id": "77795182",
        "text": "And here, like each point here has a color and the color tells you, uh, like the uh intensity of a, um, yeah, of that frequency at that like specific time. And so here, like you have this color bar that tells you like how to read like the intensity based on color, like the redder and the more intense like the that frequency is OK. So as you can see here, so we, we have uh a lot of like energy here in, at days like frequency band, which is around 256 precisely. This is a 261 which I believe like should be like the Hertz for CF four, the frequency for C four. And yes. And here we expect a lot of energy. But then we have like the, the, the partial, the harmonic partial here, which is double that frequency, which is a 500 to 12. And as you can see here, we have like",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1535s",
        "start_time": "1535.449"
    },
    {
        "id": "78777c5b",
        "text": "So as you can see here, so we, we have uh a lot of like energy here in, at days like frequency band, which is around 256 precisely. This is a 261 which I believe like should be like the Hertz for CF four, the frequency for C four. And yes. And here we expect a lot of energy. But then we have like the, the, the partial, the harmonic partial here, which is double that frequency, which is a 500 to 12. And as you can see here, we have like uh that coming in and then we multiply 256 S 61 by three and we have the third partial and you can see it here, the fourth, the fifth cool, it's super cool and you can go up and up and up and up and you still get like all of these partials. OK. And so uh as you can see, there's not much like in harmonic here. So the partials like are very well defined here.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1564s",
        "start_time": "1564.41"
    },
    {
        "id": "d7de9dab",
        "text": "frequency band, which is around 256 precisely. This is a 261 which I believe like should be like the Hertz for CF four, the frequency for C four. And yes. And here we expect a lot of energy. But then we have like the, the, the partial, the harmonic partial here, which is double that frequency, which is a 500 to 12. And as you can see here, we have like uh that coming in and then we multiply 256 S 61 by three and we have the third partial and you can see it here, the fourth, the fifth cool, it's super cool and you can go up and up and up and up and you still get like all of these partials. OK. And so uh as you can see, there's not much like in harmonic here. So the partials like are very well defined here. OK. So now let's compare this with a piano sound. OK. So now I have, I believe like it's ac five piano sound.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1573s",
        "start_time": "1573.119"
    },
    {
        "id": "6ab7cd09",
        "text": "uh that coming in and then we multiply 256 S 61 by three and we have the third partial and you can see it here, the fourth, the fifth cool, it's super cool and you can go up and up and up and up and you still get like all of these partials. OK. And so uh as you can see, there's not much like in harmonic here. So the partials like are very well defined here. OK. So now let's compare this with a piano sound. OK. So now I have, I believe like it's ac five piano sound. So let's listen now.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1599s",
        "start_time": "1599.16"
    },
    {
        "id": "eca7aa3f",
        "text": "OK. So now let's compare this with a piano sound. OK. So now I have, I believe like it's ac five piano sound. So let's listen now. OK. So you, you heard that? So this is C five. So it's an octave above uh the violin sound that we just heard. And so let's take a look at the",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1627s",
        "start_time": "1627.099"
    },
    {
        "id": "461b1b3a",
        "text": "So let's listen now. OK. So you, you heard that? So this is C five. So it's an octave above uh the violin sound that we just heard. And so let's take a look at the um",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1638s",
        "start_time": "1638.939"
    },
    {
        "id": "0dc1a25c",
        "text": "OK. So you, you heard that? So this is C five. So it's an octave above uh the violin sound that we just heard. And so let's take a look at the um um at the spectrum here",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1642s",
        "start_time": "1642.699"
    },
    {
        "id": "15d3c44d",
        "text": "um um at the spectrum here for this piano sound. And so, as you can see here, we have",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1654s",
        "start_time": "1654.689"
    },
    {
        "id": "c085fa9b",
        "text": "um at the spectrum here for this piano sound. And so, as you can see here, we have a lot",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1656s",
        "start_time": "1656.209"
    },
    {
        "id": "ced37621",
        "text": "for this piano sound. And so, as you can see here, we have a lot of activity uh around 512. And this is like a correct, right? Because this is like the fundamental frequency that we expect at um C five. And then over here you have like the first uh like har",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1658s",
        "start_time": "1658.67"
    },
    {
        "id": "ba38f2e9",
        "text": "a lot of activity uh around 512. And this is like a correct, right? Because this is like the fundamental frequency that we expect at um C five. And then over here you have like the first uh like har like partial, you go up the second, the third, the fourth. But as you can see the, the higher you go and the less presence like the partials are. And if you compare this like against this, right, you see the difference, OK.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1662s",
        "start_time": "1662.449"
    },
    {
        "id": "85127111",
        "text": "of activity uh around 512. And this is like a correct, right? Because this is like the fundamental frequency that we expect at um C five. And then over here you have like the first uh like har like partial, you go up the second, the third, the fourth. But as you can see the, the higher you go and the less presence like the partials are. And if you compare this like against this, right, you see the difference, OK. So this is like way more sustained across. So the the energy is more spread out across the whole like partials. Whereas here, like with the piano, like it, it seems to be like a little bit less. So, right? And so by looking at this, you can say, OK, so one of the reasons why like this two sounds are different is because of the",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1664s",
        "start_time": "1664.459"
    },
    {
        "id": "eff507ea",
        "text": "like partial, you go up the second, the third, the fourth. But as you can see the, the higher you go and the less presence like the partials are. And if you compare this like against this, right, you see the difference, OK. So this is like way more sustained across. So the the energy is more spread out across the whole like partials. Whereas here, like with the piano, like it, it seems to be like a little bit less. So, right? And so by looking at this, you can say, OK, so one of the reasons why like this two sounds are different is because of the a distribution of the energy across the different partials being different. Ok, cool. Ok. Now, don't worry, like if you don't have an idea about like what spectrograms are or how we, we got to uh to them or anything like that because this is gonna be a key topic and we're gonna cover this like in a lot of detail because like it's all about spectrograms and believe me, spectrograms are the key to a online sound.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1683s",
        "start_time": "1683.224"
    },
    {
        "id": "ea8aca0b",
        "text": "So this is like way more sustained across. So the the energy is more spread out across the whole like partials. Whereas here, like with the piano, like it, it seems to be like a little bit less. So, right? And so by looking at this, you can say, OK, so one of the reasons why like this two sounds are different is because of the a distribution of the energy across the different partials being different. Ok, cool. Ok. Now, don't worry, like if you don't have an idea about like what spectrograms are or how we, we got to uh to them or anything like that because this is gonna be a key topic and we're gonna cover this like in a lot of detail because like it's all about spectrograms and believe me, spectrograms are the key to a online sound. Ok. Well, I didn't want to go here. I just like spoiled what's coming next, but let's move on. OK. So now we have an idea about harmonic content. Now we have like one final aspect of timbre. So we said um envelope harmonic content and now frequency and amplitude modulation. So basically modulation in the uh the sound like itself. So what's frequency modulation?",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1700s",
        "start_time": "1700.51"
    },
    {
        "id": "a915ab0b",
        "text": "a distribution of the energy across the different partials being different. Ok, cool. Ok. Now, don't worry, like if you don't have an idea about like what spectrograms are or how we, we got to uh to them or anything like that because this is gonna be a key topic and we're gonna cover this like in a lot of detail because like it's all about spectrograms and believe me, spectrograms are the key to a online sound. Ok. Well, I didn't want to go here. I just like spoiled what's coming next, but let's move on. OK. So now we have an idea about harmonic content. Now we have like one final aspect of timbre. So we said um envelope harmonic content and now frequency and amplitude modulation. So basically modulation in the uh the sound like itself. So what's frequency modulation? So frequency mod modulation is also called in music circles as vibrato, right? And basically here you have um periodic variations in frequency",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1723s",
        "start_time": "1723.81"
    },
    {
        "id": "549e6e49",
        "text": "Ok. Well, I didn't want to go here. I just like spoiled what's coming next, but let's move on. OK. So now we have an idea about harmonic content. Now we have like one final aspect of timbre. So we said um envelope harmonic content and now frequency and amplitude modulation. So basically modulation in the uh the sound like itself. So what's frequency modulation? So frequency mod modulation is also called in music circles as vibrato, right? And basically here you have um periodic variations in frequency and in music, you have this for expressive purposes. But uh like we can create an effect, a vibrato effect by using like frequency modulation. And this can be formalized. And but basically the idea behind this is that",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1753s",
        "start_time": "1753.06"
    },
    {
        "id": "b3c08e7f",
        "text": "So frequency mod modulation is also called in music circles as vibrato, right? And basically here you have um periodic variations in frequency and in music, you have this for expressive purposes. But uh like we can create an effect, a vibrato effect by using like frequency modulation. And this can be formalized. And but basically the idea behind this is that you start like with a signal that you want to use to modulate the frequency of a a carrier signal, which is this one like in blue jam in the middle. So you apply like this message signal signal on the",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1778s",
        "start_time": "1778.099"
    },
    {
        "id": "e994360c",
        "text": "and in music, you have this for expressive purposes. But uh like we can create an effect, a vibrato effect by using like frequency modulation. And this can be formalized. And but basically the idea behind this is that you start like with a signal that you want to use to modulate the frequency of a a carrier signal, which is this one like in blue jam in the middle. So you apply like this message signal signal on the carry a signal and you get these results down below and as you can see here, that's a frequency modulation because the frequency starts uh like this. But then the frequency kind of like um uh goes down.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1792s",
        "start_time": "1792.569"
    },
    {
        "id": "2cd096ea",
        "text": "you start like with a signal that you want to use to modulate the frequency of a a carrier signal, which is this one like in blue jam in the middle. So you apply like this message signal signal on the carry a signal and you get these results down below and as you can see here, that's a frequency modulation because the frequency starts uh like this. But then the frequency kind of like um uh goes down. OK.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1809s",
        "start_time": "1809.15"
    },
    {
        "id": "a37bc5f1",
        "text": "carry a signal and you get these results down below and as you can see here, that's a frequency modulation because the frequency starts uh like this. But then the frequency kind of like um uh goes down. OK. And then it goes up and then it goes down, right? And so here we have a frequency modulation. So now you may be wondering, but how does like that sound like now if you are a musician, obviously, you know what a vibrato is. But if you're not, I'm gonna show you and this is where that amazing video comes in.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1825s",
        "start_time": "1825.25"
    },
    {
        "id": "ac643a7b",
        "text": "OK. And then it goes up and then it goes down, right? And so here we have a frequency modulation. So now you may be wondering, but how does like that sound like now if you are a musician, obviously, you know what a vibrato is. But if you're not, I'm gonna show you and this is where that amazing video comes in. And so here we have a violinist who explains how to uh play vibrato on the violin. So the first thing you'll hear is like the uh kind of like altering like two notes, right? And then after that, you'll hear like how that can become a vibrato.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1842s",
        "start_time": "1842.3"
    },
    {
        "id": "53833270",
        "text": "And then it goes up and then it goes down, right? And so here we have a frequency modulation. So now you may be wondering, but how does like that sound like now if you are a musician, obviously, you know what a vibrato is. But if you're not, I'm gonna show you and this is where that amazing video comes in. And so here we have a violinist who explains how to uh play vibrato on the violin. So the first thing you'll hear is like the uh kind of like altering like two notes, right? And then after that, you'll hear like how that can become a vibrato. Yeah, that's very dull, right? It's Tuna did did did very, very dull. But if you change the speed there, you'll hear like the typical violin sound, the vibrato sound let's hear.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1843s",
        "start_time": "1843.67"
    },
    {
        "id": "5cb74be8",
        "text": "And so here we have a violinist who explains how to uh play vibrato on the violin. So the first thing you'll hear is like the uh kind of like altering like two notes, right? And then after that, you'll hear like how that can become a vibrato. Yeah, that's very dull, right? It's Tuna did did did very, very dull. But if you change the speed there, you'll hear like the typical violin sound, the vibrato sound let's hear. And until you reach normal vibrato speed",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1863s",
        "start_time": "1863.15"
    },
    {
        "id": "f00114a6",
        "text": "Yeah, that's very dull, right? It's Tuna did did did very, very dull. But if you change the speed there, you'll hear like the typical violin sound, the vibrato sound let's hear. And until you reach normal vibrato speed and you need to, right? That's super cool. Now, every time like you listen to violins or orchestras, like the strings usually used like this vibrato sound quite uh intensively. OK. So uh one thing that I want to say is like uh as a keyboard player, unfortunately, we don't have this luxury of having like a vibrato, but sometimes like, you'll see pianists who like",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1886s",
        "start_time": "1886.77"
    },
    {
        "id": "10c591d2",
        "text": "And until you reach normal vibrato speed and you need to, right? That's super cool. Now, every time like you listen to violins or orchestras, like the strings usually used like this vibrato sound quite uh intensively. OK. So uh one thing that I want to say is like uh as a keyboard player, unfortunately, we don't have this luxury of having like a vibrato, but sometimes like, you'll see pianists who like kind of like strike a key and then after that, they kind of try to do every brow, obviously that doesn't have like any effect on the sound. But still if you have that, so it's a, it's like a nice, like quirk. OK. But if you have a midi keyboard, uh midi keyboards have this thing which is like after touch, like and after like you strike a key, then you can like vibrate the sound and it's gonna like vibrate but not on a piano. I can assure you that I've tried. It doesn't work. OK.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1898s",
        "start_time": "1898.91"
    },
    {
        "id": "102b68d1",
        "text": "and you need to, right? That's super cool. Now, every time like you listen to violins or orchestras, like the strings usually used like this vibrato sound quite uh intensively. OK. So uh one thing that I want to say is like uh as a keyboard player, unfortunately, we don't have this luxury of having like a vibrato, but sometimes like, you'll see pianists who like kind of like strike a key and then after that, they kind of try to do every brow, obviously that doesn't have like any effect on the sound. But still if you have that, so it's a, it's like a nice, like quirk. OK. But if you have a midi keyboard, uh midi keyboards have this thing which is like after touch, like and after like you strike a key, then you can like vibrate the sound and it's gonna like vibrate but not on a piano. I can assure you that I've tried. It doesn't work. OK. Uh So now the other type of like modulation that we have is called amplitude modulation. And in musical terms, this is called tremolo and this is still like a periodic variation, but it's not in frequency but in amplitude and once again, music, we can use this like for expressive purposes. OK.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1906s",
        "start_time": "1906.579"
    },
    {
        "id": "683ee55b",
        "text": "kind of like strike a key and then after that, they kind of try to do every brow, obviously that doesn't have like any effect on the sound. But still if you have that, so it's a, it's like a nice, like quirk. OK. But if you have a midi keyboard, uh midi keyboards have this thing which is like after touch, like and after like you strike a key, then you can like vibrate the sound and it's gonna like vibrate but not on a piano. I can assure you that I've tried. It doesn't work. OK. Uh So now the other type of like modulation that we have is called amplitude modulation. And in musical terms, this is called tremolo and this is still like a periodic variation, but it's not in frequency but in amplitude and once again, music, we can use this like for expressive purposes. OK. So how do we get to amplitude modulation? Basically, it's the same idea. So we have a signal message signal, we have a carrier signal and we apply the signal on the carrier signal but this time not on the frequency but on the amplitude. And so you, you're gonna have like this effect that is like ah so you like bursts and",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1932s",
        "start_time": "1932.069"
    },
    {
        "id": "8be20f7c",
        "text": "Uh So now the other type of like modulation that we have is called amplitude modulation. And in musical terms, this is called tremolo and this is still like a periodic variation, but it's not in frequency but in amplitude and once again, music, we can use this like for expressive purposes. OK. So how do we get to amplitude modulation? Basically, it's the same idea. So we have a signal message signal, we have a carrier signal and we apply the signal on the carrier signal but this time not on the frequency but on the amplitude. And so you, you're gonna have like this effect that is like ah so you like bursts and and uh release lack of amplitude like at regular intervals. Now, um I want to show you uh this once again. So, so let me just let go here. So I have a sound down here that I want to show you with with the tremolo right? Amplitude modulation. So let's hear that. But yeah, let's go back to zero.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1960s",
        "start_time": "1960.88"
    },
    {
        "id": "96374e26",
        "text": "So how do we get to amplitude modulation? Basically, it's the same idea. So we have a signal message signal, we have a carrier signal and we apply the signal on the carrier signal but this time not on the frequency but on the amplitude. And so you, you're gonna have like this effect that is like ah so you like bursts and and uh release lack of amplitude like at regular intervals. Now, um I want to show you uh this once again. So, so let me just let go here. So I have a sound down here that I want to show you with with the tremolo right? Amplitude modulation. So let's hear that. But yeah, let's go back to zero. OK. So yeah, you get the idea. So there are like you have like this tremor effects and probably you are familiar with that because like you've heard it like in many different like musical pieces and, and what not, right? But basically the idea here is that when we modulate the sound like this both like on the um amplitude and frequency uh dimensions, what happens is that we, we change the tre of sound. Now we don't really change",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=1982s",
        "start_time": "1982.689"
    },
    {
        "id": "1aad3166",
        "text": "and uh release lack of amplitude like at regular intervals. Now, um I want to show you uh this once again. So, so let me just let go here. So I have a sound down here that I want to show you with with the tremolo right? Amplitude modulation. So let's hear that. But yeah, let's go back to zero. OK. So yeah, you get the idea. So there are like you have like this tremor effects and probably you are familiar with that because like you've heard it like in many different like musical pieces and, and what not, right? But basically the idea here is that when we modulate the sound like this both like on the um amplitude and frequency uh dimensions, what happens is that we, we change the tre of sound. Now we don't really change that much like the perception like of the of the frequency itself, like of the amplitude is more like a quirk that changes the time BRA. It's as if like these things were like localized and had an effect like on time BRA.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=2005s",
        "start_time": "2005.844"
    },
    {
        "id": "5500c697",
        "text": "OK. So yeah, you get the idea. So there are like you have like this tremor effects and probably you are familiar with that because like you've heard it like in many different like musical pieces and, and what not, right? But basically the idea here is that when we modulate the sound like this both like on the um amplitude and frequency uh dimensions, what happens is that we, we change the tre of sound. Now we don't really change that much like the perception like of the of the frequency itself, like of the amplitude is more like a quirk that changes the time BRA. It's as if like these things were like localized and had an effect like on time BRA. OK. So here you have it like all the different things like that determine like time brad that we kind of like know of or have like reconstructed. So time Brad for sure is a multifunctional sound uh like dimension dimension of sound and it has like three main things. So we already review them. So amplitude envelope, the harmonic content or distribution of energy",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=2046s",
        "start_time": "2046.06"
    },
    {
        "id": "5b1a3a26",
        "text": "that much like the perception like of the of the frequency itself, like of the amplitude is more like a quirk that changes the time BRA. It's as if like these things were like localized and had an effect like on time BRA. OK. So here you have it like all the different things like that determine like time brad that we kind of like know of or have like reconstructed. So time Brad for sure is a multifunctional sound uh like dimension dimension of sound and it has like three main things. So we already review them. So amplitude envelope, the harmonic content or distribution of energy uh across different partials and then the uh signal modulation both like in frequency and amplitude. OK. So by now, you should have like a good idea of sound like and all the different uh aspects of sound. So we last time like we saw that sound like",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=2072s",
        "start_time": "2072.685"
    },
    {
        "id": "9e026d86",
        "text": "OK. So here you have it like all the different things like that determine like time brad that we kind of like know of or have like reconstructed. So time Brad for sure is a multifunctional sound uh like dimension dimension of sound and it has like three main things. So we already review them. So amplitude envelope, the harmonic content or distribution of energy uh across different partials and then the uh signal modulation both like in frequency and amplitude. OK. So by now, you should have like a good idea of sound like and all the different uh aspects of sound. So we last time like we saw that sound like is a mechanical wave that propagates through air. It is characterized by frequency like the intensity, we can describe it with frequent intensity chambre and obviously like some of these things are also like quite subjective, like pitch loud dance and of course, uh Tre",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=2088s",
        "start_time": "2088.62"
    },
    {
        "id": "5c198ae5",
        "text": "uh across different partials and then the uh signal modulation both like in frequency and amplitude. OK. So by now, you should have like a good idea of sound like and all the different uh aspects of sound. So we last time like we saw that sound like is a mechanical wave that propagates through air. It is characterized by frequency like the intensity, we can describe it with frequent intensity chambre and obviously like some of these things are also like quite subjective, like pitch loud dance and of course, uh Tre OK. So now you have like this very nice introduction and we are now ready to go to the next level, which is introducing audio signals. and specifically next time we'll are gonna tackle two very interesting like topics. Well, for for sure, we are gonna introduce like audio signals and then talk about audio to digital conversion called A DC.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=2112s",
        "start_time": "2112.35"
    },
    {
        "id": "c85a098e",
        "text": "is a mechanical wave that propagates through air. It is characterized by frequency like the intensity, we can describe it with frequent intensity chambre and obviously like some of these things are also like quite subjective, like pitch loud dance and of course, uh Tre OK. So now you have like this very nice introduction and we are now ready to go to the next level, which is introducing audio signals. and specifically next time we'll are gonna tackle two very interesting like topics. Well, for for sure, we are gonna introduce like audio signals and then talk about audio to digital conversion called A DC. The acronym and digital to audio conversion or D AC, which is kind of like the inverse process. OK. So I hope you like, you've enjoyed this video. If that's the case, remember to leave, like if you haven't subscribed yet, please do so if you have any questions or doubts or anything. Uh Please like just like leave a comment in the section below",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=2131s",
        "start_time": "2131.945"
    },
    {
        "id": "3f231357",
        "text": "OK. So now you have like this very nice introduction and we are now ready to go to the next level, which is introducing audio signals. and specifically next time we'll are gonna tackle two very interesting like topics. Well, for for sure, we are gonna introduce like audio signals and then talk about audio to digital conversion called A DC. The acronym and digital to audio conversion or D AC, which is kind of like the inverse process. OK. So I hope you like, you've enjoyed this video. If that's the case, remember to leave, like if you haven't subscribed yet, please do so if you have any questions or doubts or anything. Uh Please like just like leave a comment in the section below and talking about questions. I just want to remind you if you haven't. Uh Well, if you, if you are already here, that's great. But if you're not, please uh join the Sound of A I Slack community. So there you'll find a lot of people interested in like audio processing, machine learning A I music, all this kind of stuff, you can get feedback and talk with other people.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=2151s",
        "start_time": "2151.76"
    },
    {
        "id": "50b44837",
        "text": "The acronym and digital to audio conversion or D AC, which is kind of like the inverse process. OK. So I hope you like, you've enjoyed this video. If that's the case, remember to leave, like if you haven't subscribed yet, please do so if you have any questions or doubts or anything. Uh Please like just like leave a comment in the section below and talking about questions. I just want to remind you if you haven't. Uh Well, if you, if you are already here, that's great. But if you're not, please uh join the Sound of A I Slack community. So there you'll find a lot of people interested in like audio processing, machine learning A I music, all this kind of stuff, you can get feedback and talk with other people. So I'll leave you a uh link to sign up to the Slack community in the description below. So I hope like you really enjoyed this video. It's all for today until the next time. Cheers.",
        "video": "Intensity, Loudness, and Timbre",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "Jkoysm1fHUw",
        "youtube_link": "https://www.youtube.com/watch?v=Jkoysm1fHUw&t=2174s",
        "start_time": "2174.212"
    },
    {
        "id": "99b3dbde",
        "text": "Hi, everybody and welcome to a new exciting video in the audio signal processing for machine learning series. Last time we looked at mad frequency CEPT coefficients from a theoretical standpoint. In this video, I want to show you how you can extract MF CCS using Python and Libros. So let's get started with this Jupiter notebook. So the first thing I want to do is just import a bunch of libraries that we'll be using. So I'll import Li Brosa Libros dot display, ipython dot display, uh Pylot and then pie. So let's do that. Next thing we want to load uh an audio file. So which file are we gonna load? So it's a file that we, you should be familiar with by. Now it's a, a short passage of a piece from clothes, the Bey. So we're talking about classical music here.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "813e5356",
        "text": "So the first thing I want to do is just import a bunch of libraries that we'll be using. So I'll import Li Brosa Libros dot display, ipython dot display, uh Pylot and then pie. So let's do that. Next thing we want to load uh an audio file. So which file are we gonna load? So it's a file that we, you should be familiar with by. Now it's a, a short passage of a piece from clothes, the Bey. So we're talking about classical music here. OK. So I'll do audio and then I have the file uh that's called the BC dot WAV. OK. So now first thing let's play back this audio file in the Jupiter network. So we'll do a in IP D dot audio and then pass in the path to the file. OK? And here we go. So let's listen to a little bit of this.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=20s",
        "start_time": "20.389"
    },
    {
        "id": "01bbb1b1",
        "text": "So which file are we gonna load? So it's a file that we, you should be familiar with by. Now it's a, a short passage of a piece from clothes, the Bey. So we're talking about classical music here. OK. So I'll do audio and then I have the file uh that's called the BC dot WAV. OK. So now first thing let's play back this audio file in the Jupiter network. So we'll do a in IP D dot audio and then pass in the path to the file. OK? And here we go. So let's listen to a little bit of this. OK? So if you followed along during the series, you probably recognize",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=37s",
        "start_time": "37.47"
    },
    {
        "id": "dfabae36",
        "text": "OK. So I'll do audio and then I have the file uh that's called the BC dot WAV. OK. So now first thing let's play back this audio file in the Jupiter network. So we'll do a in IP D dot audio and then pass in the path to the file. OK? And here we go. So let's listen to a little bit of this. OK? So if you followed along during the series, you probably recognize good. So the next thing that we wanna do, we want to actually load the uh the audio file using a li browser. And so, uh again, this is something that we've done multiple times. So we'll take the signal and the sampling rate over here and then we'll do a Lisa dot load. And then what we wanna pass is the audio file over here.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=52s",
        "start_time": "52.534"
    },
    {
        "id": "cbe927db",
        "text": "OK? So if you followed along during the series, you probably recognize good. So the next thing that we wanna do, we want to actually load the uh the audio file using a li browser. And so, uh again, this is something that we've done multiple times. So we'll take the signal and the sampling rate over here and then we'll do a Lisa dot load. And then what we wanna pass is the audio file over here. And yes. And so here we have the sampling rate and here we have the signal. So let's take a look at the uh signal over here at the shape of the signal. And as you can see, we have this amount of samples uh in this uh waveform. Next step for us is extracting MF CCS, extract NF CCS. OK. So",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=82s",
        "start_time": "82.62"
    },
    {
        "id": "5c4d3813",
        "text": "good. So the next thing that we wanna do, we want to actually load the uh the audio file using a li browser. And so, uh again, this is something that we've done multiple times. So we'll take the signal and the sampling rate over here and then we'll do a Lisa dot load. And then what we wanna pass is the audio file over here. And yes. And so here we have the sampling rate and here we have the signal. So let's take a look at the uh signal over here at the shape of the signal. And as you can see, we have this amount of samples uh in this uh waveform. Next step for us is extracting MF CCS, extract NF CCS. OK. So how do we do that? Well, this is extremely simple in Li Brosa because we have a function that does dash uh almost for free for us. So MS CC is equal to libros dot feature. And then MFCC like this and we want to pass in the signal, specify the number of MF CCS and we'll",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=88s",
        "start_time": "88.879"
    },
    {
        "id": "027addc2",
        "text": "And yes. And so here we have the sampling rate and here we have the signal. So let's take a look at the uh signal over here at the shape of the signal. And as you can see, we have this amount of samples uh in this uh waveform. Next step for us is extracting MF CCS, extract NF CCS. OK. So how do we do that? Well, this is extremely simple in Li Brosa because we have a function that does dash uh almost for free for us. So MS CC is equal to libros dot feature. And then MFCC like this and we want to pass in the signal, specify the number of MF CCS and we'll use like a traditional number which is equal to 13 and then specify the sampling rate like this. OK. Let's do this. OK? So here we have the MF CCS. Now, now let's take a look at the uh the shape of this MF CCS. And so this should be a dimensional array or a matrix. And so we'll do MF CCS dot shape.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=117s",
        "start_time": "117.51"
    },
    {
        "id": "e983d5f9",
        "text": "how do we do that? Well, this is extremely simple in Li Brosa because we have a function that does dash uh almost for free for us. So MS CC is equal to libros dot feature. And then MFCC like this and we want to pass in the signal, specify the number of MF CCS and we'll use like a traditional number which is equal to 13 and then specify the sampling rate like this. OK. Let's do this. OK? So here we have the MF CCS. Now, now let's take a look at the uh the shape of this MF CCS. And so this should be a dimensional array or a matrix. And so we'll do MF CCS dot shape. And as you can see, we have, the number of rows is equal to 13 and the number of columns that we have or in other words, like the different frames is equal to 1000 almost um 1300 frames or discrete time points. Next, we want to visualize the MF CCS.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=141s",
        "start_time": "141.735"
    },
    {
        "id": "4002655a",
        "text": "use like a traditional number which is equal to 13 and then specify the sampling rate like this. OK. Let's do this. OK? So here we have the MF CCS. Now, now let's take a look at the uh the shape of this MF CCS. And so this should be a dimensional array or a matrix. And so we'll do MF CCS dot shape. And as you can see, we have, the number of rows is equal to 13 and the number of columns that we have or in other words, like the different frames is equal to 1000 almost um 1300 frames or discrete time points. Next, we want to visualize the MF CCS. So let's write, visualize MF CCS. And so we can easily do this with a native function in libros that's called spec show. So let's get started. So we'll do a plot dot uh figure as the first thing and we want to specify the figure size of yeah of our figure. So we'll do a fig size and we'll set this equal to 25 by 10. OK.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=166s",
        "start_time": "166.24"
    },
    {
        "id": "8f8a5e43",
        "text": "And as you can see, we have, the number of rows is equal to 13 and the number of columns that we have or in other words, like the different frames is equal to 1000 almost um 1300 frames or discrete time points. Next, we want to visualize the MF CCS. So let's write, visualize MF CCS. And so we can easily do this with a native function in libros that's called spec show. So let's get started. So we'll do a plot dot uh figure as the first thing and we want to specify the figure size of yeah of our figure. So we'll do a fig size and we'll set this equal to 25 by 10. OK. And now we are gonna be using the Li Brosa dot this play dot spectra function and this is a function that enables us to visualize any spectrogram like uh feature. So we'll pass in as arguments. The, obviously, the MF CCS will want to specify that the X axis uh is gonna have like time.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=191s",
        "start_time": "191.24"
    },
    {
        "id": "594adb1b",
        "text": "So let's write, visualize MF CCS. And so we can easily do this with a native function in libros that's called spec show. So let's get started. So we'll do a plot dot uh figure as the first thing and we want to specify the figure size of yeah of our figure. So we'll do a fig size and we'll set this equal to 25 by 10. OK. And now we are gonna be using the Li Brosa dot this play dot spectra function and this is a function that enables us to visualize any spectrogram like uh feature. So we'll pass in as arguments. The, obviously, the MF CCS will want to specify that the X axis uh is gonna have like time. And finally, we need to pass the sampling rate in. Now, this is gonna be a heat map like uh visualization. So we're gonna have like some colors. And what we wanna do is like having the ability to map these different colors to numerical values. So for that, we'll need a color bar. So we'll do a plot dot color bar. And here",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=211s",
        "start_time": "211.75"
    },
    {
        "id": "3a1fafee",
        "text": "And now we are gonna be using the Li Brosa dot this play dot spectra function and this is a function that enables us to visualize any spectrogram like uh feature. So we'll pass in as arguments. The, obviously, the MF CCS will want to specify that the X axis uh is gonna have like time. And finally, we need to pass the sampling rate in. Now, this is gonna be a heat map like uh visualization. So we're gonna have like some colors. And what we wanna do is like having the ability to map these different colors to numerical values. So for that, we'll need a color bar. So we'll do a plot dot color bar. And here uh we have an argument called format and here we pass percentage plus to F and the last thing is just showing the um the, this visualization. So let's take a look at this. And here we go.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=240s",
        "start_time": "240.07"
    },
    {
        "id": "9080b998",
        "text": "And finally, we need to pass the sampling rate in. Now, this is gonna be a heat map like uh visualization. So we're gonna have like some colors. And what we wanna do is like having the ability to map these different colors to numerical values. So for that, we'll need a color bar. So we'll do a plot dot color bar. And here uh we have an argument called format and here we pass percentage plus to F and the last thing is just showing the um the, this visualization. So let's take a look at this. And here we go. So here we have the visualization. And as you can see on the X axis, we have uh time and uh like this, the BC musical passage is a 32nd long. And here on the Y axis, we have the different uh coefficients. So the different MFCC coefficients and if you count, you'll see that we have 13 coefficients, right? And at each point in this diagram, we have the value for",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=266s",
        "start_time": "266.179"
    },
    {
        "id": "3f3ec290",
        "text": "uh we have an argument called format and here we pass percentage plus to F and the last thing is just showing the um the, this visualization. So let's take a look at this. And here we go. So here we have the visualization. And as you can see on the X axis, we have uh time and uh like this, the BC musical passage is a 32nd long. And here on the Y axis, we have the different uh coefficients. So the different MFCC coefficients and if you count, you'll see that we have 13 coefficients, right? And at each point in this diagram, we have the value for a given MFCCMFCC index at a certain point in time. And here you have the,",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=292s",
        "start_time": "292.26"
    },
    {
        "id": "46eba1ce",
        "text": "So here we have the visualization. And as you can see on the X axis, we have uh time and uh like this, the BC musical passage is a 32nd long. And here on the Y axis, we have the different uh coefficients. So the different MFCC coefficients and if you count, you'll see that we have 13 coefficients, right? And at each point in this diagram, we have the value for a given MFCCMFCC index at a certain point in time. And here you have the, the mapping between like the uh the colors and the different numerical values. Next, we want to calculate the 1st and 2nd derivatives of the MF CCS. We saw this already in the previous video and we can call these features delta and delta delta MF CCS. And they are very important",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=307s",
        "start_time": "307.869"
    },
    {
        "id": "66573b3d",
        "text": "a given MFCCMFCC index at a certain point in time. And here you have the, the mapping between like the uh the colors and the different numerical values. Next, we want to calculate the 1st and 2nd derivatives of the MF CCS. We saw this already in the previous video and we can call these features delta and delta delta MF CCS. And they are very important to tell us how the MS CCS change over time in an audio file. OK. So first of all, yeah, let me just add a few of this. So it becomes clear what I'm doing. And then let me just add a comment saying calculate delta and delta",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=336s",
        "start_time": "336.19"
    },
    {
        "id": "08bacfdd",
        "text": "the mapping between like the uh the colors and the different numerical values. Next, we want to calculate the 1st and 2nd derivatives of the MF CCS. We saw this already in the previous video and we can call these features delta and delta delta MF CCS. And they are very important to tell us how the MS CCS change over time in an audio file. OK. So first of all, yeah, let me just add a few of this. So it becomes clear what I'm doing. And then let me just add a comment saying calculate delta and delta to MF CCS. OK. So how can we do that. Well, this is again, very simple because Li Brosa comes with a built in function for uh doing that. So we'll say delta MF CCS is equal to Li Li Brosa dot feature dot Delta, delta. And here we just pass the MF CCS and for the delta two",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=346s",
        "start_time": "346.5"
    },
    {
        "id": "df81380b",
        "text": "to tell us how the MS CCS change over time in an audio file. OK. So first of all, yeah, let me just add a few of this. So it becomes clear what I'm doing. And then let me just add a comment saying calculate delta and delta to MF CCS. OK. So how can we do that. Well, this is again, very simple because Li Brosa comes with a built in function for uh doing that. So we'll say delta MF CCS is equal to Li Li Brosa dot feature dot Delta, delta. And here we just pass the MF CCS and for the delta two MF CCS will use the uh very same function. So it's li browser dot feature dot delta and we'll pass once again the MF CCS here, but we should specify a keyword argument called order. And here we say the order is equal to two.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=365s",
        "start_time": "365.0"
    },
    {
        "id": "2cb0c890",
        "text": "to MF CCS. OK. So how can we do that. Well, this is again, very simple because Li Brosa comes with a built in function for uh doing that. So we'll say delta MF CCS is equal to Li Li Brosa dot feature dot Delta, delta. And here we just pass the MF CCS and for the delta two MF CCS will use the uh very same function. So it's li browser dot feature dot delta and we'll pass once again the MF CCS here, but we should specify a keyword argument called order. And here we say the order is equal to two. And so this is the second derivative. OK. Yeah. So now let's take a look at the shape of this um",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=387s",
        "start_time": "387.179"
    },
    {
        "id": "2ce2e36c",
        "text": "MF CCS will use the uh very same function. So it's li browser dot feature dot delta and we'll pass once again the MF CCS here, but we should specify a keyword argument called order. And here we say the order is equal to two. And so this is the second derivative. OK. Yeah. So now let's take a look at the shape of this um delta MF CCS for example. So we'll do delta MF CCS dot A shape. And as you can see, they have the very same shape that we had for MF CCS. And if you don't remember if we do am F CCS dot uh shape, you can see. Yes. So we have 13 indexes and also like in the delta MF CCS because",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=417s",
        "start_time": "417.45"
    },
    {
        "id": "4d660370",
        "text": "And so this is the second derivative. OK. Yeah. So now let's take a look at the shape of this um delta MF CCS for example. So we'll do delta MF CCS dot A shape. And as you can see, they have the very same shape that we had for MF CCS. And if you don't remember if we do am F CCS dot uh shape, you can see. Yes. So we have 13 indexes and also like in the delta MF CCS because at the end of the day, we are just taking the first derivative, right? So 13 and then uh the number of frames that we have is equal to uh 1292. Now let me visualize delta MF CCS and delta delta MFCC. So how can we do that? Well, it's very simple. We just take like this piece of code here.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=439s",
        "start_time": "439.39"
    },
    {
        "id": "3cbd22b7",
        "text": "delta MF CCS for example. So we'll do delta MF CCS dot A shape. And as you can see, they have the very same shape that we had for MF CCS. And if you don't remember if we do am F CCS dot uh shape, you can see. Yes. So we have 13 indexes and also like in the delta MF CCS because at the end of the day, we are just taking the first derivative, right? So 13 and then uh the number of frames that we have is equal to uh 1292. Now let me visualize delta MF CCS and delta delta MFCC. So how can we do that? Well, it's very simple. We just take like this piece of code here. Let's just get this,",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=447s",
        "start_time": "447.7"
    },
    {
        "id": "7adfedd3",
        "text": "at the end of the day, we are just taking the first derivative, right? So 13 and then uh the number of frames that we have is equal to uh 1292. Now let me visualize delta MF CCS and delta delta MFCC. So how can we do that? Well, it's very simple. We just take like this piece of code here. Let's just get this, we'll put this one here and instead of MF CCS, we'll pass in delta uh MF CCS over here, let's do this. And so these are the delta MF CCS and now let's do the very same thing. But with a delta, delta MF CCS, it will pass in the delta to MF CCS. And here you go. OK.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=470s",
        "start_time": "470.885"
    },
    {
        "id": "2c20d6eb",
        "text": "Let's just get this, we'll put this one here and instead of MF CCS, we'll pass in delta uh MF CCS over here, let's do this. And so these are the delta MF CCS and now let's do the very same thing. But with a delta, delta MF CCS, it will pass in the delta to MF CCS. And here you go. OK. So here we have the original MF CCS. Here we have the first derivative of the MF CCS. And here the second uh derivative of the MF CCS, the final thing that I want to show you guys is taking the MF CCS and the 1st and 2nd derivatives of the MF CCS and co concatenating them together so that we have a unique",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=494s",
        "start_time": "494.73"
    },
    {
        "id": "20f76670",
        "text": "we'll put this one here and instead of MF CCS, we'll pass in delta uh MF CCS over here, let's do this. And so these are the delta MF CCS and now let's do the very same thing. But with a delta, delta MF CCS, it will pass in the delta to MF CCS. And here you go. OK. So here we have the original MF CCS. Here we have the first derivative of the MF CCS. And here the second uh derivative of the MF CCS, the final thing that I want to show you guys is taking the MF CCS and the 1st and 2nd derivatives of the MF CCS and co concatenating them together so that we have a unique audio uh feature that's like quite comprehensive because we have the MF CCS and we also have information about how does MF CCS change over time. Thanks to the derivatives. OK. So how can we do that? Well, this is very simple or if we use N? So we'll say that here we have uh yeah, let's call it uh comprehensive and",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=497s",
        "start_time": "497.779"
    },
    {
        "id": "cc32b886",
        "text": "So here we have the original MF CCS. Here we have the first derivative of the MF CCS. And here the second uh derivative of the MF CCS, the final thing that I want to show you guys is taking the MF CCS and the 1st and 2nd derivatives of the MF CCS and co concatenating them together so that we have a unique audio uh feature that's like quite comprehensive because we have the MF CCS and we also have information about how does MF CCS change over time. Thanks to the derivatives. OK. So how can we do that? Well, this is very simple or if we use N? So we'll say that here we have uh yeah, let's call it uh comprehensive and MF CCS. And here we'll do a NP dot concatenate. And here we need to pass the three arrays that we wanna concatenate. So the first one is MF CCS and then we'll pass delta MF CCS and delta to MF CCS. And here we go. So now let's take a look at the shape of this comprehensive MF CCS audio feature.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=522s",
        "start_time": "522.479"
    },
    {
        "id": "d49493a6",
        "text": "audio uh feature that's like quite comprehensive because we have the MF CCS and we also have information about how does MF CCS change over time. Thanks to the derivatives. OK. So how can we do that? Well, this is very simple or if we use N? So we'll say that here we have uh yeah, let's call it uh comprehensive and MF CCS. And here we'll do a NP dot concatenate. And here we need to pass the three arrays that we wanna concatenate. So the first one is MF CCS and then we'll pass delta MF CCS and delta to MF CCS. And here we go. So now let's take a look at the shape of this comprehensive MF CCS audio feature. And not surprisingly here. We have that the number of rows, the first axis is equal to 39. And that's because we basically are concatenating like 13 indexes like for each of the three arrays, right? And so we get like a",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=545s",
        "start_time": "545.299"
    },
    {
        "id": "85dface0",
        "text": "MF CCS. And here we'll do a NP dot concatenate. And here we need to pass the three arrays that we wanna concatenate. So the first one is MF CCS and then we'll pass delta MF CCS and delta to MF CCS. And here we go. So now let's take a look at the shape of this comprehensive MF CCS audio feature. And not surprisingly here. We have that the number of rows, the first axis is equal to 39. And that's because we basically are concatenating like 13 indexes like for each of the three arrays, right? And so we get like a 39 here, whereas the number of columns remains unchanged. Great. By now, you should be able to extract MF CCS and 1st and 2nd derivative MF CCS using Python and Libres. And you should also be able to visualize them using Yeah, spectra from Libres.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=570s",
        "start_time": "570.419"
    },
    {
        "id": "6a8a9671",
        "text": "And not surprisingly here. We have that the number of rows, the first axis is equal to 39. And that's because we basically are concatenating like 13 indexes like for each of the three arrays, right? And so we get like a 39 here, whereas the number of columns remains unchanged. Great. By now, you should be able to extract MF CCS and 1st and 2nd derivative MF CCS using Python and Libres. And you should also be able to visualize them using Yeah, spectra from Libres. So in the next video, we'll be looking into uh frequency domain audio features. So I guess that's all for today. I hope you enjoyed the video. If that's the case, please remember to leave a like and if you haven't subscribed yet, please consider doing it. So I'll see you next time. Cheers.",
        "video": "Extracting Mel-Frequency Cepstral Coefficients with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "WJI-17MNpdE",
        "youtube_link": "https://www.youtube.com/watch?v=WJI-17MNpdE&t=597s",
        "start_time": "597.77"
    },
    {
        "id": "7512d62f",
        "text": "Hi, everybody and welcome to New VND audio processing for machine learning series. This time, we'll look at the extraction pipelines that we need to extract both time domain features and frequency domain features. Before starting looking into this, I just want to remind you about the sound of the Eye Slack community, which is a community with people interested in all things A I audio A I music and audio digital signal processing. So if you want to improve your skills and network with cool people, just consider signing up there and Olivia sign up link to the community in the description below. Now let's go back to the cool stuff in the previous video. We took a look at a few different types of uh audio features. So time domain features which uh leave in the time domain, frequency domain features which leave in the frequency domain and finally time frequency domain features which are features that provide us with information both about time and frequency.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "a10e8669",
        "text": "A I audio A I music and audio digital signal processing. So if you want to improve your skills and network with cool people, just consider signing up there and Olivia sign up link to the community in the description below. Now let's go back to the cool stuff in the previous video. We took a look at a few different types of uh audio features. So time domain features which uh leave in the time domain, frequency domain features which leave in the frequency domain and finally time frequency domain features which are features that provide us with information both about time and frequency. Now we want to take a look at the uh extraction pipelines that we can use to extract time and frequency domain features. So let's start with time domain features first. OK. So for the extraction pipeline, obviously, the first step is just to look at an analog sound, right? So it could be the sound of a violin, a noise. And then once we get",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=26s",
        "start_time": "26.92"
    },
    {
        "id": "d2693c57",
        "text": "in the previous video. We took a look at a few different types of uh audio features. So time domain features which uh leave in the time domain, frequency domain features which leave in the frequency domain and finally time frequency domain features which are features that provide us with information both about time and frequency. Now we want to take a look at the uh extraction pipelines that we can use to extract time and frequency domain features. So let's start with time domain features first. OK. So for the extraction pipeline, obviously, the first step is just to look at an analog sound, right? So it could be the sound of a violin, a noise. And then once we get that sound, we want to convert that into something that makes sense and we can somehow like edit and manipulate with our computers. And for doing that, we need to go through the analog digital conversion process. In other words, we need to sample and quantize the analog sound sound so that we get a digital signal. OK?",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=44s",
        "start_time": "44.75"
    },
    {
        "id": "fd440ad0",
        "text": "Now we want to take a look at the uh extraction pipelines that we can use to extract time and frequency domain features. So let's start with time domain features first. OK. So for the extraction pipeline, obviously, the first step is just to look at an analog sound, right? So it could be the sound of a violin, a noise. And then once we get that sound, we want to convert that into something that makes sense and we can somehow like edit and manipulate with our computers. And for doing that, we need to go through the analog digital conversion process. In other words, we need to sample and quantize the analog sound sound so that we get a digital signal. OK? So if you are not familiar with this process, I suggest you check out my video, previous video in this series that covers this like quite in detail, you should have it over here.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=66s",
        "start_time": "66.93"
    },
    {
        "id": "eddd3b8d",
        "text": "that sound, we want to convert that into something that makes sense and we can somehow like edit and manipulate with our computers. And for doing that, we need to go through the analog digital conversion process. In other words, we need to sample and quantize the analog sound sound so that we get a digital signal. OK? So if you are not familiar with this process, I suggest you check out my video, previous video in this series that covers this like quite in detail, you should have it over here. OK? So now once you have the uh digitalized version of the sounds, the next step that we want to do here is called framing. In other words, we want to bundle together a bunch of samples. And so here you see that for example, frame one goes from sample number 1 to 100 28 frame two goes from sample 64 to 100 92 frame three from sample 100 20",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=91s",
        "start_time": "91.985"
    },
    {
        "id": "81f8a172",
        "text": "So if you are not familiar with this process, I suggest you check out my video, previous video in this series that covers this like quite in detail, you should have it over here. OK? So now once you have the uh digitalized version of the sounds, the next step that we want to do here is called framing. In other words, we want to bundle together a bunch of samples. And so here you see that for example, frame one goes from sample number 1 to 100 28 frame two goes from sample 64 to 100 92 frame three from sample 100 20 to 256 and so on and so forth. Now, um there's like an interesting thing here which is that these frames are somehow overlapped, right? But uh I'm not gonna reveal you the secret right now. I'm not gonna explain why that's the case. Now. You'll know by the end of this video. So stay tuned for that. OK. So now let's understand a little bit",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=117s",
        "start_time": "117.23"
    },
    {
        "id": "2e60655c",
        "text": "OK? So now once you have the uh digitalized version of the sounds, the next step that we want to do here is called framing. In other words, we want to bundle together a bunch of samples. And so here you see that for example, frame one goes from sample number 1 to 100 28 frame two goes from sample 64 to 100 92 frame three from sample 100 20 to 256 and so on and so forth. Now, um there's like an interesting thing here which is that these frames are somehow overlapped, right? But uh I'm not gonna reveal you the secret right now. I'm not gonna explain why that's the case. Now. You'll know by the end of this video. So stay tuned for that. OK. So now let's understand a little bit better why we use a framing before we extract uh features, acoustic features. Well, so we can think of frames as audio chunks that are perceivable things that we can perceive. Now, the problem is that if we take a look at a sample, one single sample at a sampling rate of 44.1 kilohertz, which is the sampling rate for the CD ROM.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=128s",
        "start_time": "128.089"
    },
    {
        "id": "89dc04d0",
        "text": "to 256 and so on and so forth. Now, um there's like an interesting thing here which is that these frames are somehow overlapped, right? But uh I'm not gonna reveal you the secret right now. I'm not gonna explain why that's the case. Now. You'll know by the end of this video. So stay tuned for that. OK. So now let's understand a little bit better why we use a framing before we extract uh features, acoustic features. Well, so we can think of frames as audio chunks that are perceivable things that we can perceive. Now, the problem is that if we take a look at a sample, one single sample at a sampling rate of 44.1 kilohertz, which is the sampling rate for the CD ROM. Uh We find out that that sample has a duration of NN point N 227 milliseconds, right? This is like very, very short amount of time. Now, if you're wondering how I got this number, well, that is just like the inverse of the sampling rate. So it's one divided by, in this case, 44,100 right? Uh So",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=156s",
        "start_time": "156.565"
    },
    {
        "id": "44a8f7dc",
        "text": "better why we use a framing before we extract uh features, acoustic features. Well, so we can think of frames as audio chunks that are perceivable things that we can perceive. Now, the problem is that if we take a look at a sample, one single sample at a sampling rate of 44.1 kilohertz, which is the sampling rate for the CD ROM. Uh We find out that that sample has a duration of NN point N 227 milliseconds, right? This is like very, very short amount of time. Now, if you're wondering how I got this number, well, that is just like the inverse of the sampling rate. So it's one divided by, in this case, 44,100 right? Uh So now we know that like one sample, it's like very, very short. And we know that the duration of this single sample is way below the threshold of the time resolution for our hearing, human hearing, which is around 10 milliseconds, which basically means that all the things that are below 10 milliseconds, we can just like we cannot appreciate that",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=185s",
        "start_time": "185.042"
    },
    {
        "id": "39061a4e",
        "text": "Uh We find out that that sample has a duration of NN point N 227 milliseconds, right? This is like very, very short amount of time. Now, if you're wondering how I got this number, well, that is just like the inverse of the sampling rate. So it's one divided by, in this case, 44,100 right? Uh So now we know that like one sample, it's like very, very short. And we know that the duration of this single sample is way below the threshold of the time resolution for our hearing, human hearing, which is around 10 milliseconds, which basically means that all the things that are below 10 milliseconds, we can just like we cannot appreciate that as acoustic events. So with frames, what we want to do is to have enough time of enough duration of an audio signal so that we can appreciate that. And that makes sense from an acoustic perspective because it is like what we can hear and the features that we want to extract are somehow related to what we can experience as human beings. OK. So",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=213s",
        "start_time": "213.74"
    },
    {
        "id": "e9939798",
        "text": "now we know that like one sample, it's like very, very short. And we know that the duration of this single sample is way below the threshold of the time resolution for our hearing, human hearing, which is around 10 milliseconds, which basically means that all the things that are below 10 milliseconds, we can just like we cannot appreciate that as acoustic events. So with frames, what we want to do is to have enough time of enough duration of an audio signal so that we can appreciate that. And that makes sense from an acoustic perspective because it is like what we can hear and the features that we want to extract are somehow related to what we can experience as human beings. OK. So now another thing about frames is usually that uh we, they have a frame size or in other words, they have a number of frames which is usually a power of two.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=240s",
        "start_time": "240.339"
    },
    {
        "id": "779b9ee2",
        "text": "as acoustic events. So with frames, what we want to do is to have enough time of enough duration of an audio signal so that we can appreciate that. And that makes sense from an acoustic perspective because it is like what we can hear and the features that we want to extract are somehow related to what we can experience as human beings. OK. So now another thing about frames is usually that uh we, they have a frame size or in other words, they have a number of frames which is usually a power of two. Now, why is that the case? This sounds really weird, right. Well, that's because uh usually what we want to do when we move into like the frequency domain is just like apply the fourier transform. And it turns out that when we apply the fast fourier transform, which is a variant of the fourier transform having um",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=265s",
        "start_time": "265.315"
    },
    {
        "id": "9207289b",
        "text": "now another thing about frames is usually that uh we, they have a frame size or in other words, they have a number of frames which is usually a power of two. Now, why is that the case? This sounds really weird, right. Well, that's because uh usually what we want to do when we move into like the frequency domain is just like apply the fourier transform. And it turns out that when we apply the fast fourier transform, which is a variant of the fourier transform having um uh a number of samples that that's a power of two is gonna speed up the process a lot, right? So it's just like a matter of like speed here. OK. So now typical values for the frames oscillates between 256 to 8000, 100 92. Now let's take a look at the duration of a frame",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=292s",
        "start_time": "292.54"
    },
    {
        "id": "0ac06bff",
        "text": "Now, why is that the case? This sounds really weird, right. Well, that's because uh usually what we want to do when we move into like the frequency domain is just like apply the fourier transform. And it turns out that when we apply the fast fourier transform, which is a variant of the fourier transform having um uh a number of samples that that's a power of two is gonna speed up the process a lot, right? So it's just like a matter of like speed here. OK. So now typical values for the frames oscillates between 256 to 8000, 100 92. Now let's take a look at the duration of a frame and we have this formula down here. So we have the duration of a frame that's equal to the inverse. So one of the sampling rate, so one divided by the sampling rate and we have to multiply this by capital k which is the frame size. And in other words, this first",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=305s",
        "start_time": "305.41"
    },
    {
        "id": "c112eb21",
        "text": "uh a number of samples that that's a power of two is gonna speed up the process a lot, right? So it's just like a matter of like speed here. OK. So now typical values for the frames oscillates between 256 to 8000, 100 92. Now let's take a look at the duration of a frame and we have this formula down here. So we have the duration of a frame that's equal to the inverse. So one of the sampling rate, so one divided by the sampling rate and we have to multiply this by capital k which is the frame size. And in other words, this first element here tells us the duration of a single sample. And then we multiply that by the total number of samples that we have in a frame.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=328s",
        "start_time": "328.429"
    },
    {
        "id": "9dfc86de",
        "text": "and we have this formula down here. So we have the duration of a frame that's equal to the inverse. So one of the sampling rate, so one divided by the sampling rate and we have to multiply this by capital k which is the frame size. And in other words, this first element here tells us the duration of a single sample. And then we multiply that by the total number of samples that we have in a frame. This way we get the whole duration of a frame. OK. So now let's plug some usual numbers. So we can plug 44.1 kg Hertz for the sampling rate, which is like a totally normal",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=353s",
        "start_time": "353.69"
    },
    {
        "id": "9ba0f5a6",
        "text": "element here tells us the duration of a single sample. And then we multiply that by the total number of samples that we have in a frame. This way we get the whole duration of a frame. OK. So now let's plug some usual numbers. So we can plug 44.1 kg Hertz for the sampling rate, which is like a totally normal uh sampling rate. And then we plug in 412 which is a totally normal um frame size at this sampling rate. And we get the duration of a frame to be around 11.6 milliseconds, which is just above the time resolution um time, the human hearing time resolution, which is like around 10 milliseconds. Uh Let's go back",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=374s",
        "start_time": "374.6"
    },
    {
        "id": "ff4fb5e6",
        "text": "This way we get the whole duration of a frame. OK. So now let's plug some usual numbers. So we can plug 44.1 kg Hertz for the sampling rate, which is like a totally normal uh sampling rate. And then we plug in 412 which is a totally normal um frame size at this sampling rate. And we get the duration of a frame to be around 11.6 milliseconds, which is just above the time resolution um time, the human hearing time resolution, which is like around 10 milliseconds. Uh Let's go back uh to the um fissure extraction pipeline for the time domain. OK. So the, the last uh process that we saw here was framing. Now, once we've applied framing, the next step is just to compute the features uh the time domain features on each of the different frames. But now what we want to do after that is to aggregate those results so that we get like a single",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=386s",
        "start_time": "386.209"
    },
    {
        "id": "34cd0d3e",
        "text": "uh sampling rate. And then we plug in 412 which is a totally normal um frame size at this sampling rate. And we get the duration of a frame to be around 11.6 milliseconds, which is just above the time resolution um time, the human hearing time resolution, which is like around 10 milliseconds. Uh Let's go back uh to the um fissure extraction pipeline for the time domain. OK. So the, the last uh process that we saw here was framing. Now, once we've applied framing, the next step is just to compute the features uh the time domain features on each of the different frames. But now what we want to do after that is to aggregate those results so that we get like a single uh kind of like feature vector whatever for the whole uh sound. So it's a scripture for the whole duration of a sound. And we can aggregate that by using statistical means like the mean, the median, the sum or things that are a little bit more sophisticated like Gaussian mixture models, for example GM MS here.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=400s",
        "start_time": "400.519"
    },
    {
        "id": "9bcbcdb7",
        "text": "uh to the um fissure extraction pipeline for the time domain. OK. So the, the last uh process that we saw here was framing. Now, once we've applied framing, the next step is just to compute the features uh the time domain features on each of the different frames. But now what we want to do after that is to aggregate those results so that we get like a single uh kind of like feature vector whatever for the whole uh sound. So it's a scripture for the whole duration of a sound. And we can aggregate that by using statistical means like the mean, the median, the sum or things that are a little bit more sophisticated like Gaussian mixture models, for example GM MS here. And so, out of this uh process, we get a final value, it could be like a value. So a feature value, it could be a vector, a feature vector or even a matrix, a feature matrix. And this basically is a snapshot for the",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=429s",
        "start_time": "429.334"
    },
    {
        "id": "52c89764",
        "text": "uh kind of like feature vector whatever for the whole uh sound. So it's a scripture for the whole duration of a sound. And we can aggregate that by using statistical means like the mean, the median, the sum or things that are a little bit more sophisticated like Gaussian mixture models, for example GM MS here. And so, out of this uh process, we get a final value, it could be like a value. So a feature value, it could be a vector, a feature vector or even a matrix, a feature matrix. And this basically is a snapshot for the whole duration of the audio signal that we are analyzing. OK. So this is the um feature. So the extraction pipeline for the uh for time domain features now let's move on to the frequency domain feature.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=458s",
        "start_time": "458.369"
    },
    {
        "id": "04958161",
        "text": "And so, out of this uh process, we get a final value, it could be like a value. So a feature value, it could be a vector, a feature vector or even a matrix, a feature matrix. And this basically is a snapshot for the whole duration of the audio signal that we are analyzing. OK. So this is the um feature. So the extraction pipeline for the uh for time domain features now let's move on to the frequency domain feature. As you'll see, many of the steps are basically the same as this that we found in the time domain uh uh feature extraction pipeline. So obviously, we start from an analog sound, we do some A DC. So we apply sampling, we apply quantization, we get our digitalized version of the of the of the sound. So we have this audio signal, we frame the signal",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=485s",
        "start_time": "485.17"
    },
    {
        "id": "0731f357",
        "text": "whole duration of the audio signal that we are analyzing. OK. So this is the um feature. So the extraction pipeline for the uh for time domain features now let's move on to the frequency domain feature. As you'll see, many of the steps are basically the same as this that we found in the time domain uh uh feature extraction pipeline. So obviously, we start from an analog sound, we do some A DC. So we apply sampling, we apply quantization, we get our digitalized version of the of the of the sound. So we have this audio signal, we frame the signal and now we have a bunch of frames. Now what's next? Well, what we want to do here is basically move from the time domain to the frequency domain. And we usually do that with a ma magic tool which is the fourier transform for now. Yeah, I'm just like using the this like magic wand because for us,",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=501s",
        "start_time": "501.635"
    },
    {
        "id": "f7e9bd00",
        "text": "As you'll see, many of the steps are basically the same as this that we found in the time domain uh uh feature extraction pipeline. So obviously, we start from an analog sound, we do some A DC. So we apply sampling, we apply quantization, we get our digitalized version of the of the of the sound. So we have this audio signal, we frame the signal and now we have a bunch of frames. Now what's next? Well, what we want to do here is basically move from the time domain to the frequency domain. And we usually do that with a ma magic tool which is the fourier transform for now. Yeah, I'm just like using the this like magic wand because for us, it's just like a black box for now. So it's some kind of like magic that happens and we move from the time representation to the frequency representation, but uh we'll definitely cover like the fourier transform more in detail in coming videos. So stay tuned for that.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=518s",
        "start_time": "518.359"
    },
    {
        "id": "7edb121d",
        "text": "and now we have a bunch of frames. Now what's next? Well, what we want to do here is basically move from the time domain to the frequency domain. And we usually do that with a ma magic tool which is the fourier transform for now. Yeah, I'm just like using the this like magic wand because for us, it's just like a black box for now. So it's some kind of like magic that happens and we move from the time representation to the frequency representation, but uh we'll definitely cover like the fourier transform more in detail in coming videos. So stay tuned for that. OK. So uh just like uh a little refresher on time domain and frequency domain. So the ti in time domain, we have the sound that's uh basically like visualized as a uh the amplitude as a function of time. And so we see all the events across time for a uh for, for the sound. Whereas when we move to the frequency domain",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=545s",
        "start_time": "545.979"
    },
    {
        "id": "d84ad672",
        "text": "it's just like a black box for now. So it's some kind of like magic that happens and we move from the time representation to the frequency representation, but uh we'll definitely cover like the fourier transform more in detail in coming videos. So stay tuned for that. OK. So uh just like uh a little refresher on time domain and frequency domain. So the ti in time domain, we have the sound that's uh basically like visualized as a uh the amplitude as a function of time. And so we see all the events across time for a uh for, for the sound. Whereas when we move to the frequency domain in here, we just look at all the frequency components of a sound and we see how much they contribute to the overall sound. And indeed on the X axis, we have the frequency uh domain, uh the, the frequency or Hertz. And on the Y axis, we have the magnitude which tells us how much uh each of the different frequency bands contribute to the overall sound good. OK.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=570s",
        "start_time": "570.5"
    },
    {
        "id": "fbaaa8cd",
        "text": "OK. So uh just like uh a little refresher on time domain and frequency domain. So the ti in time domain, we have the sound that's uh basically like visualized as a uh the amplitude as a function of time. And so we see all the events across time for a uh for, for the sound. Whereas when we move to the frequency domain in here, we just look at all the frequency components of a sound and we see how much they contribute to the overall sound. And indeed on the X axis, we have the frequency uh domain, uh the, the frequency or Hertz. And on the Y axis, we have the magnitude which tells us how much uh each of the different frequency bands contribute to the overall sound good. OK. Uh Now, so we would expect that's like the next natural step in the um extraction pipeline would be to apply the fourier transform. And that would be like just like natural. OK. So that we can move from 10 domain to frequency domain. But unfortunately, there's a major issue which is called spectral leakage.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=591s",
        "start_time": "591.64"
    },
    {
        "id": "7de00613",
        "text": "in here, we just look at all the frequency components of a sound and we see how much they contribute to the overall sound. And indeed on the X axis, we have the frequency uh domain, uh the, the frequency or Hertz. And on the Y axis, we have the magnitude which tells us how much uh each of the different frequency bands contribute to the overall sound good. OK. Uh Now, so we would expect that's like the next natural step in the um extraction pipeline would be to apply the fourier transform. And that would be like just like natural. OK. So that we can move from 10 domain to frequency domain. But unfortunately, there's a major issue which is called spectral leakage. OK. So let's see what spectral leakage is and whether we can solve it. OK. So spectral leakage happens when we are processing a signal. So we, when we are taking the fourier transform of a signal that",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=617s",
        "start_time": "617.674"
    },
    {
        "id": "cb0cd57a",
        "text": "Uh Now, so we would expect that's like the next natural step in the um extraction pipeline would be to apply the fourier transform. And that would be like just like natural. OK. So that we can move from 10 domain to frequency domain. But unfortunately, there's a major issue which is called spectral leakage. OK. So let's see what spectral leakage is and whether we can solve it. OK. So spectral leakage happens when we are processing a signal. So we, when we are taking the fourier transform of a signal that isn't an integer number of periods. And this basically happens all the time because like you have uh a certain amount of time worth of audio and it's rarely the case that that amount of time has an integer number of periods.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=645s",
        "start_time": "645.909"
    },
    {
        "id": "2623dfb1",
        "text": "OK. So let's see what spectral leakage is and whether we can solve it. OK. So spectral leakage happens when we are processing a signal. So we, when we are taking the fourier transform of a signal that isn't an integer number of periods. And this basically happens all the time because like you have uh a certain amount of time worth of audio and it's rarely the case that that amount of time has an integer number of periods. So what usually happens is that the end points of a signal are usually like discontinuous. In other words, these guys here are discontinuous because they are not an integer number of uh periods. And so",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=668s",
        "start_time": "668.909"
    },
    {
        "id": "3cc4e1d7",
        "text": "isn't an integer number of periods. And this basically happens all the time because like you have uh a certain amount of time worth of audio and it's rarely the case that that amount of time has an integer number of periods. So what usually happens is that the end points of a signal are usually like discontinuous. In other words, these guys here are discontinuous because they are not an integer number of uh periods. And so what happens here is that if we have these discontinuities, this discontinuities get translated into the uh spectrum or like the frequency domain as high frequency components. But this high frequency components really don't exist in the original signal. They are just some kind of artifact",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=684s",
        "start_time": "684.489"
    },
    {
        "id": "e8a53450",
        "text": "So what usually happens is that the end points of a signal are usually like discontinuous. In other words, these guys here are discontinuous because they are not an integer number of uh periods. And so what happens here is that if we have these discontinuities, this discontinuities get translated into the uh spectrum or like the frequency domain as high frequency components. But this high frequency components really don't exist in the original signal. They are just some kind of artifact that appears because of the discontinuities at the end points of the signal that we are processing with the fourier transform.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=701s",
        "start_time": "701.799"
    },
    {
        "id": "f4293141",
        "text": "what happens here is that if we have these discontinuities, this discontinuities get translated into the uh spectrum or like the frequency domain as high frequency components. But this high frequency components really don't exist in the original signal. They are just some kind of artifact that appears because of the discontinuities at the end points of the signal that we are processing with the fourier transform. So in other words, what happens is that some of these discontinuities frequencies at the discontinuities are just like leak into other higher frequencies, hence the name spectral leakage. So now let's try to visualize this. So we are here in the time uh representation, we apply the fourier transform",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=721s",
        "start_time": "721.96"
    },
    {
        "id": "2fdff735",
        "text": "that appears because of the discontinuities at the end points of the signal that we are processing with the fourier transform. So in other words, what happens is that some of these discontinuities frequencies at the discontinuities are just like leak into other higher frequencies, hence the name spectral leakage. So now let's try to visualize this. So we are here in the time uh representation, we apply the fourier transform and we get our spectrum or the frequency domain. And um and here you it may, you we may have like uh higher frequencies which have like higher high contributions, right? And it's like here like in this uh red box in the uh in the spectrum in this slide.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=747s",
        "start_time": "747.039"
    },
    {
        "id": "6c0dd25e",
        "text": "So in other words, what happens is that some of these discontinuities frequencies at the discontinuities are just like leak into other higher frequencies, hence the name spectral leakage. So now let's try to visualize this. So we are here in the time uh representation, we apply the fourier transform and we get our spectrum or the frequency domain. And um and here you it may, you we may have like uh higher frequencies which have like higher high contributions, right? And it's like here like in this uh red box in the uh in the spectrum in this slide. Uh Now this is an example of spectral leakage. So these components, frequency components don't really exist. They are just an artifact that comes from the discontinuities that we have in the original uh signal that we've analyzed. So when you have spectral leakage, usually you would get like some higher frequencies which have like some substantial contribution to the sound and which indeed",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=755s",
        "start_time": "755.289"
    },
    {
        "id": "4c012d5a",
        "text": "and we get our spectrum or the frequency domain. And um and here you it may, you we may have like uh higher frequencies which have like higher high contributions, right? And it's like here like in this uh red box in the uh in the spectrum in this slide. Uh Now this is an example of spectral leakage. So these components, frequency components don't really exist. They are just an artifact that comes from the discontinuities that we have in the original uh signal that we've analyzed. So when you have spectral leakage, usually you would get like some higher frequencies which have like some substantial contribution to the sound and which indeed shouldn't be the case because these are artifacts. OK. Now, is there a way we can",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=782s",
        "start_time": "782.2"
    },
    {
        "id": "df806d70",
        "text": "Uh Now this is an example of spectral leakage. So these components, frequency components don't really exist. They are just an artifact that comes from the discontinuities that we have in the original uh signal that we've analyzed. So when you have spectral leakage, usually you would get like some higher frequencies which have like some substantial contribution to the sound and which indeed shouldn't be the case because these are artifacts. OK. Now, is there a way we can resolve this issue? We can minimize spectral leakage? Well, luckily for us, there is one and it's called windowing, right. OK. So let's take a look at windowing. So uh the idea behind windowing is that we apply a windowing function to each frame before we feed the frames into the fourier transform.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=805s",
        "start_time": "805.659"
    },
    {
        "id": "4ea18467",
        "text": "shouldn't be the case because these are artifacts. OK. Now, is there a way we can resolve this issue? We can minimize spectral leakage? Well, luckily for us, there is one and it's called windowing, right. OK. So let's take a look at windowing. So uh the idea behind windowing is that we apply a windowing function to each frame before we feed the frames into the fourier transform. And by doing so we basically eliminate the samples which are at the end points of a frame. In other words, we completely remove the information from the end points. And what that does is it generates a periodic signal which minimizes spectral leakage. And we are happy for that.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=835s",
        "start_time": "835.21"
    },
    {
        "id": "16f50bb6",
        "text": "resolve this issue? We can minimize spectral leakage? Well, luckily for us, there is one and it's called windowing, right. OK. So let's take a look at windowing. So uh the idea behind windowing is that we apply a windowing function to each frame before we feed the frames into the fourier transform. And by doing so we basically eliminate the samples which are at the end points of a frame. In other words, we completely remove the information from the end points. And what that does is it generates a periodic signal which minimizes spectral leakage. And we are happy for that. OK. So a famous windowing function that you'll use probably 90 95% of the time when you do a fourier transforms is called the hand window. Here you have the uh this function here. And so K here it just represents the sample. So this",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=843s",
        "start_time": "843.159"
    },
    {
        "id": "4f200942",
        "text": "And by doing so we basically eliminate the samples which are at the end points of a frame. In other words, we completely remove the information from the end points. And what that does is it generates a periodic signal which minimizes spectral leakage. And we are happy for that. OK. So a famous windowing function that you'll use probably 90 95% of the time when you do a fourier transforms is called the hand window. Here you have the uh this function here. And so K here it just represents the sample. So this function is a function like of the the hand window is a function of the uh samples. And as you can see here, we use like a cosine",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=868s",
        "start_time": "868.4"
    },
    {
        "id": "7ecfd98c",
        "text": "OK. So a famous windowing function that you'll use probably 90 95% of the time when you do a fourier transforms is called the hand window. Here you have the uh this function here. And so K here it just represents the sample. So this function is a function like of the the hand window is a function of the uh samples. And as you can see here, we use like a cosine and then we have visualization of uh the hand window for 50 samples. And as you can see here, it creates like this kind of like bell shaped curve where the end points tend to be uh go to zero, right? OK. So now let's try to apply that to a signal. So we start with uh a signal. And so this is just like one frame",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=893s",
        "start_time": "893.69"
    },
    {
        "id": "843399a3",
        "text": "function is a function like of the the hand window is a function of the uh samples. And as you can see here, we use like a cosine and then we have visualization of uh the hand window for 50 samples. And as you can see here, it creates like this kind of like bell shaped curve where the end points tend to be uh go to zero, right? OK. So now let's try to apply that to a signal. So we start with uh a signal. And so this is just like one frame uh containing uh 256 samples. Here, we have the",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=914s",
        "start_time": "914.929"
    },
    {
        "id": "c3247f6b",
        "text": "and then we have visualization of uh the hand window for 50 samples. And as you can see here, it creates like this kind of like bell shaped curve where the end points tend to be uh go to zero, right? OK. So now let's try to apply that to a signal. So we start with uh a signal. And so this is just like one frame uh containing uh 256 samples. Here, we have the uh the hand window and now we can apply the hand window to the original signal so that we get a new signal that has been windowed, right? And this is the math that we run for obtaining the, for like applying the hand window to the original um uh signal. In other words, what we do is we multiply",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=925s",
        "start_time": "925.369"
    },
    {
        "id": "3d6d7e6e",
        "text": "uh containing uh 256 samples. Here, we have the uh the hand window and now we can apply the hand window to the original signal so that we get a new signal that has been windowed, right? And this is the math that we run for obtaining the, for like applying the hand window to the original um uh signal. In other words, what we do is we multiply the original signal by the hand window at each correspondent sample, right?",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=953s",
        "start_time": "953.859"
    },
    {
        "id": "9d0a6e6f",
        "text": "uh the hand window and now we can apply the hand window to the original signal so that we get a new signal that has been windowed, right? And this is the math that we run for obtaining the, for like applying the hand window to the original um uh signal. In other words, what we do is we multiply the original signal by the hand window at each correspondent sample, right? And what we obtain is something that looks like this. And as you can see here, uh because we've applied the hand window, now we've completely smoothened the end points, right? And so we don't have those discontinuities anymore now because the, the signal just goes naturally to zero thanks to the windowing that we've used. OK.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=960s",
        "start_time": "960.909"
    },
    {
        "id": "52833a6b",
        "text": "the original signal by the hand window at each correspondent sample, right? And what we obtain is something that looks like this. And as you can see here, uh because we've applied the hand window, now we've completely smoothened the end points, right? And so we don't have those discontinuities anymore now because the, the signal just goes naturally to zero thanks to the windowing that we've used. OK. But now we have another problem,",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=988s",
        "start_time": "988.539"
    },
    {
        "id": "6561ae5a",
        "text": "And what we obtain is something that looks like this. And as you can see here, uh because we've applied the hand window, now we've completely smoothened the end points, right? And so we don't have those discontinuities anymore now because the, the signal just goes naturally to zero thanks to the windowing that we've used. OK. But now we have another problem, we have another big problem. So now imagine we take",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=997s",
        "start_time": "997.82"
    },
    {
        "id": "77f033d9",
        "text": "But now we have another problem, we have another big problem. So now imagine we take a, a number of frames and we put them together, uh we just like glue them together. So here we have like three different frames that I've glued together. Now, as you can see, obviously, what happens here is that at the end points of these frames. So at the beginning and at the end we",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1026s",
        "start_time": "1026.56"
    },
    {
        "id": "362af87f",
        "text": "we have another big problem. So now imagine we take a, a number of frames and we put them together, uh we just like glue them together. So here we have like three different frames that I've glued together. Now, as you can see, obviously, what happens here is that at the end points of these frames. So at the beginning and at the end we lose signal, right? And why do we lose signal? Well, because we just uh removed it through the application of a uh windowing function. OK. And this is something that we don't want to do. We don't want to lose signal in uh when we do a fourier transform. So how do we solve that? It seems like an impossible conundrum.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1029s",
        "start_time": "1029.818"
    },
    {
        "id": "fa75be3b",
        "text": "a, a number of frames and we put them together, uh we just like glue them together. So here we have like three different frames that I've glued together. Now, as you can see, obviously, what happens here is that at the end points of these frames. So at the beginning and at the end we lose signal, right? And why do we lose signal? Well, because we just uh removed it through the application of a uh windowing function. OK. And this is something that we don't want to do. We don't want to lose signal in uh when we do a fourier transform. So how do we solve that? It seems like an impossible conundrum. Now, the solution is overlapping frames and we'll see that in a second so that we can solve the initial mystery of overlapping frames that I mentioned at the beginning of this video. But before we get there there, let's just like take a look at normal non overlapping frames. So what we would usually do like with non overlapping frames is that we just like",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1035s",
        "start_time": "1035.26"
    },
    {
        "id": "54b625b7",
        "text": "lose signal, right? And why do we lose signal? Well, because we just uh removed it through the application of a uh windowing function. OK. And this is something that we don't want to do. We don't want to lose signal in uh when we do a fourier transform. So how do we solve that? It seems like an impossible conundrum. Now, the solution is overlapping frames and we'll see that in a second so that we can solve the initial mystery of overlapping frames that I mentioned at the beginning of this video. But before we get there there, let's just like take a look at normal non overlapping frames. So what we would usually do like with non overlapping frames is that we just like choose a frame size and we apply that uh to a signal. And so this is like, for example, this vertical red bar represents like the first frame for this signal, then we move on to the second, the 3rd, 4th, 5th and 6th frames. And as you can see, we don't have any overlaps here.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1054s",
        "start_time": "1054.479"
    },
    {
        "id": "27e092bf",
        "text": "Now, the solution is overlapping frames and we'll see that in a second so that we can solve the initial mystery of overlapping frames that I mentioned at the beginning of this video. But before we get there there, let's just like take a look at normal non overlapping frames. So what we would usually do like with non overlapping frames is that we just like choose a frame size and we apply that uh to a signal. And so this is like, for example, this vertical red bar represents like the first frame for this signal, then we move on to the second, the 3rd, 4th, 5th and 6th frames. And as you can see, we don't have any overlaps here. But if we were to do this, after windowing, we would lose signal. So how do we solve that with overlapping frames? And so let's visualize overlapping frames. So we start with the first um",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1079s",
        "start_time": "1079.569"
    },
    {
        "id": "4273f7db",
        "text": "choose a frame size and we apply that uh to a signal. And so this is like, for example, this vertical red bar represents like the first frame for this signal, then we move on to the second, the 3rd, 4th, 5th and 6th frames. And as you can see, we don't have any overlaps here. But if we were to do this, after windowing, we would lose signal. So how do we solve that with overlapping frames? And so let's visualize overlapping frames. So we start with the first um frame here and then we have the second frame and as you can see there's a part there that overlaps. And yeah, this is like the way we overlap frames. And by doing so what it means is that we kind of like uh account",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1104s",
        "start_time": "1104.43"
    },
    {
        "id": "81ca54fb",
        "text": "But if we were to do this, after windowing, we would lose signal. So how do we solve that with overlapping frames? And so let's visualize overlapping frames. So we start with the first um frame here and then we have the second frame and as you can see there's a part there that overlaps. And yeah, this is like the way we overlap frames. And by doing so what it means is that we kind of like uh account for the information that we lose at the end points because we are overlapping frames. OK? And so we can continue to and have a third frame over here, 1/4 and 1/5 basically, you get the idea. So we just like overlap these frames every time. OK. So now let's take a look at a couple of important concepts. So we already so like the frame size, which is",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1127s",
        "start_time": "1127.26"
    },
    {
        "id": "a2d10589",
        "text": "frame here and then we have the second frame and as you can see there's a part there that overlaps. And yeah, this is like the way we overlap frames. And by doing so what it means is that we kind of like uh account for the information that we lose at the end points because we are overlapping frames. OK? And so we can continue to and have a third frame over here, 1/4 and 1/5 basically, you get the idea. So we just like overlap these frames every time. OK. So now let's take a look at a couple of important concepts. So we already so like the frame size, which is get this basically and it's like the number of frames that we consider. Um uh sorry, the number of samples that we consider for each frame. And then we have another very important concept which is called the hop length. Sometimes you'll hear uh this concept referred to as hop size.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1142s",
        "start_time": "1142.01"
    },
    {
        "id": "74d51770",
        "text": "for the information that we lose at the end points because we are overlapping frames. OK? And so we can continue to and have a third frame over here, 1/4 and 1/5 basically, you get the idea. So we just like overlap these frames every time. OK. So now let's take a look at a couple of important concepts. So we already so like the frame size, which is get this basically and it's like the number of frames that we consider. Um uh sorry, the number of samples that we consider for each frame. And then we have another very important concept which is called the hop length. Sometimes you'll hear uh this concept referred to as hop size. So the hop length is basically tells us the amount of samples that we shift to the right every time we take a new sample. OK. Now let's move on because now we we, we are at a point in our feature extraction pipeline for frequency domain uh features where we can apply the fourier transform.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1160s",
        "start_time": "1160.89"
    },
    {
        "id": "4a878d9f",
        "text": "get this basically and it's like the number of frames that we consider. Um uh sorry, the number of samples that we consider for each frame. And then we have another very important concept which is called the hop length. Sometimes you'll hear uh this concept referred to as hop size. So the hop length is basically tells us the amount of samples that we shift to the right every time we take a new sample. OK. Now let's move on because now we we, we are at a point in our feature extraction pipeline for frequency domain uh features where we can apply the fourier transform. So indeed, after windowing, we apply the fourier transform and we get a spectrum hopefully with minimized spectral leakage. And then after that, we just go back to like the same steps that we used for the time domain feature extraction pipe pipeline.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1186s",
        "start_time": "1186.849"
    },
    {
        "id": "c9c84d7c",
        "text": "So the hop length is basically tells us the amount of samples that we shift to the right every time we take a new sample. OK. Now let's move on because now we we, we are at a point in our feature extraction pipeline for frequency domain uh features where we can apply the fourier transform. So indeed, after windowing, we apply the fourier transform and we get a spectrum hopefully with minimized spectral leakage. And then after that, we just go back to like the same steps that we used for the time domain feature extraction pipe pipeline. So we compute frequency domain features on each frame, then we aggregate uh these results uh for the whole the entirety of the audio signal using some statistical means. And finally, we would get AAA frequency feature uh frequency domain feature value vector or matrix. OK,",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1207s",
        "start_time": "1207.709"
    },
    {
        "id": "78d6512c",
        "text": "So indeed, after windowing, we apply the fourier transform and we get a spectrum hopefully with minimized spectral leakage. And then after that, we just go back to like the same steps that we used for the time domain feature extraction pipe pipeline. So we compute frequency domain features on each frame, then we aggregate uh these results uh for the whole the entirety of the audio signal using some statistical means. And finally, we would get AAA frequency feature uh frequency domain feature value vector or matrix. OK, good. So now you have a clear idea of the pipelines that we should use when we are dealing with time domain features and frequency domain features so that we can extract them.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1236s",
        "start_time": "1236.05"
    },
    {
        "id": "7e416e10",
        "text": "So we compute frequency domain features on each frame, then we aggregate uh these results uh for the whole the entirety of the audio signal using some statistical means. And finally, we would get AAA frequency feature uh frequency domain feature value vector or matrix. OK, good. So now you have a clear idea of the pipelines that we should use when we are dealing with time domain features and frequency domain features so that we can extract them. So what's next? Well, it's time to start digging into time domain features and next time we'll start looking into time Domain features, a bunch of these and understand what they are and how we can use them for different applications in machine learning. So stay tuned for that. So",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1256s",
        "start_time": "1256.689"
    },
    {
        "id": "7877e06b",
        "text": "good. So now you have a clear idea of the pipelines that we should use when we are dealing with time domain features and frequency domain features so that we can extract them. So what's next? Well, it's time to start digging into time domain features and next time we'll start looking into time Domain features, a bunch of these and understand what they are and how we can use them for different applications in machine learning. So stay tuned for that. So that's it for today. I hope you've enjoyed this video. If that's the case, please remember to uh leave a like if you have any questions as usual, leave them in the comments section below. I'll try to answer this and I guess that's all for today and I'll see you next time. Cheers.",
        "video": "How to Extract Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "8A-W1xk7qs8",
        "youtube_link": "https://www.youtube.com/watch?v=8A-W1xk7qs8&t=1284s",
        "start_time": "1284.329"
    },
    {
        "id": "ee16b312",
        "text": "Hi, everybody and welcome to a new video in the audio processing for machine learning series. This time, we are basically starting the whole series and it's going to be quite theoretical because I want to introduce a few basic ideas. So what's sound? What are waveforms? So what's frequency and how can we use all of those uh notions in audio processing? OK. So let's get started with the basic idea here. So what what is sound well uh sound is produced by uh vibrating objects. So these objects vibrate and these vibrations cause our air molecules to oscillate and to bump into each other and by bumping into each other, these uh air molecules kind of like change the uh state of the air pressure in in like the local region where they are acting. And so they create in this process a wave. So in other words, we can think of sound as a wave that transmits transfer some energy from one point to another through air molecules. OK. But the question is what is a mechanical wave? And I'm talking about mechanical waves here because the sound is",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "94eee13f",
        "text": "well uh sound is produced by uh vibrating objects. So these objects vibrate and these vibrations cause our air molecules to oscillate and to bump into each other and by bumping into each other, these uh air molecules kind of like change the uh state of the air pressure in in like the local region where they are acting. And so they create in this process a wave. So in other words, we can think of sound as a wave that transmits transfer some energy from one point to another through air molecules. OK. But the question is what is a mechanical wave? And I'm talking about mechanical waves here because the sound is is a mechanical wave. So a mechanical wave is a wave that oscillates and that an oscillation that travels through space and the energy travels from, as I said, one point to another. And the particularity of mechanical waves is that they need a medium through with",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=27s",
        "start_time": "27.52"
    },
    {
        "id": "7592d3f8",
        "text": "And so they create in this process a wave. So in other words, we can think of sound as a wave that transmits transfer some energy from one point to another through air molecules. OK. But the question is what is a mechanical wave? And I'm talking about mechanical waves here because the sound is is a mechanical wave. So a mechanical wave is a wave that oscillates and that an oscillation that travels through space and the energy travels from, as I said, one point to another. And the particularity of mechanical waves is that they need a medium through with which the wave can expand and propagate through. In the case of sound, most of the time this medium is just air, right? And when we have this sound wave or a mechanical wave, the medium gets the forms. And in the case of sound, what happens is like as I mentioned before is that like this um",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=55s",
        "start_time": "55.299"
    },
    {
        "id": "223ab57b",
        "text": "is a mechanical wave. So a mechanical wave is a wave that oscillates and that an oscillation that travels through space and the energy travels from, as I said, one point to another. And the particularity of mechanical waves is that they need a medium through with which the wave can expand and propagate through. In the case of sound, most of the time this medium is just air, right? And when we have this sound wave or a mechanical wave, the medium gets the forms. And in the case of sound, what happens is like as I mentioned before is that like this um uh molecules tend to uh bump into each other. And when that happens, we have like higher points of uh pressure, right? And then when they just like kind of like move away from each other, we have like points of more like rare function of less air pressure. And obviously, we can",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=78s",
        "start_time": "78.629"
    },
    {
        "id": "2a9b3fd7",
        "text": "which the wave can expand and propagate through. In the case of sound, most of the time this medium is just air, right? And when we have this sound wave or a mechanical wave, the medium gets the forms. And in the case of sound, what happens is like as I mentioned before is that like this um uh molecules tend to uh bump into each other. And when that happens, we have like higher points of uh pressure, right? And then when they just like kind of like move away from each other, we have like points of more like rare function of less air pressure. And obviously, we can uh represents visualize all of this by using a pressure plot. OK. So, but obviously, this is not the type of pressure that I want to talk about here is like air pressure, right? OK. So",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=101s",
        "start_time": "101.959"
    },
    {
        "id": "9e87d898",
        "text": "uh molecules tend to uh bump into each other. And when that happens, we have like higher points of uh pressure, right? And then when they just like kind of like move away from each other, we have like points of more like rare function of less air pressure. And obviously, we can uh represents visualize all of this by using a pressure plot. OK. So, but obviously, this is not the type of pressure that I want to talk about here is like air pressure, right? OK. So basically, the idea here is uh like we can visualize a sound wave uh as a in this case, like as a simple like sine wave, right? And so we have at the center here, the average atmospheric pressure and then we have over time points of compression which are connected with this like",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=125s",
        "start_time": "125.569"
    },
    {
        "id": "81ab12ca",
        "text": "uh represents visualize all of this by using a pressure plot. OK. So, but obviously, this is not the type of pressure that I want to talk about here is like air pressure, right? OK. So basically, the idea here is uh like we can visualize a sound wave uh as a in this case, like as a simple like sine wave, right? And so we have at the center here, the average atmospheric pressure and then we have over time points of compression which are connected with this like denser uh points where like air molecules collide with each other. And then we have like these points of rare refraction where just the air molecules are more spaced out, right? So this is like the whole idea of a sound wave that just kind of like travels through space using air as a medium.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=150s",
        "start_time": "150.96"
    },
    {
        "id": "857f01db",
        "text": "basically, the idea here is uh like we can visualize a sound wave uh as a in this case, like as a simple like sine wave, right? And so we have at the center here, the average atmospheric pressure and then we have over time points of compression which are connected with this like denser uh points where like air molecules collide with each other. And then we have like these points of rare refraction where just the air molecules are more spaced out, right? So this is like the whole idea of a sound wave that just kind of like travels through space using air as a medium. OK. So now we can represent a complex uh sound using a waveform which once again is basically like a pressure plot. So we are we are plotting",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=165s",
        "start_time": "165.55"
    },
    {
        "id": "410a0888",
        "text": "denser uh points where like air molecules collide with each other. And then we have like these points of rare refraction where just the air molecules are more spaced out, right? So this is like the whole idea of a sound wave that just kind of like travels through space using air as a medium. OK. So now we can represent a complex uh sound using a waveform which once again is basically like a pressure plot. So we are we are plotting the deviation of air from this uh air pressure from this zero level against time. And so which a wave form like this that I'm sure like you'll be familiar with uh with, we can just like represent a whole piece of music, some noise, whatever we want really. And it's a nice way of visualizing and having",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=187s",
        "start_time": "187.345"
    },
    {
        "id": "a8e89b64",
        "text": "OK. So now we can represent a complex uh sound using a waveform which once again is basically like a pressure plot. So we are we are plotting the deviation of air from this uh air pressure from this zero level against time. And so which a wave form like this that I'm sure like you'll be familiar with uh with, we can just like represent a whole piece of music, some noise, whatever we want really. And it's a nice way of visualizing and having a quick understanding of visualization of what uh like a sound like looks like. OK. And as we'll see, the waveform is going to be like fundamental for doing certain transformations which are gonna be very important to get important features about audio signals. OK. So now",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=209s",
        "start_time": "209.399"
    },
    {
        "id": "68f23dad",
        "text": "the deviation of air from this uh air pressure from this zero level against time. And so which a wave form like this that I'm sure like you'll be familiar with uh with, we can just like represent a whole piece of music, some noise, whatever we want really. And it's a nice way of visualizing and having a quick understanding of visualization of what uh like a sound like looks like. OK. And as we'll see, the waveform is going to be like fundamental for doing certain transformations which are gonna be very important to get important features about audio signals. OK. So now a wave form is great because it provides us with an array of different information.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=224s",
        "start_time": "224.229"
    },
    {
        "id": "9b48ea86",
        "text": "a quick understanding of visualization of what uh like a sound like looks like. OK. And as we'll see, the waveform is going to be like fundamental for doing certain transformations which are gonna be very important to get important features about audio signals. OK. So now a wave form is great because it provides us with an array of different information. It's not just about uh like the frequency information but also about intensity timer other types of temporal information duration. So for example, if we have like a complex uh waveform for like a piece of music, we can identify onsets and the duration of the nets and all of these kind of cues and it's kind of like a mesmerizing that all of that comes from like a very simple uh graph to",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=244s",
        "start_time": "244.63"
    },
    {
        "id": "4b17db0a",
        "text": "a wave form is great because it provides us with an array of different information. It's not just about uh like the frequency information but also about intensity timer other types of temporal information duration. So for example, if we have like a complex uh waveform for like a piece of music, we can identify onsets and the duration of the nets and all of these kind of cues and it's kind of like a mesmerizing that all of that comes from like a very simple uh graph to the graph. Cool. OK. So now we can divide sound into a couple of like classes of categories. So we have periodic and a periodic sound. So in the case of periodic sound, this is like a sound where the compressions and rare fractions repeat regularly, right? So you can fix a period at which you have peaks or you have dips.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=266s",
        "start_time": "266.69"
    },
    {
        "id": "283eb4d8",
        "text": "It's not just about uh like the frequency information but also about intensity timer other types of temporal information duration. So for example, if we have like a complex uh waveform for like a piece of music, we can identify onsets and the duration of the nets and all of these kind of cues and it's kind of like a mesmerizing that all of that comes from like a very simple uh graph to the graph. Cool. OK. So now we can divide sound into a couple of like classes of categories. So we have periodic and a periodic sound. So in the case of periodic sound, this is like a sound where the compressions and rare fractions repeat regularly, right? So you can fix a period at which you have peaks or you have dips. Now, the simplest form of periodic sound is a single sine wave. And so we know like the the math behind sound sound waves uh like really, really well. So that is like very convenient for us. Now, a more complex type of sound that is like the sound, for example of an orchestra playing",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=273s",
        "start_time": "273.54"
    },
    {
        "id": "b5194565",
        "text": "the graph. Cool. OK. So now we can divide sound into a couple of like classes of categories. So we have periodic and a periodic sound. So in the case of periodic sound, this is like a sound where the compressions and rare fractions repeat regularly, right? So you can fix a period at which you have peaks or you have dips. Now, the simplest form of periodic sound is a single sine wave. And so we know like the the math behind sound sound waves uh like really, really well. So that is like very convenient for us. Now, a more complex type of sound that is like the sound, for example of an orchestra playing uh is the so called complex sound that as we'll see in a few um videos is the result of multiple sine waves that gets uh kind of like combined as superimposed together.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=302s",
        "start_time": "302.785"
    },
    {
        "id": "28bba5e1",
        "text": "Now, the simplest form of periodic sound is a single sine wave. And so we know like the the math behind sound sound waves uh like really, really well. So that is like very convenient for us. Now, a more complex type of sound that is like the sound, for example of an orchestra playing uh is the so called complex sound that as we'll see in a few um videos is the result of multiple sine waves that gets uh kind of like combined as superimposed together. Now, so this is the type of like periodic sounds. Then we have a periodic sound obviously like the the name here is a clear hint. So uh with a periodic sound, we don't have like periodicity in the audio signal.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=332s",
        "start_time": "332.57"
    },
    {
        "id": "edb1fa47",
        "text": "uh is the so called complex sound that as we'll see in a few um videos is the result of multiple sine waves that gets uh kind of like combined as superimposed together. Now, so this is the type of like periodic sounds. Then we have a periodic sound obviously like the the name here is a clear hint. So uh with a periodic sound, we don't have like periodicity in the audio signal. And we can differentiate between two types of a periodic sounds so continuous and transient. So continuous, a periodic sound is just noise. Basically, you have like a jumble of points as a wave from which don't follow any pattern whatsoever in the air pressure, it's just like some random points uh sampled through like the uh on the um air pressure uh axis. OK.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=350s",
        "start_time": "350.16"
    },
    {
        "id": "17734d14",
        "text": "Now, so this is the type of like periodic sounds. Then we have a periodic sound obviously like the the name here is a clear hint. So uh with a periodic sound, we don't have like periodicity in the audio signal. And we can differentiate between two types of a periodic sounds so continuous and transient. So continuous, a periodic sound is just noise. Basically, you have like a jumble of points as a wave from which don't follow any pattern whatsoever in the air pressure, it's just like some random points uh sampled through like the uh on the um air pressure uh axis. OK. So the transient, a periodic sound is a little bit different. So all of those like popping sounds or like clicks and things like that can be uh folded like a transient. So these are just like bursts of um energy and which change like the air pressure. Uh suddenly it's kind of like a pulse thing. And then again, there you don't have any uh type of periodicity whatsoever.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=365s",
        "start_time": "365.369"
    },
    {
        "id": "556d808e",
        "text": "And we can differentiate between two types of a periodic sounds so continuous and transient. So continuous, a periodic sound is just noise. Basically, you have like a jumble of points as a wave from which don't follow any pattern whatsoever in the air pressure, it's just like some random points uh sampled through like the uh on the um air pressure uh axis. OK. So the transient, a periodic sound is a little bit different. So all of those like popping sounds or like clicks and things like that can be uh folded like a transient. So these are just like bursts of um energy and which change like the air pressure. Uh suddenly it's kind of like a pulse thing. And then again, there you don't have any uh type of periodicity whatsoever. OK. So now let's uh try to start with the simplest things first, which is like I having like a simple",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=382s",
        "start_time": "382.1"
    },
    {
        "id": "05e0edfe",
        "text": "So the transient, a periodic sound is a little bit different. So all of those like popping sounds or like clicks and things like that can be uh folded like a transient. So these are just like bursts of um energy and which change like the air pressure. Uh suddenly it's kind of like a pulse thing. And then again, there you don't have any uh type of periodicity whatsoever. OK. So now let's uh try to start with the simplest things first, which is like I having like a simple sine wave. OK. So here we have the waveform for like a simple sine wave. And here also down here we have the math for that. And as you can see, we can easily um create the equation of the sine wave sine wave by uh using a bunch of like different parameters. So this a capital A here is the amplitude and then we have F which is the frequency T is just uh time",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=407s",
        "start_time": "407.48"
    },
    {
        "id": "026f2005",
        "text": "OK. So now let's uh try to start with the simplest things first, which is like I having like a simple sine wave. OK. So here we have the waveform for like a simple sine wave. And here also down here we have the math for that. And as you can see, we can easily um create the equation of the sine wave sine wave by uh using a bunch of like different parameters. So this a capital A here is the amplitude and then we have F which is the frequency T is just uh time and then P is the phase. Now let's try to take a look at each of these parameters in isolation. So that you get an idea of like how they influence the waveform itself. OK. So frequency is connected with the",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=434s",
        "start_time": "434.82"
    },
    {
        "id": "7ec8eb95",
        "text": "sine wave. OK. So here we have the waveform for like a simple sine wave. And here also down here we have the math for that. And as you can see, we can easily um create the equation of the sine wave sine wave by uh using a bunch of like different parameters. So this a capital A here is the amplitude and then we have F which is the frequency T is just uh time and then P is the phase. Now let's try to take a look at each of these parameters in isolation. So that you get an idea of like how they influence the waveform itself. OK. So frequency is connected with the period. So to uh the period is it is very simple to understand as a concept, right? It's just like the amount of time that we need to elapse before having to uh picks or for example, to um dips, right.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=444s",
        "start_time": "444.049"
    },
    {
        "id": "cce72f21",
        "text": "and then P is the phase. Now let's try to take a look at each of these parameters in isolation. So that you get an idea of like how they influence the waveform itself. OK. So frequency is connected with the period. So to uh the period is it is very simple to understand as a concept, right? It's just like the amount of time that we need to elapse before having to uh picks or for example, to um dips, right. OK. So this is the period. Now, the frequency is just the inverse of the period where we've um I indicated period with capital T here. And frequency is expressed in Hertz which is number of cycles per second.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=472s",
        "start_time": "472.959"
    },
    {
        "id": "627044ba",
        "text": "period. So to uh the period is it is very simple to understand as a concept, right? It's just like the amount of time that we need to elapse before having to uh picks or for example, to um dips, right. OK. So this is the period. Now, the frequency is just the inverse of the period where we've um I indicated period with capital T here. And frequency is expressed in Hertz which is number of cycles per second. OK. So now the amplitude ST uh is a yeah, quite uh simple intuitive uh concept as well to understand. And it's basically how high or low this perturbation in air pressure goes, the higher it goes",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=489s",
        "start_time": "489.45"
    },
    {
        "id": "299a21e6",
        "text": "OK. So this is the period. Now, the frequency is just the inverse of the period where we've um I indicated period with capital T here. And frequency is expressed in Hertz which is number of cycles per second. OK. So now the amplitude ST uh is a yeah, quite uh simple intuitive uh concept as well to understand. And it's basically how high or low this perturbation in air pressure goes, the higher it goes and the higher the amplitude obviously, right. So this is like uh we, we can take like this uh information just by starting from zero and then looking at the difference, for example, between the peak the the value of the amplitude at the peak and the uh and at zero, right? And so there we have the amplitude,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=508s",
        "start_time": "508.73"
    },
    {
        "id": "fcc6ed5d",
        "text": "OK. So now the amplitude ST uh is a yeah, quite uh simple intuitive uh concept as well to understand. And it's basically how high or low this perturbation in air pressure goes, the higher it goes and the higher the amplitude obviously, right. So this is like uh we, we can take like this uh information just by starting from zero and then looking at the difference, for example, between the peak the the value of the amplitude at the peak and the uh and at zero, right? And so there we have the amplitude, OK. Then the final parameter that we were kind of like uh pondering was a phase which is indicated with the P lecture from the Greek alphabet. OK. So phase, what phase tells us is basically like",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=528s",
        "start_time": "528.71"
    },
    {
        "id": "16b1a05c",
        "text": "and the higher the amplitude obviously, right. So this is like uh we, we can take like this uh information just by starting from zero and then looking at the difference, for example, between the peak the the value of the amplitude at the peak and the uh and at zero, right? And so there we have the amplitude, OK. Then the final parameter that we were kind of like uh pondering was a phase which is indicated with the P lecture from the Greek alphabet. OK. So phase, what phase tells us is basically like um it, it, it enables us to shift the waveform to the right or to the left and face basically tells us what is the position of the, the waveform at time zero, right.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=548s",
        "start_time": "548.08"
    },
    {
        "id": "dad1780d",
        "text": "OK. Then the final parameter that we were kind of like uh pondering was a phase which is indicated with the P lecture from the Greek alphabet. OK. So phase, what phase tells us is basically like um it, it, it enables us to shift the waveform to the right or to the left and face basically tells us what is the position of the, the waveform at time zero, right. OK. So with amplitude with frequency and with phase, we are able of determining uh all the parameters and have like a complete understanding of a sine wave. And as we'll see in future videos, this is extremely important because so complex sounds can just be f as a combination or super imposition of many sine waves together. OK.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=568s",
        "start_time": "568.46"
    },
    {
        "id": "11ef6e63",
        "text": "um it, it, it enables us to shift the waveform to the right or to the left and face basically tells us what is the position of the, the waveform at time zero, right. OK. So with amplitude with frequency and with phase, we are able of determining uh all the parameters and have like a complete understanding of a sine wave. And as we'll see in future videos, this is extremely important because so complex sounds can just be f as a combination or super imposition of many sine waves together. OK. Cool. OK. Now let's take a look at frequency and amplitude a little bit more. And so as you can see here, uh we have like here with this red graph like two waves. And so the, the one above here has like this frequency or like, well that actually like is the period, right?",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=587s",
        "start_time": "587.01"
    },
    {
        "id": "5639c719",
        "text": "OK. So with amplitude with frequency and with phase, we are able of determining uh all the parameters and have like a complete understanding of a sine wave. And as we'll see in future videos, this is extremely important because so complex sounds can just be f as a combination or super imposition of many sine waves together. OK. Cool. OK. Now let's take a look at frequency and amplitude a little bit more. And so as you can see here, uh we have like here with this red graph like two waves. And so the, the one above here has like this frequency or like, well that actually like is the period, right? And this one down here as this period here, which is uh like shorter. So the frequency being the inverse is higher. So what's the relationship between frequency and sound? Well, the higher the frequency, the higher the sound that we perceive right",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=603s",
        "start_time": "603.859"
    },
    {
        "id": "b183bd15",
        "text": "Cool. OK. Now let's take a look at frequency and amplitude a little bit more. And so as you can see here, uh we have like here with this red graph like two waves. And so the, the one above here has like this frequency or like, well that actually like is the period, right? And this one down here as this period here, which is uh like shorter. So the frequency being the inverse is higher. So what's the relationship between frequency and sound? Well, the higher the frequency, the higher the sound that we perceive right now, a similar thing can happen with amplitude. So we, we see here with this purple graph uh in the top right where the amplitude is quite low. And then down here uh like um bottom right, we have a sine wave with a higher amplitude and that perceptually translates to have a louder sound.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=633s",
        "start_time": "633.919"
    },
    {
        "id": "05325e20",
        "text": "And this one down here as this period here, which is uh like shorter. So the frequency being the inverse is higher. So what's the relationship between frequency and sound? Well, the higher the frequency, the higher the sound that we perceive right now, a similar thing can happen with amplitude. So we, we see here with this purple graph uh in the top right where the amplitude is quite low. And then down here uh like um bottom right, we have a sine wave with a higher amplitude and that perceptually translates to have a louder sound. So larger amplitude are connected with louder sounds. And this like makes sense uh intuitively because the amplitude just measures the amount of perturbation that uh happens in the air pressure, right? And the higher the perturbation, the higher the energy that we transfer and the the louder that will sound.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=654s",
        "start_time": "654.169"
    },
    {
        "id": "5a2b49c9",
        "text": "now, a similar thing can happen with amplitude. So we, we see here with this purple graph uh in the top right where the amplitude is quite low. And then down here uh like um bottom right, we have a sine wave with a higher amplitude and that perceptually translates to have a louder sound. So larger amplitude are connected with louder sounds. And this like makes sense uh intuitively because the amplitude just measures the amount of perturbation that uh happens in the air pressure, right? And the higher the perturbation, the higher the energy that we transfer and the the louder that will sound. OK. So now here we have like a basic uh understanding of how frequency and amplitude uh map onto like perceptual aspects. OK.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=671s",
        "start_time": "671.599"
    },
    {
        "id": "99aec0ed",
        "text": "So larger amplitude are connected with louder sounds. And this like makes sense uh intuitively because the amplitude just measures the amount of perturbation that uh happens in the air pressure, right? And the higher the perturbation, the higher the energy that we transfer and the the louder that will sound. OK. So now here we have like a basic uh understanding of how frequency and amplitude uh map onto like perceptual aspects. OK. So now an interesting thing that I want to cover here is the hearing range. And as we'll see, this is extremely important for decisions in audio processing like sample R and, and a bunch of other things. Now, different animals usually have like very different hearing ranges. So uh humans,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=696s",
        "start_time": "696.575"
    },
    {
        "id": "d0eaf35e",
        "text": "OK. So now here we have like a basic uh understanding of how frequency and amplitude uh map onto like perceptual aspects. OK. So now an interesting thing that I want to cover here is the hearing range. And as we'll see, this is extremely important for decisions in audio processing like sample R and, and a bunch of other things. Now, different animals usually have like very different hearing ranges. So uh humans, for example, have a hearing range which is between 20 Hertz and 20,000 Hertz. But if we go and we take a look at the hearing range for cats and dogs, we see that they are capable of hearing also higher frequencies, the frequencies that are",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=721s",
        "start_time": "721.9"
    },
    {
        "id": "338e3f64",
        "text": "So now an interesting thing that I want to cover here is the hearing range. And as we'll see, this is extremely important for decisions in audio processing like sample R and, and a bunch of other things. Now, different animals usually have like very different hearing ranges. So uh humans, for example, have a hearing range which is between 20 Hertz and 20,000 Hertz. But if we go and we take a look at the hearing range for cats and dogs, we see that they are capable of hearing also higher frequencies, the frequencies that are we humans call ultrasounds just because like we, I mean, guess like we everything is trapo percent. And so what we can hear and goes beyond like what the the highest rhythms we can hear is called ultrasounds and basically like the both cats and dogs and definitely bats can hear ultrasounds, right? So now let's take a look at a few um",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=733s",
        "start_time": "733.039"
    },
    {
        "id": "c262ead4",
        "text": "for example, have a hearing range which is between 20 Hertz and 20,000 Hertz. But if we go and we take a look at the hearing range for cats and dogs, we see that they are capable of hearing also higher frequencies, the frequencies that are we humans call ultrasounds just because like we, I mean, guess like we everything is trapo percent. And so what we can hear and goes beyond like what the the highest rhythms we can hear is called ultrasounds and basically like the both cats and dogs and definitely bats can hear ultrasounds, right? So now let's take a look at a few um examples of like sounds and where they are mapped into the hearing range. So uh I if we take a look at the concert orchestra, for example, in the middle here, so you'll see that the concert orchestra more or less like covers the whole uh uh spectrum of hearing range of human hearing range. And like, like, I mean, that's like makes sense. And so here you have, for example, like the male singing voice,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=753s",
        "start_time": "753.403"
    },
    {
        "id": "e5e7200e",
        "text": "we humans call ultrasounds just because like we, I mean, guess like we everything is trapo percent. And so what we can hear and goes beyond like what the the highest rhythms we can hear is called ultrasounds and basically like the both cats and dogs and definitely bats can hear ultrasounds, right? So now let's take a look at a few um examples of like sounds and where they are mapped into the hearing range. So uh I if we take a look at the concert orchestra, for example, in the middle here, so you'll see that the concert orchestra more or less like covers the whole uh uh spectrum of hearing range of human hearing range. And like, like, I mean, that's like makes sense. And so here you have, for example, like the male singing voice, which is between somewhere I'd say like 60 to 2000, even like 5000. And then the female speaking voice, which is usually like an octave above the male speaking voice as a rule of thumb. And so it starts around like 200 Hertz and it goes all the way up to 10,000 Hertz. OK. So you're like, you get an idea of different types of sounds and how they mapped onto uh Hertz and our hearing range. OK. So now",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=773s",
        "start_time": "773.765"
    },
    {
        "id": "ab02aaba",
        "text": "examples of like sounds and where they are mapped into the hearing range. So uh I if we take a look at the concert orchestra, for example, in the middle here, so you'll see that the concert orchestra more or less like covers the whole uh uh spectrum of hearing range of human hearing range. And like, like, I mean, that's like makes sense. And so here you have, for example, like the male singing voice, which is between somewhere I'd say like 60 to 2000, even like 5000. And then the female speaking voice, which is usually like an octave above the male speaking voice as a rule of thumb. And so it starts around like 200 Hertz and it goes all the way up to 10,000 Hertz. OK. So you're like, you get an idea of different types of sounds and how they mapped onto uh Hertz and our hearing range. OK. So now uh what we've talked about until up until now is a kind of objective measure of sound, which is frequency. Now, the way we hear frequency and we perceive frequency, it's kind of like very subjective and it's not really like that objective, like a tool. And now pit is the,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=799s",
        "start_time": "799.07"
    },
    {
        "id": "932e5cdf",
        "text": "which is between somewhere I'd say like 60 to 2000, even like 5000. And then the female speaking voice, which is usually like an octave above the male speaking voice as a rule of thumb. And so it starts around like 200 Hertz and it goes all the way up to 10,000 Hertz. OK. So you're like, you get an idea of different types of sounds and how they mapped onto uh Hertz and our hearing range. OK. So now uh what we've talked about until up until now is a kind of objective measure of sound, which is frequency. Now, the way we hear frequency and we perceive frequency, it's kind of like very subjective and it's not really like that objective, like a tool. And now pit is the, the concept that we use uh for the perception of frequency, right? OK. So the, the great thing and the interesting thing I should say like about pitch is that uh we don't hear pitch in a kind we don't hear, I should say frequency in a linear way, but rather in a logarithmic way. So um one other like interesting thing here is that two frequencies",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=826s",
        "start_time": "826.895"
    },
    {
        "id": "3cfa39d2",
        "text": "uh what we've talked about until up until now is a kind of objective measure of sound, which is frequency. Now, the way we hear frequency and we perceive frequency, it's kind of like very subjective and it's not really like that objective, like a tool. And now pit is the, the concept that we use uh for the perception of frequency, right? OK. So the, the great thing and the interesting thing I should say like about pitch is that uh we don't hear pitch in a kind we don't hear, I should say frequency in a linear way, but rather in a logarithmic way. So um one other like interesting thing here is that two frequencies are perceived to be more or less like the same if they differ by a power of two.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=857s",
        "start_time": "857.03"
    },
    {
        "id": "b20bb868",
        "text": "the concept that we use uh for the perception of frequency, right? OK. So the, the great thing and the interesting thing I should say like about pitch is that uh we don't hear pitch in a kind we don't hear, I should say frequency in a linear way, but rather in a logarithmic way. So um one other like interesting thing here is that two frequencies are perceived to be more or less like the same if they differ by a power of two. And so this is like the concept of octave that's coming into place and we'll cover this. But for now, I want you to understand that the way we perceive uh pitch or the way we perceive frequency through page. It's kind of like very different and different from like frequency itself because we have this kind of like logarithmic perception of frequency. OK. So to understand the concept of pitch, we need to understand the concept of me. Now,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=879s",
        "start_time": "879.679"
    },
    {
        "id": "d9066431",
        "text": "are perceived to be more or less like the same if they differ by a power of two. And so this is like the concept of octave that's coming into place and we'll cover this. But for now, I want you to understand that the way we perceive uh pitch or the way we perceive frequency through page. It's kind of like very different and different from like frequency itself because we have this kind of like logarithmic perception of frequency. OK. So to understand the concept of pitch, we need to understand the concept of me. Now, uh media notes um are kind of like very common and very handy uh convention for just like uh transferring information about like musical notes and stuff like that. But what we want to uh understand here is that how they map",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=908s",
        "start_time": "908.979"
    },
    {
        "id": "ca189893",
        "text": "And so this is like the concept of octave that's coming into place and we'll cover this. But for now, I want you to understand that the way we perceive uh pitch or the way we perceive frequency through page. It's kind of like very different and different from like frequency itself because we have this kind of like logarithmic perception of frequency. OK. So to understand the concept of pitch, we need to understand the concept of me. Now, uh media notes um are kind of like very common and very handy uh convention for just like uh transferring information about like musical notes and stuff like that. But what we want to uh understand here is that how they map uh to onto like pitch and a keyboard. So now here we have like a piano keyboard, right? And the idea of midi notes is that we can attach to each keyboard, a number, a a midi number, right? So for example, the middle C down here uh has a midi note uh which is equal to 60.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=916s",
        "start_time": "916.789"
    },
    {
        "id": "7e93f2ca",
        "text": "uh media notes um are kind of like very common and very handy uh convention for just like uh transferring information about like musical notes and stuff like that. But what we want to uh understand here is that how they map uh to onto like pitch and a keyboard. So now here we have like a piano keyboard, right? And the idea of midi notes is that we can attach to each keyboard, a number, a a midi number, right? So for example, the middle C down here uh has a midi note uh which is equal to 60. Now, uh we can map this midi notes to note names. So this midi note 60 is equal to C four. Now, what does that stand for? So we have like a four and note name like two parameters there. So one is a letter and yeah, that's just like the, the the note names, right? CD eff sharp, all these things, right? And the other one is a number. So what's that number? Well, that",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=945s",
        "start_time": "945.599"
    },
    {
        "id": "a9f4cd29",
        "text": "uh to onto like pitch and a keyboard. So now here we have like a piano keyboard, right? And the idea of midi notes is that we can attach to each keyboard, a number, a a midi number, right? So for example, the middle C down here uh has a midi note uh which is equal to 60. Now, uh we can map this midi notes to note names. So this midi note 60 is equal to C four. Now, what does that stand for? So we have like a four and note name like two parameters there. So one is a letter and yeah, that's just like the, the the note names, right? CD eff sharp, all these things, right? And the other one is a number. So what's that number? Well, that number represents the octave we are at. So now, I guess like most of you are familiar with the concept of like a scale like or an octave and you've like perhaps played around like with a, with a keyboard, like a piano yourself. But if you're not, the basic idea is that we have",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=964s",
        "start_time": "964.15"
    },
    {
        "id": "9a82093b",
        "text": "Now, uh we can map this midi notes to note names. So this midi note 60 is equal to C four. Now, what does that stand for? So we have like a four and note name like two parameters there. So one is a letter and yeah, that's just like the, the the note names, right? CD eff sharp, all these things, right? And the other one is a number. So what's that number? Well, that number represents the octave we are at. So now, I guess like most of you are familiar with the concept of like a scale like or an octave and you've like perhaps played around like with a, with a keyboard, like a piano yourself. But if you're not, the basic idea is that we have a pattern of notes that always repeats itself, so we start like with C we go to C# D and Efgab and then we can go up and save and here we have like the very same pattern, the very same 12 notes",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=986s",
        "start_time": "986.919"
    },
    {
        "id": "53b5276e",
        "text": "number represents the octave we are at. So now, I guess like most of you are familiar with the concept of like a scale like or an octave and you've like perhaps played around like with a, with a keyboard, like a piano yourself. But if you're not, the basic idea is that we have a pattern of notes that always repeats itself, so we start like with C we go to C# D and Efgab and then we can go up and save and here we have like the very same pattern, the very same 12 notes which we call uh sentences. So there are like 12 sentences in the whole octave. OK. So now what's the difference between this, this, this and this? Well, basically like the, the notes uh that get like the same uh name. So C will sound basically the same but they'll sound somehow like higher. So it's difficult to explain but basically like the tone itself is the same, but it will be perceived as higher. OK.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1013s",
        "start_time": "1013.83"
    },
    {
        "id": "4690df94",
        "text": "a pattern of notes that always repeats itself, so we start like with C we go to C# D and Efgab and then we can go up and save and here we have like the very same pattern, the very same 12 notes which we call uh sentences. So there are like 12 sentences in the whole octave. OK. So now what's the difference between this, this, this and this? Well, basically like the, the notes uh that get like the same uh name. So C will sound basically the same but they'll sound somehow like higher. So it's difficult to explain but basically like the tone itself is the same, but it will be perceived as higher. OK. So, and basically like with the number here in the note name, like the C four, the four just expresses the octave we are at and obviously the octave like is this interval that we have like between uh for example, like C four and C five? OK. Cool. So how does that map onto the idea of frequency?",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1031s",
        "start_time": "1031.78"
    },
    {
        "id": "6c8091da",
        "text": "which we call uh sentences. So there are like 12 sentences in the whole octave. OK. So now what's the difference between this, this, this and this? Well, basically like the, the notes uh that get like the same uh name. So C will sound basically the same but they'll sound somehow like higher. So it's difficult to explain but basically like the tone itself is the same, but it will be perceived as higher. OK. So, and basically like with the number here in the note name, like the C four, the four just expresses the octave we are at and obviously the octave like is this interval that we have like between uh for example, like C four and C five? OK. Cool. So how does that map onto the idea of frequency? So let's take a look at that. So we start with the, with a simple note, a fundamental note which is a four or pitch or midi note uh 69 and this is set at 440 hertz. Now, the idea is that if we go up a knot, so to a five will",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1052s",
        "start_time": "1052.569"
    },
    {
        "id": "8ac453e1",
        "text": "So, and basically like with the number here in the note name, like the C four, the four just expresses the octave we are at and obviously the octave like is this interval that we have like between uh for example, like C four and C five? OK. Cool. So how does that map onto the idea of frequency? So let's take a look at that. So we start with the, with a simple note, a fundamental note which is a four or pitch or midi note uh 69 and this is set at 440 hertz. Now, the idea is that if we go up a knot, so to a five will have the frequency that's double that it's 880 Hertz. So the, the basic uh takeaway point here is that when you have like notes that are like a, an octave above, they'll have a frequency that's double the frequency of the same note at the octave below, right? OK. So",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1081s",
        "start_time": "1081.16"
    },
    {
        "id": "d8029b9d",
        "text": "So let's take a look at that. So we start with the, with a simple note, a fundamental note which is a four or pitch or midi note uh 69 and this is set at 440 hertz. Now, the idea is that if we go up a knot, so to a five will have the frequency that's double that it's 880 Hertz. So the, the basic uh takeaway point here is that when you have like notes that are like a, an octave above, they'll have a frequency that's double the frequency of the same note at the octave below, right? OK. So now if we take all of these ideas and we plot them on a pitch frequency chart, you'll see that. Now the um connection like the, the mapping between frequency and pitch is a logarithmic one. And you can see like here the nice logarithm",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1103s",
        "start_time": "1103.436"
    },
    {
        "id": "af680a2e",
        "text": "have the frequency that's double that it's 880 Hertz. So the, the basic uh takeaway point here is that when you have like notes that are like a, an octave above, they'll have a frequency that's double the frequency of the same note at the octave below, right? OK. So now if we take all of these ideas and we plot them on a pitch frequency chart, you'll see that. Now the um connection like the, the mapping between frequency and pitch is a logarithmic one. And you can see like here the nice logarithm uh function here and here uh on the y axis on the pitch axis, we just have like a zero, a one, a two, a three, a four, so on and so forth. And you'll see that basically like at every octave we go above, we would just like double the uh frequency in Hertz down here. OK.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1125s",
        "start_time": "1125.712"
    },
    {
        "id": "8c929596",
        "text": "now if we take all of these ideas and we plot them on a pitch frequency chart, you'll see that. Now the um connection like the, the mapping between frequency and pitch is a logarithmic one. And you can see like here the nice logarithm uh function here and here uh on the y axis on the pitch axis, we just have like a zero, a one, a two, a three, a four, so on and so forth. And you'll see that basically like at every octave we go above, we would just like double the uh frequency in Hertz down here. OK. So this is like a very interesting aspect and it seems that in sound like the way we perceive sound across the board, not just like for frequency, but also for example, for um",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1149s",
        "start_time": "1149.68"
    },
    {
        "id": "affdec0c",
        "text": "uh function here and here uh on the y axis on the pitch axis, we just have like a zero, a one, a two, a three, a four, so on and so forth. And you'll see that basically like at every octave we go above, we would just like double the uh frequency in Hertz down here. OK. So this is like a very interesting aspect and it seems that in sound like the way we perceive sound across the board, not just like for frequency, but also for example, for um uh amplitude for like intensity is a logarithmic. And not only that, but if you're talking about music, even like the, the very concept of time and rhythm is somehow logarithm, a logarithmic and based on powers of two, OK. But yeah, I guess we'll look into depth in the coming views. OK. So how do we map pit onto frequency? Here? We can use a very simple equation.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1171s",
        "start_time": "1171.54"
    },
    {
        "id": "9c1376ef",
        "text": "So this is like a very interesting aspect and it seems that in sound like the way we perceive sound across the board, not just like for frequency, but also for example, for um uh amplitude for like intensity is a logarithmic. And not only that, but if you're talking about music, even like the, the very concept of time and rhythm is somehow logarithm, a logarithmic and based on powers of two, OK. But yeah, I guess we'll look into depth in the coming views. OK. So how do we map pit onto frequency? Here? We can use a very simple equation. So you have like this frequency F as a function of P which is the peach and we will pass the peach in as a media note. And here you have this nice function. So you have two to the power of P minus 69 divided by 12. And all of this guy is multiplied by 440 0. And this 440 0 you can recognize that's just the frequency of pit",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1193s",
        "start_time": "1193.459"
    },
    {
        "id": "3a850576",
        "text": "uh amplitude for like intensity is a logarithmic. And not only that, but if you're talking about music, even like the, the very concept of time and rhythm is somehow logarithm, a logarithmic and based on powers of two, OK. But yeah, I guess we'll look into depth in the coming views. OK. So how do we map pit onto frequency? Here? We can use a very simple equation. So you have like this frequency F as a function of P which is the peach and we will pass the peach in as a media note. And here you have this nice function. So you have two to the power of P minus 69 divided by 12. And all of this guy is multiplied by 440 0. And this 440 0 you can recognize that's just the frequency of pit uh 69 at maybe not 69. Because like if you plug into this P here, 69 this the whole thing here will become one. And if you multiply that by 440 basically you just get 440 which is the frequency for uh pitch 69 or a four. OK? So now let's try to",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1206s",
        "start_time": "1206.949"
    },
    {
        "id": "60ff4180",
        "text": "So you have like this frequency F as a function of P which is the peach and we will pass the peach in as a media note. And here you have this nice function. So you have two to the power of P minus 69 divided by 12. And all of this guy is multiplied by 440 0. And this 440 0 you can recognize that's just the frequency of pit uh 69 at maybe not 69. Because like if you plug into this P here, 69 this the whole thing here will become one. And if you multiply that by 440 basically you just get 440 which is the frequency for uh pitch 69 or a four. OK? So now let's try to plug like a, an example number here. OK. So here we are plugging 60 which is middle C or uh I think it's C four, right? And uh we plug it in here and when we, when it's all said and done, we get back 261.6 and that's the frequency for pitch 60. Now,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1235s",
        "start_time": "1235.685"
    },
    {
        "id": "70348887",
        "text": "uh 69 at maybe not 69. Because like if you plug into this P here, 69 this the whole thing here will become one. And if you multiply that by 440 basically you just get 440 which is the frequency for uh pitch 69 or a four. OK? So now let's try to plug like a, an example number here. OK. So here we are plugging 60 which is middle C or uh I think it's C four, right? And uh we plug it in here and when we, when it's all said and done, we get back 261.6 and that's the frequency for pitch 60. Now, so as we said, an octave is divided into 12 semitones. OK? So 12 different notes that are uh that have like the same distance like a among themselves, right? OK. So they basically divide the whole octave in 12, equal parts. OK.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1264s",
        "start_time": "1264.67"
    },
    {
        "id": "208ca65c",
        "text": "plug like a, an example number here. OK. So here we are plugging 60 which is middle C or uh I think it's C four, right? And uh we plug it in here and when we, when it's all said and done, we get back 261.6 and that's the frequency for pitch 60. Now, so as we said, an octave is divided into 12 semitones. OK? So 12 different notes that are uh that have like the same distance like a among themselves, right? OK. So they basically divide the whole octave in 12, equal parts. OK. So the uh so what's the relationship between two subsequent pitches? OK. And so we can just like take a look at that by using like this uh function, this uh equation here. So the frequency of P plus one which is like the subsequent pitch to uh to the one that we are analyzing. So the subsequent semi turn",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1288s",
        "start_time": "1288.88"
    },
    {
        "id": "49b4df8a",
        "text": "so as we said, an octave is divided into 12 semitones. OK? So 12 different notes that are uh that have like the same distance like a among themselves, right? OK. So they basically divide the whole octave in 12, equal parts. OK. So the uh so what's the relationship between two subsequent pitches? OK. And so we can just like take a look at that by using like this uh function, this uh equation here. So the frequency of P plus one which is like the subsequent pitch to uh to the one that we are analyzing. So the subsequent semi turn it divided by FP the ratio is given by two to the power of one divided by 12.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1314s",
        "start_time": "1314.77"
    },
    {
        "id": "f2720a3c",
        "text": "So the uh so what's the relationship between two subsequent pitches? OK. And so we can just like take a look at that by using like this uh function, this uh equation here. So the frequency of P plus one which is like the subsequent pitch to uh to the one that we are analyzing. So the subsequent semi turn it divided by FP the ratio is given by two to the power of one divided by 12. OK. And that's equal to one point no 59. So this is like the ratio between two subsequent sentences and this remains always the same throughout like the entire like keyboard of a piano if you will. OK. So now one thing that you, I want you to notice here is that like this is like quite intuitive and it's because uh if we,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1335s",
        "start_time": "1335.199"
    },
    {
        "id": "39d3bb5d",
        "text": "it divided by FP the ratio is given by two to the power of one divided by 12. OK. And that's equal to one point no 59. So this is like the ratio between two subsequent sentences and this remains always the same throughout like the entire like keyboard of a piano if you will. OK. So now one thing that you, I want you to notice here is that like this is like quite intuitive and it's because uh if we, so basically we are saying that uh like every time we go up an octave like this factor uh is gonna be like equal to two like the factor like the, the multiplication factor of the frequency is gonna be equal to two. But if you could just up up a semi turn, so this is gonna be equal to two",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1359s",
        "start_time": "1359.67"
    },
    {
        "id": "8fc3dd8f",
        "text": "OK. And that's equal to one point no 59. So this is like the ratio between two subsequent sentences and this remains always the same throughout like the entire like keyboard of a piano if you will. OK. So now one thing that you, I want you to notice here is that like this is like quite intuitive and it's because uh if we, so basically we are saying that uh like every time we go up an octave like this factor uh is gonna be like equal to two like the factor like the, the multiplication factor of the frequency is gonna be equal to two. But if you could just up up a semi turn, so this is gonna be equal to two to the, to the power of one divided by 12 because we know that we are dividing the, the, the octave in 12 semis. OK. Cool. OK. So now to finish uh this um video, I want to talk about a sense. So this is like another very important um concept and it's related with the idea of like music perception mainly. So the idea is basically that we have,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1368s",
        "start_time": "1368.0"
    },
    {
        "id": "66c58584",
        "text": "so basically we are saying that uh like every time we go up an octave like this factor uh is gonna be like equal to two like the factor like the, the multiplication factor of the frequency is gonna be equal to two. But if you could just up up a semi turn, so this is gonna be equal to two to the, to the power of one divided by 12 because we know that we are dividing the, the, the octave in 12 semis. OK. Cool. OK. So now to finish uh this um video, I want to talk about a sense. So this is like another very important um concept and it's related with the idea of like music perception mainly. So the idea is basically that we have, we, we have like a quite decent resolution when it comes time to pitch. Obviously, we are capable of like picking up like different semitones and understanding that there's a difference in pitch there. But we are better than that. So we are capable of like, um,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1393s",
        "start_time": "1393.079"
    },
    {
        "id": "654f903d",
        "text": "to the, to the power of one divided by 12 because we know that we are dividing the, the, the octave in 12 semis. OK. Cool. OK. So now to finish uh this um video, I want to talk about a sense. So this is like another very important um concept and it's related with the idea of like music perception mainly. So the idea is basically that we have, we, we have like a quite decent resolution when it comes time to pitch. Obviously, we are capable of like picking up like different semitones and understanding that there's a difference in pitch there. But we are better than that. So we are capable of like, um, appreciating pitch differences that are smaller than a Samsung. So how can we measure that? Well, here is where scents come into place. So the idea of a scent is that, uh, it, the, the whole octave is divided in 1200 cents. So we have 100 cents for each semi. Ok. And so here, the cool thing is that's obviously like there's a,",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1414s",
        "start_time": "1414.38"
    },
    {
        "id": "ff964dae",
        "text": "we, we have like a quite decent resolution when it comes time to pitch. Obviously, we are capable of like picking up like different semitones and understanding that there's a difference in pitch there. But we are better than that. So we are capable of like, um, appreciating pitch differences that are smaller than a Samsung. So how can we measure that? Well, here is where scents come into place. So the idea of a scent is that, uh, it, the, the whole octave is divided in 1200 cents. So we have 100 cents for each semi. Ok. And so here, the cool thing is that's obviously like there's a, as a threshold below which we can't just like really tell the difference between two pitches, but that threshold of so called noticeable pitch difference is between 10 and 25 cents depending like on who you are. Your, I guess your age and your background, if you are like a musician or you're not, probably if you're a musician, like you are capable of appreciating",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1442s",
        "start_time": "1442.5"
    },
    {
        "id": "0cd49872",
        "text": "appreciating pitch differences that are smaller than a Samsung. So how can we measure that? Well, here is where scents come into place. So the idea of a scent is that, uh, it, the, the whole octave is divided in 1200 cents. So we have 100 cents for each semi. Ok. And so here, the cool thing is that's obviously like there's a, as a threshold below which we can't just like really tell the difference between two pitches, but that threshold of so called noticeable pitch difference is between 10 and 25 cents depending like on who you are. Your, I guess your age and your background, if you are like a musician or you're not, probably if you're a musician, like you are capable of appreciating uh pitch, uh difference is way better than non musicians. Ok. Cool. So by now, you should have like a fair understanding of what, what sound is, what a waveform is and everything that has to do with frequency and pitch. So",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1459s",
        "start_time": "1459.28"
    },
    {
        "id": "32be0f92",
        "text": "as a threshold below which we can't just like really tell the difference between two pitches, but that threshold of so called noticeable pitch difference is between 10 and 25 cents depending like on who you are. Your, I guess your age and your background, if you are like a musician or you're not, probably if you're a musician, like you are capable of appreciating uh pitch, uh difference is way better than non musicians. Ok. Cool. So by now, you should have like a fair understanding of what, what sound is, what a waveform is and everything that has to do with frequency and pitch. So the next time we'll continue delving into a sound and all the different parameters which actually describe sound precisely. We're going to talk about intensity, power and loudness that are all features that are somehow uh correlated or connected with the idea of amplitude. And then we'll also uh",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1486s",
        "start_time": "1486.005"
    },
    {
        "id": "14aebe1f",
        "text": "uh pitch, uh difference is way better than non musicians. Ok. Cool. So by now, you should have like a fair understanding of what, what sound is, what a waveform is and everything that has to do with frequency and pitch. So the next time we'll continue delving into a sound and all the different parameters which actually describe sound precisely. We're going to talk about intensity, power and loudness that are all features that are somehow uh correlated or connected with the idea of amplitude. And then we'll also uh venture into a very cool topic, which is that of tre the timer of sound. And this is like a very difficult one like to grasp because it's very nuanced and it's very, very, I don't know, like very ambiguous, I would say and subjective. OK. So I hope you enjoyed this video. If that's the case, please remember to leave a like and if you haven't subscribed to the channel, please do. So you'll get like all the",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1512s",
        "start_time": "1512.969"
    },
    {
        "id": "15d59809",
        "text": "the next time we'll continue delving into a sound and all the different parameters which actually describe sound precisely. We're going to talk about intensity, power and loudness that are all features that are somehow uh correlated or connected with the idea of amplitude. And then we'll also uh venture into a very cool topic, which is that of tre the timer of sound. And this is like a very difficult one like to grasp because it's very nuanced and it's very, very, I don't know, like very ambiguous, I would say and subjective. OK. So I hope you enjoyed this video. If that's the case, please remember to leave a like and if you haven't subscribed to the channel, please do. So you'll get like all the um videos that I'm posting. And finally, I want to remember uh that we have a community which is on a Slack workspace that's called the Sound of A I where if you join, you're gonna be able to just like join the discussion, ask questions with the community. Get feedback and yeah. Happen to know like very cool people. OK. That's all for today. I hope I'll see you next time. Cheers.",
        "video": "Sound and Waveforms",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "bnHHVo3j124",
        "youtube_link": "https://www.youtube.com/watch?v=bnHHVo3j124&t=1532s",
        "start_time": "1532.459"
    },
    {
        "id": "938f3906",
        "text": "Hi, everybody and welcome to a new video in the audio signal processing for machine learning series. This is not only a new video but it's gonna be the last one in the series. So what I'm gonna do today is basically show you how you can calculate the spectral Centroid and the spectral bandwidth using libres. OK. So let's get started. So here, as you can see, I already fired up a notebook and wrote down like some stuff. So I'm not gonna go through this like in detail because we've seen this time and again, time and again throughout the series. But basically, so we have like all sorts of imports here with Lisa and other stuff that we'll use here. We are loading some audio files and we are always using the free usual audio files. So the bey piece, an orchestral piece, then a song by Red Hot Chili Peppers and then a nice ballad by Jake Eton. So this is a jazzy music. So here I just like display all of them. I'm not gonna play us back because like we've heard them like multiple times and if you're interested, you can download the uh the network and hear for yourself. And then what I do here, I load all the audio files with LIB browser so that we get both the uh the signal as a non power and the sample rate. OK. So it's now time to calculate the spectral Centroid with libros.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=0s",
        "start_time": "0.31"
    },
    {
        "id": "85d5bbf0",
        "text": "detail because we've seen this time and again, time and again throughout the series. But basically, so we have like all sorts of imports here with Lisa and other stuff that we'll use here. We are loading some audio files and we are always using the free usual audio files. So the bey piece, an orchestral piece, then a song by Red Hot Chili Peppers and then a nice ballad by Jake Eton. So this is a jazzy music. So here I just like display all of them. I'm not gonna play us back because like we've heard them like multiple times and if you're interested, you can download the uh the network and hear for yourself. And then what I do here, I load all the audio files with LIB browser so that we get both the uh the signal as a non power and the sample rate. OK. So it's now time to calculate the spectral Centroid with libros. Now, the first thing that we want to do is just like uh set up the uh frame size and the hop length. And we're gonna need uh these values for extracting the spectral Centroid. OK. So now let's move on and uh actually extract the spectral Centroid. So we'll do a SC, this is like uh stands for spectral uh Centroid and we'll do uh the BC over here.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=28s",
        "start_time": "28.805"
    },
    {
        "id": "990049a4",
        "text": "play us back because like we've heard them like multiple times and if you're interested, you can download the uh the network and hear for yourself. And then what I do here, I load all the audio files with LIB browser so that we get both the uh the signal as a non power and the sample rate. OK. So it's now time to calculate the spectral Centroid with libros. Now, the first thing that we want to do is just like uh set up the uh frame size and the hop length. And we're gonna need uh these values for extracting the spectral Centroid. OK. So now let's move on and uh actually extract the spectral Centroid. So we'll do a SC, this is like uh stands for spectral uh Centroid and we'll do uh the BC over here. And uh so what we'll do is a lib rosa dot feature and then we'll get the spectral Centroid feature and here we should pass uh a few arguments. So the first of which is just the signal itself. So we'll do A Y which stands for signal",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=57s",
        "start_time": "57.56"
    },
    {
        "id": "ba0d115c",
        "text": "Now, the first thing that we want to do is just like uh set up the uh frame size and the hop length. And we're gonna need uh these values for extracting the spectral Centroid. OK. So now let's move on and uh actually extract the spectral Centroid. So we'll do a SC, this is like uh stands for spectral uh Centroid and we'll do uh the BC over here. And uh so what we'll do is a lib rosa dot feature and then we'll get the spectral Centroid feature and here we should pass uh a few arguments. So the first of which is just the signal itself. So we'll do A Y which stands for signal uh for in Lisa. And here we'll pass the, the BC signal, so we'll pass the BC, then we want to pass the sample rate and we know that this is equal to SR because we took it here, then uh we need to pass the frame size that's called N dash N underscore FFT.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=82s",
        "start_time": "82.949"
    },
    {
        "id": "6986ed78",
        "text": "And uh so what we'll do is a lib rosa dot feature and then we'll get the spectral Centroid feature and here we should pass uh a few arguments. So the first of which is just the signal itself. So we'll do A Y which stands for signal uh for in Lisa. And here we'll pass the, the BC signal, so we'll pass the BC, then we want to pass the sample rate and we know that this is equal to SR because we took it here, then uh we need to pass the frame size that's called N dash N underscore FFT. Uh And so this is gonna be equal to the frame size. And finally, we'll pass in the H length that's equal to the constants that we uh just like sat over here. OK. So uh let me just do the same thing for the three other signals that we have. So for the red hot chili peppers and for uh for the two other signals that we have. So red hot chili pepper song and Jake song.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=108s",
        "start_time": "108.699"
    },
    {
        "id": "5e5e480e",
        "text": "uh for in Lisa. And here we'll pass the, the BC signal, so we'll pass the BC, then we want to pass the sample rate and we know that this is equal to SR because we took it here, then uh we need to pass the frame size that's called N dash N underscore FFT. Uh And so this is gonna be equal to the frame size. And finally, we'll pass in the H length that's equal to the constants that we uh just like sat over here. OK. So uh let me just do the same thing for the three other signals that we have. So for the red hot chili peppers and for uh for the two other signals that we have. So red hot chili pepper song and Jake song. So this one, I, I'll just call uh red hots and let me pass this guy",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=129s",
        "start_time": "129.24"
    },
    {
        "id": "a12d996c",
        "text": "Uh And so this is gonna be equal to the frame size. And finally, we'll pass in the H length that's equal to the constants that we uh just like sat over here. OK. So uh let me just do the same thing for the three other signals that we have. So for the red hot chili peppers and for uh for the two other signals that we have. So red hot chili pepper song and Jake song. So this one, I, I'll just call uh red hots and let me pass this guy over here as the signal and then we'll do ISC, the spectral Centroid of Duke",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=151s",
        "start_time": "151.869"
    },
    {
        "id": "e1354c52",
        "text": "So this one, I, I'll just call uh red hots and let me pass this guy over here as the signal and then we'll do ISC, the spectral Centroid of Duke and I'll pass the signal for GKL inter over here. So if I don't have any mistakes, so we should just like run this and get the spectral cent, right. Yeah, it seems like it works. Let's now calculate the shape of the features that we just got. So we can do SC",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=180s",
        "start_time": "180.5"
    },
    {
        "id": "3d04d624",
        "text": "over here as the signal and then we'll do ISC, the spectral Centroid of Duke and I'll pass the signal for GKL inter over here. So if I don't have any mistakes, so we should just like run this and get the spectral cent, right. Yeah, it seems like it works. Let's now calculate the shape of the features that we just got. So we can do SC uh the BC dot uh shape. And as you can see here, we have a bi dimensional array. The first dimension is equal to one and the second dimension is equal to uh 1292. So the second dimension",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=188s",
        "start_time": "188.929"
    },
    {
        "id": "1992d9bf",
        "text": "and I'll pass the signal for GKL inter over here. So if I don't have any mistakes, so we should just like run this and get the spectral cent, right. Yeah, it seems like it works. Let's now calculate the shape of the features that we just got. So we can do SC uh the BC dot uh shape. And as you can see here, we have a bi dimensional array. The first dimension is equal to one and the second dimension is equal to uh 1292. So the second dimension is equal to the number of frames uh that we get to say it's all like the, we are like in the time domain and it's all the, the frames that we have there. OK. But we are mainly interested like in those uh",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=198s",
        "start_time": "198.139"
    },
    {
        "id": "8cac0d87",
        "text": "uh the BC dot uh shape. And as you can see here, we have a bi dimensional array. The first dimension is equal to one and the second dimension is equal to uh 1292. So the second dimension is equal to the number of frames uh that we get to say it's all like the, we are like in the time domain and it's all the, the frames that we have there. OK. But we are mainly interested like in those uh 1292 values of the spectral cent across time. So what we should do here is just get item",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=213s",
        "start_time": "213.86"
    },
    {
        "id": "17b7a8eb",
        "text": "is equal to the number of frames uh that we get to say it's all like the, we are like in the time domain and it's all the, the frames that we have there. OK. But we are mainly interested like in those uh 1292 values of the spectral cent across time. So what we should do here is just get item zero over here. So let's do that. And if we rerun this, as you can see, obviously here, we are just getting like a one dimensional array which this almost 1300 values. And that is like the value for each frame, the value of the spectral cent for",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=229s",
        "start_time": "229.119"
    },
    {
        "id": "f21d02fa",
        "text": "1292 values of the spectral cent across time. So what we should do here is just get item zero over here. So let's do that. And if we rerun this, as you can see, obviously here, we are just getting like a one dimensional array which this almost 1300 values. And that is like the value for each frame, the value of the spectral cent for frame. The next thing that we want to do is to visualize the spectral center right across time for the three pieces of music. OK. So let's start by just creating some markdowns say spectral Centroid. OK.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=245s",
        "start_time": "245.119"
    },
    {
        "id": "b0b033f4",
        "text": "zero over here. So let's do that. And if we rerun this, as you can see, obviously here, we are just getting like a one dimensional array which this almost 1300 values. And that is like the value for each frame, the value of the spectral cent for frame. The next thing that we want to do is to visualize the spectral center right across time for the three pieces of music. OK. So let's start by just creating some markdowns say spectral Centroid. OK. So here, so how do we do this? Well, we'll use map plot Libs. So the first thing that we'll do is plots dot uh figure. So we'll create a figure with a given fig size figure size and this is gonna be equal to uh 15 or like, yeah, let's say 25 by 10.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=254s",
        "start_time": "254.94"
    },
    {
        "id": "a0770d6d",
        "text": "frame. The next thing that we want to do is to visualize the spectral center right across time for the three pieces of music. OK. So let's start by just creating some markdowns say spectral Centroid. OK. So here, so how do we do this? Well, we'll use map plot Libs. So the first thing that we'll do is plots dot uh figure. So we'll create a figure with a given fig size figure size and this is gonna be equal to uh 15 or like, yeah, let's say 25 by 10. And then we want to move on and here start adding uh like the curve. So we're gonna have like a single figure with three different curves for the three different pieces that we are analyzing. OK. So we'll do a plots dot plots. And here we need to pass the value for the X axis, the Y axis and a color.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=275s",
        "start_time": "275.964"
    },
    {
        "id": "2330d2fb",
        "text": "So here, so how do we do this? Well, we'll use map plot Libs. So the first thing that we'll do is plots dot uh figure. So we'll create a figure with a given fig size figure size and this is gonna be equal to uh 15 or like, yeah, let's say 25 by 10. And then we want to move on and here start adding uh like the curve. So we're gonna have like a single figure with three different curves for the three different pieces that we are analyzing. OK. So we'll do a plots dot plots. And here we need to pass the value for the X axis, the Y axis and a color. OK. So for the X axis, we need to pass time. So I haven't still, I calculated that. So yeah, I'll do that like once we have like all of this ready, the for the Y axis, obviously, what we want to pass is the,",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=297s",
        "start_time": "297.309"
    },
    {
        "id": "a1baa89a",
        "text": "And then we want to move on and here start adding uh like the curve. So we're gonna have like a single figure with three different curves for the three different pieces that we are analyzing. OK. So we'll do a plots dot plots. And here we need to pass the value for the X axis, the Y axis and a color. OK. So for the X axis, we need to pass time. So I haven't still, I calculated that. So yeah, I'll do that like once we have like all of this ready, the for the Y axis, obviously, what we want to pass is the, the values of the spectral Centroid uh And finally, we want to pass the uh a color. And so for the BC, we'll use a blue. Now, let's just add the same thing. But for the other",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=319s",
        "start_time": "319.95"
    },
    {
        "id": "48e6c972",
        "text": "OK. So for the X axis, we need to pass time. So I haven't still, I calculated that. So yeah, I'll do that like once we have like all of this ready, the for the Y axis, obviously, what we want to pass is the, the values of the spectral Centroid uh And finally, we want to pass the uh a color. And so for the BC, we'll use a blue. Now, let's just add the same thing. But for the other uh songs, so we'll have like the red hot chili pepper song here and this is gonna be a red and finally, we'll have like the Duke Ellington uh piece and this is gonna be uh yellow like this. And finally, we'll do a plot dot uh show. Now, as I said, uh we still don't have time. Uh So we need just like to, to, to create that. And so we'll create that starting from the frame start by",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=343s",
        "start_time": "343.14"
    },
    {
        "id": "8f340660",
        "text": "the values of the spectral Centroid uh And finally, we want to pass the uh a color. And so for the BC, we'll use a blue. Now, let's just add the same thing. But for the other uh songs, so we'll have like the red hot chili pepper song here and this is gonna be a red and finally, we'll have like the Duke Ellington uh piece and this is gonna be uh yellow like this. And finally, we'll do a plot dot uh show. Now, as I said, uh we still don't have time. Uh So we need just like to, to, to create that. And so we'll create that starting from the frame start by getting the frames and we can easily get, get them by doing the length of, well, the range of the length of any of these uh spectral Centroid features.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=357s",
        "start_time": "357.589"
    },
    {
        "id": "0afac594",
        "text": "uh songs, so we'll have like the red hot chili pepper song here and this is gonna be a red and finally, we'll have like the Duke Ellington uh piece and this is gonna be uh yellow like this. And finally, we'll do a plot dot uh show. Now, as I said, uh we still don't have time. Uh So we need just like to, to, to create that. And so we'll create that starting from the frame start by getting the frames and we can easily get, get them by doing the length of, well, the range of the length of any of these uh spectral Centroid features. We'll do, yeah, we'll take uh the spectral centro uh feature for the BC and we can't use any of this because they all have like the original signals all have the same uh duration. So they'll have like the, the same number of frames. OK. So, and then we can build uh time here.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=376s",
        "start_time": "376.51"
    },
    {
        "id": "e8f61f85",
        "text": "getting the frames and we can easily get, get them by doing the length of, well, the range of the length of any of these uh spectral Centroid features. We'll do, yeah, we'll take uh the spectral centro uh feature for the BC and we can't use any of this because they all have like the original signals all have the same uh duration. So they'll have like the, the same number of frames. OK. So, and then we can build uh time here. And so we'll do a li li browser dot frames to uh time. And here we should just pass the frames like this. OK. Let's do this and see if it works well. I have an error.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=406s",
        "start_time": "406.73"
    },
    {
        "id": "3e348279",
        "text": "We'll do, yeah, we'll take uh the spectral centro uh feature for the BC and we can't use any of this because they all have like the original signals all have the same uh duration. So they'll have like the, the same number of frames. OK. So, and then we can build uh time here. And so we'll do a li li browser dot frames to uh time. And here we should just pass the frames like this. OK. Let's do this and see if it works well. I have an error. So yeah, this is not figi size but it is fig size. So yeah, let's",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=422s",
        "start_time": "422.64"
    },
    {
        "id": "7daaeaf7",
        "text": "And so we'll do a li li browser dot frames to uh time. And here we should just pass the frames like this. OK. Let's do this and see if it works well. I have an error. So yeah, this is not figi size but it is fig size. So yeah, let's work this out. OK. Nice. OK. So here we have the",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=441s",
        "start_time": "441.769"
    },
    {
        "id": "5321098b",
        "text": "So yeah, this is not figi size but it is fig size. So yeah, let's work this out. OK. Nice. OK. So here we have the uh yeah a graph with the three curves for the spectral Centroid uh for like Duke Allington for the red hot chili peppers as well as for uh the D BC orchestral piece. So on the X axis, we have time and as you can probably see it's difficult but it's there, we have 30 so this is like 30 seconds worth",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=457s",
        "start_time": "457.119"
    },
    {
        "id": "f7bb28db",
        "text": "work this out. OK. Nice. OK. So here we have the uh yeah a graph with the three curves for the spectral Centroid uh for like Duke Allington for the red hot chili peppers as well as for uh the D BC orchestral piece. So on the X axis, we have time and as you can probably see it's difficult but it's there, we have 30 so this is like 30 seconds worth data of like a piece. And then here like on the Y axis, we just have the value of the spectral Centroid. And as you can see the spectral Centroid for the red hot chili peppers is overall like higher across time than the same",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=466s",
        "start_time": "466.109"
    },
    {
        "id": "37d08181",
        "text": "uh yeah a graph with the three curves for the spectral Centroid uh for like Duke Allington for the red hot chili peppers as well as for uh the D BC orchestral piece. So on the X axis, we have time and as you can probably see it's difficult but it's there, we have 30 so this is like 30 seconds worth data of like a piece. And then here like on the Y axis, we just have the value of the spectral Centroid. And as you can see the spectral Centroid for the red hot chili peppers is overall like higher across time than the same for uh the Beau C which is like this curve in blue. And for the Duke Ellington piece, uh now, uh this is something that is usually the case. So usually like which rock music you or like EDM popular music, you tend to have spectral cent rates that are like a little bit higher than uh",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=471s",
        "start_time": "471.589"
    },
    {
        "id": "a3db44fa",
        "text": "data of like a piece. And then here like on the Y axis, we just have the value of the spectral Centroid. And as you can see the spectral Centroid for the red hot chili peppers is overall like higher across time than the same for uh the Beau C which is like this curve in blue. And for the Duke Ellington piece, uh now, uh this is something that is usually the case. So usually like which rock music you or like EDM popular music, you tend to have spectral cent rates that are like a little bit higher than uh the other like pieces that are that use tend to use like acoustic instruments like classical music or like jazz.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=494s",
        "start_time": "494.135"
    },
    {
        "id": "d4549f0e",
        "text": "for uh the Beau C which is like this curve in blue. And for the Duke Ellington piece, uh now, uh this is something that is usually the case. So usually like which rock music you or like EDM popular music, you tend to have spectral cent rates that are like a little bit higher than uh the other like pieces that are that use tend to use like acoustic instruments like classical music or like jazz. OK. So uh now, of course, like if you don't remember like what the spectral cent is, I highly suggest you to Coche couch like this video will explain like what the spectral centro is. But in a nutshell, we are talking about the center of gravity,",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=516s",
        "start_time": "516.682"
    },
    {
        "id": "78c5b722",
        "text": "the other like pieces that are that use tend to use like acoustic instruments like classical music or like jazz. OK. So uh now, of course, like if you don't remember like what the spectral cent is, I highly suggest you to Coche couch like this video will explain like what the spectral centro is. But in a nutshell, we are talking about the center of gravity, see the frequency, that's the main center of gravity for a piece and it's calculated like for each frame. And so here we have just like a curve that goes through the whole duration of the signal. And at each frame we're getting a value for the spectral Centroid. OK. So now we can move on uh to the spectral bandwidth and we can calculate it with uh libros. So",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=539s",
        "start_time": "539.84"
    },
    {
        "id": "83517ac6",
        "text": "OK. So uh now, of course, like if you don't remember like what the spectral cent is, I highly suggest you to Coche couch like this video will explain like what the spectral centro is. But in a nutshell, we are talking about the center of gravity, see the frequency, that's the main center of gravity for a piece and it's calculated like for each frame. And so here we have just like a curve that goes through the whole duration of the signal. And at each frame we're getting a value for the spectral Centroid. OK. So now we can move on uh to the spectral bandwidth and we can calculate it with uh libros. So let me just write some mark down here just to keep things neat. So we'll say spectral",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=548s",
        "start_time": "548.359"
    },
    {
        "id": "f9a11292",
        "text": "see the frequency, that's the main center of gravity for a piece and it's calculated like for each frame. And so here we have just like a curve that goes through the whole duration of the signal. And at each frame we're getting a value for the spectral Centroid. OK. So now we can move on uh to the spectral bandwidth and we can calculate it with uh libros. So let me just write some mark down here just to keep things neat. So we'll say spectral or yeah, let's say just like calculate uh band",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=566s",
        "start_time": "566.729"
    },
    {
        "id": "2fb7d9be",
        "text": "let me just write some mark down here just to keep things neat. So we'll say spectral or yeah, let's say just like calculate uh band with",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=594s",
        "start_time": "594.44"
    },
    {
        "id": "8bb65a12",
        "text": "or yeah, let's say just like calculate uh band with OK. And so here",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=604s",
        "start_time": "604.969"
    },
    {
        "id": "34d744c8",
        "text": "with OK. And so here I can easily calculate all of this. And so I can just like copy the stuff that we use for uh the spectral Centroid. And here instead of like cool the spectral Centroid function in a libres do feature I can call the spectral underscore",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=610s",
        "start_time": "610.77"
    },
    {
        "id": "d53806ea",
        "text": "OK. And so here I can easily calculate all of this. And so I can just like copy the stuff that we use for uh the spectral Centroid. And here instead of like cool the spectral Centroid function in a libres do feature I can call the spectral underscore bandwidth. That's all I need to do. And now magically, I'm gonna get the spectral bandwidth, obviously, I need to change like the the name of this variable because we're not dealing with spectral cent but with a spectral uh bandwidth. And so I can just like put uh yeah, let's put band which stands for bandwidth and we'll do the same thing for",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=612s",
        "start_time": "612.69"
    },
    {
        "id": "93268250",
        "text": "I can easily calculate all of this. And so I can just like copy the stuff that we use for uh the spectral Centroid. And here instead of like cool the spectral Centroid function in a libres do feature I can call the spectral underscore bandwidth. That's all I need to do. And now magically, I'm gonna get the spectral bandwidth, obviously, I need to change like the the name of this variable because we're not dealing with spectral cent but with a spectral uh bandwidth. And so I can just like put uh yeah, let's put band which stands for bandwidth and we'll do the same thing for the red hot chili peppers signal and for the gig Allinson signal and",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=616s",
        "start_time": "616.489"
    },
    {
        "id": "9d587e9a",
        "text": "bandwidth. That's all I need to do. And now magically, I'm gonna get the spectral bandwidth, obviously, I need to change like the the name of this variable because we're not dealing with spectral cent but with a spectral uh bandwidth. And so I can just like put uh yeah, let's put band which stands for bandwidth and we'll do the same thing for the red hot chili peppers signal and for the gig Allinson signal and we need to change this thing to bandwidth, same thing down here. And the rest uh remains the same because the arguments to spectral bandwidth are the same as we use for the spectral center. So the signal itself the sampling rate, the frame size and the hop length. And yeah, we need to always like take the the the first item. So the item zero and once we do this,",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=639s",
        "start_time": "639.96"
    },
    {
        "id": "14734163",
        "text": "the red hot chili peppers signal and for the gig Allinson signal and we need to change this thing to bandwidth, same thing down here. And the rest uh remains the same because the arguments to spectral bandwidth are the same as we use for the spectral center. So the signal itself the sampling rate, the frame size and the hop length. And yeah, we need to always like take the the the first item. So the item zero and once we do this, OK. So here we have like the um the the spectral bandwidth like for all of these three signals. Now let's take a look at the shape. And here once again, you should have yes, this 1292.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=663s",
        "start_time": "663.32"
    },
    {
        "id": "418683b8",
        "text": "we need to change this thing to bandwidth, same thing down here. And the rest uh remains the same because the arguments to spectral bandwidth are the same as we use for the spectral center. So the signal itself the sampling rate, the frame size and the hop length. And yeah, we need to always like take the the the first item. So the item zero and once we do this, OK. So here we have like the um the the spectral bandwidth like for all of these three signals. Now let's take a look at the shape. And here once again, you should have yes, this 1292. And this is like a one dimensional ray once again because we are taking like the item, the the zero item here, right? And this is like equal. So basically what this means is that we have a value for the bandwidth for each frames uh that we are analyzing. OK. So now let's move on and just visualize this. And once again, we are gonna basically reuse the kit that we just wrote. So we'll do visualize",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=669s",
        "start_time": "669.78"
    },
    {
        "id": "971a99ad",
        "text": "OK. So here we have like the um the the spectral bandwidth like for all of these three signals. Now let's take a look at the shape. And here once again, you should have yes, this 1292. And this is like a one dimensional ray once again because we are taking like the item, the the zero item here, right? And this is like equal. So basically what this means is that we have a value for the bandwidth for each frames uh that we are analyzing. OK. So now let's move on and just visualize this. And once again, we are gonna basically reuse the kit that we just wrote. So we'll do visualize uh band with",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=699s",
        "start_time": "699.599"
    },
    {
        "id": "1fb8b079",
        "text": "And this is like a one dimensional ray once again because we are taking like the item, the the zero item here, right? And this is like equal. So basically what this means is that we have a value for the bandwidth for each frames uh that we are analyzing. OK. So now let's move on and just visualize this. And once again, we are gonna basically reuse the kit that we just wrote. So we'll do visualize uh band with OK, like this. And then I'm just gonna get this stuff",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=715s",
        "start_time": "715.729"
    },
    {
        "id": "21910783",
        "text": "uh band with OK, like this. And then I'm just gonna get this stuff over here and we can reuse this. And so I'll use the instead of like the spectral centr the BC, I'm passing the,",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=745s",
        "start_time": "745.679"
    },
    {
        "id": "6e364f06",
        "text": "OK, like this. And then I'm just gonna get this stuff over here and we can reuse this. And so I'll use the instead of like the spectral centr the BC, I'm passing the, the bandwidth for the beauty for the red hot chili peppers and for Duke Ellington like this. And now tea remains the same because and well, I I ask you first thing so just like post the video and just like think about why a tea for the bandwidth is the same as the one that we have for the spectral cent, I'll give you a couple of seconds. Well,",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=748s",
        "start_time": "748.78"
    },
    {
        "id": "5cf7ead2",
        "text": "over here and we can reuse this. And so I'll use the instead of like the spectral centr the BC, I'm passing the, the bandwidth for the beauty for the red hot chili peppers and for Duke Ellington like this. And now tea remains the same because and well, I I ask you first thing so just like post the video and just like think about why a tea for the bandwidth is the same as the one that we have for the spectral cent, I'll give you a couple of seconds. Well, the the answer is, is super easy and that's because uh the, the shape is the same if, if you take a look at this, right. So the shape, so the number of frames that we have like for the bandwidth are the same number of frames uh that we have like for the uh spectral Centroid. So we can use like the same X axis. So we are using like the same times. OK. So now let's just uh",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=754s",
        "start_time": "754.69"
    },
    {
        "id": "5226c2e7",
        "text": "the bandwidth for the beauty for the red hot chili peppers and for Duke Ellington like this. And now tea remains the same because and well, I I ask you first thing so just like post the video and just like think about why a tea for the bandwidth is the same as the one that we have for the spectral cent, I'll give you a couple of seconds. Well, the the answer is, is super easy and that's because uh the, the shape is the same if, if you take a look at this, right. So the shape, so the number of frames that we have like for the bandwidth are the same number of frames uh that we have like for the uh spectral Centroid. So we can use like the same X axis. So we are using like the same times. OK. So now let's just uh plot this and as you can see, yeah, we have like a nice uh yeah, the three curves that more or less like resemble uh like the, the ones that we have for the spectral Centroid. And uh that's because like in a sense like the bandwidth is derived from the spectral uh Centroid and it gives us basically like the amount of frequencies that are like relt there are significant around the spectral",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=764s",
        "start_time": "764.369"
    },
    {
        "id": "fa615cc3",
        "text": "the the answer is, is super easy and that's because uh the, the shape is the same if, if you take a look at this, right. So the shape, so the number of frames that we have like for the bandwidth are the same number of frames uh that we have like for the uh spectral Centroid. So we can use like the same X axis. So we are using like the same times. OK. So now let's just uh plot this and as you can see, yeah, we have like a nice uh yeah, the three curves that more or less like resemble uh like the, the ones that we have for the spectral Centroid. And uh that's because like in a sense like the bandwidth is derived from the spectral uh Centroid and it gives us basically like the amount of frequencies that are like relt there are significant around the spectral uh Centroid and the, the the kind of like comment that we can apply to this plot. And to uh yeah, I would say like to the spectral bandwidth is similar to the one that we had regarding the spectral Centroid in a sense like the spectral bandwidth of uh",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=790s",
        "start_time": "790.989"
    },
    {
        "id": "dec19aeb",
        "text": "plot this and as you can see, yeah, we have like a nice uh yeah, the three curves that more or less like resemble uh like the, the ones that we have for the spectral Centroid. And uh that's because like in a sense like the bandwidth is derived from the spectral uh Centroid and it gives us basically like the amount of frequencies that are like relt there are significant around the spectral uh Centroid and the, the the kind of like comment that we can apply to this plot. And to uh yeah, I would say like to the spectral bandwidth is similar to the one that we had regarding the spectral Centroid in a sense like the spectral bandwidth of uh acoustic pieces like classical music or uh jazz music tends to be like smaller than the one that we have for. Um yeah, rock music or yeah, music which like electronic instruments usually or percussions for sure. OK. So yeah, this is it. So now you are able to calculate the spectral centro and the bandwidth using uh Liza.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=818s",
        "start_time": "818.28"
    },
    {
        "id": "86980cad",
        "text": "uh Centroid and the, the the kind of like comment that we can apply to this plot. And to uh yeah, I would say like to the spectral bandwidth is similar to the one that we had regarding the spectral Centroid in a sense like the spectral bandwidth of uh acoustic pieces like classical music or uh jazz music tends to be like smaller than the one that we have for. Um yeah, rock music or yeah, music which like electronic instruments usually or percussions for sure. OK. So yeah, this is it. So now you are able to calculate the spectral centro and the bandwidth using uh Liza. And yeah, I think like that's it for like this video and I just wanted to yeah close like this series like with a few uh with a couple of comments. So one thing I think like this was like an amazing ride. So we went through a lot of things and if you follow it along with like my series now, yeah, you have like a very strong background in audio processing and audio",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=847s",
        "start_time": "847.075"
    },
    {
        "id": "a2c07538",
        "text": "acoustic pieces like classical music or uh jazz music tends to be like smaller than the one that we have for. Um yeah, rock music or yeah, music which like electronic instruments usually or percussions for sure. OK. So yeah, this is it. So now you are able to calculate the spectral centro and the bandwidth using uh Liza. And yeah, I think like that's it for like this video and I just wanted to yeah close like this series like with a few uh with a couple of comments. So one thing I think like this was like an amazing ride. So we went through a lot of things and if you follow it along with like my series now, yeah, you have like a very strong background in audio processing and audio uh features for machine learning. So you now know about the difference of like time domain features, frequency domain features. You should have a very good understanding of the fourier transform",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=865s",
        "start_time": "865.119"
    },
    {
        "id": "e957d7e5",
        "text": "And yeah, I think like that's it for like this video and I just wanted to yeah close like this series like with a few uh with a couple of comments. So one thing I think like this was like an amazing ride. So we went through a lot of things and if you follow it along with like my series now, yeah, you have like a very strong background in audio processing and audio uh features for machine learning. So you now know about the difference of like time domain features, frequency domain features. You should have a very good understanding of the fourier transform and uh great understanding about MF CCS uh male spectrograms, spectrograms, uh log spectrograms. And so these are all ingredients that are will be necessary for your activity as an A I audio or A I music engineer or researcher. So yeah, congratulate like yourself because you did a lot of stuff if you follow it uh so far.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=891s",
        "start_time": "891.229"
    },
    {
        "id": "69594540",
        "text": "uh features for machine learning. So you now know about the difference of like time domain features, frequency domain features. You should have a very good understanding of the fourier transform and uh great understanding about MF CCS uh male spectrograms, spectrograms, uh log spectrograms. And so these are all ingredients that are will be necessary for your activity as an A I audio or A I music engineer or researcher. So yeah, congratulate like yourself because you did a lot of stuff if you follow it uh so far. And then a final like personal l like I was very happy with all the the feedback that I got the questions that you guys like asked throughout the series. It was like amazing also like the the feedback that I got like on the sound of the eyes like channel. By the way, if you want to sign up",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=915s",
        "start_time": "915.65"
    },
    {
        "id": "df55de16",
        "text": "and uh great understanding about MF CCS uh male spectrograms, spectrograms, uh log spectrograms. And so these are all ingredients that are will be necessary for your activity as an A I audio or A I music engineer or researcher. So yeah, congratulate like yourself because you did a lot of stuff if you follow it uh so far. And then a final like personal l like I was very happy with all the the feedback that I got the questions that you guys like asked throughout the series. It was like amazing also like the the feedback that I got like on the sound of the eyes like channel. By the way, if you want to sign up to that community you have the uh sign up link in the description box below. So yeah, I hope like uh I can continue to produce, I will continue to produce like content like this and get like uh feedback, positive like feedback uh from you.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=928s",
        "start_time": "928.76"
    },
    {
        "id": "c871fcf5",
        "text": "And then a final like personal l like I was very happy with all the the feedback that I got the questions that you guys like asked throughout the series. It was like amazing also like the the feedback that I got like on the sound of the eyes like channel. By the way, if you want to sign up to that community you have the uh sign up link in the description box below. So yeah, I hope like uh I can continue to produce, I will continue to produce like content like this and get like uh feedback, positive like feedback uh from you. And yeah, I think like that's all for today. If you enjoyed the video, the series and you haven't like subscribed yet to the sign of VA channel, please consider doing so. And uh if you like the video just hit the like button and I guess I'll see you in the next series. Cheers.",
        "video": "Extracting Spectral Centroid and Bandwidth with Python and Librosa",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "j6NTatoi928",
        "youtube_link": "https://www.youtube.com/watch?v=j6NTatoi928&t=954s",
        "start_time": "954.53"
    },
    {
        "id": "9ca9870a",
        "text": "Hi, everybody and welcome to a new video in the audio signal processing for machine learning series. This time, I'll be introducing the fourier transform. Now, when I was uh planning this video, I thought what's the best way of introducing the fourier transform? And I thought, well, perhaps I should just give you a little bit of intuition there and then go straight into uh implementation or even just use the browser for extracting a spectrum through the fourier transform. But then I thought, well, the fourier transform is at core of signal processing. So just sprinkling a little bit of math here and there and intuition is not going to be enough for you. So you really want to have a deep understanding of the fourier transform and all of its implications. This is why I've decided to divide this topic into a number of videos. And this first video is going to give you a",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "c9aa27f5",
        "text": "Now, when I was uh planning this video, I thought what's the best way of introducing the fourier transform? And I thought, well, perhaps I should just give you a little bit of intuition there and then go straight into uh implementation or even just use the browser for extracting a spectrum through the fourier transform. But then I thought, well, the fourier transform is at core of signal processing. So just sprinkling a little bit of math here and there and intuition is not going to be enough for you. So you really want to have a deep understanding of the fourier transform and all of its implications. This is why I've decided to divide this topic into a number of videos. And this first video is going to give you a an introduction and the high level of intuition of the fourier transform. But towards the end, I'll just be giving you a little bit of math to formalize what we'll be discussing throughout the video. So stay tuned for more about the fourier transform. But before we get started, I want to invite you once again to the sound of the eye is lacking",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=10s",
        "start_time": "10.8"
    },
    {
        "id": "60e5275e",
        "text": "core of signal processing. So just sprinkling a little bit of math here and there and intuition is not going to be enough for you. So you really want to have a deep understanding of the fourier transform and all of its implications. This is why I've decided to divide this topic into a number of videos. And this first video is going to give you a an introduction and the high level of intuition of the fourier transform. But towards the end, I'll just be giving you a little bit of math to formalize what we'll be discussing throughout the video. So stay tuned for more about the fourier transform. But before we get started, I want to invite you once again to the sound of the eye is lacking community. If you don't know it, this is a community where uh we are a bunch of people interested in A I music signal processing A I audio, all of these kind of things. And there you can ask for feedback, improve your skills, share research and talk with very cool people. So if you're interested in that, just go check out the sign up link in the description below. OK. Now on to the real stuff",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=34s",
        "start_time": "34.24"
    },
    {
        "id": "e63e67d8",
        "text": "an introduction and the high level of intuition of the fourier transform. But towards the end, I'll just be giving you a little bit of math to formalize what we'll be discussing throughout the video. So stay tuned for more about the fourier transform. But before we get started, I want to invite you once again to the sound of the eye is lacking community. If you don't know it, this is a community where uh we are a bunch of people interested in A I music signal processing A I audio, all of these kind of things. And there you can ask for feedback, improve your skills, share research and talk with very cool people. So if you're interested in that, just go check out the sign up link in the description below. OK. Now on to the real stuff to introduce the concept of that are behind um the fourier transform. I want to use an analogy here. And for that, I'm gonna use one of the most iconic cover ever created in the rock world and from one of the bands that I love the most pink Floyd. So this is the cover for dark side of the moon. So what does this",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=57s",
        "start_time": "57.68"
    },
    {
        "id": "5780a176",
        "text": "community. If you don't know it, this is a community where uh we are a bunch of people interested in A I music signal processing A I audio, all of these kind of things. And there you can ask for feedback, improve your skills, share research and talk with very cool people. So if you're interested in that, just go check out the sign up link in the description below. OK. Now on to the real stuff to introduce the concept of that are behind um the fourier transform. I want to use an analogy here. And for that, I'm gonna use one of the most iconic cover ever created in the rock world and from one of the bands that I love the most pink Floyd. So this is the cover for dark side of the moon. So what does this have to do with the fourier transform? Well, it's the analogy here that counts a lot. So here we have a beam, a light beam, right? So this is a complex waveform, it goes through a prism and then out of the prism, we get the",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=81s",
        "start_time": "81.12"
    },
    {
        "id": "1bfbc08c",
        "text": "to introduce the concept of that are behind um the fourier transform. I want to use an analogy here. And for that, I'm gonna use one of the most iconic cover ever created in the rock world and from one of the bands that I love the most pink Floyd. So this is the cover for dark side of the moon. So what does this have to do with the fourier transform? Well, it's the analogy here that counts a lot. So here we have a beam, a light beam, right? So this is a complex waveform, it goes through a prism and then out of the prism, we get the different like spectral bands, the different colors divided, right. So the basic idea is complex waveform. We have kind of like a machine or an algorithm which in this case is the prism. And then we get all the different components which make up uh light. OK. So with the fourier transform, we do something that's very, very similar to this.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=108s",
        "start_time": "108.12"
    },
    {
        "id": "6af75684",
        "text": "have to do with the fourier transform? Well, it's the analogy here that counts a lot. So here we have a beam, a light beam, right? So this is a complex waveform, it goes through a prism and then out of the prism, we get the different like spectral bands, the different colors divided, right. So the basic idea is complex waveform. We have kind of like a machine or an algorithm which in this case is the prism. And then we get all the different components which make up uh light. OK. So with the fourier transform, we do something that's very, very similar to this. So the high level intuition here is that we have a complex sound and then using the fourier transform, which in our analogy is the prism, we decompose the complex sound into its frequency components. Because if you remember from one of my initial videos, uh complex sounds are made up of many different uh pure turns",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=131s",
        "start_time": "131.139"
    },
    {
        "id": "fe4c7d24",
        "text": "different like spectral bands, the different colors divided, right. So the basic idea is complex waveform. We have kind of like a machine or an algorithm which in this case is the prism. And then we get all the different components which make up uh light. OK. So with the fourier transform, we do something that's very, very similar to this. So the high level intuition here is that we have a complex sound and then using the fourier transform, which in our analogy is the prism, we decompose the complex sound into its frequency components. Because if you remember from one of my initial videos, uh complex sounds are made up of many different uh pure turns added up superimposed together. OK. So this is like basically the first intuition that you have. Now when we do this, when we use the four year transform, what we do really is a journey from the",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=149s",
        "start_time": "149.96"
    },
    {
        "id": "f0d09c19",
        "text": "So the high level intuition here is that we have a complex sound and then using the fourier transform, which in our analogy is the prism, we decompose the complex sound into its frequency components. Because if you remember from one of my initial videos, uh complex sounds are made up of many different uh pure turns added up superimposed together. OK. So this is like basically the first intuition that you have. Now when we do this, when we use the four year transform, what we do really is a journey from the time domain to the frequency domain. So probably if you followed along this series, you've seen like this type of like slide time and again, but let me repeat that uh for yeah, the sake of yeah, understanding everything very deeply. OK. So here we have like a sound you know wave plot and here we have time on the X axis right.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=174s",
        "start_time": "174.97"
    },
    {
        "id": "42448476",
        "text": "added up superimposed together. OK. So this is like basically the first intuition that you have. Now when we do this, when we use the four year transform, what we do really is a journey from the time domain to the frequency domain. So probably if you followed along this series, you've seen like this type of like slide time and again, but let me repeat that uh for yeah, the sake of yeah, understanding everything very deeply. OK. So here we have like a sound you know wave plot and here we have time on the X axis right. Then we use the fourier transfer some magic happens and we get the same sound but this time uh plotted in the frequency domain. So we have a frequency analysis of the very same sound. And in other words, on the x axis, we have frequency and where you see this spot,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=201s",
        "start_time": "201.149"
    },
    {
        "id": "a5a6c051",
        "text": "time domain to the frequency domain. So probably if you followed along this series, you've seen like this type of like slide time and again, but let me repeat that uh for yeah, the sake of yeah, understanding everything very deeply. OK. So here we have like a sound you know wave plot and here we have time on the X axis right. Then we use the fourier transfer some magic happens and we get the same sound but this time uh plotted in the frequency domain. So we have a frequency analysis of the very same sound. And in other words, on the x axis, we have frequency and where you see this spot, it means that the relative um frequencies at that specific Hertz are an important component of that original sound. Now, the question is how do we do this? So how do we go from the time domain to the frequency domain? Well, that's the topic of today's video at an intuition level. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=216s",
        "start_time": "216.729"
    },
    {
        "id": "8513d3e2",
        "text": "Then we use the fourier transfer some magic happens and we get the same sound but this time uh plotted in the frequency domain. So we have a frequency analysis of the very same sound. And in other words, on the x axis, we have frequency and where you see this spot, it means that the relative um frequencies at that specific Hertz are an important component of that original sound. Now, the question is how do we do this? So how do we go from the time domain to the frequency domain? Well, that's the topic of today's video at an intuition level. OK. Now let's go one level of resolution further uh down and get like a better intuition of what's going on with the fourier transform. So",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=240s",
        "start_time": "240.779"
    },
    {
        "id": "fb44cdb2",
        "text": "it means that the relative um frequencies at that specific Hertz are an important component of that original sound. Now, the question is how do we do this? So how do we go from the time domain to the frequency domain? Well, that's the topic of today's video at an intuition level. OK. Now let's go one level of resolution further uh down and get like a better intuition of what's going on with the fourier transform. So uh when we apply the fourier transform, we do a bunch of things. So the first thing that we do is we compare the original signal with a bunch of sinusoids with various frequencies, right. So we have the original signal",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=261s",
        "start_time": "261.959"
    },
    {
        "id": "e3584ba4",
        "text": "Now let's go one level of resolution further uh down and get like a better intuition of what's going on with the fourier transform. So uh when we apply the fourier transform, we do a bunch of things. So the first thing that we do is we compare the original signal with a bunch of sinusoids with various frequencies, right. So we have the original signal and then we have uh a bunch of sine waves and we want to compare the original signal to those sine waves which are gonna have different frequencies. And so out of that comparison, what we get is a magnitude and a phase. Now what does the magnitude tells us? Well,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=284s",
        "start_time": "284.47"
    },
    {
        "id": "d659baba",
        "text": "uh when we apply the fourier transform, we do a bunch of things. So the first thing that we do is we compare the original signal with a bunch of sinusoids with various frequencies, right. So we have the original signal and then we have uh a bunch of sine waves and we want to compare the original signal to those sine waves which are gonna have different frequencies. And so out of that comparison, what we get is a magnitude and a phase. Now what does the magnitude tells us? Well, uh the magnitude tells us how similar the uh original signal and the P turn or the sign turn are. So the higher the magnitude, the higher the similarity between the original signal and the sinusoid with that specific uh frequency. Now we'll take a look at the phase in uh uh a few minutes. But here",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=297s",
        "start_time": "297.04"
    },
    {
        "id": "8a9b7096",
        "text": "and then we have uh a bunch of sine waves and we want to compare the original signal to those sine waves which are gonna have different frequencies. And so out of that comparison, what we get is a magnitude and a phase. Now what does the magnitude tells us? Well, uh the magnitude tells us how similar the uh original signal and the P turn or the sign turn are. So the higher the magnitude, the higher the similarity between the original signal and the sinusoid with that specific uh frequency. Now we'll take a look at the phase in uh uh a few minutes. But here uh this is kind of like the, the deeper intuition, but still, it may feel a little bit abstract. So now let's go to check out some code that I've written in a Jupiter notebook and see how this applies specifically and just like visualize some of these intuitions.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=317s",
        "start_time": "317.179"
    },
    {
        "id": "8eb0c12e",
        "text": "uh the magnitude tells us how similar the uh original signal and the P turn or the sign turn are. So the higher the magnitude, the higher the similarity between the original signal and the sinusoid with that specific uh frequency. Now we'll take a look at the phase in uh uh a few minutes. But here uh this is kind of like the, the deeper intuition, but still, it may feel a little bit abstract. So now let's go to check out some code that I've written in a Jupiter notebook and see how this applies specifically and just like visualize some of these intuitions. OK? So now uh it's not important uh to look at the code here. And many of the stuff that I've uh like many of like these lines of code that I've read here, I already explained to you like in previous videos. Uh But what's important is to look at the results of these things. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=340s",
        "start_time": "340.679"
    },
    {
        "id": "24871b10",
        "text": "uh this is kind of like the, the deeper intuition, but still, it may feel a little bit abstract. So now let's go to check out some code that I've written in a Jupiter notebook and see how this applies specifically and just like visualize some of these intuitions. OK? So now uh it's not important uh to look at the code here. And many of the stuff that I've uh like many of like these lines of code that I've read here, I already explained to you like in previous videos. Uh But what's important is to look at the results of these things. OK. So now let me get started. So I'll import uh all of these libraries which will need to plot and analyze sounds. OK. So the first thing that I want to do is to uh actually show you the sound that we'll be analyzing. So let's listen to this.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=367s",
        "start_time": "367.98"
    },
    {
        "id": "236fae66",
        "text": "OK? So now uh it's not important uh to look at the code here. And many of the stuff that I've uh like many of like these lines of code that I've read here, I already explained to you like in previous videos. Uh But what's important is to look at the results of these things. OK. So now let me get started. So I'll import uh all of these libraries which will need to plot and analyze sounds. OK. So the first thing that I want to do is to uh actually show you the sound that we'll be analyzing. So let's listen to this. OK. It's just a piano key specifically, it's C five note C five. OK? And it's held for almost like a second and a half. OK. So here we'll just load the audio file which uh Li Brosa, as, as I said, uh you've seen how to do this in previous video if you followed the series, if you haven't just like, check that out,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=384s",
        "start_time": "384.859"
    },
    {
        "id": "952e9158",
        "text": "So now let me get started. So I'll import uh all of these libraries which will need to plot and analyze sounds. OK. So the first thing that I want to do is to uh actually show you the sound that we'll be analyzing. So let's listen to this. OK. It's just a piano key specifically, it's C five note C five. OK? And it's held for almost like a second and a half. OK. So here we'll just load the audio file which uh Li Brosa, as, as I said, uh you've seen how to do this in previous video if you followed the series, if you haven't just like, check that out, OK. So the next step is to actually plot the waveform. And yeah, here we have like our nice little waveform. So once again, here we are in the time domain. So X axis is time, Y axis is amplitude and as you can see, so we have the attack of the sound and then we have a little bit of the DC and then yeah, the sound ends towards like 1.4 seconds. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=404s",
        "start_time": "404.07"
    },
    {
        "id": "fe0dc9e0",
        "text": "OK. It's just a piano key specifically, it's C five note C five. OK? And it's held for almost like a second and a half. OK. So here we'll just load the audio file which uh Li Brosa, as, as I said, uh you've seen how to do this in previous video if you followed the series, if you haven't just like, check that out, OK. So the next step is to actually plot the waveform. And yeah, here we have like our nice little waveform. So once again, here we are in the time domain. So X axis is time, Y axis is amplitude and as you can see, so we have the attack of the sound and then we have a little bit of the DC and then yeah, the sound ends towards like 1.4 seconds. OK. So the next step is the crucial one is the one that we use for deriving the free transform.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=423s",
        "start_time": "423.019"
    },
    {
        "id": "8540c295",
        "text": "OK. So the next step is to actually plot the waveform. And yeah, here we have like our nice little waveform. So once again, here we are in the time domain. So X axis is time, Y axis is amplitude and as you can see, so we have the attack of the sound and then we have a little bit of the DC and then yeah, the sound ends towards like 1.4 seconds. OK. So the next step is the crucial one is the one that we use for deriving the free transform. And we do that by using uh S IP specifically the FFT module. And we use the FFT which stands for fast fourier transform um function in the FFT module. So we pass it in the signal and we get the fourier transform. Now, I'm not going to show you what that result actually is because you need to have like mm",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=445s",
        "start_time": "445.75"
    },
    {
        "id": "45a2ccaa",
        "text": "So the next step is the crucial one is the one that we use for deriving the free transform. And we do that by using uh S IP specifically the FFT module. And we use the FFT which stands for fast fourier transform um function in the FFT module. So we pass it in the signal and we get the fourier transform. Now, I'm not going to show you what that result actually is because you need to have like mm a deeper understanding of math complex numbers and things that, that which you'll have in a few videos. But for now, the important thing to understand is that if we do the absolute, if we take the absolute value of the fourier transform, we actually get the magnitudes and this is the y axis of our spectrum.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=473s",
        "start_time": "473.679"
    },
    {
        "id": "e62dc102",
        "text": "And we do that by using uh S IP specifically the FFT module. And we use the FFT which stands for fast fourier transform um function in the FFT module. So we pass it in the signal and we get the fourier transform. Now, I'm not going to show you what that result actually is because you need to have like mm a deeper understanding of math complex numbers and things that, that which you'll have in a few videos. But for now, the important thing to understand is that if we do the absolute, if we take the absolute value of the fourier transform, we actually get the magnitudes and this is the y axis of our spectrum. Whereas on the X axis obviously, we have the frequencies. Now how should we like distribute the frequencies on the X axis? So the frequencies are gonna be between zero Hertz and the sampling rate. OK. And then we'll divide this in in a number of steps that's equal to the length of the, of the magnitude array.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=483s",
        "start_time": "483.109"
    },
    {
        "id": "ae682ac5",
        "text": "a deeper understanding of math complex numbers and things that, that which you'll have in a few videos. But for now, the important thing to understand is that if we do the absolute, if we take the absolute value of the fourier transform, we actually get the magnitudes and this is the y axis of our spectrum. Whereas on the X axis obviously, we have the frequencies. Now how should we like distribute the frequencies on the X axis? So the frequencies are gonna be between zero Hertz and the sampling rate. OK. And then we'll divide this in in a number of steps that's equal to the length of the, of the magnitude array. OK. So with all of this in mind, we can now plot the spectrum",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=507s",
        "start_time": "507.66"
    },
    {
        "id": "b5d54de3",
        "text": "Whereas on the X axis obviously, we have the frequencies. Now how should we like distribute the frequencies on the X axis? So the frequencies are gonna be between zero Hertz and the sampling rate. OK. And then we'll divide this in in a number of steps that's equal to the length of the, of the magnitude array. OK. So with all of this in mind, we can now plot the spectrum and here we go. So here we have the spectrum and as you can see, we can see a peak in the magnitude around 523 Hertz. And this is more or less the frequency the specific frequency for the note C five.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=531s",
        "start_time": "531.799"
    },
    {
        "id": "c058a2ba",
        "text": "OK. So with all of this in mind, we can now plot the spectrum and here we go. So here we have the spectrum and as you can see, we can see a peak in the magnitude around 523 Hertz. And this is more or less the frequency the specific frequency for the note C five. And so this is the so called fundamental of the, of the, of the sound that we've heard and then we have a few spikes. So here around uh 1004, um 1 hun 1040 Hertz something and here",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=561s",
        "start_time": "561.099"
    },
    {
        "id": "0cfa0a29",
        "text": "and here we go. So here we have the spectrum and as you can see, we can see a peak in the magnitude around 523 Hertz. And this is more or less the frequency the specific frequency for the note C five. And so this is the so called fundamental of the, of the, of the sound that we've heard and then we have a few spikes. So here around uh 1004, um 1 hun 1040 Hertz something and here and 1/4 1 here and a little bit of 1/5 down here, but it's almost like unnoticeable that one. OK. So the, this profile is quite specific, right? And, and it feels like there there's some like symmetry like natural uh thing to it. And it's the fact that this is a harmonic sound. In other words, these guys over here",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=567s",
        "start_time": "567.869"
    },
    {
        "id": "e55450fb",
        "text": "And so this is the so called fundamental of the, of the, of the sound that we've heard and then we have a few spikes. So here around uh 1004, um 1 hun 1040 Hertz something and here and 1/4 1 here and a little bit of 1/5 down here, but it's almost like unnoticeable that one. OK. So the, this profile is quite specific, right? And, and it feels like there there's some like symmetry like natural uh thing to it. And it's the fact that this is a harmonic sound. In other words, these guys over here are harmonics or overtones of the fundamental frequency. In other words, this frequency is just twice the frequency of the fundamental, the frequency associated with this third spike is three times the frequency of the fundamental. And here it's four times that OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=586s",
        "start_time": "586.78"
    },
    {
        "id": "a7208f7e",
        "text": "and 1/4 1 here and a little bit of 1/5 down here, but it's almost like unnoticeable that one. OK. So the, this profile is quite specific, right? And, and it feels like there there's some like symmetry like natural uh thing to it. And it's the fact that this is a harmonic sound. In other words, these guys over here are harmonics or overtones of the fundamental frequency. In other words, this frequency is just twice the frequency of the fundamental, the frequency associated with this third spike is three times the frequency of the fundamental. And here it's four times that OK. Good. Now, if you don't remember uh what all the turns are like or partials or these things I have a video about that is one of the initial videos and it should be linked up here. OK. So now we've seen how we can move basically from the time domain to the frequency domain. But still the fourier transform is a completely black box for us. Now,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=607s",
        "start_time": "607.57"
    },
    {
        "id": "ba575f68",
        "text": "are harmonics or overtones of the fundamental frequency. In other words, this frequency is just twice the frequency of the fundamental, the frequency associated with this third spike is three times the frequency of the fundamental. And here it's four times that OK. Good. Now, if you don't remember uh what all the turns are like or partials or these things I have a video about that is one of the initial videos and it should be linked up here. OK. So now we've seen how we can move basically from the time domain to the frequency domain. But still the fourier transform is a completely black box for us. Now, I want to start unravel it for you guys and give you an understanding of how the this process and the fourier transform works. But for that, what I want to do is zoom into the waveform and actually uh be at a resolution where we can actually see like all the cycles in um the wave associated with this sound. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=630s",
        "start_time": "630.534"
    },
    {
        "id": "e3dd7480",
        "text": "Good. Now, if you don't remember uh what all the turns are like or partials or these things I have a video about that is one of the initial videos and it should be linked up here. OK. So now we've seen how we can move basically from the time domain to the frequency domain. But still the fourier transform is a completely black box for us. Now, I want to start unravel it for you guys and give you an understanding of how the this process and the fourier transform works. But for that, what I want to do is zoom into the waveform and actually uh be at a resolution where we can actually see like all the cycles in um the wave associated with this sound. OK. So for doing that be before we do that. Yeah, let's, let's take a look at it at a few numbers. So the length of the signal or in other words, the number of samples that we have, it's around 34,000 samples. The sampling rate, if I remember correctly here should be a 22,050 Hertz. And the duration of each sample is the inverse of Dutch, which is four times 10 to",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=654s",
        "start_time": "654.109"
    },
    {
        "id": "a5a0b9d0",
        "text": "I want to start unravel it for you guys and give you an understanding of how the this process and the fourier transform works. But for that, what I want to do is zoom into the waveform and actually uh be at a resolution where we can actually see like all the cycles in um the wave associated with this sound. OK. So for doing that be before we do that. Yeah, let's, let's take a look at it at a few numbers. So the length of the signal or in other words, the number of samples that we have, it's around 34,000 samples. The sampling rate, if I remember correctly here should be a 22,050 Hertz. And the duration of each sample is the inverse of Dutch, which is four times 10 to um the power of minus five. So it's a very, very short amount of time for each sample.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=679s",
        "start_time": "679.95"
    },
    {
        "id": "0afeafb2",
        "text": "So for doing that be before we do that. Yeah, let's, let's take a look at it at a few numbers. So the length of the signal or in other words, the number of samples that we have, it's around 34,000 samples. The sampling rate, if I remember correctly here should be a 22,050 Hertz. And the duration of each sample is the inverse of Dutch, which is four times 10 to um the power of minus five. So it's a very, very short amount of time for each sample. Now, uh let's take a look at the duration of a cycle for our fundamental frequency. So 523 Hertz and so obviously, this is one divided by 523. And if you take a look at these results, so this is uh almost two milliseconds. So we have a cycle every two milliseconds. So now what I want to do,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=705s",
        "start_time": "705.419"
    },
    {
        "id": "15b32e6e",
        "text": "um the power of minus five. So it's a very, very short amount of time for each sample. Now, uh let's take a look at the duration of a cycle for our fundamental frequency. So 523 Hertz and so obviously, this is one divided by 523. And if you take a look at these results, so this is uh almost two milliseconds. So we have a cycle every two milliseconds. So now what I want to do, it's just like zoom in and to the waveform and only consider 400 samples because 400 samples are gonna give us uh 0.02 seconds. And this is, this will enable us to see more or less like 10 cycles of our waveforms. And that is something like that, we can definitely visualize. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=733s",
        "start_time": "733.869"
    },
    {
        "id": "71004fb7",
        "text": "Now, uh let's take a look at the duration of a cycle for our fundamental frequency. So 523 Hertz and so obviously, this is one divided by 523. And if you take a look at these results, so this is uh almost two milliseconds. So we have a cycle every two milliseconds. So now what I want to do, it's just like zoom in and to the waveform and only consider 400 samples because 400 samples are gonna give us uh 0.02 seconds. And this is, this will enable us to see more or less like 10 cycles of our waveforms. And that is something like that, we can definitely visualize. OK. So all of that to say that um I have like this new plot where we zoom into the waveform. And here I'm only considering 400 samples. In other words,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=740s",
        "start_time": "740.33"
    },
    {
        "id": "ed2a3e41",
        "text": "it's just like zoom in and to the waveform and only consider 400 samples because 400 samples are gonna give us uh 0.02 seconds. And this is, this will enable us to see more or less like 10 cycles of our waveforms. And that is something like that, we can definitely visualize. OK. So all of that to say that um I have like this new plot where we zoom into the waveform. And here I'm only considering 400 samples. In other words, I'm slicing the signal which is on the Y axis. And I'm only considering the, the samples between like 20,400. OK. So let's take a look at this and here you have the results. So now we are at a resolution where we can actually see the um all the cycles. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=767s",
        "start_time": "767.554"
    },
    {
        "id": "9203b719",
        "text": "So all of that to say that um I have like this new plot where we zoom into the waveform. And here I'm only considering 400 samples. In other words, I'm slicing the signal which is on the Y axis. And I'm only considering the, the samples between like 20,400. OK. So let's take a look at this and here you have the results. So now we are at a resolution where we can actually see the um all the cycles. OK. So now what's the point of all of this? Well, the point is that uh what we want to do is to actually compare this signal against sinusoids with different um frequencies. OK. And see the similarity there. So, and I want to show you how we can do that visually.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=794s",
        "start_time": "794.94"
    },
    {
        "id": "fab6e49c",
        "text": "I'm slicing the signal which is on the Y axis. And I'm only considering the, the samples between like 20,400. OK. So let's take a look at this and here you have the results. So now we are at a resolution where we can actually see the um all the cycles. OK. So now what's the point of all of this? Well, the point is that uh what we want to do is to actually compare this signal against sinusoids with different um frequencies. OK. And see the similarity there. So, and I want to show you how we can do that visually. OK. So now let me just go back here and I want to uh remind you of the equation for a sine wave, which is given by this formula here. So it's like the sine of two pi uh which uh multiplies F",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=807s",
        "start_time": "807.83"
    },
    {
        "id": "0a60c3a5",
        "text": "So now what's the point of all of this? Well, the point is that uh what we want to do is to actually compare this signal against sinusoids with different um frequencies. OK. And see the similarity there. So, and I want to show you how we can do that visually. OK. So now let me just go back here and I want to uh remind you of the equation for a sine wave, which is given by this formula here. So it's like the sine of two pi uh which uh multiplies F uh times T where F is the frequency T is just time minus P where P is the face and we'll see what the face does in a second. OK. So yeah, let's just go back here.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=832s",
        "start_time": "832.729"
    },
    {
        "id": "473088dc",
        "text": "OK. So now let me just go back here and I want to uh remind you of the equation for a sine wave, which is given by this formula here. So it's like the sine of two pi uh which uh multiplies F uh times T where F is the frequency T is just time minus P where P is the face and we'll see what the face does in a second. OK. So yeah, let's just go back here. So the thing that we want to do here is just create a sinusoid. And here I'll put the frequency equal to 523 which is our fundamental frequency for the time being, we'll have a phase of zero. And here we have our function, our sine wave over here.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=854s",
        "start_time": "854.469"
    },
    {
        "id": "00d0b9f8",
        "text": "uh times T where F is the frequency T is just time minus P where P is the face and we'll see what the face does in a second. OK. So yeah, let's just go back here. So the thing that we want to do here is just create a sinusoid. And here I'll put the frequency equal to 523 which is our fundamental frequency for the time being, we'll have a phase of zero. And here we have our function, our sine wave over here. OK. And then I'm just gonna plot that once again uh between 10,000 samples and 10,000 of 400 samples. So just 400 samples so that we can uh kind of like superimpose this to the original signal uh in a few minutes. OK. So here we have our nice uh SYO at 5, 523",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=873s",
        "start_time": "873.82"
    },
    {
        "id": "ffe79dde",
        "text": "So the thing that we want to do here is just create a sinusoid. And here I'll put the frequency equal to 523 which is our fundamental frequency for the time being, we'll have a phase of zero. And here we have our function, our sine wave over here. OK. And then I'm just gonna plot that once again uh between 10,000 samples and 10,000 of 400 samples. So just 400 samples so that we can uh kind of like superimpose this to the original signal uh in a few minutes. OK. So here we have our nice uh SYO at 5, 523 hurts. Now, let's take a look at the face. So what does the uh face do? Well, for that, what I suggest doing is just create another um",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=889s",
        "start_time": "889.659"
    },
    {
        "id": "15014708",
        "text": "OK. And then I'm just gonna plot that once again uh between 10,000 samples and 10,000 of 400 samples. So just 400 samples so that we can uh kind of like superimpose this to the original signal uh in a few minutes. OK. So here we have our nice uh SYO at 5, 523 hurts. Now, let's take a look at the face. So what does the uh face do? Well, for that, what I suggest doing is just create another um another um uh sinusoid which will call",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=908s",
        "start_time": "908.799"
    },
    {
        "id": "adc284ba",
        "text": "hurts. Now, let's take a look at the face. So what does the uh face do? Well, for that, what I suggest doing is just create another um another um uh sinusoid which will call over here. Yeah, it's a little bit difficult to work with this extra big cursor.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=936s",
        "start_time": "936.119"
    },
    {
        "id": "db945256",
        "text": "another um uh sinusoid which will call over here. Yeah, it's a little bit difficult to work with this extra big cursor. So over here, so this is phase two. Everything else will be uh like the same. So we'll just like change the phase here. And obviously I need to",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=951s",
        "start_time": "951.64"
    },
    {
        "id": "c904cd5b",
        "text": "over here. Yeah, it's a little bit difficult to work with this extra big cursor. So over here, so this is phase two. Everything else will be uh like the same. So we'll just like change the phase here. And obviously I need to plots signed to as well over here. It's not there, it's down here. And I'll use say, I don't know like a yellow color color for this.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=957s",
        "start_time": "957.359"
    },
    {
        "id": "930e939b",
        "text": "So over here, so this is phase two. Everything else will be uh like the same. So we'll just like change the phase here. And obviously I need to plots signed to as well over here. It's not there, it's down here. And I'll use say, I don't know like a yellow color color for this. OK. So now let's use the same face. And so if we use the same face, uh so we, we have a mistake here.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=965s",
        "start_time": "965.059"
    },
    {
        "id": "21eeaf0c",
        "text": "plots signed to as well over here. It's not there, it's down here. And I'll use say, I don't know like a yellow color color for this. OK. So now let's use the same face. And so if we use the same face, uh so we, we have a mistake here. Oh Yes, because this is face two. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=978s",
        "start_time": "978.739"
    },
    {
        "id": "de70698a",
        "text": "OK. So now let's use the same face. And so if we use the same face, uh so we, we have a mistake here. Oh Yes, because this is face two. OK. So let's go. Yeah. And obviously like the two sinusoids are completely aligned on top of each other. But now if I change the phase, say like I go 0.2 for uh sinus, the the yellow sinusoid, uh you'll see that the",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=989s",
        "start_time": "989.979"
    },
    {
        "id": "4557a21c",
        "text": "Oh Yes, because this is face two. OK. So let's go. Yeah. And obviously like the two sinusoids are completely aligned on top of each other. But now if I change the phase, say like I go 0.2 for uh sinus, the the yellow sinusoid, uh you'll see that the um sinusoid is now shifting, shifting towards the right. If I get to 0.5",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=999s",
        "start_time": "999.21"
    },
    {
        "id": "ea8d6dbf",
        "text": "So let's go. Yeah. And obviously like the two sinusoids are completely aligned on top of each other. But now if I change the phase, say like I go 0.2 for uh sinus, the the yellow sinusoid, uh you'll see that the um sinusoid is now shifting, shifting towards the right. If I get to 0.5 the sinusoids are completely like in opposite phase. So in other words, when we have a pick with the red sinusoid, we have a deep uh with the uh yellow sinusoid and vice versa. And the closer I get to, to one and the closer I'm just like realigning them. And so once we are back at one, so they are aligned once again.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1003s",
        "start_time": "1003.15"
    },
    {
        "id": "4331e6f8",
        "text": "um sinusoid is now shifting, shifting towards the right. If I get to 0.5 the sinusoids are completely like in opposite phase. So in other words, when we have a pick with the red sinusoid, we have a deep uh with the uh yellow sinusoid and vice versa. And the closer I get to, to one and the closer I'm just like realigning them. And so once we are back at one, so they are aligned once again. So what this means is that if I continue giving like higher numbers for phase two, we'll just like start again the, the circle because yeah, I mean, we are like using a, a periodic function in the end. OK. So 1.5 it's basically equal to uh we get like the same function",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1021s",
        "start_time": "1021.729"
    },
    {
        "id": "5d31e50f",
        "text": "the sinusoids are completely like in opposite phase. So in other words, when we have a pick with the red sinusoid, we have a deep uh with the uh yellow sinusoid and vice versa. And the closer I get to, to one and the closer I'm just like realigning them. And so once we are back at one, so they are aligned once again. So what this means is that if I continue giving like higher numbers for phase two, we'll just like start again the, the circle because yeah, I mean, we are like using a, a periodic function in the end. OK. So 1.5 it's basically equal to uh we get like the same function of the same sine wave that we had for 0.5. And if we go to, we change the phase to two. Well, once again, we are just back to the um to being aligned once again with the uh initial sine wave. OK. So all of this to say that we can assume that the phase moves between zero and one just because uh the, the function is periodic. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1029s",
        "start_time": "1029.849"
    },
    {
        "id": "95f1e4d0",
        "text": "So what this means is that if I continue giving like higher numbers for phase two, we'll just like start again the, the circle because yeah, I mean, we are like using a, a periodic function in the end. OK. So 1.5 it's basically equal to uh we get like the same function of the same sine wave that we had for 0.5. And if we go to, we change the phase to two. Well, once again, we are just back to the um to being aligned once again with the uh initial sine wave. OK. So all of this to say that we can assume that the phase moves between zero and one just because uh the, the function is periodic. OK. So now what's the big deal like with this uh sine waves? Right? So the point is that we want to compare uh the signal and the sino if you guys remember what I said with the deeper intuition. So",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1057s",
        "start_time": "1057.099"
    },
    {
        "id": "9bf6ca1d",
        "text": "of the same sine wave that we had for 0.5. And if we go to, we change the phase to two. Well, once again, we are just back to the um to being aligned once again with the uh initial sine wave. OK. So all of this to say that we can assume that the phase moves between zero and one just because uh the, the function is periodic. OK. So now what's the big deal like with this uh sine waves? Right? So the point is that we want to compare uh the signal and the sino if you guys remember what I said with the deeper intuition. So uh in order to, to move from the frequent, from the time domain to the frequency domain, we compare the signal with sinusoids of various frequencies. And then for each frequency, we get a magnitude and the phase. OK. And the and high magnitude indicates high similarity between the signal and a sinusoid. Now, let's see how this actually applies here visually. OK. So what I'm doing here is I'm plotting",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1074s",
        "start_time": "1074.989"
    },
    {
        "id": "6df1ef72",
        "text": "So now what's the big deal like with this uh sine waves? Right? So the point is that we want to compare uh the signal and the sino if you guys remember what I said with the deeper intuition. So uh in order to, to move from the frequent, from the time domain to the frequency domain, we compare the signal with sinusoids of various frequencies. And then for each frequency, we get a magnitude and the phase. OK. And the and high magnitude indicates high similarity between the signal and a sinusoid. Now, let's see how this actually applies here visually. OK. So what I'm doing here is I'm plotting uh both the original signal as well as a uh sinusoid here and the sinusoid is going to be shown in red. OK. And here for the sinusoid, I'm using the fundamental frequency. So 523 and the phase is set at uh zero. OK. So let's take a look at this. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1104s",
        "start_time": "1104.42"
    },
    {
        "id": "1842ba29",
        "text": "uh in order to, to move from the frequent, from the time domain to the frequency domain, we compare the signal with sinusoids of various frequencies. And then for each frequency, we get a magnitude and the phase. OK. And the and high magnitude indicates high similarity between the signal and a sinusoid. Now, let's see how this actually applies here visually. OK. So what I'm doing here is I'm plotting uh both the original signal as well as a uh sinusoid here and the sinusoid is going to be shown in red. OK. And here for the sinusoid, I'm using the fundamental frequency. So 523 and the phase is set at uh zero. OK. So let's take a look at this. OK. That's really cool. But as you can see, we, I mean, like the two waveforms like more or less like seem to have like a similar uh like cycle, right? So similar frequency. And that's because obviously the original signal has the fundamental which is at 523.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1123s",
        "start_time": "1123.979"
    },
    {
        "id": "636cc86c",
        "text": "uh both the original signal as well as a uh sinusoid here and the sinusoid is going to be shown in red. OK. And here for the sinusoid, I'm using the fundamental frequency. So 523 and the phase is set at uh zero. OK. So let's take a look at this. OK. That's really cool. But as you can see, we, I mean, like the two waveforms like more or less like seem to have like a similar uh like cycle, right? So similar frequency. And that's because obviously the original signal has the fundamental which is at 523. Uh but there are somewhat out of phase. So we want to align this. So how do we do that? Well, we can just uh increase the phase and see what happens. So let me redo this and as you can see now we are kind of align the two and I know because I've tried it before that we have a very good alignment when we use a face, which is equal to N 0.55 55. And here we go.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1153s",
        "start_time": "1153.3"
    },
    {
        "id": "cbe98959",
        "text": "That's really cool. But as you can see, we, I mean, like the two waveforms like more or less like seem to have like a similar uh like cycle, right? So similar frequency. And that's because obviously the original signal has the fundamental which is at 523. Uh but there are somewhat out of phase. So we want to align this. So how do we do that? Well, we can just uh increase the phase and see what happens. So let me redo this and as you can see now we are kind of align the two and I know because I've tried it before that we have a very good alignment when we use a face, which is equal to N 0.55 55. And here we go. So as you can see now, the two signals are quite uh like aligned, they're quite similar. But here we are talking about similarities. So how do we calculate similarity? Well,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1176s",
        "start_time": "1176.01"
    },
    {
        "id": "0ab61039",
        "text": "Uh but there are somewhat out of phase. So we want to align this. So how do we do that? Well, we can just uh increase the phase and see what happens. So let me redo this and as you can see now we are kind of align the two and I know because I've tried it before that we have a very good alignment when we use a face, which is equal to N 0.55 55. And here we go. So as you can see now, the two signals are quite uh like aligned, they're quite similar. But here we are talking about similarities. So how do we calculate similarity? Well, we have many ways of calculating similarities for two functions But one that's like very intuitive and simple is to combine the two signals by multiplying them. And in this case, it would be a multiplication sample by sample. So point by point we multiply the two signals and then we just look at the area. So we add all the positive areas and we feel like the, the area below",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1197s",
        "start_time": "1197.239"
    },
    {
        "id": "3e777801",
        "text": "So as you can see now, the two signals are quite uh like aligned, they're quite similar. But here we are talking about similarities. So how do we calculate similarity? Well, we have many ways of calculating similarities for two functions But one that's like very intuitive and simple is to combine the two signals by multiplying them. And in this case, it would be a multiplication sample by sample. So point by point we multiply the two signals and then we just look at the area. So we add all the positive areas and we feel like the, the area below uh the um the new combined signal and we add the positive area and we subtract the negative area. So let me explain what that is. And for doing that, we just add",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1223s",
        "start_time": "1223.579"
    },
    {
        "id": "acbb371f",
        "text": "we have many ways of calculating similarities for two functions But one that's like very intuitive and simple is to combine the two signals by multiplying them. And in this case, it would be a multiplication sample by sample. So point by point we multiply the two signals and then we just look at the area. So we add all the positive areas and we feel like the, the area below uh the um the new combined signal and we add the positive area and we subtract the negative area. So let me explain what that is. And for doing that, we just add this line of code here which I already had prepared. So, and as you can see here on the Y axis, we are now multiplying the sign um signal with the original signal and we'll have the color of this combined signal in yellow. So let's see",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1239s",
        "start_time": "1239.05"
    },
    {
        "id": "6aada71b",
        "text": "uh the um the new combined signal and we add the positive area and we subtract the negative area. So let me explain what that is. And for doing that, we just add this line of code here which I already had prepared. So, and as you can see here on the Y axis, we are now multiplying the sign um signal with the original signal and we'll have the color of this combined signal in yellow. So let's see what happens here. OK. And you can see this now, I'm taking the area. In other words, I'm filling up the area below um the, the waveform. And as you can see here, we have basically, it's all like positive area.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1267s",
        "start_time": "1267.28"
    },
    {
        "id": "172194b9",
        "text": "this line of code here which I already had prepared. So, and as you can see here on the Y axis, we are now multiplying the sign um signal with the original signal and we'll have the color of this combined signal in yellow. So let's see what happens here. OK. And you can see this now, I'm taking the area. In other words, I'm filling up the area below um the, the waveform. And as you can see here, we have basically, it's all like positive area. And the intuition here is that the higher the area, the positive area that we have for the combined signal and the higher the similarity between the two original, the two signals. So the Sinusoid, as well as the original signal",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1280s",
        "start_time": "1280.64"
    },
    {
        "id": "40ea29db",
        "text": "what happens here. OK. And you can see this now, I'm taking the area. In other words, I'm filling up the area below um the, the waveform. And as you can see here, we have basically, it's all like positive area. And the intuition here is that the higher the area, the positive area that we have for the combined signal and the higher the similarity between the two original, the two signals. So the Sinusoid, as well as the original signal and the intuition behind this is that if I, when I multiply both of this and they have the same sign, so they are both. So the signal and the uh Sinusoid have both like positive sign or both negative sign, then",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1299s",
        "start_time": "1299.67"
    },
    {
        "id": "c0aa5104",
        "text": "And the intuition here is that the higher the area, the positive area that we have for the combined signal and the higher the similarity between the two original, the two signals. So the Sinusoid, as well as the original signal and the intuition behind this is that if I, when I multiply both of this and they have the same sign, so they are both. So the signal and the uh Sinusoid have both like positive sign or both negative sign, then they're gonna end up with like a positive value, right? Whereas uh if they have an alternate opposite science, then we'll end up like with negative values. And so when we add like that positive area to the negative area, we just end up with a uh value that tells us how much similar these two signals are really are",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1318s",
        "start_time": "1318.5"
    },
    {
        "id": "e6589553",
        "text": "and the intuition behind this is that if I, when I multiply both of this and they have the same sign, so they are both. So the signal and the uh Sinusoid have both like positive sign or both negative sign, then they're gonna end up with like a positive value, right? Whereas uh if they have an alternate opposite science, then we'll end up like with negative values. And so when we add like that positive area to the negative area, we just end up with a uh value that tells us how much similar these two signals are really are OK. But uh one thing that I want to show you here is that if I change the phase here, so let's assume we go down to zero. So phase is equal to zero. Now all of a sudden we have all negative areas, right? And so there's no similarity at all, even if we have the same",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1338s",
        "start_time": "1338.9"
    },
    {
        "id": "27ef917c",
        "text": "they're gonna end up with like a positive value, right? Whereas uh if they have an alternate opposite science, then we'll end up like with negative values. And so when we add like that positive area to the negative area, we just end up with a uh value that tells us how much similar these two signals are really are OK. But uh one thing that I want to show you here is that if I change the phase here, so let's assume we go down to zero. So phase is equal to zero. Now all of a sudden we have all negative areas, right? And so there's no similarity at all, even if we have the same sine wave. So with the with the same frequency and amplitude, but still the phase is extremely important. But because in this case, uh is just there's no similarity whatsoever. So we only have like negative areas here, negative values for the combined um sine wave, not sine wave for the combined signal. OK. So if we go and say take a 0.2 for example,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1358s",
        "start_time": "1358.0"
    },
    {
        "id": "51a16f37",
        "text": "OK. But uh one thing that I want to show you here is that if I change the phase here, so let's assume we go down to zero. So phase is equal to zero. Now all of a sudden we have all negative areas, right? And so there's no similarity at all, even if we have the same sine wave. So with the with the same frequency and amplitude, but still the phase is extremely important. But because in this case, uh is just there's no similarity whatsoever. So we only have like negative areas here, negative values for the combined um sine wave, not sine wave for the combined signal. OK. So if we go and say take a 0.2 for example, uh yeah, still this is mostly like negative but now let's try 0.4. OK. Yeah, this is a good example. So which phase at no 0.4 we have uh a bit of negative areas as well as a bit of positive areas. So yeah, it's a little bit of mixed bag. There's some level of like similarity between the two signals, but it's not uh perfect. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1383s",
        "start_time": "1383.869"
    },
    {
        "id": "42a81154",
        "text": "sine wave. So with the with the same frequency and amplitude, but still the phase is extremely important. But because in this case, uh is just there's no similarity whatsoever. So we only have like negative areas here, negative values for the combined um sine wave, not sine wave for the combined signal. OK. So if we go and say take a 0.2 for example, uh yeah, still this is mostly like negative but now let's try 0.4. OK. Yeah, this is a good example. So which phase at no 0.4 we have uh a bit of negative areas as well as a bit of positive areas. So yeah, it's a little bit of mixed bag. There's some level of like similarity between the two signals, but it's not uh perfect. OK. So this is the intuition that we can use to understand how we move from uh the time domain to the frequency domain. And it's just",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1405s",
        "start_time": "1405.55"
    },
    {
        "id": "f7fb12a6",
        "text": "uh yeah, still this is mostly like negative but now let's try 0.4. OK. Yeah, this is a good example. So which phase at no 0.4 we have uh a bit of negative areas as well as a bit of positive areas. So yeah, it's a little bit of mixed bag. There's some level of like similarity between the two signals, but it's not uh perfect. OK. So this is the intuition that we can use to understand how we move from uh the time domain to the frequency domain. And it's just as simple as",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1435s",
        "start_time": "1435.099"
    },
    {
        "id": "790f1e52",
        "text": "So this is the intuition that we can use to understand how we move from uh the time domain to the frequency domain. And it's just as simple as doing a bunch of things. So we choose a frequency first and then we create a sine wave out of like that frequency. Then the next step is we want to optimize for the face so that we know that we've chosen the phase that gives the maximum similarity.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1461s",
        "start_time": "1461.14"
    },
    {
        "id": "c3b9fd81",
        "text": "as simple as doing a bunch of things. So we choose a frequency first and then we create a sine wave out of like that frequency. Then the next step is we want to optimize for the face so that we know that we've chosen the phase that gives the maximum similarity. So the maximum area possible. And finally, we calculate the magnitude and the magnitude is basically the similarity or in other words, the positive area of the combined signal uh minus the negative area. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1475s",
        "start_time": "1475.4"
    },
    {
        "id": "7bd44572",
        "text": "doing a bunch of things. So we choose a frequency first and then we create a sine wave out of like that frequency. Then the next step is we want to optimize for the face so that we know that we've chosen the phase that gives the maximum similarity. So the maximum area possible. And finally, we calculate the magnitude and the magnitude is basically the similarity or in other words, the positive area of the combined signal uh minus the negative area. OK. But this is just for one single frequency in a fourier transform, we have to try and do this for all possible frequencies in the real set. OK. So we start with a frequency, then we, we continue and do like the same steps for the next frequency and we move on and on and on and on.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1477s",
        "start_time": "1477.29"
    },
    {
        "id": "52d22051",
        "text": "So the maximum area possible. And finally, we calculate the magnitude and the magnitude is basically the similarity or in other words, the positive area of the combined signal uh minus the negative area. OK. But this is just for one single frequency in a fourier transform, we have to try and do this for all possible frequencies in the real set. OK. So we start with a frequency, then we, we continue and do like the same steps for the next frequency and we move on and on and on and on. And by doing so we are able to see which frequencies have a, an important are important components of the original signals. And those will be um kind of indicated by because they have a high value for the magnitude. And this is like what happens here. So if we go back here,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1500s",
        "start_time": "1500.06"
    },
    {
        "id": "abdcb170",
        "text": "But this is just for one single frequency in a fourier transform, we have to try and do this for all possible frequencies in the real set. OK. So we start with a frequency, then we, we continue and do like the same steps for the next frequency and we move on and on and on and on. And by doing so we are able to see which frequencies have a, an important are important components of the original signals. And those will be um kind of indicated by because they have a high value for the magnitude. And this is like what happens here. So if we go back here, so here we see like the the spectrum once again. So yeah, let me just find, let's just go back to, to the best uh phase that I could find. So 0.55. So as you can see here, uh the fact that we have like this nice alignment between the frequency at the sage at 523 Hertz and the original signal is reflected in the power spectrum here.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1522s",
        "start_time": "1522.4"
    },
    {
        "id": "6a3b99e5",
        "text": "And by doing so we are able to see which frequencies have a, an important are important components of the original signals. And those will be um kind of indicated by because they have a high value for the magnitude. And this is like what happens here. So if we go back here, so here we see like the the spectrum once again. So yeah, let me just find, let's just go back to, to the best uh phase that I could find. So 0.55. So as you can see here, uh the fact that we have like this nice alignment between the frequency at the sage at 523 Hertz and the original signal is reflected in the power spectrum here. And that's because we have this peak in the magnitude which is around 523 Hertz. So that tells us that we have this high level of similarity between the sine wave at the pure to at 523 Hertz and the original signal.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1545s",
        "start_time": "1545.979"
    },
    {
        "id": "56fec6d0",
        "text": "so here we see like the the spectrum once again. So yeah, let me just find, let's just go back to, to the best uh phase that I could find. So 0.55. So as you can see here, uh the fact that we have like this nice alignment between the frequency at the sage at 523 Hertz and the original signal is reflected in the power spectrum here. And that's because we have this peak in the magnitude which is around 523 Hertz. So that tells us that we have this high level of similarity between the sine wave at the pure to at 523 Hertz and the original signal. OK. So this is uh interesting. So now I hope like you, you get the understanding and the intuition behind the fourier transform. Now, the next step is to take a look at a little bit of mathematical formalization of what we've said so far and don't be scared about like this massive creation here.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1573s",
        "start_time": "1573.63"
    },
    {
        "id": "56f6a770",
        "text": "And that's because we have this peak in the magnitude which is around 523 Hertz. So that tells us that we have this high level of similarity between the sine wave at the pure to at 523 Hertz and the original signal. OK. So this is uh interesting. So now I hope like you, you get the understanding and the intuition behind the fourier transform. Now, the next step is to take a look at a little bit of mathematical formalization of what we've said so far and don't be scared about like this massive creation here. So I'll just break it down for you. But in, in a nutshell, this is what we've just said. So what we are looking for here is the phase for like the optimized phase for a given frequency F OK. So",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1599s",
        "start_time": "1599.959"
    },
    {
        "id": "ed3309b2",
        "text": "OK. So this is uh interesting. So now I hope like you, you get the understanding and the intuition behind the fourier transform. Now, the next step is to take a look at a little bit of mathematical formalization of what we've said so far and don't be scared about like this massive creation here. So I'll just break it down for you. But in, in a nutshell, this is what we've just said. So what we are looking for here is the phase for like the optimized phase for a given frequency F OK. So these two things that we have in here is just the multiplication of the signal and the sinusoid. So SFT is just our original signal. And here we have our sinusoid and we multiply those.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1622s",
        "start_time": "1622.079"
    },
    {
        "id": "132ebae8",
        "text": "So I'll just break it down for you. But in, in a nutshell, this is what we've just said. So what we are looking for here is the phase for like the optimized phase for a given frequency F OK. So these two things that we have in here is just the multiplication of the signal and the sinusoid. So SFT is just our original signal. And here we have our sinusoid and we multiply those. Now the next step is to calculate the area and we do that with this integral symbol. Now you don't need to understand calculus. But the intuition here is that we are basically calculating the",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1645s",
        "start_time": "1645.01"
    },
    {
        "id": "d532d1f1",
        "text": "these two things that we have in here is just the multiplication of the signal and the sinusoid. So SFT is just our original signal. And here we have our sinusoid and we multiply those. Now the next step is to calculate the area and we do that with this integral symbol. Now you don't need to understand calculus. But the intuition here is that we are basically calculating the uh positive area of the combined signal and then subtracted that's the negative area. And that's what we are doing with the integral, which is across time. So it's basically we, we take like all uh the, the, the time throughout like the uh a given signal.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1664s",
        "start_time": "1664.199"
    },
    {
        "id": "3a97a928",
        "text": "Now the next step is to calculate the area and we do that with this integral symbol. Now you don't need to understand calculus. But the intuition here is that we are basically calculating the uh positive area of the combined signal and then subtracted that's the negative area. And that's what we are doing with the integral, which is across time. So it's basically we, we take like all uh the, the, the time throughout like the uh a given signal. And finally, what we do is we take the arc marks. In other words, we want to select the phase in the interval 01 and we show why we only consider 01. And so we want to take like this phase that maximizes the area. And in other words, what we are doing here is what I was doing manually before tweaking the phase in order to find the phase that would actually,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1677s",
        "start_time": "1677.91"
    },
    {
        "id": "e5cd972d",
        "text": "uh positive area of the combined signal and then subtracted that's the negative area. And that's what we are doing with the integral, which is across time. So it's basically we, we take like all uh the, the, the time throughout like the uh a given signal. And finally, what we do is we take the arc marks. In other words, we want to select the phase in the interval 01 and we show why we only consider 01. And so we want to take like this phase that maximizes the area. And in other words, what we are doing here is what I was doing manually before tweaking the phase in order to find the phase that would actually, and maximize the positive area that we have when we combine the two signals. OK. So it's this is simple intuition to understand,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1697s",
        "start_time": "1697.18"
    },
    {
        "id": "4b8d0e28",
        "text": "And finally, what we do is we take the arc marks. In other words, we want to select the phase in the interval 01 and we show why we only consider 01. And so we want to take like this phase that maximizes the area. And in other words, what we are doing here is what I was doing manually before tweaking the phase in order to find the phase that would actually, and maximize the positive area that we have when we combine the two signals. OK. So it's this is simple intuition to understand, OK. The next step that we want to do is to actually take the um magnitude. And so how do we take the magnitude? Well, it's basically the same thing, right? So we are once again calculating the, combining the two signals and then we calculate the area uh that we have once we multiply the original signal with the synod.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1717s",
        "start_time": "1717.599"
    },
    {
        "id": "aa4a1317",
        "text": "and maximize the positive area that we have when we combine the two signals. OK. So it's this is simple intuition to understand, OK. The next step that we want to do is to actually take the um magnitude. And so how do we take the magnitude? Well, it's basically the same thing, right? So we are once again calculating the, combining the two signals and then we calculate the area uh that we have once we multiply the original signal with the synod. And finally, this is the only, the only step that's different from the, the previous one that we, so uh we just want to select the max area. And uh for selecting that we are going to try like all the possible like uh values for a phase between zero and one. But if we have that value already, because we've optimized before we'll just use that value and plug it in here directly.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1746s",
        "start_time": "1746.42"
    },
    {
        "id": "c98b0c8f",
        "text": "OK. The next step that we want to do is to actually take the um magnitude. And so how do we take the magnitude? Well, it's basically the same thing, right? So we are once again calculating the, combining the two signals and then we calculate the area uh that we have once we multiply the original signal with the synod. And finally, this is the only, the only step that's different from the, the previous one that we, so uh we just want to select the max area. And uh for selecting that we are going to try like all the possible like uh values for a phase between zero and one. But if we have that value already, because we've optimized before we'll just use that value and plug it in here directly. It's important to say that the calculations that we've carried out here are in a continuous space. In other words, uh because we use this integral here, we are basically assuming that time is continuous or in other words, uh like time is a real number, it changes as a real number. OK. So time is continuous, but frequency is continuous as well, right?",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1758s",
        "start_time": "1758.92"
    },
    {
        "id": "ab937f4f",
        "text": "And finally, this is the only, the only step that's different from the, the previous one that we, so uh we just want to select the max area. And uh for selecting that we are going to try like all the possible like uh values for a phase between zero and one. But if we have that value already, because we've optimized before we'll just use that value and plug it in here directly. It's important to say that the calculations that we've carried out here are in a continuous space. In other words, uh because we use this integral here, we are basically assuming that time is continuous or in other words, uh like time is a real number, it changes as a real number. OK. So time is continuous, but frequency is continuous as well, right? So while this is the case for the, the theory, well, what we've done already, for example, uh with our code uh doesn't uh use like continuous representation. Uh Just because uh computers can't use continuous representation, we have to sample a continuous function and use a discrete version of that.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1785s",
        "start_time": "1785.43"
    },
    {
        "id": "73a34d71",
        "text": "It's important to say that the calculations that we've carried out here are in a continuous space. In other words, uh because we use this integral here, we are basically assuming that time is continuous or in other words, uh like time is a real number, it changes as a real number. OK. So time is continuous, but frequency is continuous as well, right? So while this is the case for the, the theory, well, what we've done already, for example, uh with our code uh doesn't uh use like continuous representation. Uh Just because uh computers can't use continuous representation, we have to sample a continuous function and use a discrete version of that. Now, uh if you want to know more about continuous versus um discrete uh signals, I suggest you to go check out one of my first videos on this topic. Uh But the whole point here that I want to make is that the theory that we're going to study at least like over the next couple of videos is going to be about continuous for a",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1815s",
        "start_time": "1815.75"
    },
    {
        "id": "276c8668",
        "text": "So while this is the case for the, the theory, well, what we've done already, for example, uh with our code uh doesn't uh use like continuous representation. Uh Just because uh computers can't use continuous representation, we have to sample a continuous function and use a discrete version of that. Now, uh if you want to know more about continuous versus um discrete uh signals, I suggest you to go check out one of my first videos on this topic. Uh But the whole point here that I want to make is that the theory that we're going to study at least like over the next couple of videos is going to be about continuous for a uh transform. But then we'll move on and take a look at the discrete fourier transform. And there the math is going to be slightly different. And uh that is going to be the math that we will actually be using for understanding how to implement an algorithm that can extract the fourier transform in the case of discrete signals. OK. So",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1844s",
        "start_time": "1844.14"
    },
    {
        "id": "e1dfd250",
        "text": "Now, uh if you want to know more about continuous versus um discrete uh signals, I suggest you to go check out one of my first videos on this topic. Uh But the whole point here that I want to make is that the theory that we're going to study at least like over the next couple of videos is going to be about continuous for a uh transform. But then we'll move on and take a look at the discrete fourier transform. And there the math is going to be slightly different. And uh that is going to be the math that we will actually be using for understanding how to implement an algorithm that can extract the fourier transform in the case of discrete signals. OK. So uh up until now you you've learned how to move from the time domain to the frequency domain. But you may be wondering is there a way of going the other way around? So the round trip, so we go from time",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1867s",
        "start_time": "1867.13"
    },
    {
        "id": "1e8b1e83",
        "text": "uh transform. But then we'll move on and take a look at the discrete fourier transform. And there the math is going to be slightly different. And uh that is going to be the math that we will actually be using for understanding how to implement an algorithm that can extract the fourier transform in the case of discrete signals. OK. So uh up until now you you've learned how to move from the time domain to the frequency domain. But you may be wondering is there a way of going the other way around? So the round trip, so we go from time to frequency domain but then can we go to frequency domain to time domain? And this would be the equivalent of saying OK. So now we start with these uh different like color bands, we put them into the prism, this fantastic algorithm and we end up with the,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1889s",
        "start_time": "1889.67"
    },
    {
        "id": "a3bc4439",
        "text": "uh up until now you you've learned how to move from the time domain to the frequency domain. But you may be wondering is there a way of going the other way around? So the round trip, so we go from time to frequency domain but then can we go to frequency domain to time domain? And this would be the equivalent of saying OK. So now we start with these uh different like color bands, we put them into the prism, this fantastic algorithm and we end up with the, with a complex light wave. OK. Can we do that with the fourier transform? And the answer is yes, we can reconstruct a signal by superimposing all the sinusoids, all the frequency components that we've extracted out of the original signal. So we superimpose the sinusoids and we weigh them by their relative magnitude.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1911s",
        "start_time": "1911.949"
    },
    {
        "id": "a13d73f3",
        "text": "to frequency domain but then can we go to frequency domain to time domain? And this would be the equivalent of saying OK. So now we start with these uh different like color bands, we put them into the prism, this fantastic algorithm and we end up with the, with a complex light wave. OK. Can we do that with the fourier transform? And the answer is yes, we can reconstruct a signal by superimposing all the sinusoids, all the frequency components that we've extracted out of the original signal. So we superimpose the sinusoids and we weigh them by their relative magnitude. So the ones that uh have a higher magnitude are gonna have like a higher impact in making the original signal. And then obviously, we want to use the uh the original, the phase that we extracted as well. And the point here is that the original signal and the fourier transform have the same information and we can go back and forth from time to frequency domain as we please. So",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1928s",
        "start_time": "1928.26"
    },
    {
        "id": "d333d923",
        "text": "with a complex light wave. OK. Can we do that with the fourier transform? And the answer is yes, we can reconstruct a signal by superimposing all the sinusoids, all the frequency components that we've extracted out of the original signal. So we superimpose the sinusoids and we weigh them by their relative magnitude. So the ones that uh have a higher magnitude are gonna have like a higher impact in making the original signal. And then obviously, we want to use the uh the original, the phase that we extracted as well. And the point here is that the original signal and the fourier transform have the same information and we can go back and forth from time to frequency domain as we please. So and the process of going back from frequency domain to time domain is called inverse fourier transform. So we start from the frequency domain, we apply an inverse fourier transform and we go all the way back to the original signal and the time domain. OK. So",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1947s",
        "start_time": "1947.359"
    },
    {
        "id": "e5d179ac",
        "text": "So the ones that uh have a higher magnitude are gonna have like a higher impact in making the original signal. And then obviously, we want to use the uh the original, the phase that we extracted as well. And the point here is that the original signal and the fourier transform have the same information and we can go back and forth from time to frequency domain as we please. So and the process of going back from frequency domain to time domain is called inverse fourier transform. So we start from the frequency domain, we apply an inverse fourier transform and we go all the way back to the original signal and the time domain. OK. So this is like an idea that's not exactly the same as that of adjective synthesis, but it's kind of like similar. So in adjective synthesis, you have synthesizers which basically use different sinusoids and you can superimpose them. So combined by adding them up um in order to create complex sounds. And if you think about it, this is somewhat similar to what we were saying with the inverse fourier transform.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=1975s",
        "start_time": "1975.839"
    },
    {
        "id": "83a3efe6",
        "text": "and the process of going back from frequency domain to time domain is called inverse fourier transform. So we start from the frequency domain, we apply an inverse fourier transform and we go all the way back to the original signal and the time domain. OK. So this is like an idea that's not exactly the same as that of adjective synthesis, but it's kind of like similar. So in adjective synthesis, you have synthesizers which basically use different sinusoids and you can superimpose them. So combined by adding them up um in order to create complex sounds. And if you think about it, this is somewhat similar to what we were saying with the inverse fourier transform. So let me show you these in a an example. So I found uh like this very cool uh website. So where you can try out adjective sensor. So here like you have uh a bunch of different frequencies. So here this is like C four. So tw uh 261 Hertz. Then we have 523 Hertz, which is C five, which is the fundamental frequency of R uh sound. And then",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2004s",
        "start_time": "2004.689"
    },
    {
        "id": "53660dc3",
        "text": "this is like an idea that's not exactly the same as that of adjective synthesis, but it's kind of like similar. So in adjective synthesis, you have synthesizers which basically use different sinusoids and you can superimpose them. So combined by adding them up um in order to create complex sounds. And if you think about it, this is somewhat similar to what we were saying with the inverse fourier transform. So let me show you these in a an example. So I found uh like this very cool uh website. So where you can try out adjective sensor. So here like you have uh a bunch of different frequencies. So here this is like C four. So tw uh 261 Hertz. Then we have 523 Hertz, which is C five, which is the fundamental frequency of R uh sound. And then you go up. So basically here you have like all the overturns of these C four fundamental frequency. OK. So let's unmute this. So you should be able to, yeah, let me",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2027s",
        "start_time": "2027.14"
    },
    {
        "id": "8be3215f",
        "text": "So let me show you these in a an example. So I found uh like this very cool uh website. So where you can try out adjective sensor. So here like you have uh a bunch of different frequencies. So here this is like C four. So tw uh 261 Hertz. Then we have 523 Hertz, which is C five, which is the fundamental frequency of R uh sound. And then you go up. So basically here you have like all the overturns of these C four fundamental frequency. OK. So let's unmute this. So you should be able to, yeah, let me put this down a little bit. OK. So you should be able to, to hear this C four, right? So now let me add",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2056s",
        "start_time": "2056.81"
    },
    {
        "id": "c8de66e6",
        "text": "you go up. So basically here you have like all the overturns of these C four fundamental frequency. OK. So let's unmute this. So you should be able to, yeah, let me put this down a little bit. OK. So you should be able to, to hear this C four, right? So now let me add uh the first overturn and take a look at how the main signal is gonna be modified,",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2085s",
        "start_time": "2085.28"
    },
    {
        "id": "a6375c14",
        "text": "put this down a little bit. OK. So you should be able to, to hear this C four, right? So now let me add uh the first overturn and take a look at how the main signal is gonna be modified, right?",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2098s",
        "start_time": "2098.879"
    },
    {
        "id": "23a4413a",
        "text": "uh the first overturn and take a look at how the main signal is gonna be modified, right? OK. So let me add some other overtones.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2108s",
        "start_time": "2108.419"
    },
    {
        "id": "9a3378d7",
        "text": "right? OK. So let me add some other overtones. Yeah, let's just go down a little bit.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2119s",
        "start_time": "2119.0"
    },
    {
        "id": "4798ce0e",
        "text": "OK. So let me add some other overtones. Yeah, let's just go down a little bit. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2120s",
        "start_time": "2120.969"
    },
    {
        "id": "c1939e03",
        "text": "Yeah, let's just go down a little bit. OK. That's cool. Isn't it",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2136s",
        "start_time": "2136.34"
    },
    {
        "id": "c553ce69",
        "text": "OK. That's cool. Isn't it cool? So this is like a really cool example of how we can superimpose PP turns or syn sides and get a complex waveform like this one. OK.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2141s",
        "start_time": "2141.149"
    },
    {
        "id": "fa5f0695",
        "text": "That's cool. Isn't it cool? So this is like a really cool example of how we can superimpose PP turns or syn sides and get a complex waveform like this one. OK. So yeah, I guess like this was it for this introductory session on a fourier transform. So now where do we go from here? Well, we want to start looking into the math, the real math behind the fourier transform. And for doing that, we are gonna need to get familiar with an",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2142s",
        "start_time": "2142.959"
    },
    {
        "id": "6dd77c41",
        "text": "cool? So this is like a really cool example of how we can superimpose PP turns or syn sides and get a complex waveform like this one. OK. So yeah, I guess like this was it for this introductory session on a fourier transform. So now where do we go from here? Well, we want to start looking into the math, the real math behind the fourier transform. And for doing that, we are gonna need to get familiar with an another type of numbers that perhaps you've heard or you're familiar with. But if you're not, I suggest you check out my next video which is going to be on complex numbers. And as we'll see you in a few videos, complex numbers are going to be very important for working with fourier transforms because they're very handy for representing what we are doing here.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2150s",
        "start_time": "2150.8"
    },
    {
        "id": "ac75bc6b",
        "text": "So yeah, I guess like this was it for this introductory session on a fourier transform. So now where do we go from here? Well, we want to start looking into the math, the real math behind the fourier transform. And for doing that, we are gonna need to get familiar with an another type of numbers that perhaps you've heard or you're familiar with. But if you're not, I suggest you check out my next video which is going to be on complex numbers. And as we'll see you in a few videos, complex numbers are going to be very important for working with fourier transforms because they're very handy for representing what we are doing here. OK? So that's all for today. I hope you enjoyed this video and you found it useful. If that's the case, please remember to leave a like. And if you haven't subscribed to the channel, please consider doing so. If you have any questions or doubts in your mind about what I've covered today, please don't hesitate to leave a comment in the comment section below. I guess I'll see you next time. Cheers.",
        "video": "Demystifying the Fourier Transform: The Intuition",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "XQ45IgG6rJ4",
        "youtube_link": "https://www.youtube.com/watch?v=XQ45IgG6rJ4&t=2166s",
        "start_time": "2166.33"
    },
    {
        "id": "4a38757f",
        "text": "Hi, everybody and welcome to a new exciting video in the audio signal processing for machine learning series. This time, we'll look into frequency domain audio features specifically. We'll be looking at the math and theory behind it. And I'm also gonna give you the intuition. But before we get started, I want to remind you about the sound of A I is Luck community, which is a community of people who are interested in all things A I audio music and signal processing. So you can join, get feedback on your projects and network with very cool people. So if you haven't joined yet, I highly suggest you to go check out the sign up link in the description box below. But now let's move on to the cool stuff. So in the last couple of videos, we looked into mal frequency subs coefficients or MF CCS. And now it's time to move on to frequency domain audio features and we can do this because now if you followed along with the series,",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "34f57e75",
        "text": "I want to remind you about the sound of A I is Luck community, which is a community of people who are interested in all things A I audio music and signal processing. So you can join, get feedback on your projects and network with very cool people. So if you haven't joined yet, I highly suggest you to go check out the sign up link in the description box below. But now let's move on to the cool stuff. So in the last couple of videos, we looked into mal frequency subs coefficients or MF CCS. And now it's time to move on to frequency domain audio features and we can do this because now if you followed along with the series, uh we have a deep knowledge of fourier transform, short term fourier transform. And so we know how to move to the frequency domain today, we'll be looking into three frequency domain audio features specifically, these are the band energy ratio, the spectral centr and the bandwidth. I just want to say that there are a lot more frequency domain features. But for today, we'll just be focusing on these ones. OK. So",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=20s",
        "start_time": "20.01"
    },
    {
        "id": "6cd2b97c",
        "text": "in the description box below. But now let's move on to the cool stuff. So in the last couple of videos, we looked into mal frequency subs coefficients or MF CCS. And now it's time to move on to frequency domain audio features and we can do this because now if you followed along with the series, uh we have a deep knowledge of fourier transform, short term fourier transform. And so we know how to move to the frequency domain today, we'll be looking into three frequency domain audio features specifically, these are the band energy ratio, the spectral centr and the bandwidth. I just want to say that there are a lot more frequency domain features. But for today, we'll just be focusing on these ones. OK. So um I just want to remind you about how we can extract frequent frequency domain uh features in a nutshell in a very simplistic way. What we do is we start from the waveform, then we apply a short time fourier transform and so that we get a spectrogram. And then at this point, we can compute a feature",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=39s",
        "start_time": "39.689"
    },
    {
        "id": "494f0030",
        "text": "uh we have a deep knowledge of fourier transform, short term fourier transform. And so we know how to move to the frequency domain today, we'll be looking into three frequency domain audio features specifically, these are the band energy ratio, the spectral centr and the bandwidth. I just want to say that there are a lot more frequency domain features. But for today, we'll just be focusing on these ones. OK. So um I just want to remind you about how we can extract frequent frequency domain uh features in a nutshell in a very simplistic way. What we do is we start from the waveform, then we apply a short time fourier transform and so that we get a spectrogram. And then at this point, we can compute a feature computation so that we can arrive at a feature. Now, as I said, this is a simplification of the whole process and I have a whole video that details how to extract audio features in general and the frequency domain audio features specifically. So I highly suggest you to go check out that video. Then another thing that is gonna be a",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=59s",
        "start_time": "59.59"
    },
    {
        "id": "1b20e6cf",
        "text": "um I just want to remind you about how we can extract frequent frequency domain uh features in a nutshell in a very simplistic way. What we do is we start from the waveform, then we apply a short time fourier transform and so that we get a spectrogram. And then at this point, we can compute a feature computation so that we can arrive at a feature. Now, as I said, this is a simplification of the whole process and I have a whole video that details how to extract audio features in general and the frequency domain audio features specifically. So I highly suggest you to go check out that video. Then another thing that is gonna be a required for this video if you want to really understand it is the understanding of short time fourier transform and the understanding of what a spectrogram is. Now, once again, I have a bunch of videos on that. So just go check them out in this series. I just want to share a couple of math conventions that we'll be using during the video. So the first one is this MTON",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=86s",
        "start_time": "86.889"
    },
    {
        "id": "9907f301",
        "text": "computation so that we can arrive at a feature. Now, as I said, this is a simplification of the whole process and I have a whole video that details how to extract audio features in general and the frequency domain audio features specifically. So I highly suggest you to go check out that video. Then another thing that is gonna be a required for this video if you want to really understand it is the understanding of short time fourier transform and the understanding of what a spectrogram is. Now, once again, I have a bunch of videos on that. So just go check them out in this series. I just want to share a couple of math conventions that we'll be using during the video. So the first one is this MTON and this will stand for the magnitude of the signal at a given frequency bin and, and at a given frame T",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=108s",
        "start_time": "108.462"
    },
    {
        "id": "6fca785c",
        "text": "required for this video if you want to really understand it is the understanding of short time fourier transform and the understanding of what a spectrogram is. Now, once again, I have a bunch of videos on that. So just go check them out in this series. I just want to share a couple of math conventions that we'll be using during the video. So the first one is this MTON and this will stand for the magnitude of the signal at a given frequency bin and, and at a given frame T and then the other convention that we'll be using is that capital N is equal to the number of frequency bins that we have in the uh spectrogram. Let's jump to the first audio feature that's band energy ratio. And this feature provides us information about the relation between the energy in the lower and the higher frequency bands. So uh we can think of this as a measure of how dominant the lower frequency",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=130s",
        "start_time": "130.035"
    },
    {
        "id": "64ac8d72",
        "text": "and this will stand for the magnitude of the signal at a given frequency bin and, and at a given frame T and then the other convention that we'll be using is that capital N is equal to the number of frequency bins that we have in the uh spectrogram. Let's jump to the first audio feature that's band energy ratio. And this feature provides us information about the relation between the energy in the lower and the higher frequency bands. So uh we can think of this as a measure of how dominant the lower frequency are. Let's take a look at the math behind the band energy ratio so that we can understand how it actually works. So as you can see here, we have a formula and there's a fraction and the fraction needs to be expected because we are talking about a ratio of two elements, right? So now let's take a look at each of the items. Uh So here, both in the numerator and denominator, we have the power of the signal at time",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=151s",
        "start_time": "151.77"
    },
    {
        "id": "cac15de4",
        "text": "and then the other convention that we'll be using is that capital N is equal to the number of frequency bins that we have in the uh spectrogram. Let's jump to the first audio feature that's band energy ratio. And this feature provides us information about the relation between the energy in the lower and the higher frequency bands. So uh we can think of this as a measure of how dominant the lower frequency are. Let's take a look at the math behind the band energy ratio so that we can understand how it actually works. So as you can see here, we have a formula and there's a fraction and the fraction needs to be expected because we are talking about a ratio of two elements, right? So now let's take a look at each of the items. Uh So here, both in the numerator and denominator, we have the power of the signal at time uh T and frequency BN. And this is the power because we're talking about the um magnitude of the signal squared, which is the power as you should know by now. OK.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=162s",
        "start_time": "162.57"
    },
    {
        "id": "8f3443b3",
        "text": "are. Let's take a look at the math behind the band energy ratio so that we can understand how it actually works. So as you can see here, we have a formula and there's a fraction and the fraction needs to be expected because we are talking about a ratio of two elements, right? So now let's take a look at each of the items. Uh So here, both in the numerator and denominator, we have the power of the signal at time uh T and frequency BN. And this is the power because we're talking about the um magnitude of the signal squared, which is the power as you should know by now. OK. So uh the other thing that I want to draw your attention to is this capital F. OK? Because we are talking about, we, we are considering a couple of like uh sums here right? In the uh denominator, we sum from the frequency being F capital F to capital N. And then in",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=191s",
        "start_time": "191.49"
    },
    {
        "id": "c6fd88c1",
        "text": "uh T and frequency BN. And this is the power because we're talking about the um magnitude of the signal squared, which is the power as you should know by now. OK. So uh the other thing that I want to draw your attention to is this capital F. OK? Because we are talking about, we, we are considering a couple of like uh sums here right? In the uh denominator, we sum from the frequency being F capital F to capital N. And then in up here in the numerator, we are summing the powers from the first frequency being to F capital F minus one. OK. So that capital F is called the split frequency. So what's the split frequency? Well, it is that frequency that tells us or like provides the difference between the",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=220s",
        "start_time": "220.539"
    },
    {
        "id": "dbeb77e7",
        "text": "So uh the other thing that I want to draw your attention to is this capital F. OK? Because we are talking about, we, we are considering a couple of like uh sums here right? In the uh denominator, we sum from the frequency being F capital F to capital N. And then in up here in the numerator, we are summing the powers from the first frequency being to F capital F minus one. OK. So that capital F is called the split frequency. So what's the split frequency? Well, it is that frequency that tells us or like provides the difference between the lower frequencies and the higher frequencies. So now let me visualize this because it's gonna become very, very easy to understand. OK. So here we have like our usual spectrogram, we have time on the X axis and on the y axis we have frequency. Now the split frequency is just a horizontal line like this. So in this case, the",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=233s",
        "start_time": "233.029"
    },
    {
        "id": "4c342ae2",
        "text": "up here in the numerator, we are summing the powers from the first frequency being to F capital F minus one. OK. So that capital F is called the split frequency. So what's the split frequency? Well, it is that frequency that tells us or like provides the difference between the lower frequencies and the higher frequencies. So now let me visualize this because it's gonna become very, very easy to understand. OK. So here we have like our usual spectrogram, we have time on the X axis and on the y axis we have frequency. Now the split frequency is just a horizontal line like this. So in this case, the um split frequency is at 2000 Hertz. Now, so this is the the threshold below that frequency, we have lower frequencies and above that split frequency, we have higher frequencies. Now this",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=253s",
        "start_time": "253.102"
    },
    {
        "id": "afd5c656",
        "text": "lower frequencies and the higher frequencies. So now let me visualize this because it's gonna become very, very easy to understand. OK. So here we have like our usual spectrogram, we have time on the X axis and on the y axis we have frequency. Now the split frequency is just a horizontal line like this. So in this case, the um split frequency is at 2000 Hertz. Now, so this is the the threshold below that frequency, we have lower frequencies and above that split frequency, we have higher frequencies. Now this frequency is completely arbitrary. So you can put it wherever you want really. But uh so we can just like move it down here, for example, say around 800 Hertz, but a usual split frequency value is 2000 Hertz. It makes a lot of sense.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=273s",
        "start_time": "273.175"
    },
    {
        "id": "3508a0d7",
        "text": "um split frequency is at 2000 Hertz. Now, so this is the the threshold below that frequency, we have lower frequencies and above that split frequency, we have higher frequencies. Now this frequency is completely arbitrary. So you can put it wherever you want really. But uh so we can just like move it down here, for example, say around 800 Hertz, but a usual split frequency value is 2000 Hertz. It makes a lot of sense. OK. So now let's go back to the math here. And so with this idea in mind, we can recognize that in the lower part of this fraction. So at the numerator, we have uh the power in the lower frequency bands. And so how do we get to that? Well, we have a sum here. And we are",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=293s",
        "start_time": "293.559"
    },
    {
        "id": "7d304b45",
        "text": "frequency is completely arbitrary. So you can put it wherever you want really. But uh so we can just like move it down here, for example, say around 800 Hertz, but a usual split frequency value is 2000 Hertz. It makes a lot of sense. OK. So now let's go back to the math here. And so with this idea in mind, we can recognize that in the lower part of this fraction. So at the numerator, we have uh the power in the lower frequency bands. And so how do we get to that? Well, we have a sum here. And we are summing the power for each frequency bin at a given point in time. And that is starting from the first frequency bin up to the split frequency minus one. So this is all the power that we have in the lower frequencies. OK.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=310s",
        "start_time": "310.075"
    },
    {
        "id": "25540846",
        "text": "OK. So now let's go back to the math here. And so with this idea in mind, we can recognize that in the lower part of this fraction. So at the numerator, we have uh the power in the lower frequency bands. And so how do we get to that? Well, we have a sum here. And we are summing the power for each frequency bin at a given point in time. And that is starting from the first frequency bin up to the split frequency minus one. So this is all the power that we have in the lower frequencies. OK. And at the denominator, we actually have the opposite of that. So we have the power in the higher frequency bands. And we can see that because of the indexes here in the sum. So we start at the split frequency and then we go all the way up to uh capital N which is the, the higher number, the higher frequency bin that we have.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=326s",
        "start_time": "326.91"
    },
    {
        "id": "a814e2c6",
        "text": "summing the power for each frequency bin at a given point in time. And that is starting from the first frequency bin up to the split frequency minus one. So this is all the power that we have in the lower frequencies. OK. And at the denominator, we actually have the opposite of that. So we have the power in the higher frequency bands. And we can see that because of the indexes here in the sum. So we start at the split frequency and then we go all the way up to uh capital N which is the, the higher number, the higher frequency bin that we have. OK. One thing that I want to stress here is that the B we are taking the band energy ratio at a specific frame. So in other words, here, once you have like all your frames in the spectrogram, you're gonna apply this uh formula to each frame to each point uh in time that you have in the spectrogram.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=347s",
        "start_time": "347.809"
    },
    {
        "id": "d745f7ab",
        "text": "And at the denominator, we actually have the opposite of that. So we have the power in the higher frequency bands. And we can see that because of the indexes here in the sum. So we start at the split frequency and then we go all the way up to uh capital N which is the, the higher number, the higher frequency bin that we have. OK. One thing that I want to stress here is that the B we are taking the band energy ratio at a specific frame. So in other words, here, once you have like all your frames in the spectrogram, you're gonna apply this uh formula to each frame to each point uh in time that you have in the spectrogram. OK? So this is the band's energy ratio. Now let's try to visualize this. So as we said here, we have that uh purple horizontal line that is split frequency. So now let's take a frame. So it's this rectangle red rectangle here, obviously, this is not a frame because we have a lot of frames in it. But let's assume this is just like a frame. OK? So now we take the,",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=368s",
        "start_time": "368.859"
    },
    {
        "id": "121788ab",
        "text": "OK. One thing that I want to stress here is that the B we are taking the band energy ratio at a specific frame. So in other words, here, once you have like all your frames in the spectrogram, you're gonna apply this uh formula to each frame to each point uh in time that you have in the spectrogram. OK? So this is the band's energy ratio. Now let's try to visualize this. So as we said here, we have that uh purple horizontal line that is split frequency. So now let's take a frame. So it's this rectangle red rectangle here, obviously, this is not a frame because we have a lot of frames in it. But let's assume this is just like a frame. OK? So now we take the, the power and we summit here in the um higher frequencies, we do the same thing like in the lower frequencies and then we just apply the uh a fraction ratio there. So we divide like this green power here by the uh the power here like in this uh blue bar and that is the band energy ratio.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=391s",
        "start_time": "391.25"
    },
    {
        "id": "2fe1222f",
        "text": "OK? So this is the band's energy ratio. Now let's try to visualize this. So as we said here, we have that uh purple horizontal line that is split frequency. So now let's take a frame. So it's this rectangle red rectangle here, obviously, this is not a frame because we have a lot of frames in it. But let's assume this is just like a frame. OK? So now we take the, the power and we summit here in the um higher frequencies, we do the same thing like in the lower frequencies and then we just apply the uh a fraction ratio there. So we divide like this green power here by the uh the power here like in this uh blue bar and that is the band energy ratio. Ok. So what can we use the band to energy ratio for? Well, we can use it for all sorts of things uh in music and speech processing and specifically band energy ratio has been extensively used in to discriminate music from speech and for certain music classification problems like music genre classification or mood uh classification. OK.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=414s",
        "start_time": "414.019"
    },
    {
        "id": "b95d9a60",
        "text": "the power and we summit here in the um higher frequencies, we do the same thing like in the lower frequencies and then we just apply the uh a fraction ratio there. So we divide like this green power here by the uh the power here like in this uh blue bar and that is the band energy ratio. Ok. So what can we use the band to energy ratio for? Well, we can use it for all sorts of things uh in music and speech processing and specifically band energy ratio has been extensively used in to discriminate music from speech and for certain music classification problems like music genre classification or mood uh classification. OK. Now let's move on to the second uh audio feature which is the spectral Centroid and this is a very common famous one I should say, right?",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=438s",
        "start_time": "438.589"
    },
    {
        "id": "8b9e39ff",
        "text": "Ok. So what can we use the band to energy ratio for? Well, we can use it for all sorts of things uh in music and speech processing and specifically band energy ratio has been extensively used in to discriminate music from speech and for certain music classification problems like music genre classification or mood uh classification. OK. Now let's move on to the second uh audio feature which is the spectral Centroid and this is a very common famous one I should say, right? And so the spectral Centroid in a nutshell. So the intuition is that it it's gonna tell us it's gonna provide us the center of gravity of the magnitude spectrum. In other words, it'll give us the frequency band where we have uh most of the energy concentrated, right?",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=463s",
        "start_time": "463.38"
    },
    {
        "id": "ae80d943",
        "text": "Now let's move on to the second uh audio feature which is the spectral Centroid and this is a very common famous one I should say, right? And so the spectral Centroid in a nutshell. So the intuition is that it it's gonna tell us it's gonna provide us the center of gravity of the magnitude spectrum. In other words, it'll give us the frequency band where we have uh most of the energy concentrated, right? And the cool thing about the spectral centro is that it uh nicely maps onto a very prominent timbrel feature which is brightness. So how open or dull a certain sound is. So now let's take a look at uh the the math behind the spectral Centroid. And before we actually look at the formalization like which uh mathematical symbols, let's take a look at the,",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=488s",
        "start_time": "488.97"
    },
    {
        "id": "925909f6",
        "text": "And so the spectral Centroid in a nutshell. So the intuition is that it it's gonna tell us it's gonna provide us the center of gravity of the magnitude spectrum. In other words, it'll give us the frequency band where we have uh most of the energy concentrated, right? And the cool thing about the spectral centro is that it uh nicely maps onto a very prominent timbrel feature which is brightness. So how open or dull a certain sound is. So now let's take a look at uh the the math behind the spectral Centroid. And before we actually look at the formalization like which uh mathematical symbols, let's take a look at the, the kind of like uh quality formation. So the spectral Centroid is the weighted min of the frequencies or the frequency bins if you will.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=500s",
        "start_time": "500.019"
    },
    {
        "id": "3390e157",
        "text": "And the cool thing about the spectral centro is that it uh nicely maps onto a very prominent timbrel feature which is brightness. So how open or dull a certain sound is. So now let's take a look at uh the the math behind the spectral Centroid. And before we actually look at the formalization like which uh mathematical symbols, let's take a look at the, the kind of like uh quality formation. So the spectral Centroid is the weighted min of the frequencies or the frequency bins if you will. So what are we talking about here? Well, this is like easier likes in math than, than said, right. And so this is like the, the, the formula for the spectral Centroid at a given frame T. So, and as you can see here, we have like all the usual stuff that we have in a weighted uh min. So here we have like the uh fre frequency bin N and then here we have the, the weights for N",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=520s",
        "start_time": "520.799"
    },
    {
        "id": "23e23146",
        "text": "the kind of like uh quality formation. So the spectral Centroid is the weighted min of the frequencies or the frequency bins if you will. So what are we talking about here? Well, this is like easier likes in math than, than said, right. And so this is like the, the, the formula for the spectral Centroid at a given frame T. So, and as you can see here, we have like all the usual stuff that we have in a weighted uh min. So here we have like the uh fre frequency bin N and then here we have the, the weights for N so and the weights obviously are not, they are like the magnitude for that frequency bin at that specifically at that specific uh frame T. OK.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=549s",
        "start_time": "549.51"
    },
    {
        "id": "e90fccde",
        "text": "So what are we talking about here? Well, this is like easier likes in math than, than said, right. And so this is like the, the, the formula for the spectral Centroid at a given frame T. So, and as you can see here, we have like all the usual stuff that we have in a weighted uh min. So here we have like the uh fre frequency bin N and then here we have the, the weights for N so and the weights obviously are not, they are like the magnitude for that frequency bin at that specifically at that specific uh frame T. OK. So we can also see that down here we have the sum weight. And yeah, as you can see here, we're talking about a weighted uh min and this is the weighted min of the frequency bins, right. OK. So",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=560s",
        "start_time": "560.84"
    },
    {
        "id": "eaf67dc4",
        "text": "so and the weights obviously are not, they are like the magnitude for that frequency bin at that specifically at that specific uh frame T. OK. So we can also see that down here we have the sum weight. And yeah, as you can see here, we're talking about a weighted uh min and this is the weighted min of the frequency bins, right. OK. So where can we use the spectral Centroid? Well, once again, uh the spectral Centroid has been extensively used in audio classification or in music classification uh problems. And it's one of like the key uh frequency domain audio features. And yeah, it's been like very, very extensively used throughout time and different applications.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=590s",
        "start_time": "590.289"
    },
    {
        "id": "795b7b30",
        "text": "So we can also see that down here we have the sum weight. And yeah, as you can see here, we're talking about a weighted uh min and this is the weighted min of the frequency bins, right. OK. So where can we use the spectral Centroid? Well, once again, uh the spectral Centroid has been extensively used in audio classification or in music classification uh problems. And it's one of like the key uh frequency domain audio features. And yeah, it's been like very, very extensively used throughout time and different applications. OK. So now let's move on to the bandwidth. So the bandwidth is somewhat uh related to the uh spectral Centroid. And we can think of the bandwidth as that spectral range which is of interest and it's around the Centroid. Or in other words, we can think",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=606s",
        "start_time": "606.229"
    },
    {
        "id": "d4656c3d",
        "text": "where can we use the spectral Centroid? Well, once again, uh the spectral Centroid has been extensively used in audio classification or in music classification uh problems. And it's one of like the key uh frequency domain audio features. And yeah, it's been like very, very extensively used throughout time and different applications. OK. So now let's move on to the bandwidth. So the bandwidth is somewhat uh related to the uh spectral Centroid. And we can think of the bandwidth as that spectral range which is of interest and it's around the Centroid. Or in other words, we can think the bandwidth as the variant from the spectral Centroid. Once again, the bandwidth has a direct relation or a correlation with the perceived timer. Now let's take a look at the formalization here. So the bandwidth once again is a way",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=623s",
        "start_time": "623.229"
    },
    {
        "id": "fb090e15",
        "text": "OK. So now let's move on to the bandwidth. So the bandwidth is somewhat uh related to the uh spectral Centroid. And we can think of the bandwidth as that spectral range which is of interest and it's around the Centroid. Or in other words, we can think the bandwidth as the variant from the spectral Centroid. Once again, the bandwidth has a direct relation or a correlation with the perceived timer. Now let's take a look at the formalization here. So the bandwidth once again is a way mean but this time it's not a weighted mean of the frequencies but rather weighted mean of the distances of frequency band from the spectral Centroid. OK. I know this can sound a little bit uh yeah difficult complex. So now let's take a look at the math and understand that at the end of it today, it's quite simple. So here we have the formula for the bandwidth at a specific frame T OK.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=646s",
        "start_time": "646.5"
    },
    {
        "id": "0a01b9dc",
        "text": "the bandwidth as the variant from the spectral Centroid. Once again, the bandwidth has a direct relation or a correlation with the perceived timer. Now let's take a look at the formalization here. So the bandwidth once again is a way mean but this time it's not a weighted mean of the frequencies but rather weighted mean of the distances of frequency band from the spectral Centroid. OK. I know this can sound a little bit uh yeah difficult complex. So now let's take a look at the math and understand that at the end of it today, it's quite simple. So here we have the formula for the bandwidth at a specific frame T OK. And as you can see, we are once again talking about a weighted M here. So here we have the weights and the weights as always are just like the, the, the magnitude for the signal at the specific time T we are analyzing at end at the frequency band. And then we have like this element here",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=666s",
        "start_time": "666.882"
    },
    {
        "id": "2c4eb3f8",
        "text": "mean but this time it's not a weighted mean of the frequencies but rather weighted mean of the distances of frequency band from the spectral Centroid. OK. I know this can sound a little bit uh yeah difficult complex. So now let's take a look at the math and understand that at the end of it today, it's quite simple. So here we have the formula for the bandwidth at a specific frame T OK. And as you can see, we are once again talking about a weighted M here. So here we have the weights and the weights as always are just like the, the, the magnitude for the signal at the specific time T we are analyzing at end at the frequency band. And then we have like this element here and this element is indeed the distance of the frequency band from the spectral Centroid. And we can like easily see it. This is like the absolute value uh between the frequency band uh N minus the uh spectral Centroid cal the value of the spectral Centroid calculated at time T OK.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=687s",
        "start_time": "687.265"
    },
    {
        "id": "c58d5e97",
        "text": "And as you can see, we are once again talking about a weighted M here. So here we have the weights and the weights as always are just like the, the, the magnitude for the signal at the specific time T we are analyzing at end at the frequency band. And then we have like this element here and this element is indeed the distance of the frequency band from the spectral Centroid. And we can like easily see it. This is like the absolute value uh between the frequency band uh N minus the uh spectral Centroid cal the value of the spectral Centroid calculated at time T OK. And down here, once again, we have the sum of weights. And as you can see, this is once again a weighted min.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=716s",
        "start_time": "716.969"
    },
    {
        "id": "e549393a",
        "text": "and this element is indeed the distance of the frequency band from the spectral Centroid. And we can like easily see it. This is like the absolute value uh between the frequency band uh N minus the uh spectral Centroid cal the value of the spectral Centroid calculated at time T OK. And down here, once again, we have the sum of weights. And as you can see, this is once again a weighted min. OK. But now let's try to understand how the bandwidth like um changes depending on how the energy is uh distributed across all the different frequency bands. So, so if the energy is spread across the frequency bands, so it's kind of, yeah, it's just like spread, then uh what happens is",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=743s",
        "start_time": "743.049"
    },
    {
        "id": "b4d5f8eb",
        "text": "And down here, once again, we have the sum of weights. And as you can see, this is once again a weighted min. OK. But now let's try to understand how the bandwidth like um changes depending on how the energy is uh distributed across all the different frequency bands. So, so if the energy is spread across the frequency bands, so it's kind of, yeah, it's just like spread, then uh what happens is that the the bandwidth is going to the value for the bandwidth is going to increase. On the other hand, if the energy is kind of concentrated in a in a kind of like small frequency band in just like a few frequency bands, then what happens is that the bandwidth is going to go, the value for the bandwidth",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=768s",
        "start_time": "768.659"
    },
    {
        "id": "417c1a4f",
        "text": "OK. But now let's try to understand how the bandwidth like um changes depending on how the energy is uh distributed across all the different frequency bands. So, so if the energy is spread across the frequency bands, so it's kind of, yeah, it's just like spread, then uh what happens is that the the bandwidth is going to the value for the bandwidth is going to increase. On the other hand, if the energy is kind of concentrated in a in a kind of like small frequency band in just like a few frequency bands, then what happens is that the bandwidth is going to go, the value for the bandwidth is going to go down. And as you can see here, this is somewhat correlated with the idea of variance, right. So if the energy is spread across the spectrogram across the different sorry across the different frequency bands, then we have like a higher Varian for the energy. And if it's not, if it's just concentrated, we then have like a AAA",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=776s",
        "start_time": "776.489"
    },
    {
        "id": "7b0df642",
        "text": "that the the bandwidth is going to the value for the bandwidth is going to increase. On the other hand, if the energy is kind of concentrated in a in a kind of like small frequency band in just like a few frequency bands, then what happens is that the bandwidth is going to go, the value for the bandwidth is going to go down. And as you can see here, this is somewhat correlated with the idea of variance, right. So if the energy is spread across the spectrogram across the different sorry across the different frequency bands, then we have like a higher Varian for the energy. And if it's not, if it's just concentrated, we then have like a AAA low uh variant. And now uh the bandwidth can also be called spectral spread. So this is just another way of calling uh bandwidth. And you can now understand why that's the case, right? Because spectral spread is yeah, that idea of like where, how much the energy is spread across the frequency bands.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=799s",
        "start_time": "799.724"
    },
    {
        "id": "ae189d74",
        "text": "is going to go down. And as you can see here, this is somewhat correlated with the idea of variance, right. So if the energy is spread across the spectrogram across the different sorry across the different frequency bands, then we have like a higher Varian for the energy. And if it's not, if it's just concentrated, we then have like a AAA low uh variant. And now uh the bandwidth can also be called spectral spread. So this is just another way of calling uh bandwidth. And you can now understand why that's the case, right? Because spectral spread is yeah, that idea of like where, how much the energy is spread across the frequency bands. OK. So let's take a look at the applications of the uh bandwidth. Well, uh bandwidth just like the banter and duration spectral centric has been extensively used for music processing, for example, in problems like music, genre, classification of music, mood uh classification. Now all of these features we've talked about today",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=822s",
        "start_time": "822.96"
    },
    {
        "id": "5c6618fd",
        "text": "low uh variant. And now uh the bandwidth can also be called spectral spread. So this is just another way of calling uh bandwidth. And you can now understand why that's the case, right? Because spectral spread is yeah, that idea of like where, how much the energy is spread across the frequency bands. OK. So let's take a look at the applications of the uh bandwidth. Well, uh bandwidth just like the banter and duration spectral centric has been extensively used for music processing, for example, in problems like music, genre, classification of music, mood uh classification. Now all of these features we've talked about today uh as I said, have been uh used quite a lot uh during like the traditional machine learning era, right, where like we were using like knowledge uh engineering, like for coming up with uh interesting and significant um audio features.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=846s",
        "start_time": "846.195"
    },
    {
        "id": "7ae55a62",
        "text": "OK. So let's take a look at the applications of the uh bandwidth. Well, uh bandwidth just like the banter and duration spectral centric has been extensively used for music processing, for example, in problems like music, genre, classification of music, mood uh classification. Now all of these features we've talked about today uh as I said, have been uh used quite a lot uh during like the traditional machine learning era, right, where like we were using like knowledge uh engineering, like for coming up with uh interesting and significant um audio features. And so yeah, all of these frequency domain audio features have been extensively used like in that period now, they are little bit less. So because like moving to deep learning uh applications, well, we tend to use um audio features that are more role like like spectrograms, mouse spectrograms or even just waveforms.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=869s",
        "start_time": "869.57"
    },
    {
        "id": "4b1b5cad",
        "text": "uh as I said, have been uh used quite a lot uh during like the traditional machine learning era, right, where like we were using like knowledge uh engineering, like for coming up with uh interesting and significant um audio features. And so yeah, all of these frequency domain audio features have been extensively used like in that period now, they are little bit less. So because like moving to deep learning uh applications, well, we tend to use um audio features that are more role like like spectrograms, mouse spectrograms or even just waveforms. OK. So by now, you should have a good understanding of this basic uh frequency domain audio features. Uh As I said, there are way more than this. But now with this idea in mind and with this basic understanding, you can go there and you can easily understand all the other ones that are out there. So what's up next? Well, we talked about the theory behind this uh frequency domain audio features in this video. Next,",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=895s",
        "start_time": "895.239"
    },
    {
        "id": "02d64d81",
        "text": "And so yeah, all of these frequency domain audio features have been extensively used like in that period now, they are little bit less. So because like moving to deep learning uh applications, well, we tend to use um audio features that are more role like like spectrograms, mouse spectrograms or even just waveforms. OK. So by now, you should have a good understanding of this basic uh frequency domain audio features. Uh As I said, there are way more than this. But now with this idea in mind and with this basic understanding, you can go there and you can easily understand all the other ones that are out there. So what's up next? Well, we talked about the theory behind this uh frequency domain audio features in this video. Next, we're gonna be implementing uh one of these audio features from scratch and that's the band energy ratio and we use Python for doing that as we've done throughout this series. And then we're going to be visualizing the uh band energy ratio for pieces of music in different genres and try to understand if we can tell them apart based only on the band energy uh ratio.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=915s",
        "start_time": "915.325"
    },
    {
        "id": "656cc019",
        "text": "OK. So by now, you should have a good understanding of this basic uh frequency domain audio features. Uh As I said, there are way more than this. But now with this idea in mind and with this basic understanding, you can go there and you can easily understand all the other ones that are out there. So what's up next? Well, we talked about the theory behind this uh frequency domain audio features in this video. Next, we're gonna be implementing uh one of these audio features from scratch and that's the band energy ratio and we use Python for doing that as we've done throughout this series. And then we're going to be visualizing the uh band energy ratio for pieces of music in different genres and try to understand if we can tell them apart based only on the band energy uh ratio. Ok, I hope you enjoyed the video. If that's the case, please remember to leave a like if you haven't subscribed and you want to watch more videos like this consider uh doing so. If you have any questions as usual, please leave them in the comment section below. That's all for today. I'll see you next time. Cheers.",
        "video": "Frequency-Domain Audio Features",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3-bjAoAxQ9o",
        "youtube_link": "https://www.youtube.com/watch?v=3-bjAoAxQ9o&t=935s",
        "start_time": "935.729"
    },
    {
        "id": "ac227d1a",
        "text": "Hi, everybody and welcome to a new exciting video series called audio signal processing for machine learning. Many of you guys have asked me to dig deeper into audio digital signal processing and so here you have a whole series on that. In this video, I'm gonna give you a quick overview of the series, the different content, the things that you learn, the perquisites and the resources. Now, what's the problem? Why do we need this series? So the main issue that probably most deep learning engineers know is that when it comes time to work on deep learning applications for images, that's not that much of an issue because we have a lot of resources that explain how you can process image a road data and make it viable for deep learning and machine learning models. But that's not necessarily the case for audio, there's a sort of mist around audio data and how you should use it for",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=0s",
        "start_time": "0.1"
    },
    {
        "id": "a67c1c3d",
        "text": "Many of you guys have asked me to dig deeper into audio digital signal processing and so here you have a whole series on that. In this video, I'm gonna give you a quick overview of the series, the different content, the things that you learn, the perquisites and the resources. Now, what's the problem? Why do we need this series? So the main issue that probably most deep learning engineers know is that when it comes time to work on deep learning applications for images, that's not that much of an issue because we have a lot of resources that explain how you can process image a road data and make it viable for deep learning and machine learning models. But that's not necessarily the case for audio, there's a sort of mist around audio data and how you should use it for deep learning applications. And so this is why you're getting like this series. Now, when we talk about audio um A I applications, we can divide two stages here. So one is the development and the evaluation of models and this is a part that I covered in another series that I have on my channel that's called Deep learning for Rodeo uh in Python.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=7s",
        "start_time": "7.429"
    },
    {
        "id": "3ff8998f",
        "text": "when it comes time to work on deep learning applications for images, that's not that much of an issue because we have a lot of resources that explain how you can process image a road data and make it viable for deep learning and machine learning models. But that's not necessarily the case for audio, there's a sort of mist around audio data and how you should use it for deep learning applications. And so this is why you're getting like this series. Now, when we talk about audio um A I applications, we can divide two stages here. So one is the development and the evaluation of models and this is a part that I covered in another series that I have on my channel that's called Deep learning for Rodeo uh in Python. And you should find it over here in case you want to check that out. And then there's the other level which is that of preparing the audio row data in order to uh make it viable for injections in the models. Now, I have",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=33s",
        "start_time": "33.745"
    },
    {
        "id": "b5d14de2",
        "text": "deep learning applications. And so this is why you're getting like this series. Now, when we talk about audio um A I applications, we can divide two stages here. So one is the development and the evaluation of models and this is a part that I covered in another series that I have on my channel that's called Deep learning for Rodeo uh in Python. And you should find it over here in case you want to check that out. And then there's the other level which is that of preparing the audio row data in order to uh make it viable for injections in the models. Now, I have a couple of videos in that series that I've just mentioned on deep learning for all year where I talk about audio preprocessing and all your features and all these sorts of things. But I realized that that wasn't really enough. And many of you guys have asked me to dig deeper. OK.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=60s",
        "start_time": "60.062"
    },
    {
        "id": "7bcf730c",
        "text": "And you should find it over here in case you want to check that out. And then there's the other level which is that of preparing the audio row data in order to uh make it viable for injections in the models. Now, I have a couple of videos in that series that I've just mentioned on deep learning for all year where I talk about audio preprocessing and all your features and all these sorts of things. But I realized that that wasn't really enough. And many of you guys have asked me to dig deeper. OK. So now the question is uh so where do we use audio, digital signal processing for machine learning and for um deep learning specifically? Well, there are a bunch of applications in A I audio where we use uh audio signal processing.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=87s",
        "start_time": "87.019"
    },
    {
        "id": "970932c9",
        "text": "a couple of videos in that series that I've just mentioned on deep learning for all year where I talk about audio preprocessing and all your features and all these sorts of things. But I realized that that wasn't really enough. And many of you guys have asked me to dig deeper. OK. So now the question is uh so where do we use audio, digital signal processing for machine learning and for um deep learning specifically? Well, there are a bunch of applications in A I audio where we use uh audio signal processing. So obviously, you have all sorts of audio classification of problems, then speech recognition, speaker verification, uh speaker diar organization, for example, and then audio de noising audio up sampling. And if you are a music type of guy, there's a whole field",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=102s",
        "start_time": "102.449"
    },
    {
        "id": "9c5f7161",
        "text": "So now the question is uh so where do we use audio, digital signal processing for machine learning and for um deep learning specifically? Well, there are a bunch of applications in A I audio where we use uh audio signal processing. So obviously, you have all sorts of audio classification of problems, then speech recognition, speaker verification, uh speaker diar organization, for example, and then audio de noising audio up sampling. And if you are a music type of guy, there's a whole field that's called music information retrieval that uses uh tools from digital signal processing along with machine learning to uh crack certain problems like music instruments, uh identification or music mood and a genre classification.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=119s",
        "start_time": "119.29"
    },
    {
        "id": "d868fbc2",
        "text": "So obviously, you have all sorts of audio classification of problems, then speech recognition, speaker verification, uh speaker diar organization, for example, and then audio de noising audio up sampling. And if you are a music type of guy, there's a whole field that's called music information retrieval that uses uh tools from digital signal processing along with machine learning to uh crack certain problems like music instruments, uh identification or music mood and a genre classification. And there's a bunch bunch more of those? Cool. OK. So what are we gonna cover in uh this series? So it's a lot of stuff really and it's not set uh on the stone yet. So I'll, I, I'm open to, to get feedback from you guys on like what topics like to cover during the, the process like of this uh series. But",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=138s",
        "start_time": "138.33"
    },
    {
        "id": "ce0cf69f",
        "text": "that's called music information retrieval that uses uh tools from digital signal processing along with machine learning to uh crack certain problems like music instruments, uh identification or music mood and a genre classification. And there's a bunch bunch more of those? Cool. OK. So what are we gonna cover in uh this series? So it's a lot of stuff really and it's not set uh on the stone yet. So I'll, I, I'm open to, to get feedback from you guys on like what topics like to cover during the, the process like of this uh series. But for sure, I'm gonna cover a sound waves, digital to analog converters, analog to digital converters. And then I'll jump into audio features and we'll take a look at time and frequency domain audio features like R MS spectral Centroid MF CCS. Then we're gonna also look at a bunch of very important audio transformations.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=156s",
        "start_time": "156.339"
    },
    {
        "id": "fff1901c",
        "text": "And there's a bunch bunch more of those? Cool. OK. So what are we gonna cover in uh this series? So it's a lot of stuff really and it's not set uh on the stone yet. So I'll, I, I'm open to, to get feedback from you guys on like what topics like to cover during the, the process like of this uh series. But for sure, I'm gonna cover a sound waves, digital to analog converters, analog to digital converters. And then I'll jump into audio features and we'll take a look at time and frequency domain audio features like R MS spectral Centroid MF CCS. Then we're gonna also look at a bunch of very important audio transformations. We'll take a look at the fourier transform, the short time fourier transform that leads to spectrograms. Then we'll compare that against other transformations like the constant to transform the male spectrograms and chromo grams of",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=174s",
        "start_time": "174.779"
    },
    {
        "id": "938b4509",
        "text": "for sure, I'm gonna cover a sound waves, digital to analog converters, analog to digital converters. And then I'll jump into audio features and we'll take a look at time and frequency domain audio features like R MS spectral Centroid MF CCS. Then we're gonna also look at a bunch of very important audio transformations. We'll take a look at the fourier transform, the short time fourier transform that leads to spectrograms. Then we'll compare that against other transformations like the constant to transform the male spectrograms and chromo grams of top of that. We're gonna also take a look at um topics in audio and music perception which we can leverage to preprocess the audio data in a way that makes sense for the current problem that we're trying to solve. OK. So what should you expect from this series",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=198s",
        "start_time": "198.91"
    },
    {
        "id": "e967805c",
        "text": "We'll take a look at the fourier transform, the short time fourier transform that leads to spectrograms. Then we'll compare that against other transformations like the constant to transform the male spectrograms and chromo grams of top of that. We're gonna also take a look at um topics in audio and music perception which we can leverage to preprocess the audio data in a way that makes sense for the current problem that we're trying to solve. OK. So what should you expect from this series if you usually follow the sound of the I channel, you know that I love to cover both theoretical stuff and uh implementation stuff. So this series is gonna be no different. So we're gonna have theoretical sessions where I dig deeper into the theoretical ideas behind the the stuff that we are uh discussing and then we're gonna have coding uh sessions where I implement all the theoretical stuff that we've discussed.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=223s",
        "start_time": "223.27"
    },
    {
        "id": "199af168",
        "text": "top of that. We're gonna also take a look at um topics in audio and music perception which we can leverage to preprocess the audio data in a way that makes sense for the current problem that we're trying to solve. OK. So what should you expect from this series if you usually follow the sound of the I channel, you know that I love to cover both theoretical stuff and uh implementation stuff. So this series is gonna be no different. So we're gonna have theoretical sessions where I dig deeper into the theoretical ideas behind the the stuff that we are uh discussing and then we're gonna have coding uh sessions where I implement all the theoretical stuff that we've discussed. Now, you may be wondering, but where do I get all the material that you'll be posting with these videos? Well, I have a github uh page that's linked in the uh descript description section below and there you can find the code samples as well as the slides just to, to have all the material with yourself for review.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=238s",
        "start_time": "238.744"
    },
    {
        "id": "7a33b388",
        "text": "if you usually follow the sound of the I channel, you know that I love to cover both theoretical stuff and uh implementation stuff. So this series is gonna be no different. So we're gonna have theoretical sessions where I dig deeper into the theoretical ideas behind the the stuff that we are uh discussing and then we're gonna have coding uh sessions where I implement all the theoretical stuff that we've discussed. Now, you may be wondering, but where do I get all the material that you'll be posting with these videos? Well, I have a github uh page that's linked in the uh descript description section below and there you can find the code samples as well as the slides just to, to have all the material with yourself for review. So uh if you're familiar with my channel, you know that I literally love Python and it's not, this shouldn't come as a surprise to you that throughout the series, I'm gonna be using Python. And then on top of that, I'm gonna be using li browser, which is an open source audio processing libraries that we can use to extract loads of all your features in a very handy way.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=260s",
        "start_time": "260.609"
    },
    {
        "id": "361145c0",
        "text": "Now, you may be wondering, but where do I get all the material that you'll be posting with these videos? Well, I have a github uh page that's linked in the uh descript description section below and there you can find the code samples as well as the slides just to, to have all the material with yourself for review. So uh if you're familiar with my channel, you know that I literally love Python and it's not, this shouldn't come as a surprise to you that throughout the series, I'm gonna be using Python. And then on top of that, I'm gonna be using li browser, which is an open source audio processing libraries that we can use to extract loads of all your features in a very handy way. OK. So what will you learn from an operational standpoint? First of all, you're gonna get a deep dive into all your data so that you really know what you are talking about there and how to manipulate and preprocess all of this data. Then obviously you'll familiarize with um frequency and time domain or",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=290s",
        "start_time": "290.579"
    },
    {
        "id": "eac51322",
        "text": "So uh if you're familiar with my channel, you know that I literally love Python and it's not, this shouldn't come as a surprise to you that throughout the series, I'm gonna be using Python. And then on top of that, I'm gonna be using li browser, which is an open source audio processing libraries that we can use to extract loads of all your features in a very handy way. OK. So what will you learn from an operational standpoint? First of all, you're gonna get a deep dive into all your data so that you really know what you are talking about there and how to manipulate and preprocess all of this data. Then obviously you'll familiarize with um frequency and time domain or features and you're gonna be able to extract these features from rare audio. Most importantly, you recognize what are your audio features to use in your audio ML applications. So what makes the most sense for different types of applications? And along with that throughout this series, we're gonna, I'm gonna show you how to preprocess all your data and make it ready for your uh deep learning applications.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=312s",
        "start_time": "312.529"
    },
    {
        "id": "4a97d927",
        "text": "OK. So what will you learn from an operational standpoint? First of all, you're gonna get a deep dive into all your data so that you really know what you are talking about there and how to manipulate and preprocess all of this data. Then obviously you'll familiarize with um frequency and time domain or features and you're gonna be able to extract these features from rare audio. Most importantly, you recognize what are your audio features to use in your audio ML applications. So what makes the most sense for different types of applications? And along with that throughout this series, we're gonna, I'm gonna show you how to preprocess all your data and make it ready for your uh deep learning applications. OK? And then there's uh a thing that's very dear to me. So I'm gonna cover a little bit of math uh behind all the audio transformations that we're gonna uh touch upon. And I think that's very important for you to understand that so that, you know, really dd down what uh like audio features are and how we can extract them and how you can uh",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=340s",
        "start_time": "340.47"
    },
    {
        "id": "f3c186b0",
        "text": "features and you're gonna be able to extract these features from rare audio. Most importantly, you recognize what are your audio features to use in your audio ML applications. So what makes the most sense for different types of applications? And along with that throughout this series, we're gonna, I'm gonna show you how to preprocess all your data and make it ready for your uh deep learning applications. OK? And then there's uh a thing that's very dear to me. So I'm gonna cover a little bit of math uh behind all the audio transformations that we're gonna uh touch upon. And I think that's very important for you to understand that so that, you know, really dd down what uh like audio features are and how we can extract them and how you can uh basically uh trick the parameters for extracting those features in a way that makes the most sense for your problem. And finally, on top of that, obviously, you're gonna be able to use a lib browser efficiently so that you can extract all the features that you need for your audio ML projects.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=360s",
        "start_time": "360.204"
    },
    {
        "id": "8f0dd970",
        "text": "OK? And then there's uh a thing that's very dear to me. So I'm gonna cover a little bit of math uh behind all the audio transformations that we're gonna uh touch upon. And I think that's very important for you to understand that so that, you know, really dd down what uh like audio features are and how we can extract them and how you can uh basically uh trick the parameters for extracting those features in a way that makes the most sense for your problem. And finally, on top of that, obviously, you're gonna be able to use a lib browser efficiently so that you can extract all the features that you need for your audio ML projects. But mainly",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=390s",
        "start_time": "390.119"
    },
    {
        "id": "76d8ba3c",
        "text": "basically uh trick the parameters for extracting those features in a way that makes the most sense for your problem. And finally, on top of that, obviously, you're gonna be able to use a lib browser efficiently so that you can extract all the features that you need for your audio ML projects. But mainly uh the success of this series is gonna be measured against this thing. So the moment you'll see a spectrogram like this, you just don't freak out but rather know what we're talking about and what this image is actually telling you all your wise and you're gonna be able to interpret it good.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=416s",
        "start_time": "416.649"
    },
    {
        "id": "0678116c",
        "text": "But mainly uh the success of this series is gonna be measured against this thing. So the moment you'll see a spectrogram like this, you just don't freak out but rather know what we're talking about and what this image is actually telling you all your wise and you're gonna be able to interpret it good. Who's this series for? Well, if you are a machine learning or more specifically deep learning engineer and you're tapping your feet into the audio domain, this is a perfect series for you. Same thing. If you are a computer science student,",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=436s",
        "start_time": "436.54"
    },
    {
        "id": "ea8022ee",
        "text": "uh the success of this series is gonna be measured against this thing. So the moment you'll see a spectrogram like this, you just don't freak out but rather know what we're talking about and what this image is actually telling you all your wise and you're gonna be able to interpret it good. Who's this series for? Well, if you are a machine learning or more specifically deep learning engineer and you're tapping your feet into the audio domain, this is a perfect series for you. Same thing. If you are a computer science student, I've received a ton of requests from CS students who have asked me, how can I uh preprocess audio data for this specific audio A I application? Well, you're going to get most of those answers here in this um series. Now, if you are a software engineer with an interest in audio and music, again, this is uh a series that's for you.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=438s",
        "start_time": "438.329"
    },
    {
        "id": "f8468ebb",
        "text": "Who's this series for? Well, if you are a machine learning or more specifically deep learning engineer and you're tapping your feet into the audio domain, this is a perfect series for you. Same thing. If you are a computer science student, I've received a ton of requests from CS students who have asked me, how can I uh preprocess audio data for this specific audio A I application? Well, you're going to get most of those answers here in this um series. Now, if you are a software engineer with an interest in audio and music, again, this is uh a series that's for you. And of course, if you are a music technologist or a tech oriented musician who wants to dig deeper into uh audio and computation, again, this is an ideal series for you. Great. So obviously, this is not gonna be a series for uh beginners, Python, uh beginners rather you should have intermediate Python skills in order to follow the coding um sessions.",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=461s",
        "start_time": "461.25"
    },
    {
        "id": "52eec228",
        "text": "I've received a ton of requests from CS students who have asked me, how can I uh preprocess audio data for this specific audio A I application? Well, you're going to get most of those answers here in this um series. Now, if you are a software engineer with an interest in audio and music, again, this is uh a series that's for you. And of course, if you are a music technologist or a tech oriented musician who wants to dig deeper into uh audio and computation, again, this is an ideal series for you. Great. So obviously, this is not gonna be a series for uh beginners, Python, uh beginners rather you should have intermediate Python skills in order to follow the coding um sessions. And finally, I invite you to join the Sound of A I Slack community. So why should you do that? Because there you'll find a growing community of like minded people who are interested in A I music A I audio, audio and music processing. And so you can",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=475s",
        "start_time": "475.329"
    },
    {
        "id": "fc5b578b",
        "text": "And of course, if you are a music technologist or a tech oriented musician who wants to dig deeper into uh audio and computation, again, this is an ideal series for you. Great. So obviously, this is not gonna be a series for uh beginners, Python, uh beginners rather you should have intermediate Python skills in order to follow the coding um sessions. And finally, I invite you to join the Sound of A I Slack community. So why should you do that? Because there you'll find a growing community of like minded people who are interested in A I music A I audio, audio and music processing. And so you can really ask a bunch of questions and grow your understanding of the topic while uh networking with a lot of like cool and knowledgeable people. Good. So I'll leave the link to the sound of the eyes lack workspace below in the description. Just go check that out and sign up. OK. So this was, was all for today. I'm looking forward to starting this journey with you and I hope you'll join in",
        "video": "Audio Signal Processing for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "iCwMQJnKk2c",
        "youtube_link": "https://www.youtube.com/watch?v=iCwMQJnKk2c&t=499s",
        "start_time": "499.41"
    },
    {
        "id": "022c3ad7",
        "text": "Hi, everybody and welcome to a new video in the audio signal processing for machine learning series. This time we'll pick up where we left last time where we talked about fourier transform and arrived at the point where we said, well, we need to know about complex numbers for moving forward and understand fourier transform even better than just like the intuition level. So this time, we'll look into the fundamental concepts behind numbers. And all of this information is going to be very useful for you to understand the deeper concepts behind uh audio signal processing. But before we get started, I want to remind you once again about the sound of the Ice Luck community, which is a community with people interested in all things A I music A and all your signal processing. So you can go there, hang out with very interesting people and uh network with them, ask for feedback and just share your ideas. So if you're interested and you're not signed up yet, just go check out the sign up link to the workspace Slack workspace in the description below. But now let's move on to the topic of today's video. The complex numbers. So why should we butter with complex numbers in the first place? Well,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "8b5e5771",
        "text": "numbers. And all of this information is going to be very useful for you to understand the deeper concepts behind uh audio signal processing. But before we get started, I want to remind you once again about the sound of the Ice Luck community, which is a community with people interested in all things A I music A and all your signal processing. So you can go there, hang out with very interesting people and uh network with them, ask for feedback and just share your ideas. So if you're interested and you're not signed up yet, just go check out the sign up link to the workspace Slack workspace in the description below. But now let's move on to the topic of today's video. The complex numbers. So why should we butter with complex numbers in the first place? Well, aren't real numbers are good enough. Well, it turns out that when we are dealing with the fourier transform, if you remember from my previous video, we usually get a couple of parameters for each of the um pure turns pure frequencies that we decompose a complex sound into. And this two para",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=27s",
        "start_time": "27.565"
    },
    {
        "id": "347881a9",
        "text": "and uh network with them, ask for feedback and just share your ideas. So if you're interested and you're not signed up yet, just go check out the sign up link to the workspace Slack workspace in the description below. But now let's move on to the topic of today's video. The complex numbers. So why should we butter with complex numbers in the first place? Well, aren't real numbers are good enough. Well, it turns out that when we are dealing with the fourier transform, if you remember from my previous video, we usually get a couple of parameters for each of the um pure turns pure frequencies that we decompose a complex sound into. And this two para are the magnitude and phase. Now magnitude is a real number. And if we are just interested into looking at the um magnitude spectrum of a complex sounds, well, we really don't need complex numbers at all. But if you are interested to have like a deeper understanding of the full four transform and we want to factor in all the phase,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=55s",
        "start_time": "55.27"
    },
    {
        "id": "d1a23335",
        "text": "aren't real numbers are good enough. Well, it turns out that when we are dealing with the fourier transform, if you remember from my previous video, we usually get a couple of parameters for each of the um pure turns pure frequencies that we decompose a complex sound into. And this two para are the magnitude and phase. Now magnitude is a real number. And if we are just interested into looking at the um magnitude spectrum of a complex sounds, well, we really don't need complex numbers at all. But if you are interested to have like a deeper understanding of the full four transform and we want to factor in all the phase, now we have a problem and that's because we don't have a compact way of dealing with these two parameters at once. So magnitude and phase. And so wouldn't it be wonderful if we had something that we could use to deal in a handy way, both with magnitude and phase with both of these parameters at once? Well,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=80s",
        "start_time": "80.847"
    },
    {
        "id": "5525eab2",
        "text": "are the magnitude and phase. Now magnitude is a real number. And if we are just interested into looking at the um magnitude spectrum of a complex sounds, well, we really don't need complex numbers at all. But if you are interested to have like a deeper understanding of the full four transform and we want to factor in all the phase, now we have a problem and that's because we don't have a compact way of dealing with these two parameters at once. So magnitude and phase. And so wouldn't it be wonderful if we had something that we could use to deal in a handy way, both with magnitude and phase with both of these parameters at once? Well, it turns out that we have that, that math can provide it with that and it's",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=106s",
        "start_time": "106.424"
    },
    {
        "id": "d24ef569",
        "text": "now we have a problem and that's because we don't have a compact way of dealing with these two parameters at once. So magnitude and phase. And so wouldn't it be wonderful if we had something that we could use to deal in a handy way, both with magnitude and phase with both of these parameters at once? Well, it turns out that we have that, that math can provide it with that and it's complicated. Uh Yeah. No, it's not really complicated numbers. Well, it could be complicated the topic in itself, but it's not complicated numbers. It's actually complex numbers. And uh as I said, this topic can be a little bit uh complicated, but I'll try to make it as easy and accessible as possible for you in today's video. OK. So",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=132s",
        "start_time": "132.001"
    },
    {
        "id": "53eec23d",
        "text": "it turns out that we have that, that math can provide it with that and it's complicated. Uh Yeah. No, it's not really complicated numbers. Well, it could be complicated the topic in itself, but it's not complicated numbers. It's actually complex numbers. And uh as I said, this topic can be a little bit uh complicated, but I'll try to make it as easy and accessible as possible for you in today's video. OK. So what's the genesis of complex numbers? So why do we need them? Well, for a long time, mathematicians have been scared of a very simple thing",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=157s",
        "start_time": "157.74"
    },
    {
        "id": "0ea2b285",
        "text": "complicated. Uh Yeah. No, it's not really complicated numbers. Well, it could be complicated the topic in itself, but it's not complicated numbers. It's actually complex numbers. And uh as I said, this topic can be a little bit uh complicated, but I'll try to make it as easy and accessible as possible for you in today's video. OK. So what's the genesis of complex numbers? So why do we need them? Well, for a long time, mathematicians have been scared of a very simple thing and that's square root of minus one or more in general, the square root of negative numbers. So uh with real numbers, there's no way we can solve like this very simple thing like the square root of a negative number. So basically math is broken in the in terms",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=164s",
        "start_time": "164.369"
    },
    {
        "id": "5e839645",
        "text": "what's the genesis of complex numbers? So why do we need them? Well, for a long time, mathematicians have been scared of a very simple thing and that's square root of minus one or more in general, the square root of negative numbers. So uh with real numbers, there's no way we can solve like this very simple thing like the square root of a negative number. So basically math is broken in the in terms sort of like real numbers if we want to deal with this type of things. So how do we deal with these guys? Well, mathematicians are quite creative people sometimes quite nuts and they came out with something completely made up and it was so made up that they called it the imaginary units,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=190s",
        "start_time": "190.21"
    },
    {
        "id": "18ebea8b",
        "text": "and that's square root of minus one or more in general, the square root of negative numbers. So uh with real numbers, there's no way we can solve like this very simple thing like the square root of a negative number. So basically math is broken in the in terms sort of like real numbers if we want to deal with this type of things. So how do we deal with these guys? Well, mathematicians are quite creative people sometimes quite nuts and they came out with something completely made up and it was so made up that they called it the imaginary units, the famous I symbol. And I has the wonderful property that when it is squared, it is equal to minus one. So now all of a sudden we have a way of working with square roots with negative arguments using complex numbers. And isn't that fantastic? Yeah, I guess you bet that's fantastic. Now, this may feel",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=204s",
        "start_time": "204.509"
    },
    {
        "id": "681acfc1",
        "text": "sort of like real numbers if we want to deal with this type of things. So how do we deal with these guys? Well, mathematicians are quite creative people sometimes quite nuts and they came out with something completely made up and it was so made up that they called it the imaginary units, the famous I symbol. And I has the wonderful property that when it is squared, it is equal to minus one. So now all of a sudden we have a way of working with square roots with negative arguments using complex numbers. And isn't that fantastic? Yeah, I guess you bet that's fantastic. Now, this may feel somewhat uh like cheating or like weird and in a sense it is, but I can assure you that complex numbers work for real. And for example, in audio processing in signal processing, they are used ubiquitously. So",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=224s",
        "start_time": "224.1"
    },
    {
        "id": "d94a414e",
        "text": "the famous I symbol. And I has the wonderful property that when it is squared, it is equal to minus one. So now all of a sudden we have a way of working with square roots with negative arguments using complex numbers. And isn't that fantastic? Yeah, I guess you bet that's fantastic. Now, this may feel somewhat uh like cheating or like weird and in a sense it is, but I can assure you that complex numbers work for real. And for example, in audio processing in signal processing, they are used ubiquitously. So now let's just get familiar with the definition of a complex number or like just C a complex number, even if it's just like a generic complex number for the time being. So we define it as C, which is equal to A plus I times B now A and",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=243s",
        "start_time": "243.88"
    },
    {
        "id": "b0886b1c",
        "text": "somewhat uh like cheating or like weird and in a sense it is, but I can assure you that complex numbers work for real. And for example, in audio processing in signal processing, they are used ubiquitously. So now let's just get familiar with the definition of a complex number or like just C a complex number, even if it's just like a generic complex number for the time being. So we define it as C, which is equal to A plus I times B now A and B are both real numbers. And this I is the imaginary units, right? And so the cool thing about this, we we, it's like that this complex number can be divided into two parts, the real parts that provides us information about the real",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=270s",
        "start_time": "270.445"
    },
    {
        "id": "36ef163f",
        "text": "now let's just get familiar with the definition of a complex number or like just C a complex number, even if it's just like a generic complex number for the time being. So we define it as C, which is equal to A plus I times B now A and B are both real numbers. And this I is the imaginary units, right? And so the cool thing about this, we we, it's like that this complex number can be divided into two parts, the real parts that provides us information about the real domain of this complex number. And the so called imaginary parts that provides us information about the imaginary part. Now, given mathematicians love visualizing stuff quite a lot. So they thought that we could just take these complex numbers and put them on a plane. Because if you think of this uh the real part and imaginary part as two separate axis,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=287s",
        "start_time": "287.54"
    },
    {
        "id": "e8aedee0",
        "text": "B are both real numbers. And this I is the imaginary units, right? And so the cool thing about this, we we, it's like that this complex number can be divided into two parts, the real parts that provides us information about the real domain of this complex number. And the so called imaginary parts that provides us information about the imaginary part. Now, given mathematicians love visualizing stuff quite a lot. So they thought that we could just take these complex numbers and put them on a plane. Because if you think of this uh the real part and imaginary part as two separate axis, why don't you put them on a plane? And that's what they do. And so they can end up plotting complex numbers onto the complex plane. So in the complex plane, the X axis is the real axis, whereas the y axis is the imaginary axis. Now let's try to pinpoint a complex number. For example, this three plus two. I.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=305s",
        "start_time": "305.1"
    },
    {
        "id": "cddc376a",
        "text": "domain of this complex number. And the so called imaginary parts that provides us information about the imaginary part. Now, given mathematicians love visualizing stuff quite a lot. So they thought that we could just take these complex numbers and put them on a plane. Because if you think of this uh the real part and imaginary part as two separate axis, why don't you put them on a plane? And that's what they do. And so they can end up plotting complex numbers onto the complex plane. So in the complex plane, the X axis is the real axis, whereas the y axis is the imaginary axis. Now let's try to pinpoint a complex number. For example, this three plus two. I. So for thats basically both three and two can be thought of as Cartesian coordinates. So three is the Cartesian coordinate for the X axis for the real part. And it's over here. And then",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=322s",
        "start_time": "322.91"
    },
    {
        "id": "607d4ba8",
        "text": "why don't you put them on a plane? And that's what they do. And so they can end up plotting complex numbers onto the complex plane. So in the complex plane, the X axis is the real axis, whereas the y axis is the imaginary axis. Now let's try to pinpoint a complex number. For example, this three plus two. I. So for thats basically both three and two can be thought of as Cartesian coordinates. So three is the Cartesian coordinate for the X axis for the real part. And it's over here. And then uh two is the Cartesian coordinate for the imaginary axis. And so when we just connect these two points to two coordinates, we get like this point over here, which is three plus two. I. Now, here we have another example, which is minus one minus one plus four I. And as you can see, we go down to minus one on the real axis and four I on the imaginary axis. And here we have this point. OK. So",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=349s",
        "start_time": "349.13"
    },
    {
        "id": "bc19bc7e",
        "text": "So for thats basically both three and two can be thought of as Cartesian coordinates. So three is the Cartesian coordinate for the X axis for the real part. And it's over here. And then uh two is the Cartesian coordinate for the imaginary axis. And so when we just connect these two points to two coordinates, we get like this point over here, which is three plus two. I. Now, here we have another example, which is minus one minus one plus four I. And as you can see, we go down to minus one on the real axis and four I on the imaginary axis. And here we have this point. OK. So this is like all good and well. But uh now we are interested in another way of representing complex numbers using the so called polar coordinates. And now you'll see why this is like very handy for us in the context of audio signal processing. OK.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=375s",
        "start_time": "375.619"
    },
    {
        "id": "edc4a083",
        "text": "uh two is the Cartesian coordinate for the imaginary axis. And so when we just connect these two points to two coordinates, we get like this point over here, which is three plus two. I. Now, here we have another example, which is minus one minus one plus four I. And as you can see, we go down to minus one on the real axis and four I on the imaginary axis. And here we have this point. OK. So this is like all good and well. But uh now we are interested in another way of representing complex numbers using the so called polar coordinates. And now you'll see why this is like very handy for us in the context of audio signal processing. OK. So here from moving from the carti coordinate representation onto the polar representation, we need a couple of parameters. The first of which is this absolute value of C which visually represents the distance of the complex number from the origin. And visually you can visually, you can see it like as a red line here.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=390s",
        "start_time": "390.47"
    },
    {
        "id": "76d09f81",
        "text": "this is like all good and well. But uh now we are interested in another way of representing complex numbers using the so called polar coordinates. And now you'll see why this is like very handy for us in the context of audio signal processing. OK. So here from moving from the carti coordinate representation onto the polar representation, we need a couple of parameters. The first of which is this absolute value of C which visually represents the distance of the complex number from the origin. And visually you can visually, you can see it like as a red line here. Now this is one parameter that we need. The second parameter is the angle called gamma.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=419s",
        "start_time": "419.269"
    },
    {
        "id": "2402c5a5",
        "text": "So here from moving from the carti coordinate representation onto the polar representation, we need a couple of parameters. The first of which is this absolute value of C which visually represents the distance of the complex number from the origin. And visually you can visually, you can see it like as a red line here. Now this is one parameter that we need. The second parameter is the angle called gamma. So what's gamma? Well, gamma is the angle between the positive uh real axis and the line that connects the origin with the complex number. So in other words, it's this angle over here. Now the moment we have these two parameters, we can move easily from the uh from the cars coordinate representation to the polar coordinate representation. But before we do that, we need to calculate both absolute value of C and gamma.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=440s",
        "start_time": "440.54"
    },
    {
        "id": "dead40ac",
        "text": "Now this is one parameter that we need. The second parameter is the angle called gamma. So what's gamma? Well, gamma is the angle between the positive uh real axis and the line that connects the origin with the complex number. So in other words, it's this angle over here. Now the moment we have these two parameters, we can move easily from the uh from the cars coordinate representation to the polar coordinate representation. But before we do that, we need to calculate both absolute value of C and gamma. Let's start with absolute value of C. And here we can use like some very basic math, something that probably you, you've done like in high school, if not elementary school.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=465s",
        "start_time": "465.73"
    },
    {
        "id": "840ab6fd",
        "text": "So what's gamma? Well, gamma is the angle between the positive uh real axis and the line that connects the origin with the complex number. So in other words, it's this angle over here. Now the moment we have these two parameters, we can move easily from the uh from the cars coordinate representation to the polar coordinate representation. But before we do that, we need to calculate both absolute value of C and gamma. Let's start with absolute value of C. And here we can use like some very basic math, something that probably you, you've done like in high school, if not elementary school. So here we, I just like reread this and added uh like these um colors, colored lines for clarity. So the purple line here is a, here we have B which is represented by this um yellow line. And obviously, we have the hypotenuse of this triangle which is absolute value of C.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=472s",
        "start_time": "472.63"
    },
    {
        "id": "4131cadf",
        "text": "Let's start with absolute value of C. And here we can use like some very basic math, something that probably you, you've done like in high school, if not elementary school. So here we, I just like reread this and added uh like these um colors, colored lines for clarity. So the purple line here is a, here we have B which is represented by this um yellow line. And obviously, we have the hypotenuse of this triangle which is absolute value of C. Now, um as you can see, this is a right angled triangle because we have a right angle here. And given, we know I given, we know B because these are like the, this is like the um",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=500s",
        "start_time": "500.209"
    },
    {
        "id": "5b20a640",
        "text": "So here we, I just like reread this and added uh like these um colors, colored lines for clarity. So the purple line here is a, here we have B which is represented by this um yellow line. And obviously, we have the hypotenuse of this triangle which is absolute value of C. Now, um as you can see, this is a right angled triangle because we have a right angle here. And given, we know I given, we know B because these are like the, this is like the um the real coordinate for the, yeah, the real part A and B is the coordinate for the imaginary part of the number. Then we can easily get the hypotenuse of this triangle by using",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=511s",
        "start_time": "511.25"
    },
    {
        "id": "dceb8e44",
        "text": "Now, um as you can see, this is a right angled triangle because we have a right angle here. And given, we know I given, we know B because these are like the, this is like the um the real coordinate for the, yeah, the real part A and B is the coordinate for the imaginary part of the number. Then we can easily get the hypotenuse of this triangle by using Patreon theorem here. And so you should be familiar with this. But basically, if we square the absolute value of C, this is equal to A squared plus B squared. Now we're not interested in the square of absolute value of C. So we want to take the absolute value of C. So we have to square,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=537s",
        "start_time": "537.429"
    },
    {
        "id": "4dd254e1",
        "text": "the real coordinate for the, yeah, the real part A and B is the coordinate for the imaginary part of the number. Then we can easily get the hypotenuse of this triangle by using Patreon theorem here. And so you should be familiar with this. But basically, if we square the absolute value of C, this is equal to A squared plus B squared. Now we're not interested in the square of absolute value of C. So we want to take the absolute value of C. So we have to square, well, we have to use the square root, sorry uh uh and apply that to A squared plus B squared. So this way we can easily get the absolute value of C great. So what about the gamma value the gamma angle? So now for calculating that we need to brush off on some trigonometry. So basic cosine and sine functions. OK. So let's get started. So here",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=552s",
        "start_time": "552.869"
    },
    {
        "id": "b4b59d89",
        "text": "Patreon theorem here. And so you should be familiar with this. But basically, if we square the absolute value of C, this is equal to A squared plus B squared. Now we're not interested in the square of absolute value of C. So we want to take the absolute value of C. So we have to square, well, we have to use the square root, sorry uh uh and apply that to A squared plus B squared. So this way we can easily get the absolute value of C great. So what about the gamma value the gamma angle? So now for calculating that we need to brush off on some trigonometry. So basic cosine and sine functions. OK. So let's get started. So here the cosine of gamma is equal to this side. A divided by the hypotenuse absolute value of C and the sine of gamma is equal to B divided by the hypotenuse of this triangle. Once again, the absolute value of C. OK. Now,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=567s",
        "start_time": "567.57"
    },
    {
        "id": "bbdeef01",
        "text": "well, we have to use the square root, sorry uh uh and apply that to A squared plus B squared. So this way we can easily get the absolute value of C great. So what about the gamma value the gamma angle? So now for calculating that we need to brush off on some trigonometry. So basic cosine and sine functions. OK. So let's get started. So here the cosine of gamma is equal to this side. A divided by the hypotenuse absolute value of C and the sine of gamma is equal to B divided by the hypotenuse of this triangle. Once again, the absolute value of C. OK. Now, uh we are interested in gamma. So we want to get gamma and get rid of this absolute value of C. So how can we do that? Well, we can, for example, divide the sign with the cosine of gamma and then obviously divide this guy by this guy. And if we do so we end up with this formula here. So we have sine of gamma divided by cosine of gamma, which is equal to B divided by A",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=589s",
        "start_time": "589.02"
    },
    {
        "id": "f8d0ad94",
        "text": "the cosine of gamma is equal to this side. A divided by the hypotenuse absolute value of C and the sine of gamma is equal to B divided by the hypotenuse of this triangle. Once again, the absolute value of C. OK. Now, uh we are interested in gamma. So we want to get gamma and get rid of this absolute value of C. So how can we do that? Well, we can, for example, divide the sign with the cosine of gamma and then obviously divide this guy by this guy. And if we do so we end up with this formula here. So we have sine of gamma divided by cosine of gamma, which is equal to B divided by A and the, the, the absolute value of C has been canceled out right. Now. If you are familiar with trigonometry, I'm sure you've recognized that this sine divided by cosine is indeed the tangent of gamma.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=618s",
        "start_time": "618.0"
    },
    {
        "id": "8241f0d6",
        "text": "uh we are interested in gamma. So we want to get gamma and get rid of this absolute value of C. So how can we do that? Well, we can, for example, divide the sign with the cosine of gamma and then obviously divide this guy by this guy. And if we do so we end up with this formula here. So we have sine of gamma divided by cosine of gamma, which is equal to B divided by A and the, the, the absolute value of C has been canceled out right. Now. If you are familiar with trigonometry, I'm sure you've recognized that this sine divided by cosine is indeed the tangent of gamma. Now, obviously we're not interested in the tangent of gamma, we are interested in gamma. So we, how do we get gamma? Well, we need to apply the inverse function of the tangent to be divided by A. And in other words, what we do to get gamma, we have to apply the arc tangent to be divided by a",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=640s",
        "start_time": "640.0"
    },
    {
        "id": "4e6451e8",
        "text": "and the, the, the absolute value of C has been canceled out right. Now. If you are familiar with trigonometry, I'm sure you've recognized that this sine divided by cosine is indeed the tangent of gamma. Now, obviously we're not interested in the tangent of gamma, we are interested in gamma. So we, how do we get gamma? Well, we need to apply the inverse function of the tangent to be divided by A. And in other words, what we do to get gamma, we have to apply the arc tangent to be divided by a good. So this way we can get gamma or yeah, the the gamma angle directly from the Cartesian coordinates. So now I'll store this couple of parameters here. And this is like the way we can get both the angle gamma and the absolute value C starting from the Cartesian coordinates and information about A AND B.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=668s",
        "start_time": "668.919"
    },
    {
        "id": "17be07c9",
        "text": "Now, obviously we're not interested in the tangent of gamma, we are interested in gamma. So we, how do we get gamma? Well, we need to apply the inverse function of the tangent to be divided by A. And in other words, what we do to get gamma, we have to apply the arc tangent to be divided by a good. So this way we can get gamma or yeah, the the gamma angle directly from the Cartesian coordinates. So now I'll store this couple of parameters here. And this is like the way we can get both the angle gamma and the absolute value C starting from the Cartesian coordinates and information about A AND B. That's great, but we haven't learned yet about the polar coordinates. So now how do we move to polar coordinates? Well, once again, we have to go back to some uh trigonometry. And here,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=687s",
        "start_time": "687.679"
    },
    {
        "id": "3866935a",
        "text": "good. So this way we can get gamma or yeah, the the gamma angle directly from the Cartesian coordinates. So now I'll store this couple of parameters here. And this is like the way we can get both the angle gamma and the absolute value C starting from the Cartesian coordinates and information about A AND B. That's great, but we haven't learned yet about the polar coordinates. So now how do we move to polar coordinates? Well, once again, we have to go back to some uh trigonometry. And here, so you see that the side A is equal to the hypotenuse the absolute value of C multiplied by the cosine of gamma. And it's this guy here and then B, the side B is equal to the hypotenuse multiplied by the",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=709s",
        "start_time": "709.27"
    },
    {
        "id": "332d95f3",
        "text": "That's great, but we haven't learned yet about the polar coordinates. So now how do we move to polar coordinates? Well, once again, we have to go back to some uh trigonometry. And here, so you see that the side A is equal to the hypotenuse the absolute value of C multiplied by the cosine of gamma. And it's this guy here and then B, the side B is equal to the hypotenuse multiplied by the line of gamma. Right. Now here we have the Cartesian representation of a complex number. So what we can do next is get A and substitute A with this formula over here because it's the same thing as we've seen",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=733s",
        "start_time": "733.02"
    },
    {
        "id": "11823486",
        "text": "so you see that the side A is equal to the hypotenuse the absolute value of C multiplied by the cosine of gamma. And it's this guy here and then B, the side B is equal to the hypotenuse multiplied by the line of gamma. Right. Now here we have the Cartesian representation of a complex number. So what we can do next is get A and substitute A with this formula over here because it's the same thing as we've seen and then substitute B with this other formula here. And so here, basically we are getting rid of the Cartesian coordinates A and B and we are using instead the absolute value C and gamma, which is our angle. So if we do that substitution, we end up with this nice formula down here,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=748s",
        "start_time": "748.57"
    },
    {
        "id": "bb72127e",
        "text": "line of gamma. Right. Now here we have the Cartesian representation of a complex number. So what we can do next is get A and substitute A with this formula over here because it's the same thing as we've seen and then substitute B with this other formula here. And so here, basically we are getting rid of the Cartesian coordinates A and B and we are using instead the absolute value C and gamma, which is our angle. So if we do that substitution, we end up with this nice formula down here, which is a complex number of C in polar coordinates can be expressed as the absolute value of that number multiplied by cosine of gamma plus I times sine gamma.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=765s",
        "start_time": "765.875"
    },
    {
        "id": "435a8ed1",
        "text": "and then substitute B with this other formula here. And so here, basically we are getting rid of the Cartesian coordinates A and B and we are using instead the absolute value C and gamma, which is our angle. So if we do that substitution, we end up with this nice formula down here, which is a complex number of C in polar coordinates can be expressed as the absolute value of that number multiplied by cosine of gamma plus I times sine gamma. Nice. OK. So, but now you may be wondering, well, we are doing all of this like mental gymnastics and mathematical gymnastics. But how does this apply to our fourier transform? Well, I'm not going to give you like the mapping, the 1 to 1 mapping here because you need to know more about complex numbers. But I want to give you a little bit of a Hinch. So with the fourier transform, we are dealing with sine waves, right?",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=783s",
        "start_time": "783.44"
    },
    {
        "id": "69f36839",
        "text": "which is a complex number of C in polar coordinates can be expressed as the absolute value of that number multiplied by cosine of gamma plus I times sine gamma. Nice. OK. So, but now you may be wondering, well, we are doing all of this like mental gymnastics and mathematical gymnastics. But how does this apply to our fourier transform? Well, I'm not going to give you like the mapping, the 1 to 1 mapping here because you need to know more about complex numbers. But I want to give you a little bit of a Hinch. So with the fourier transform, we are dealing with sine waves, right? And uh here you have a hint that still we're dealing with cosine and science. So that is somewhat uh like close in a sense, right? And so that is a little hint that perhaps this representation can be useful for the fourier transform. But more than that with the fourier transform when we decompose a sound into a single frequency pure tone.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=807s",
        "start_time": "807.419"
    },
    {
        "id": "1ebb1186",
        "text": "Nice. OK. So, but now you may be wondering, well, we are doing all of this like mental gymnastics and mathematical gymnastics. But how does this apply to our fourier transform? Well, I'm not going to give you like the mapping, the 1 to 1 mapping here because you need to know more about complex numbers. But I want to give you a little bit of a Hinch. So with the fourier transform, we are dealing with sine waves, right? And uh here you have a hint that still we're dealing with cosine and science. So that is somewhat uh like close in a sense, right? And so that is a little hint that perhaps this representation can be useful for the fourier transform. But more than that with the fourier transform when we decompose a sound into a single frequency pure tone. Well, we get two values an absolute value and a an angle, a face, right. And",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=820s",
        "start_time": "820.549"
    },
    {
        "id": "9ef11519",
        "text": "And uh here you have a hint that still we're dealing with cosine and science. So that is somewhat uh like close in a sense, right? And so that is a little hint that perhaps this representation can be useful for the fourier transform. But more than that with the fourier transform when we decompose a sound into a single frequency pure tone. Well, we get two values an absolute value and a an angle, a face, right. And the the hint here is that the magnitude can be somewhat mapped onto the absolute value here and the phase onto this gamma angle. So now probably you start to see here what's happening, right. So we have all of these different elements and uh like both in the fourier transform. And here in this comp the polar coordinate representation of color of complex numbers and they somewhat align",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=848s",
        "start_time": "848.039"
    },
    {
        "id": "dae59da6",
        "text": "Well, we get two values an absolute value and a an angle, a face, right. And the the hint here is that the magnitude can be somewhat mapped onto the absolute value here and the phase onto this gamma angle. So now probably you start to see here what's happening, right. So we have all of these different elements and uh like both in the fourier transform. And here in this comp the polar coordinate representation of color of complex numbers and they somewhat align and that's great. And we can use this to our own advantage to make sense of the fourier transform with complex numbers. But this is not today's topic. We aren't done yet with complex numbers though. So there are a few other concepts that we use and that are super useful for representing in a nicer handy way our fourier transform. So let's just take a look at them. So one of this is the Euler formula,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=872s",
        "start_time": "872.76"
    },
    {
        "id": "4037e9e9",
        "text": "the the hint here is that the magnitude can be somewhat mapped onto the absolute value here and the phase onto this gamma angle. So now probably you start to see here what's happening, right. So we have all of these different elements and uh like both in the fourier transform. And here in this comp the polar coordinate representation of color of complex numbers and they somewhat align and that's great. And we can use this to our own advantage to make sense of the fourier transform with complex numbers. But this is not today's topic. We aren't done yet with complex numbers though. So there are a few other concepts that we use and that are super useful for representing in a nicer handy way our fourier transform. So let's just take a look at them. So one of this is the Euler formula, basically what we are looking at here is E to the I times gamma, which is equal to cosine of gamma plus I sine gamma. Now, if you're wondering about e this is the base of the natural logarithm. Now this is like a very, very nice formula. And uh it's kind of looks similar to what we are looking at at our um",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=880s",
        "start_time": "880.83"
    },
    {
        "id": "2b2427d3",
        "text": "and that's great. And we can use this to our own advantage to make sense of the fourier transform with complex numbers. But this is not today's topic. We aren't done yet with complex numbers though. So there are a few other concepts that we use and that are super useful for representing in a nicer handy way our fourier transform. So let's just take a look at them. So one of this is the Euler formula, basically what we are looking at here is E to the I times gamma, which is equal to cosine of gamma plus I sine gamma. Now, if you're wondering about e this is the base of the natural logarithm. Now this is like a very, very nice formula. And uh it's kind of looks similar to what we are looking at at our um a polar representation of the complex numbers. But let's try to visualize and understand what this is on a plot, what this exponential is on a plot. And so as you can see here, so what the exponential does, it basically traces the unit circle. So what's the unit circle is the circle that has a radius one",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=908s",
        "start_time": "908.77"
    },
    {
        "id": "2e66fa16",
        "text": "basically what we are looking at here is E to the I times gamma, which is equal to cosine of gamma plus I sine gamma. Now, if you're wondering about e this is the base of the natural logarithm. Now this is like a very, very nice formula. And uh it's kind of looks similar to what we are looking at at our um a polar representation of the complex numbers. But let's try to visualize and understand what this is on a plot, what this exponential is on a plot. And so as you can see here, so what the exponential does, it basically traces the unit circle. So what's the unit circle is the circle that has a radius one and it traces it counter clockwise. And in order to like just trace the circle, what we do is we just increase the angle gamma. The eer formula is going to be very useful to rewrite our polar coordinates for the complex numbers. But before we get there, I want to take a little detail to show you one of the beauty, one of the jewels in mathematics and that's the Euler identity.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=936s",
        "start_time": "936.27"
    },
    {
        "id": "02126fdd",
        "text": "a polar representation of the complex numbers. But let's try to visualize and understand what this is on a plot, what this exponential is on a plot. And so as you can see here, so what the exponential does, it basically traces the unit circle. So what's the unit circle is the circle that has a radius one and it traces it counter clockwise. And in order to like just trace the circle, what we do is we just increase the angle gamma. The eer formula is going to be very useful to rewrite our polar coordinates for the complex numbers. But before we get there, I want to take a little detail to show you one of the beauty, one of the jewels in mathematics and that's the Euler identity. So mathematicians, physicists even engineers look at this formula and they say, well, it's wonderful. It's just perfect.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=963s",
        "start_time": "963.7"
    },
    {
        "id": "311400c1",
        "text": "and it traces it counter clockwise. And in order to like just trace the circle, what we do is we just increase the angle gamma. The eer formula is going to be very useful to rewrite our polar coordinates for the complex numbers. But before we get there, I want to take a little detail to show you one of the beauty, one of the jewels in mathematics and that's the Euler identity. So mathematicians, physicists even engineers look at this formula and they say, well, it's wonderful. It's just perfect. So let's take a look at it. So it's e to the I times pi plus one is equal to zero. So why is it so wonderful? Well, because it has all the fundamental elements of mathematics and symbols, it has zero, it has one, it has pi which is so omnipresent in mathematics, it has I this made of imaginary unit and it has E so this is just wonderful.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=992s",
        "start_time": "992.03"
    },
    {
        "id": "1b860945",
        "text": "So mathematicians, physicists even engineers look at this formula and they say, well, it's wonderful. It's just perfect. So let's take a look at it. So it's e to the I times pi plus one is equal to zero. So why is it so wonderful? Well, because it has all the fundamental elements of mathematics and symbols, it has zero, it has one, it has pi which is so omnipresent in mathematics, it has I this made of imaginary unit and it has E so this is just wonderful. But for this to work, we need for this exponential right to be equal to minus one. So let's check out, let's see like if this actually is the case, let's start from the Euler formula here and here we can see uh that. Um so if we plug pi into like this formula, we'll see that cosine of pi is equal to one",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1020s",
        "start_time": "1020.0"
    },
    {
        "id": "e4e93657",
        "text": "So let's take a look at it. So it's e to the I times pi plus one is equal to zero. So why is it so wonderful? Well, because it has all the fundamental elements of mathematics and symbols, it has zero, it has one, it has pi which is so omnipresent in mathematics, it has I this made of imaginary unit and it has E so this is just wonderful. But for this to work, we need for this exponential right to be equal to minus one. So let's check out, let's see like if this actually is the case, let's start from the Euler formula here and here we can see uh that. Um so if we plug pi into like this formula, we'll see that cosine of pi is equal to one equal to sorry minus one plus I times sine of pi and sine of pi is equal to zero. In other words, we are adding up minus one plus I times zero, which is zero. So all of this guy gives us back minus one. So it checks out that's great. But let's try to visualize this and see if it works. And for that, we should go back to the complex plane.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1028s",
        "start_time": "1028.63"
    },
    {
        "id": "b998fb2b",
        "text": "But for this to work, we need for this exponential right to be equal to minus one. So let's check out, let's see like if this actually is the case, let's start from the Euler formula here and here we can see uh that. Um so if we plug pi into like this formula, we'll see that cosine of pi is equal to one equal to sorry minus one plus I times sine of pi and sine of pi is equal to zero. In other words, we are adding up minus one plus I times zero, which is zero. So all of this guy gives us back minus one. So it checks out that's great. But let's try to visualize this and see if it works. And for that, we should go back to the complex plane. And now we want to see uh E to the I times pi and that is at this point here. So it's basically the value that we are tracing on the unit circle is the one that is resting on the negative real axis. And so this is equal to minus one and again, visual it",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1055s",
        "start_time": "1055.199"
    },
    {
        "id": "b02580c9",
        "text": "equal to sorry minus one plus I times sine of pi and sine of pi is equal to zero. In other words, we are adding up minus one plus I times zero, which is zero. So all of this guy gives us back minus one. So it checks out that's great. But let's try to visualize this and see if it works. And for that, we should go back to the complex plane. And now we want to see uh E to the I times pi and that is at this point here. So it's basically the value that we are tracing on the unit circle is the one that is resting on the negative real axis. And so this is equal to minus one and again, visual it out. So Euler, you are a great man and you were right, by the way, this chap is Leonard Euler, one of the most brilliant mathematicians to ever live on planet Earth. OK. But now let's go back to our polar coordinates for the uh complex numbers. OK. So",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1084s",
        "start_time": "1084.53"
    },
    {
        "id": "c77c1d7d",
        "text": "And now we want to see uh E to the I times pi and that is at this point here. So it's basically the value that we are tracing on the unit circle is the one that is resting on the negative real axis. And so this is equal to minus one and again, visual it out. So Euler, you are a great man and you were right, by the way, this chap is Leonard Euler, one of the most brilliant mathematicians to ever live on planet Earth. OK. But now let's go back to our polar coordinates for the uh complex numbers. OK. So uh the top equation here gives us the port code units as we've come to know them. And here we have the Euler formula. Do you notice something interesting there that we could use to our advantage? Well, yes. So this guy here is basically our exponential here. So what we can do is take this exponential and plug it in here,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1113s",
        "start_time": "1113.26"
    },
    {
        "id": "6bee4b1d",
        "text": "out. So Euler, you are a great man and you were right, by the way, this chap is Leonard Euler, one of the most brilliant mathematicians to ever live on planet Earth. OK. But now let's go back to our polar coordinates for the uh complex numbers. OK. So uh the top equation here gives us the port code units as we've come to know them. And here we have the Euler formula. Do you notice something interesting there that we could use to our advantage? Well, yes. So this guy here is basically our exponential here. So what we can do is take this exponential and plug it in here, right? And so we can rewrite our polar coordinates like this. So a complex number C is equal to the absolute value of C times E",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1139s",
        "start_time": "1139.505"
    },
    {
        "id": "e2e72824",
        "text": "uh the top equation here gives us the port code units as we've come to know them. And here we have the Euler formula. Do you notice something interesting there that we could use to our advantage? Well, yes. So this guy here is basically our exponential here. So what we can do is take this exponential and plug it in here, right? And so we can rewrite our polar coordinates like this. So a complex number C is equal to the absolute value of C times E uh to the I times gamma. Great. So isn't this wonderful? It's wonderful because it's a very, very compact way of uh indicating representing a complex number. So obviously, here we have two parts of this uh complex number.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1163s",
        "start_time": "1163.13"
    },
    {
        "id": "98c6ecc5",
        "text": "right? And so we can rewrite our polar coordinates like this. So a complex number C is equal to the absolute value of C times E uh to the I times gamma. Great. So isn't this wonderful? It's wonderful because it's a very, very compact way of uh indicating representing a complex number. So obviously, here we have two parts of this uh complex number. So we obviously have the exponential and we have the absolute value. Now, the last exercise that I want to do today is to just visualize what these two components contribute to the overall complex number in a visual way. Now, this is a suggestion for you guys. So if you're learning some math or anything",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1189s",
        "start_time": "1189.16"
    },
    {
        "id": "7bbe4b7b",
        "text": "uh to the I times gamma. Great. So isn't this wonderful? It's wonderful because it's a very, very compact way of uh indicating representing a complex number. So obviously, here we have two parts of this uh complex number. So we obviously have the exponential and we have the absolute value. Now, the last exercise that I want to do today is to just visualize what these two components contribute to the overall complex number in a visual way. Now, this is a suggestion for you guys. So if you're learning some math or anything really, uh even like in engineering, I really suggest you to go deep and try to visualize all of this with like formulas and things because that is gonna give you give you like such an understanding, a deep understanding and it's gonna go well beyond just like memorizing like a formula without really understanding what's going on there. And this is what I'm trying to do like with my videos in general. And yeah, and this like formula in particular right now.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1200s",
        "start_time": "1200.43"
    },
    {
        "id": "6519fff8",
        "text": "So we obviously have the exponential and we have the absolute value. Now, the last exercise that I want to do today is to just visualize what these two components contribute to the overall complex number in a visual way. Now, this is a suggestion for you guys. So if you're learning some math or anything really, uh even like in engineering, I really suggest you to go deep and try to visualize all of this with like formulas and things because that is gonna give you give you like such an understanding, a deep understanding and it's gonna go well beyond just like memorizing like a formula without really understanding what's going on there. And this is what I'm trying to do like with my videos in general. And yeah, and this like formula in particular right now. But yeah, without further ado let me just jump into this because it's so wonderful. So the this exponential component we already know it's the uh basically like traces the unit circle in the complex plane counter clockwise. And so what this component gives us to the complex number is the direction of the number in the complex plane on the unit circle.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1222s",
        "start_time": "1222.17"
    },
    {
        "id": "f0b95c3d",
        "text": "really, uh even like in engineering, I really suggest you to go deep and try to visualize all of this with like formulas and things because that is gonna give you give you like such an understanding, a deep understanding and it's gonna go well beyond just like memorizing like a formula without really understanding what's going on there. And this is what I'm trying to do like with my videos in general. And yeah, and this like formula in particular right now. But yeah, without further ado let me just jump into this because it's so wonderful. So the this exponential component we already know it's the uh basically like traces the unit circle in the complex plane counter clockwise. And so what this component gives us to the complex number is the direction of the number in the complex plane on the unit circle. And here we have it. For example, this is like for gamma, which is equal to pi divided by four. In other words, for 4045 degrees right now, if we change a gamma and we go to gamma equal to pi well, we are just changing the direction of the number which now rests",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1244s",
        "start_time": "1244.729"
    },
    {
        "id": "1622e582",
        "text": "But yeah, without further ado let me just jump into this because it's so wonderful. So the this exponential component we already know it's the uh basically like traces the unit circle in the complex plane counter clockwise. And so what this component gives us to the complex number is the direction of the number in the complex plane on the unit circle. And here we have it. For example, this is like for gamma, which is equal to pi divided by four. In other words, for 4045 degrees right now, if we change a gamma and we go to gamma equal to pi well, we are just changing the direction of the number which now rests on the negative real axis. Now, if we change gamma once again, we change the direction, for example, minus pi divided by two, we are resting on the negative imaginary axis. And the value of our exponential here is going to be equal to minus.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1272s",
        "start_time": "1272.75"
    },
    {
        "id": "00bb1b55",
        "text": "And here we have it. For example, this is like for gamma, which is equal to pi divided by four. In other words, for 4045 degrees right now, if we change a gamma and we go to gamma equal to pi well, we are just changing the direction of the number which now rests on the negative real axis. Now, if we change gamma once again, we change the direction, for example, minus pi divided by two, we are resting on the negative imaginary axis. And the value of our exponential here is going to be equal to minus. So minus the imaginary unit. OK. So you get the idea of what this exponential does to our number, it just gives us the direction. Then what about uh absolute value of C? Well, this acts as a scaler basically scales the distance from the origin of our number. So if we start with a uh with the unit, so basically the absolute value of C is equal to one, well,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1300s",
        "start_time": "1300.56"
    },
    {
        "id": "67545878",
        "text": "on the negative real axis. Now, if we change gamma once again, we change the direction, for example, minus pi divided by two, we are resting on the negative imaginary axis. And the value of our exponential here is going to be equal to minus. So minus the imaginary unit. OK. So you get the idea of what this exponential does to our number, it just gives us the direction. Then what about uh absolute value of C? Well, this acts as a scaler basically scales the distance from the origin of our number. So if we start with a uh with the unit, so basically the absolute value of C is equal to one, well, we are just resting on the unit circle obviously. But if the absolute value of C was equal to two, for example, well, what would happen is that we double the distance of the number from the origin, but we stay on the same line on the same direction.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1321s",
        "start_time": "1321.062"
    },
    {
        "id": "009c4187",
        "text": "So minus the imaginary unit. OK. So you get the idea of what this exponential does to our number, it just gives us the direction. Then what about uh absolute value of C? Well, this acts as a scaler basically scales the distance from the origin of our number. So if we start with a uh with the unit, so basically the absolute value of C is equal to one, well, we are just resting on the unit circle obviously. But if the absolute value of C was equal to two, for example, well, what would happen is that we double the distance of the number from the origin, but we stay on the same line on the same direction. But when I say that the absolute value C so this component is a scalar, it can obviously stretch things but it can also uh shrink them. So if we had, for example, C equal to 0.5 what we would get is half",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1341s",
        "start_time": "1341.566"
    },
    {
        "id": "30655bc6",
        "text": "we are just resting on the unit circle obviously. But if the absolute value of C was equal to two, for example, well, what would happen is that we double the distance of the number from the origin, but we stay on the same line on the same direction. But when I say that the absolute value C so this component is a scalar, it can obviously stretch things but it can also uh shrink them. So if we had, for example, C equal to 0.5 what we would get is half the distance from the origin calculated with respect to the unit circle over here.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1370s",
        "start_time": "1370.03"
    },
    {
        "id": "037af415",
        "text": "But when I say that the absolute value C so this component is a scalar, it can obviously stretch things but it can also uh shrink them. So if we had, for example, C equal to 0.5 what we would get is half the distance from the origin calculated with respect to the unit circle over here. That's great. So yeah, this is like so fascinating, like an interesting and I, and I think like it's gonna be like very useful moving forward. Definitely, it's gonna be very useful to like understand how to map the fourier transform onto uh like the complex representation. And now, once again, I want to give you a hint into that. So",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1391s",
        "start_time": "1391.579"
    },
    {
        "id": "35b6d4ae",
        "text": "the distance from the origin calculated with respect to the unit circle over here. That's great. So yeah, this is like so fascinating, like an interesting and I, and I think like it's gonna be like very useful moving forward. Definitely, it's gonna be very useful to like understand how to map the fourier transform onto uh like the complex representation. And now, once again, I want to give you a hint into that. So now we have a very complex way of representing complex numbers. And we are going to be using uh this absolute value as a way of mapping our magnitude that we have like in our fourier transform. And we're going to be using this exponential and specifically this gamma to talk about the face that we get out of a fourier transform for a given pure tone. And so this is the type of like",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1409s",
        "start_time": "1409.56"
    },
    {
        "id": "2e0d00b2",
        "text": "That's great. So yeah, this is like so fascinating, like an interesting and I, and I think like it's gonna be like very useful moving forward. Definitely, it's gonna be very useful to like understand how to map the fourier transform onto uh like the complex representation. And now, once again, I want to give you a hint into that. So now we have a very complex way of representing complex numbers. And we are going to be using uh this absolute value as a way of mapping our magnitude that we have like in our fourier transform. And we're going to be using this exponential and specifically this gamma to talk about the face that we get out of a fourier transform for a given pure tone. And so this is the type of like kind of like mental exercise that we'll use in the next video where we are actually gonna put into place all of the knowledge that we've acquired today about complex numbers in order to create a complex representation of the fourier transform. And you'll be delighted by that because it's gonna look like so wonderful and so simple,",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1417s",
        "start_time": "1417.849"
    },
    {
        "id": "4f6e5ebf",
        "text": "now we have a very complex way of representing complex numbers. And we are going to be using uh this absolute value as a way of mapping our magnitude that we have like in our fourier transform. And we're going to be using this exponential and specifically this gamma to talk about the face that we get out of a fourier transform for a given pure tone. And so this is the type of like kind of like mental exercise that we'll use in the next video where we are actually gonna put into place all of the knowledge that we've acquired today about complex numbers in order to create a complex representation of the fourier transform. And you'll be delighted by that because it's gonna look like so wonderful and so simple, good. So this was like a quite intense video film, but I hope that you've enjoyed it because now you should have a good understanding, an operational understanding of complex numbers, uh their uh representation in Cartesian coordinates in polar coordinates, the Euler formula, the exponential and all of this like nice things here.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1441s",
        "start_time": "1441.16"
    },
    {
        "id": "6129579c",
        "text": "kind of like mental exercise that we'll use in the next video where we are actually gonna put into place all of the knowledge that we've acquired today about complex numbers in order to create a complex representation of the fourier transform. And you'll be delighted by that because it's gonna look like so wonderful and so simple, good. So this was like a quite intense video film, but I hope that you've enjoyed it because now you should have a good understanding, an operational understanding of complex numbers, uh their uh representation in Cartesian coordinates in polar coordinates, the Euler formula, the exponential and all of this like nice things here. So yeah, that's it for today. I hope you've enjoyed the video. If that's the case, please remember to leave a like and if you haven't subscribed P, please consider doing so. And as usual, if you have any questions. Please leave them in the comments section below. I'll try to answer to those and I guess that's it. I'll see you next time. Cheers.",
        "video": "Complex Numbers for Audio Signal Processing",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "DgF4m0AWCgA",
        "youtube_link": "https://www.youtube.com/watch?v=DgF4m0AWCgA&t=1470s",
        "start_time": "1470.839"
    },
    {
        "id": "c5b62fa2",
        "text": "Hi, everybody and welcome to a new exciting video in the audio signal processing for machine learning series. Last time we extracted the discrete fourier transform with Python and li browser from a bunch of audio files. This time we are back to theory specifically, we'll be addressing a key topic in A I audio, the short time fourier transform. So why is the short time four so important? Well, that's because it enables us to extract spectrograms and spectrograms are really the probably the most important feature that you can feed to deep learning audio models. But before we get into the in and out of the of the short time period transform, I want to remind you once again about the sound of the Eye Slack community on this community, you'll find people with interests in A I music A I audio, audio digital signal processing. And so if you're there, you can ask for feedback, you can share your projects and network with a bunch of very cool people. So if you're interested in joining, I'll leave you the sign up link in the description section below. OK. Now on to",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=0s",
        "start_time": "0.23"
    },
    {
        "id": "2d3b5897",
        "text": "so important? Well, that's because it enables us to extract spectrograms and spectrograms are really the probably the most important feature that you can feed to deep learning audio models. But before we get into the in and out of the of the short time period transform, I want to remind you once again about the sound of the Eye Slack community on this community, you'll find people with interests in A I music A I audio, audio digital signal processing. And so if you're there, you can ask for feedback, you can share your projects and network with a bunch of very cool people. So if you're interested in joining, I'll leave you the sign up link in the description section below. OK. Now on to the real cool stuff. So before we get to the SDFT, I want to remind you about the discrete fourier transport. And here we have its mathematical formulation. Now, I'm not going to get too much into the details here. That's just because I have a whole video on the discrete fourier transform. So if you're interested, you can go and check that out. But",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=26s",
        "start_time": "26.2"
    },
    {
        "id": "d47375b8",
        "text": "on this community, you'll find people with interests in A I music A I audio, audio digital signal processing. And so if you're there, you can ask for feedback, you can share your projects and network with a bunch of very cool people. So if you're interested in joining, I'll leave you the sign up link in the description section below. OK. Now on to the real cool stuff. So before we get to the SDFT, I want to remind you about the discrete fourier transport. And here we have its mathematical formulation. Now, I'm not going to get too much into the details here. That's just because I have a whole video on the discrete fourier transform. So if you're interested, you can go and check that out. But what we need here is the high level intuition. So we start with our signal in the time domain. So waveform like this, then we apply the discrete transform. And what we get back is basically a picture of the presence of the different frequency components in the original signal.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=52s",
        "start_time": "52.36"
    },
    {
        "id": "f166cbb2",
        "text": "the real cool stuff. So before we get to the SDFT, I want to remind you about the discrete fourier transport. And here we have its mathematical formulation. Now, I'm not going to get too much into the details here. That's just because I have a whole video on the discrete fourier transform. So if you're interested, you can go and check that out. But what we need here is the high level intuition. So we start with our signal in the time domain. So waveform like this, then we apply the discrete transform. And what we get back is basically a picture of the presence of the different frequency components in the original signal. And usually we get like a magnitude spectrum like this. But this is a still image. What do I mean by that? Well, it's a still image in the sense that it only provides us like one picture that averages the presence of the frequency components across the whole duration of the signal.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=76s",
        "start_time": "76.595"
    },
    {
        "id": "40413492",
        "text": "what we need here is the high level intuition. So we start with our signal in the time domain. So waveform like this, then we apply the discrete transform. And what we get back is basically a picture of the presence of the different frequency components in the original signal. And usually we get like a magnitude spectrum like this. But this is a still image. What do I mean by that? Well, it's a still image in the sense that it only provides us like one picture that averages the presence of the frequency components across the whole duration of the signal. And here we actually have a problem because we know what um frequency components are present in the signal. But we don't know when they are more or less present because all of them are averaged across the whole duration of the entire signal. And this is a little bit of a problem because we know which audio data, it's all about the evolution of frequency components like over",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=101s",
        "start_time": "101.029"
    },
    {
        "id": "4d16fdbd",
        "text": "And usually we get like a magnitude spectrum like this. But this is a still image. What do I mean by that? Well, it's a still image in the sense that it only provides us like one picture that averages the presence of the frequency components across the whole duration of the signal. And here we actually have a problem because we know what um frequency components are present in the signal. But we don't know when they are more or less present because all of them are averaged across the whole duration of the entire signal. And this is a little bit of a problem because we know which audio data, it's all about the evolution of frequency components like over time. And so audio data is very, very dynamic and we want to know how like this different frequency components evolve over time. And this is the whole point of the short time four transfer. So moving from a still image to a video that provides us information about the um different frequency components across time. So how can we do that? Well, that's the whole point of the short time career transform. And the high level idea here is that we don't",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=123s",
        "start_time": "123.87"
    },
    {
        "id": "5a41855e",
        "text": "And here we actually have a problem because we know what um frequency components are present in the signal. But we don't know when they are more or less present because all of them are averaged across the whole duration of the entire signal. And this is a little bit of a problem because we know which audio data, it's all about the evolution of frequency components like over time. And so audio data is very, very dynamic and we want to know how like this different frequency components evolve over time. And this is the whole point of the short time four transfer. So moving from a still image to a video that provides us information about the um different frequency components across time. So how can we do that? Well, that's the whole point of the short time career transform. And the high level idea here is that we don't perform the fourier transform across the whole duration of the signal. But rather we consider a small segments or chunks of the signal, which technically we call frames. And then we apply a discrete fourier transform for each frame. Now I know this could sound a little bit like abstract. So let's visualize this. So we start with a signal audio signal like this.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=146s",
        "start_time": "146.889"
    },
    {
        "id": "fcafc639",
        "text": "time. And so audio data is very, very dynamic and we want to know how like this different frequency components evolve over time. And this is the whole point of the short time four transfer. So moving from a still image to a video that provides us information about the um different frequency components across time. So how can we do that? Well, that's the whole point of the short time career transform. And the high level idea here is that we don't perform the fourier transform across the whole duration of the signal. But rather we consider a small segments or chunks of the signal, which technically we call frames. And then we apply a discrete fourier transform for each frame. Now I know this could sound a little bit like abstract. So let's visualize this. So we start with a signal audio signal like this. Then we consider only like the first chunk, the first frame. And at this point on only on this the samples belonging to this frame, we apply the discrete free transform and we get back like our nice magnitude spectrum. Then we slide on to the next frame. And once again, we apply to that",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=176s",
        "start_time": "176.425"
    },
    {
        "id": "60808dc8",
        "text": "perform the fourier transform across the whole duration of the signal. But rather we consider a small segments or chunks of the signal, which technically we call frames. And then we apply a discrete fourier transform for each frame. Now I know this could sound a little bit like abstract. So let's visualize this. So we start with a signal audio signal like this. Then we consider only like the first chunk, the first frame. And at this point on only on this the samples belonging to this frame, we apply the discrete free transform and we get back like our nice magnitude spectrum. Then we slide on to the next frame. And once again, we apply to that frame, the discrete fourier transform third frame, same thing until we burst through all the duration of the signal. One way we can use to derive the segments is through windowing. In other words, we apply a window function to a signal. What does that mean? Well, it means that we take the original signal",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=206s",
        "start_time": "206.649"
    },
    {
        "id": "31a571ae",
        "text": "Then we consider only like the first chunk, the first frame. And at this point on only on this the samples belonging to this frame, we apply the discrete free transform and we get back like our nice magnitude spectrum. Then we slide on to the next frame. And once again, we apply to that frame, the discrete fourier transform third frame, same thing until we burst through all the duration of the signal. One way we can use to derive the segments is through windowing. In other words, we apply a window function to a signal. What does that mean? Well, it means that we take the original signal and then we multiply that by a window function sample by sample and we obtain a windowed signal. Now this feels a little bit abstract, doesn't it? So let me give you an example. So we start from a an audio signal and then here we're gonna be applying a rectangle uh winnowing function",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=231s",
        "start_time": "231.979"
    },
    {
        "id": "e15a8554",
        "text": "frame, the discrete fourier transform third frame, same thing until we burst through all the duration of the signal. One way we can use to derive the segments is through windowing. In other words, we apply a window function to a signal. What does that mean? Well, it means that we take the original signal and then we multiply that by a window function sample by sample and we obtain a windowed signal. Now this feels a little bit abstract, doesn't it? So let me give you an example. So we start from a an audio signal and then here we're gonna be applying a rectangle uh winnowing function and this is the result. So the rectangle window function is this red curve here. And yeah, it has like a rectangle shape, right? And this function is zero everywhere apart from a segment where it is equal to one. So if we multiply the signal with the um rectangle window, we obtain this windowed signal down here. Now,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=252s",
        "start_time": "252.529"
    },
    {
        "id": "b6d8b6ca",
        "text": "and then we multiply that by a window function sample by sample and we obtain a windowed signal. Now this feels a little bit abstract, doesn't it? So let me give you an example. So we start from a an audio signal and then here we're gonna be applying a rectangle uh winnowing function and this is the result. So the rectangle window function is this red curve here. And yeah, it has like a rectangle shape, right? And this function is zero everywhere apart from a segment where it is equal to one. So if we multiply the signal with the um rectangle window, we obtain this windowed signal down here. Now, uh I want to introduce a couple of parameters that are very important for what we are discussing today. And now uh so one is the window size, the other one is the frame size, they're both measured in number of samples, but they refer to two slightly different things. So let's take a look at the window size first. So the window uh size is basically the amount of samples we apply windowing to",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=273s",
        "start_time": "273.079"
    },
    {
        "id": "fbd1ee38",
        "text": "and this is the result. So the rectangle window function is this red curve here. And yeah, it has like a rectangle shape, right? And this function is zero everywhere apart from a segment where it is equal to one. So if we multiply the signal with the um rectangle window, we obtain this windowed signal down here. Now, uh I want to introduce a couple of parameters that are very important for what we are discussing today. And now uh so one is the window size, the other one is the frame size, they're both measured in number of samples, but they refer to two slightly different things. So let's take a look at the window size first. So the window uh size is basically the amount of samples we apply windowing to the frame size. On the other hand is the kind of like the the number of samples that we consider in each chunk of the signal when we segment the the signal and then we pass it to the the short time fourier transform for just like calculating the fourier transform for each frame for each segment. OK.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=293s",
        "start_time": "293.97"
    },
    {
        "id": "ed6bec6d",
        "text": "uh I want to introduce a couple of parameters that are very important for what we are discussing today. And now uh so one is the window size, the other one is the frame size, they're both measured in number of samples, but they refer to two slightly different things. So let's take a look at the window size first. So the window uh size is basically the amount of samples we apply windowing to the frame size. On the other hand is the kind of like the the number of samples that we consider in each chunk of the signal when we segment the the signal and then we pass it to the the short time fourier transform for just like calculating the fourier transform for each frame for each segment. OK. Usually the window size and the frame size coin site, they are, they have the same value the same number of samples. But sometimes it happens that the frame size is a larger than the window size. Now, this is like quite unusual, I would say. And most of the time in most of your applications,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=320s",
        "start_time": "320.399"
    },
    {
        "id": "78a45e00",
        "text": "the frame size. On the other hand is the kind of like the the number of samples that we consider in each chunk of the signal when we segment the the signal and then we pass it to the the short time fourier transform for just like calculating the fourier transform for each frame for each segment. OK. Usually the window size and the frame size coin site, they are, they have the same value the same number of samples. But sometimes it happens that the frame size is a larger than the window size. Now, this is like quite unusual, I would say. And most of the time in most of your applications, the window size and the frame size will inside. And this is like so uh so true that for example, in Libres and when we extract the shorts and transform, we we are not forced to pass the window size. And the default value for the window size is the uh frame size, right?",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=346s",
        "start_time": "346.679"
    },
    {
        "id": "a7872297",
        "text": "Usually the window size and the frame size coin site, they are, they have the same value the same number of samples. But sometimes it happens that the frame size is a larger than the window size. Now, this is like quite unusual, I would say. And most of the time in most of your applications, the window size and the frame size will inside. And this is like so uh so true that for example, in Libres and when we extract the shorts and transform, we we are not forced to pass the window size. And the default value for the window size is the uh frame size, right? OK. But what happens if the window size is smaller than the frame size? Well, still we the chunk. So we, we apply the fourier transform to is the whole frame. But then the windowing happens only on the window size number of samples, right?",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=369s",
        "start_time": "369.75"
    },
    {
        "id": "1f279e76",
        "text": "the window size and the frame size will inside. And this is like so uh so true that for example, in Libres and when we extract the shorts and transform, we we are not forced to pass the window size. And the default value for the window size is the uh frame size, right? OK. But what happens if the window size is smaller than the frame size? Well, still we the chunk. So we, we apply the fourier transform to is the whole frame. But then the windowing happens only on the window size number of samples, right? And we apply the window function on those samples and then the remaining samples which are the difference between the frame size and the window size are gonna be zero padded. OK? But for the sake of this video, we'll assume that we have uh the window size which is equal to the frame size. So if that happens,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=391s",
        "start_time": "391.255"
    },
    {
        "id": "1dcca889",
        "text": "OK. But what happens if the window size is smaller than the frame size? Well, still we the chunk. So we, we apply the fourier transform to is the whole frame. But then the windowing happens only on the window size number of samples, right? And we apply the window function on those samples and then the remaining samples which are the difference between the frame size and the window size are gonna be zero padded. OK? But for the sake of this video, we'll assume that we have uh the window size which is equal to the frame size. So if that happens, what this is at this point is already the um windowed. Uh So we, we have like one frame here and we've also applied the window function here. And at this point is when we apply the uh fourier uh discrete fourier transform so that we can get the frequency components out of this frame,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=413s",
        "start_time": "413.14"
    },
    {
        "id": "4f1e2da6",
        "text": "And we apply the window function on those samples and then the remaining samples which are the difference between the frame size and the window size are gonna be zero padded. OK? But for the sake of this video, we'll assume that we have uh the window size which is equal to the frame size. So if that happens, what this is at this point is already the um windowed. Uh So we, we have like one frame here and we've also applied the window function here. And at this point is when we apply the uh fourier uh discrete fourier transform so that we can get the frequency components out of this frame, then we move on, we slide to the right and we get like a second frame like this sliding also like the window function. And here once again, we apply the discrete fourier transform, we move to the third frame same thing until we get to the end of the signal. But this is a simplified version of",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=433s",
        "start_time": "433.63"
    },
    {
        "id": "2efda026",
        "text": "what this is at this point is already the um windowed. Uh So we, we have like one frame here and we've also applied the window function here. And at this point is when we apply the uh fourier uh discrete fourier transform so that we can get the frequency components out of this frame, then we move on, we slide to the right and we get like a second frame like this sliding also like the window function. And here once again, we apply the discrete fourier transform, we move to the third frame same thing until we get to the end of the signal. But this is a simplified version of what usually happens in a short time period transform because the frames are overlapping like this. So the second frame is overlapping with the first one as you can see from here. Now I need to introduce another parameter here that's called H size or capital H. And it's given by this visually and",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=457s",
        "start_time": "457.67"
    },
    {
        "id": "0c432338",
        "text": "then we move on, we slide to the right and we get like a second frame like this sliding also like the window function. And here once again, we apply the discrete fourier transform, we move to the third frame same thing until we get to the end of the signal. But this is a simplified version of what usually happens in a short time period transform because the frames are overlapping like this. So the second frame is overlapping with the first one as you can see from here. Now I need to introduce another parameter here that's called H size or capital H. And it's given by this visually and this basically provides us uh it tells us uh how many samples we slide to the right when we take a new uh frame. OK. If you want to know why um",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=482s",
        "start_time": "482.35"
    },
    {
        "id": "c1878213",
        "text": "what usually happens in a short time period transform because the frames are overlapping like this. So the second frame is overlapping with the first one as you can see from here. Now I need to introduce another parameter here that's called H size or capital H. And it's given by this visually and this basically provides us uh it tells us uh how many samples we slide to the right when we take a new uh frame. OK. If you want to know why um the hub sound is like it's so important and we need overlapping frames. I really suggest you to go check out my video on audio feature extraction pipelines. It's up here and there you'll learn about a lot of topics about, for example, like spectral leakage and a bunch of other things that are related to like the points that I'm making here. And I'm not going to get into the details here because I've already done that. OK.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=505s",
        "start_time": "505.225"
    },
    {
        "id": "4dbfdfa8",
        "text": "this basically provides us uh it tells us uh how many samples we slide to the right when we take a new uh frame. OK. If you want to know why um the hub sound is like it's so important and we need overlapping frames. I really suggest you to go check out my video on audio feature extraction pipelines. It's up here and there you'll learn about a lot of topics about, for example, like spectral leakage and a bunch of other things that are related to like the points that I'm making here. And I'm not going to get into the details here because I've already done that. OK. Moving on, it's time to move from this kind of like visual intuition of the short term fourier transform to its mathematical formulation. So what I want to do here and don't be scanned is to compare the digital fourier transform or its mathematical form, which is this formula in the top with the formulation for mathematical formulation for the short time fourier transform uh which is this one down here.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=528s",
        "start_time": "528.979"
    },
    {
        "id": "7ca1288a",
        "text": "the hub sound is like it's so important and we need overlapping frames. I really suggest you to go check out my video on audio feature extraction pipelines. It's up here and there you'll learn about a lot of topics about, for example, like spectral leakage and a bunch of other things that are related to like the points that I'm making here. And I'm not going to get into the details here because I've already done that. OK. Moving on, it's time to move from this kind of like visual intuition of the short term fourier transform to its mathematical formulation. So what I want to do here and don't be scanned is to compare the digital fourier transform or its mathematical form, which is this formula in the top with the formulation for mathematical formulation for the short time fourier transform uh which is this one down here. OK. So let's go uh item by item and see how they map to each other. OK. The first one is just like the definition right, the output that we get. So in the case of the discrete fourier transform and we get like this X hat as a function of K where K is a proxy for a frequency.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=544s",
        "start_time": "544.409"
    },
    {
        "id": "ce70c1e6",
        "text": "Moving on, it's time to move from this kind of like visual intuition of the short term fourier transform to its mathematical formulation. So what I want to do here and don't be scanned is to compare the digital fourier transform or its mathematical form, which is this formula in the top with the formulation for mathematical formulation for the short time fourier transform uh which is this one down here. OK. So let's go uh item by item and see how they map to each other. OK. The first one is just like the definition right, the output that we get. So in the case of the discrete fourier transform and we get like this X hat as a function of K where K is a proxy for a frequency. And um so in other words, like uh the disco fourier transform depends on the frequency. And this means that uh each of these like formulas is gonna give us a complex uh fourier coefficient for the KF frequency",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=571s",
        "start_time": "571.294"
    },
    {
        "id": "ee5255d3",
        "text": "OK. So let's go uh item by item and see how they map to each other. OK. The first one is just like the definition right, the output that we get. So in the case of the discrete fourier transform and we get like this X hat as a function of K where K is a proxy for a frequency. And um so in other words, like uh the disco fourier transform depends on the frequency. And this means that uh each of these like formulas is gonna give us a complex uh fourier coefficient for the KF frequency and the complex fourier coefficient provides this information to uh about two parameters the face and the magnitude. Now, if you want to really know what phase and magnitude mean in terms of the fourier transform, I suggest you to go check out this video on uh the fourier transform and there like you'll get like a better picture but I hope like you've already watched that.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=598s",
        "start_time": "598.39"
    },
    {
        "id": "57edb794",
        "text": "And um so in other words, like uh the disco fourier transform depends on the frequency. And this means that uh each of these like formulas is gonna give us a complex uh fourier coefficient for the KF frequency and the complex fourier coefficient provides this information to uh about two parameters the face and the magnitude. Now, if you want to really know what phase and magnitude mean in terms of the fourier transform, I suggest you to go check out this video on uh the fourier transform and there like you'll get like a better picture but I hope like you've already watched that. OK. So this is for the discrete period transform. What about the short time period transform? As we can see here, a capital S depends not only on K on frequency but also on M. So what's this M well M is a proxy for time. So that the short time period transform depends both on frequency and time.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=620s",
        "start_time": "620.25"
    },
    {
        "id": "b508d57e",
        "text": "and the complex fourier coefficient provides this information to uh about two parameters the face and the magnitude. Now, if you want to really know what phase and magnitude mean in terms of the fourier transform, I suggest you to go check out this video on uh the fourier transform and there like you'll get like a better picture but I hope like you've already watched that. OK. So this is for the discrete period transform. What about the short time period transform? As we can see here, a capital S depends not only on K on frequency but also on M. So what's this M well M is a proxy for time. So that the short time period transform depends both on frequency and time. So now let's understand a little bit better like what this is and uh like nominally uh it's just like the, the frame, the, the, the frame number we are currently in. But let's visualize this. OK. So here we have like we are back again with our um original signal and like the different frames here. And so",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=638s",
        "start_time": "638.039"
    },
    {
        "id": "74b060fa",
        "text": "OK. So this is for the discrete period transform. What about the short time period transform? As we can see here, a capital S depends not only on K on frequency but also on M. So what's this M well M is a proxy for time. So that the short time period transform depends both on frequency and time. So now let's understand a little bit better like what this is and uh like nominally uh it's just like the, the frame, the, the, the frame number we are currently in. But let's visualize this. OK. So here we have like we are back again with our um original signal and like the different frames here. And so for the first frame we have M which is equal to one. Here we have for the second frame M equal to two and M equal to three, you get the idea, right? So M is just like the frame number. So in other words, uh the result that we get from the short time period transform is the uh fourier",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=663s",
        "start_time": "663.679"
    },
    {
        "id": "7c984c7a",
        "text": "So now let's understand a little bit better like what this is and uh like nominally uh it's just like the, the frame, the, the, the frame number we are currently in. But let's visualize this. OK. So here we have like we are back again with our um original signal and like the different frames here. And so for the first frame we have M which is equal to one. Here we have for the second frame M equal to two and M equal to three, you get the idea, right? So M is just like the frame number. So in other words, uh the result that we get from the short time period transform is the uh fourier coefficient for the KF frequency at the N temporal bin or M frame. OK. But still like the fourier coefficient that we get, it's still like a complex number that has information about phase and magnitude. OK moving on.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=689s",
        "start_time": "689.469"
    },
    {
        "id": "61bca22a",
        "text": "for the first frame we have M which is equal to one. Here we have for the second frame M equal to two and M equal to three, you get the idea, right? So M is just like the frame number. So in other words, uh the result that we get from the short time period transform is the uh fourier coefficient for the KF frequency at the N temporal bin or M frame. OK. But still like the fourier coefficient that we get, it's still like a complex number that has information about phase and magnitude. OK moving on. So the next step is to compare these two sums if you guys remember from the DFT. Um So what what was happening here is we were summing all across like all the uh samples. So basically we were summing across like all the time, all the duration of the signal.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=714s",
        "start_time": "714.14"
    },
    {
        "id": "b978cd79",
        "text": "coefficient for the KF frequency at the N temporal bin or M frame. OK. But still like the fourier coefficient that we get, it's still like a complex number that has information about phase and magnitude. OK moving on. So the next step is to compare these two sums if you guys remember from the DFT. Um So what what was happening here is we were summing all across like all the uh samples. So basically we were summing across like all the time, all the duration of the signal. And we do like something similar also in the STFT kind of like intuitively, we are doing the same thing we are summing like across time or given like we are in a discrete uh domain across like all the different samples. But what's different between these two sums is capital N. In the case of the discrete fourier transform, we are summing across all of the samples",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=736s",
        "start_time": "736.692"
    },
    {
        "id": "4a06d145",
        "text": "So the next step is to compare these two sums if you guys remember from the DFT. Um So what what was happening here is we were summing all across like all the uh samples. So basically we were summing across like all the time, all the duration of the signal. And we do like something similar also in the STFT kind of like intuitively, we are doing the same thing we are summing like across time or given like we are in a discrete uh domain across like all the different samples. But what's different between these two sums is capital N. In the case of the discrete fourier transform, we are summing across all of the samples in the signal. So N is equal to all the samples in the signal. In the case of the short time period transform capital N over here is equal to the frame size. And that's because we're not considering all of the signal but just one frame. And in one frame, we have a number of samples that's equal to the frame size by definition,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=759s",
        "start_time": "759.245"
    },
    {
        "id": "ae4e65ea",
        "text": "And we do like something similar also in the STFT kind of like intuitively, we are doing the same thing we are summing like across time or given like we are in a discrete uh domain across like all the different samples. But what's different between these two sums is capital N. In the case of the discrete fourier transform, we are summing across all of the samples in the signal. So N is equal to all the samples in the signal. In the case of the short time period transform capital N over here is equal to the frame size. And that's because we're not considering all of the signal but just one frame. And in one frame, we have a number of samples that's equal to the frame size by definition, right? So now you start to get the idea of how like this STFT works. But to see this like even more, um specifically, we need to move to the next element of this uh formulas which is the signal itself. So let's analyze the one in the top",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=782s",
        "start_time": "782.059"
    },
    {
        "id": "261d9e52",
        "text": "in the signal. So N is equal to all the samples in the signal. In the case of the short time period transform capital N over here is equal to the frame size. And that's because we're not considering all of the signal but just one frame. And in one frame, we have a number of samples that's equal to the frame size by definition, right? So now you start to get the idea of how like this STFT works. But to see this like even more, um specifically, we need to move to the next element of this uh formulas which is the signal itself. So let's analyze the one in the top formula. So for the disc fourier transform X of N is just like the signal considered, I mean the whole signal. So all of these samples in the short time fourier transform, by contrast, we are only considering the signal",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=806s",
        "start_time": "806.169"
    },
    {
        "id": "ba5d27bc",
        "text": "right? So now you start to get the idea of how like this STFT works. But to see this like even more, um specifically, we need to move to the next element of this uh formulas which is the signal itself. So let's analyze the one in the top formula. So for the disc fourier transform X of N is just like the signal considered, I mean the whole signal. So all of these samples in the short time fourier transform, by contrast, we are only considering the signal uh that's uh present like in the current frame. So in other words, we're considering all the samples that are present in the current M frame. And so why is that the case? Well, that's the case because first of all this M multiplied by capital H is the starting sample of the current frame",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=830s",
        "start_time": "830.479"
    },
    {
        "id": "e074fab6",
        "text": "formula. So for the disc fourier transform X of N is just like the signal considered, I mean the whole signal. So all of these samples in the short time fourier transform, by contrast, we are only considering the signal uh that's uh present like in the current frame. So in other words, we're considering all the samples that are present in the current M frame. And so why is that the case? Well, that's the case because first of all this M multiplied by capital H is the starting sample of the current frame M is the current frame and H capital H is the H size. So if you multiply two, you realize that this is like the starting sample of the current frame and then we add N but this N moves like from zero to the frame size minus one, which basically means like that we are kind of like covering all the samples in that frame. OK.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=849s",
        "start_time": "849.674"
    },
    {
        "id": "7864f1bc",
        "text": "uh that's uh present like in the current frame. So in other words, we're considering all the samples that are present in the current M frame. And so why is that the case? Well, that's the case because first of all this M multiplied by capital H is the starting sample of the current frame M is the current frame and H capital H is the H size. So if you multiply two, you realize that this is like the starting sample of the current frame and then we add N but this N moves like from zero to the frame size minus one, which basically means like that we are kind of like covering all the samples in that frame. OK. Now, if we want to visualize this, we can just get back to the signal. And here, like we have like this rectangle and here we have like all of the uh the signal like for one frame. Uh And here like on the left of this rectangle, this vertical line here is the starting sample of the frame which is equal to N multiplied by capital H OK.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=869s",
        "start_time": "869.179"
    },
    {
        "id": "6e94febd",
        "text": "M is the current frame and H capital H is the H size. So if you multiply two, you realize that this is like the starting sample of the current frame and then we add N but this N moves like from zero to the frame size minus one, which basically means like that we are kind of like covering all the samples in that frame. OK. Now, if we want to visualize this, we can just get back to the signal. And here, like we have like this rectangle and here we have like all of the uh the signal like for one frame. Uh And here like on the left of this rectangle, this vertical line here is the starting sample of the frame which is equal to N multiplied by capital H OK. So now um we like the next step though in the case of the short time fourier transform is that we should multiply this signal by the uh windowing function. And we do that with this uh like representation here. So we have like the signal where the, the signal for one frame and we multiply that by the windowing function. And",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=893s",
        "start_time": "893.815"
    },
    {
        "id": "4e5152fd",
        "text": "Now, if we want to visualize this, we can just get back to the signal. And here, like we have like this rectangle and here we have like all of the uh the signal like for one frame. Uh And here like on the left of this rectangle, this vertical line here is the starting sample of the frame which is equal to N multiplied by capital H OK. So now um we like the next step though in the case of the short time fourier transform is that we should multiply this signal by the uh windowing function. And we do that with this uh like representation here. So we have like the signal where the, the signal for one frame and we multiply that by the windowing function. And uh again, so we already saw this right. And so we are multiplying the uh original uh signal like for a specific frame by uh the windowing function and we obtain the windowed um signal, OK? For that one frame,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=918s",
        "start_time": "918.82"
    },
    {
        "id": "f927ae36",
        "text": "So now um we like the next step though in the case of the short time fourier transform is that we should multiply this signal by the uh windowing function. And we do that with this uh like representation here. So we have like the signal where the, the signal for one frame and we multiply that by the windowing function. And uh again, so we already saw this right. And so we are multiplying the uh original uh signal like for a specific frame by uh the windowing function and we obtain the windowed um signal, OK? For that one frame, OK. So moving on, we have the last step. And the last step is the same for both the discrete fourier transform and the short term fourier transform. In other words, we are multiplying by a pure turn that has frequency given by K divided by capital N.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=947s",
        "start_time": "947.71"
    },
    {
        "id": "6e4500ba",
        "text": "uh again, so we already saw this right. And so we are multiplying the uh original uh signal like for a specific frame by uh the windowing function and we obtain the windowed um signal, OK? For that one frame, OK. So moving on, we have the last step. And the last step is the same for both the discrete fourier transform and the short term fourier transform. In other words, we are multiplying by a pure turn that has frequency given by K divided by capital N. So by doing so what we are doing is we are taking the uh the signal uh And then we are decomposing it and projecting it onto uh the pure turn with frequency um K divided by capital N.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=973s",
        "start_time": "973.239"
    },
    {
        "id": "a62c7226",
        "text": "OK. So moving on, we have the last step. And the last step is the same for both the discrete fourier transform and the short term fourier transform. In other words, we are multiplying by a pure turn that has frequency given by K divided by capital N. So by doing so what we are doing is we are taking the uh the signal uh And then we are decomposing it and projecting it onto uh the pure turn with frequency um K divided by capital N. OK. So here you have the comparison between the math behind the discrete fourier transform and the math for the short time fourier transform. But now you may be wondering, OK, now, more or less like I get like the math here but what are like the outputs? So what what do we get out of AD FT and A NSTFT? Let's take a look at that.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=992s",
        "start_time": "992.849"
    },
    {
        "id": "ddb7b6ae",
        "text": "So by doing so what we are doing is we are taking the uh the signal uh And then we are decomposing it and projecting it onto uh the pure turn with frequency um K divided by capital N. OK. So here you have the comparison between the math behind the discrete fourier transform and the math for the short time fourier transform. But now you may be wondering, OK, now, more or less like I get like the math here but what are like the outputs? So what what do we get out of AD FT and A NSTFT? Let's take a look at that. OK. So for DFT we extract uh a spectral vector and which uh for a number of like frequency beams. In other words, like we, we get a fourier coefficient for each of the frequency components we've um decomposed our original",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1011s",
        "start_time": "1011.03"
    },
    {
        "id": "28e18f3c",
        "text": "OK. So here you have the comparison between the math behind the discrete fourier transform and the math for the short time fourier transform. But now you may be wondering, OK, now, more or less like I get like the math here but what are like the outputs? So what what do we get out of AD FT and A NSTFT? Let's take a look at that. OK. So for DFT we extract uh a spectral vector and which uh for a number of like frequency beams. In other words, like we, we get a fourier coefficient for each of the frequency components we've um decomposed our original signal into and this is a one dimensional array. It's just like a vector, right? And there's no mention of time in here because everything is averaged across the whole duration of a of a signal. But with the STFT, we have like something that's quite different. In this case, we don't have a one dimension",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1029s",
        "start_time": "1029.588"
    },
    {
        "id": "3db2ac5f",
        "text": "OK. So for DFT we extract uh a spectral vector and which uh for a number of like frequency beams. In other words, like we, we get a fourier coefficient for each of the frequency components we've um decomposed our original signal into and this is a one dimensional array. It's just like a vector, right? And there's no mention of time in here because everything is averaged across the whole duration of a of a signal. But with the STFT, we have like something that's quite different. In this case, we don't have a one dimension value one dimensional array, but rather a two dimensional array or in other words, a spectral matrix that has a number of frequency bins and a number of frames. And in other words, we get a complex fourier coefficient for each frequency bin that we are considering for each frame.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1051s",
        "start_time": "1051.729"
    },
    {
        "id": "eead1f53",
        "text": "signal into and this is a one dimensional array. It's just like a vector, right? And there's no mention of time in here because everything is averaged across the whole duration of a of a signal. But with the STFT, we have like something that's quite different. In this case, we don't have a one dimension value one dimensional array, but rather a two dimensional array or in other words, a spectral matrix that has a number of frequency bins and a number of frames. And in other words, we get a complex fourier coefficient for each frequency bin that we are considering for each frame. OK. And so in other words, we have both a reference, a reference to frequency as we had with this quid for transform. But now we've gained all the information about time through the different frames which are proxies for time. OK. But um you may be wondering, but can we calculate the actual number of frequency bins and number of frames that we get out of us? And Stft? Well, yes, of course, we can and we'll do that.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1076s",
        "start_time": "1076.196"
    },
    {
        "id": "dd875d4d",
        "text": "value one dimensional array, but rather a two dimensional array or in other words, a spectral matrix that has a number of frequency bins and a number of frames. And in other words, we get a complex fourier coefficient for each frequency bin that we are considering for each frame. OK. And so in other words, we have both a reference, a reference to frequency as we had with this quid for transform. But now we've gained all the information about time through the different frames which are proxies for time. OK. But um you may be wondering, but can we calculate the actual number of frequency bins and number of frames that we get out of us? And Stft? Well, yes, of course, we can and we'll do that. OK. So how do we get the number of frequency bins? Well, this is quite easy to get. And uh you have the formula here, so you get the frame size, you divide the frame size by two and then you add up one and this gives us the number of frequency bins. Now let's try to understand why this is the case. If you guys remember from my uh earlier video on the descript fourier transform,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1100s",
        "start_time": "1100.661"
    },
    {
        "id": "456746ab",
        "text": "OK. And so in other words, we have both a reference, a reference to frequency as we had with this quid for transform. But now we've gained all the information about time through the different frames which are proxies for time. OK. But um you may be wondering, but can we calculate the actual number of frequency bins and number of frames that we get out of us? And Stft? Well, yes, of course, we can and we'll do that. OK. So how do we get the number of frequency bins? Well, this is quite easy to get. And uh you have the formula here, so you get the frame size, you divide the frame size by two and then you add up one and this gives us the number of frequency bins. Now let's try to understand why this is the case. If you guys remember from my uh earlier video on the descript fourier transform, you should know that the number of frequency bins that we get out of a discrete fourier transform is equal to the number of samples that we have in the, in the on the whole signal. Now, in the case of an STFT, we don't uh average uh",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1125s",
        "start_time": "1125.459"
    },
    {
        "id": "02466254",
        "text": "OK. So how do we get the number of frequency bins? Well, this is quite easy to get. And uh you have the formula here, so you get the frame size, you divide the frame size by two and then you add up one and this gives us the number of frequency bins. Now let's try to understand why this is the case. If you guys remember from my uh earlier video on the descript fourier transform, you should know that the number of frequency bins that we get out of a discrete fourier transform is equal to the number of samples that we have in the, in the on the whole signal. Now, in the case of an STFT, we don't uh average uh don't consider the whole uh samples at once but rather like a frame size number of samples. So we would expect that the number of frequency bins uh for each four year transform that we get is equal to the frame size. But",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1154s",
        "start_time": "1154.43"
    },
    {
        "id": "293c458c",
        "text": "you should know that the number of frequency bins that we get out of a discrete fourier transform is equal to the number of samples that we have in the, in the on the whole signal. Now, in the case of an STFT, we don't uh average uh don't consider the whole uh samples at once but rather like a frame size number of samples. So we would expect that the number of frequency bins uh for each four year transform that we get is equal to the frame size. But we don't get that we get the frame size divided by two plus one. Was that the case? Well, if you remember once again from the discrete fourier transform video, we saw that the discrete fourier transform is symmetrical has a mirror symmetry around the center frequency which is the niquet frequency.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1183s",
        "start_time": "1183.13"
    },
    {
        "id": "d5780bb8",
        "text": "don't consider the whole uh samples at once but rather like a frame size number of samples. So we would expect that the number of frequency bins uh for each four year transform that we get is equal to the frame size. But we don't get that we get the frame size divided by two plus one. Was that the case? Well, if you remember once again from the discrete fourier transform video, we saw that the discrete fourier transform is symmetrical has a mirror symmetry around the center frequency which is the niquet frequency. And uh what happens there is that's basically like the, the first half uh has some information and then that gets like mirrored in the second half. And so in a short time for trans, we, we consider that and so we don't need to take information about all of those bins because it's just like a redundant. We only take informa the information",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1201s",
        "start_time": "1201.67"
    },
    {
        "id": "274f8b8e",
        "text": "we don't get that we get the frame size divided by two plus one. Was that the case? Well, if you remember once again from the discrete fourier transform video, we saw that the discrete fourier transform is symmetrical has a mirror symmetry around the center frequency which is the niquet frequency. And uh what happens there is that's basically like the, the first half uh has some information and then that gets like mirrored in the second half. And so in a short time for trans, we, we consider that and so we don't need to take information about all of those bins because it's just like a redundant. We only take informa the information about like the first half. So frame size divided by two plus one. So that's the reason why now if you haven't fallen that uh completely, I highly suggest you to go check out my video on the descript fourier transform to understand what it meant more specifically there. OK. Now let's move on to the number of frames. And so here we have another very nice,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1220s",
        "start_time": "1220.489"
    },
    {
        "id": "10dc58ca",
        "text": "And uh what happens there is that's basically like the, the first half uh has some information and then that gets like mirrored in the second half. And so in a short time for trans, we, we consider that and so we don't need to take information about all of those bins because it's just like a redundant. We only take informa the information about like the first half. So frame size divided by two plus one. So that's the reason why now if you haven't fallen that uh completely, I highly suggest you to go check out my video on the descript fourier transform to understand what it meant more specifically there. OK. Now let's move on to the number of frames. And so here we have another very nice, a little formula and the number of frames is given by the total number of samples that we have in a signal minus the frame size divided by the H size plus one. Now, I'm not gonna get into the details of explaining this visually and I highly suggest you as an exercise to play around with this and understand why this formula gives us the number of frames.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1240s",
        "start_time": "1240.849"
    },
    {
        "id": "45b1e8ab",
        "text": "about like the first half. So frame size divided by two plus one. So that's the reason why now if you haven't fallen that uh completely, I highly suggest you to go check out my video on the descript fourier transform to understand what it meant more specifically there. OK. Now let's move on to the number of frames. And so here we have another very nice, a little formula and the number of frames is given by the total number of samples that we have in a signal minus the frame size divided by the H size plus one. Now, I'm not gonna get into the details of explaining this visually and I highly suggest you as an exercise to play around with this and understand why this formula gives us the number of frames. OK? But I know this can feel a little bit abstract. So let's go move on with an example. So here we have like a bunch of like uh STFT uh parameters and we want to find the actual output shape. So we have a signal with 10,000 samples, we have a frame size which is equal to 1000 samples and we have a H size of 500 samples. So",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1262s",
        "start_time": "1262.225"
    },
    {
        "id": "e0b3e449",
        "text": "a little formula and the number of frames is given by the total number of samples that we have in a signal minus the frame size divided by the H size plus one. Now, I'm not gonna get into the details of explaining this visually and I highly suggest you as an exercise to play around with this and understand why this formula gives us the number of frames. OK? But I know this can feel a little bit abstract. So let's go move on with an example. So here we have like a bunch of like uh STFT uh parameters and we want to find the actual output shape. So we have a signal with 10,000 samples, we have a frame size which is equal to 1000 samples and we have a H size of 500 samples. So uh for the number of frequency bins, so we take the frame size, we divided it by two, we add one and we get 501 frequency bins. OK? But these are frequency bins and we know that uh they divide a certain frequency range um equally. And so we have like a frequency range that's divided in 501 bins in this case.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1283s",
        "start_time": "1283.602"
    },
    {
        "id": "f20c87c4",
        "text": "OK? But I know this can feel a little bit abstract. So let's go move on with an example. So here we have like a bunch of like uh STFT uh parameters and we want to find the actual output shape. So we have a signal with 10,000 samples, we have a frame size which is equal to 1000 samples and we have a H size of 500 samples. So uh for the number of frequency bins, so we take the frame size, we divided it by two, we add one and we get 501 frequency bins. OK? But these are frequency bins and we know that uh they divide a certain frequency range um equally. And so we have like a frequency range that's divided in 501 bins in this case. Uh Now what, what, what is that range? Well, that range and the frequency range is between zero Hertz and the sampling range divided by two Hertz. And that is the NS frequency once again.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1305s",
        "start_time": "1305.339"
    },
    {
        "id": "3dd78d93",
        "text": "uh for the number of frequency bins, so we take the frame size, we divided it by two, we add one and we get 501 frequency bins. OK? But these are frequency bins and we know that uh they divide a certain frequency range um equally. And so we have like a frequency range that's divided in 501 bins in this case. Uh Now what, what, what is that range? Well, that range and the frequency range is between zero Hertz and the sampling range divided by two Hertz. And that is the NS frequency once again. Uh So if you want to know why that's the case, once again, just go back to my video on this grid fourier transform. OK. So moving on the number of frames. So here uh we have like a little formula and so we have to take the number of samples in the signal. So it's 10,000 minus the uh the frame size. And this is gonna be divided by the hub size. And all of this, we have to, all of this, we have to add one and",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1329s",
        "start_time": "1329.15"
    },
    {
        "id": "2d38f75c",
        "text": "Uh Now what, what, what is that range? Well, that range and the frequency range is between zero Hertz and the sampling range divided by two Hertz. And that is the NS frequency once again. Uh So if you want to know why that's the case, once again, just go back to my video on this grid fourier transform. OK. So moving on the number of frames. So here uh we have like a little formula and so we have to take the number of samples in the signal. So it's 10,000 minus the uh the frame size. And this is gonna be divided by the hub size. And all of this, we have to, all of this, we have to add one and the result is 19. So we have 19 frames, this signal is going to be divided into. So the overall uh output shape of the stft in this particular case is gonna be 501 and 19. So it's a two dimensional ray. The first dimension uh provides us information about frequency. The second uh provides us information about the temporal um bins or the number of frames.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1354s",
        "start_time": "1354.285"
    },
    {
        "id": "3a8e3760",
        "text": "Uh So if you want to know why that's the case, once again, just go back to my video on this grid fourier transform. OK. So moving on the number of frames. So here uh we have like a little formula and so we have to take the number of samples in the signal. So it's 10,000 minus the uh the frame size. And this is gonna be divided by the hub size. And all of this, we have to, all of this, we have to add one and the result is 19. So we have 19 frames, this signal is going to be divided into. So the overall uh output shape of the stft in this particular case is gonna be 501 and 19. So it's a two dimensional ray. The first dimension uh provides us information about frequency. The second uh provides us information about the temporal um bins or the number of frames. OK. So now uh I think like we should uh take a look at the short time um fourier transform and try to understand the different parameters. So the important thing that you should understand here is that really the short time fourier transform",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1370s",
        "start_time": "1370.29"
    },
    {
        "id": "d656d507",
        "text": "the result is 19. So we have 19 frames, this signal is going to be divided into. So the overall uh output shape of the stft in this particular case is gonna be 501 and 19. So it's a two dimensional ray. The first dimension uh provides us information about frequency. The second uh provides us information about the temporal um bins or the number of frames. OK. So now uh I think like we should uh take a look at the short time um fourier transform and try to understand the different parameters. So the important thing that you should understand here is that really the short time fourier transform depends on a bunch of parameters that we pass. So depending on the parameters that we pass, we're gonna get an output that's gonna be different. So one of these parameters and we've already encountered it is the frame size or in other words, how big are the chunks we divide our original signal into? And this is measured in frames. And",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1400s",
        "start_time": "1400.06"
    },
    {
        "id": "9260acb1",
        "text": "OK. So now uh I think like we should uh take a look at the short time um fourier transform and try to understand the different parameters. So the important thing that you should understand here is that really the short time fourier transform depends on a bunch of parameters that we pass. So depending on the parameters that we pass, we're gonna get an output that's gonna be different. So one of these parameters and we've already encountered it is the frame size or in other words, how big are the chunks we divide our original signal into? And this is measured in frames. And the usual values that we have here are like like this like 512 1024 8, 1000 100 and 92. As you can see, these are power of two numbers. And as we already discussed in a previous video, it's important that the frame size is a power of two",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1430s",
        "start_time": "1430.329"
    },
    {
        "id": "2de40d43",
        "text": "depends on a bunch of parameters that we pass. So depending on the parameters that we pass, we're gonna get an output that's gonna be different. So one of these parameters and we've already encountered it is the frame size or in other words, how big are the chunks we divide our original signal into? And this is measured in frames. And the usual values that we have here are like like this like 512 1024 8, 1000 100 and 92. As you can see, these are power of two numbers. And as we already discussed in a previous video, it's important that the frame size is a power of two number. And that's because um with that specific number, we can use the fast fourier transform to calculate the discrete fourier transform, which is a very quick and computationally efficient way of extracting the discrete fourier transform.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1449s",
        "start_time": "1449.099"
    },
    {
        "id": "725aeffb",
        "text": "the usual values that we have here are like like this like 512 1024 8, 1000 100 and 92. As you can see, these are power of two numbers. And as we already discussed in a previous video, it's important that the frame size is a power of two number. And that's because um with that specific number, we can use the fast fourier transform to calculate the discrete fourier transform, which is a very quick and computationally efficient way of extracting the discrete fourier transform. Now, there's a an interesting aspect uh in when we choose the frame size and it's called the time frequency trade off. So if we get like a larger a large frame size, what usually what happens is that the frequency resolution is gonna increase and the time resolution uh is gonna be degraded. So where is that the case? Well, so we know that if we",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1472s",
        "start_time": "1472.599"
    },
    {
        "id": "4800aacd",
        "text": "number. And that's because um with that specific number, we can use the fast fourier transform to calculate the discrete fourier transform, which is a very quick and computationally efficient way of extracting the discrete fourier transform. Now, there's a an interesting aspect uh in when we choose the frame size and it's called the time frequency trade off. So if we get like a larger a large frame size, what usually what happens is that the frequency resolution is gonna increase and the time resolution uh is gonna be degraded. So where is that the case? Well, so we know that if we uh uh enlarge the frame size, so we take more samples, we are gonna be having like more frequency bins. And so if you have more frequency bins, it means that your frequency resolution overall improves. But if you take more, more samples in one frame size, it means that you are taking like a larger, you're considering a larger chunk of",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1491s",
        "start_time": "1491.15"
    },
    {
        "id": "47ac3fe8",
        "text": "Now, there's a an interesting aspect uh in when we choose the frame size and it's called the time frequency trade off. So if we get like a larger a large frame size, what usually what happens is that the frequency resolution is gonna increase and the time resolution uh is gonna be degraded. So where is that the case? Well, so we know that if we uh uh enlarge the frame size, so we take more samples, we are gonna be having like more frequency bins. And so if you have more frequency bins, it means that your frequency resolution overall improves. But if you take more, more samples in one frame size, it means that you are taking like a larger, you're considering a larger chunk of time because samples like are pro for for time. OK. And in other words, if you're taking like like a larger chunk of time, it means that the time resolution goes down, right? And uh the opposite is also true. In other words, if you take a smaller frame size, then",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1510s",
        "start_time": "1510.18"
    },
    {
        "id": "a986cdba",
        "text": "uh uh enlarge the frame size, so we take more samples, we are gonna be having like more frequency bins. And so if you have more frequency bins, it means that your frequency resolution overall improves. But if you take more, more samples in one frame size, it means that you are taking like a larger, you're considering a larger chunk of time because samples like are pro for for time. OK. And in other words, if you're taking like like a larger chunk of time, it means that the time resolution goes down, right? And uh the opposite is also true. In other words, if you take a smaller frame size, then the frequency resolution is gonna go down. And that's because you're gonna have like a, a smaller number of frequency bins as output. But you're gonna have a higher, better time resolution just because you're considering less samples which equates like to uh like a, a less amount of time.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1540s",
        "start_time": "1540.239"
    },
    {
        "id": "2af76738",
        "text": "time because samples like are pro for for time. OK. And in other words, if you're taking like like a larger chunk of time, it means that the time resolution goes down, right? And uh the opposite is also true. In other words, if you take a smaller frame size, then the frequency resolution is gonna go down. And that's because you're gonna have like a, a smaller number of frequency bins as output. But you're gonna have a higher, better time resolution just because you're considering less samples which equates like to uh like a, a less amount of time. And so you're gonna be calculating like the, the, the fourier transform like on smaller chunks of time. So your time resolution is gonna be better. Now, this is like a time frequency trade off as you can see here. So when, when you try to improve the frequency resolution, then the time resolution is gonna go down and vice versa.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1566s",
        "start_time": "1566.699"
    },
    {
        "id": "63830c06",
        "text": "the frequency resolution is gonna go down. And that's because you're gonna have like a, a smaller number of frequency bins as output. But you're gonna have a higher, better time resolution just because you're considering less samples which equates like to uh like a, a less amount of time. And so you're gonna be calculating like the, the, the fourier transform like on smaller chunks of time. So your time resolution is gonna be better. Now, this is like a time frequency trade off as you can see here. So when, when you try to improve the frequency resolution, then the time resolution is gonna go down and vice versa. Now how do we solve that? Well, we don't really solve that. We just have like some heuristics most of the time you want to find a value of the frame size, that's a K and it's a good trade off between frequency and time resolution. Um",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1587s",
        "start_time": "1587.329"
    },
    {
        "id": "de4f72dd",
        "text": "And so you're gonna be calculating like the, the, the fourier transform like on smaller chunks of time. So your time resolution is gonna be better. Now, this is like a time frequency trade off as you can see here. So when, when you try to improve the frequency resolution, then the time resolution is gonna go down and vice versa. Now how do we solve that? Well, we don't really solve that. We just have like some heuristics most of the time you want to find a value of the frame size, that's a K and it's a good trade off between frequency and time resolution. Um But this really depends on the type of application that you're um a problem that you are interested in. So certain problems for certain problems, it's more important that you have a higher frequency resolution. And in that case, you should take like a,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1608s",
        "start_time": "1608.079"
    },
    {
        "id": "e092ed13",
        "text": "Now how do we solve that? Well, we don't really solve that. We just have like some heuristics most of the time you want to find a value of the frame size, that's a K and it's a good trade off between frequency and time resolution. Um But this really depends on the type of application that you're um a problem that you are interested in. So certain problems for certain problems, it's more important that you have a higher frequency resolution. And in that case, you should take like a, a bigger uh frame size uh for other applications like for example, onset detection, you're not really super interested in the frequency frequency resolution, perhaps you're more interested in just like having like a um very precise or like high, highly ol like time. Um so that you a very good like time resolution so that you can really know what happens like at each point in time.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1629s",
        "start_time": "1629.88"
    },
    {
        "id": "7804590d",
        "text": "But this really depends on the type of application that you're um a problem that you are interested in. So certain problems for certain problems, it's more important that you have a higher frequency resolution. And in that case, you should take like a, a bigger uh frame size uh for other applications like for example, onset detection, you're not really super interested in the frequency frequency resolution, perhaps you're more interested in just like having like a um very precise or like high, highly ol like time. Um so that you a very good like time resolution so that you can really know what happens like at each point in time. OK. So I think like this is like very uh important to keep in mind when you decide like which frame size like to, to take. Because I mean, the two things frequent and temporal resolution are uh related uh together and inversely",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1645s",
        "start_time": "1645.06"
    },
    {
        "id": "e3e8d5e8",
        "text": "a bigger uh frame size uh for other applications like for example, onset detection, you're not really super interested in the frequency frequency resolution, perhaps you're more interested in just like having like a um very precise or like high, highly ol like time. Um so that you a very good like time resolution so that you can really know what happens like at each point in time. OK. So I think like this is like very uh important to keep in mind when you decide like which frame size like to, to take. Because I mean, the two things frequent and temporal resolution are uh related uh together and inversely um related in a sense, right. OK. So now um let's move on to the next uh short term fourier transform parameter. And that's the hop size. We already saw that multiple times in this video and in earlier videos, and we know that's the number of samples that we slide to the right when we want to take a new frame. So usual values here, once again, 256 512 I mean all power of two most of the time.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1660s",
        "start_time": "1660.114"
    },
    {
        "id": "f70b3524",
        "text": "OK. So I think like this is like very uh important to keep in mind when you decide like which frame size like to, to take. Because I mean, the two things frequent and temporal resolution are uh related uh together and inversely um related in a sense, right. OK. So now um let's move on to the next uh short term fourier transform parameter. And that's the hop size. We already saw that multiple times in this video and in earlier videos, and we know that's the number of samples that we slide to the right when we want to take a new frame. So usual values here, once again, 256 512 I mean all power of two most of the time. And we can also define this as a fraction of the frame size. So a half of the frame size or 1/4 or 1/8 of the uh frame size. So you have like both definitions, absolute and relative. OK.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1690s",
        "start_time": "1690.209"
    },
    {
        "id": "df639901",
        "text": "um related in a sense, right. OK. So now um let's move on to the next uh short term fourier transform parameter. And that's the hop size. We already saw that multiple times in this video and in earlier videos, and we know that's the number of samples that we slide to the right when we want to take a new frame. So usual values here, once again, 256 512 I mean all power of two most of the time. And we can also define this as a fraction of the frame size. So a half of the frame size or 1/4 or 1/8 of the uh frame size. So you have like both definitions, absolute and relative. OK. Now moving on uh a third, very important parameter is the windowing function. Obviously, the short time fourier transform is not only like a function of the signal itself, but it's a function also of the windowing function that we choose. Because different windowing functions are gonna",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1706s",
        "start_time": "1706.26"
    },
    {
        "id": "6ab101ed",
        "text": "And we can also define this as a fraction of the frame size. So a half of the frame size or 1/4 or 1/8 of the uh frame size. So you have like both definitions, absolute and relative. OK. Now moving on uh a third, very important parameter is the windowing function. Obviously, the short time fourier transform is not only like a function of the signal itself, but it's a function also of the windowing function that we choose. Because different windowing functions are gonna uh kind of like modulate the original signal in different manners and and this is going to have an effect on the um short time period transform results. OK. So we introduced the rectangle window function, but that's not really used at all in um",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1734s",
        "start_time": "1734.65"
    },
    {
        "id": "b2c69584",
        "text": "Now moving on uh a third, very important parameter is the windowing function. Obviously, the short time fourier transform is not only like a function of the signal itself, but it's a function also of the windowing function that we choose. Because different windowing functions are gonna uh kind of like modulate the original signal in different manners and and this is going to have an effect on the um short time period transform results. OK. So we introduced the rectangle window function, but that's not really used at all in um in digital signal processing. And that's because like it creates discontinuities like on the edges, like all of the windows rather to avoid those you want to use like a bell shaped curve. One of which the most important probably is the hand window. So 90% of the time probably you are going to be used",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1750s",
        "start_time": "1750.979"
    },
    {
        "id": "d7d731f1",
        "text": "uh kind of like modulate the original signal in different manners and and this is going to have an effect on the um short time period transform results. OK. So we introduced the rectangle window function, but that's not really used at all in um in digital signal processing. And that's because like it creates discontinuities like on the edges, like all of the windows rather to avoid those you want to use like a bell shaped curve. One of which the most important probably is the hand window. So 90% of the time probably you are going to be used the hand window when you perform a short time for transform perhaps without even knowing that. And so this uh function is is given by this formula here, which is obviously like a periodic uh formula, a periodic like function over here and here like you have visualization of this. So",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1769s",
        "start_time": "1769.589"
    },
    {
        "id": "181c5f35",
        "text": "in digital signal processing. And that's because like it creates discontinuities like on the edges, like all of the windows rather to avoid those you want to use like a bell shaped curve. One of which the most important probably is the hand window. So 90% of the time probably you are going to be used the hand window when you perform a short time for transform perhaps without even knowing that. And so this uh function is is given by this formula here, which is obviously like a periodic uh formula, a periodic like function over here and here like you have visualization of this. So um so let's say this like in action. So here we have like a signal here, we have like our bell shaped hand window. So when we apply the hand window to the signal, you see that the signal gets modulated and towards the end, the um the values of the",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1791s",
        "start_time": "1791.06"
    },
    {
        "id": "253f6d5a",
        "text": "the hand window when you perform a short time for transform perhaps without even knowing that. And so this uh function is is given by this formula here, which is obviously like a periodic uh formula, a periodic like function over here and here like you have visualization of this. So um so let's say this like in action. So here we have like a signal here, we have like our bell shaped hand window. So when we apply the hand window to the signal, you see that the signal gets modulated and towards the end, the um the values of the of the samples tend to get squashed right towards zero, so that we avoid these continuities on the edges. Once again, if you want to know why that is so important, you should go check out my video on audio feature extraction pipelines where I talk about spectral leakage. OK. So now let's move on to the final topic of this video",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1810s",
        "start_time": "1810.449"
    },
    {
        "id": "cc3e0c1e",
        "text": "um so let's say this like in action. So here we have like a signal here, we have like our bell shaped hand window. So when we apply the hand window to the signal, you see that the signal gets modulated and towards the end, the um the values of the of the samples tend to get squashed right towards zero, so that we avoid these continuities on the edges. Once again, if you want to know why that is so important, you should go check out my video on audio feature extraction pipelines where I talk about spectral leakage. OK. So now let's move on to the final topic of this video and this is like what you probably came here for and that's the spectrogram. So through the spectrogram, we can visualize sound. So, but how do we get to the spectrograms? Because up until now we know that we have the short time period transform and that's a matrix that has like",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1831s",
        "start_time": "1831.089"
    },
    {
        "id": "06a68ce9",
        "text": "of the samples tend to get squashed right towards zero, so that we avoid these continuities on the edges. Once again, if you want to know why that is so important, you should go check out my video on audio feature extraction pipelines where I talk about spectral leakage. OK. So now let's move on to the final topic of this video and this is like what you probably came here for and that's the spectrogram. So through the spectrogram, we can visualize sound. So, but how do we get to the spectrograms? Because up until now we know that we have the short time period transform and that's a matrix that has like complex numbers or fourier coefficients for each item in the matrix. So what we do is we take the squared magnitude of the short time fourier transform. And what we get is a",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1851s",
        "start_time": "1851.094"
    },
    {
        "id": "bbec9fbb",
        "text": "and this is like what you probably came here for and that's the spectrogram. So through the spectrogram, we can visualize sound. So, but how do we get to the spectrograms? Because up until now we know that we have the short time period transform and that's a matrix that has like complex numbers or fourier coefficients for each item in the matrix. So what we do is we take the squared magnitude of the short time fourier transform. And what we get is a uh matrix which has the same shape as the original short time period transform. But the difference is that now we have uh all, all of the items are not complex numbers anymore, but they are real numbers. And now we can visualize them using a hip map and the visualization is called a spectrogram. And this is",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1877s",
        "start_time": "1877.709"
    },
    {
        "id": "2e17f7c8",
        "text": "complex numbers or fourier coefficients for each item in the matrix. So what we do is we take the squared magnitude of the short time fourier transform. And what we get is a uh matrix which has the same shape as the original short time period transform. But the difference is that now we have uh all, all of the items are not complex numbers anymore, but they are real numbers. And now we can visualize them using a hip map and the visualization is called a spectrogram. And this is I mean this is like so important for all applications in A I audio because like so many times we are gonna be using spectrograms as features that we feed into the algorithms. So now let's take a look at the spectrogram here. So",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1901s",
        "start_time": "1901.849"
    },
    {
        "id": "a0ce5b24",
        "text": "uh matrix which has the same shape as the original short time period transform. But the difference is that now we have uh all, all of the items are not complex numbers anymore, but they are real numbers. And now we can visualize them using a hip map and the visualization is called a spectrogram. And this is I mean this is like so important for all applications in A I audio because like so many times we are gonna be using spectrograms as features that we feed into the algorithms. So now let's take a look at the spectrogram here. So on the X axis, we have time and these are like discrete times and you can see it here that you have like this tiny like discontinuities. And these are like all the frames, all the temporal bins. And on the Y axis, we have uh frequency with all of the different frequency bins. And so what we",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1918s",
        "start_time": "1918.069"
    },
    {
        "id": "af3ce443",
        "text": "I mean this is like so important for all applications in A I audio because like so many times we are gonna be using spectrograms as features that we feed into the algorithms. So now let's take a look at the spectrogram here. So on the X axis, we have time and these are like discrete times and you can see it here that you have like this tiny like discontinuities. And these are like all the frames, all the temporal bins. And on the Y axis, we have uh frequency with all of the different frequency bins. And so what we we are seeing here is how the different frequency beams, how the different frequency components evolve over time across the different um frames that we have in the original signal. And so, and now this is actually the dream that we wanted to come true.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1941s",
        "start_time": "1941.92"
    },
    {
        "id": "b8fc54cf",
        "text": "on the X axis, we have time and these are like discrete times and you can see it here that you have like this tiny like discontinuities. And these are like all the frames, all the temporal bins. And on the Y axis, we have uh frequency with all of the different frequency bins. And so what we we are seeing here is how the different frequency beams, how the different frequency components evolve over time across the different um frames that we have in the original signal. And so, and now this is actually the dream that we wanted to come true. So now, not only we have information about the frequency components, which was something that we already had with the spectrum, the magnitude spectrum, but we also have information about the components evolving over time,",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1957s",
        "start_time": "1957.55"
    },
    {
        "id": "693e3d05",
        "text": "we are seeing here is how the different frequency beams, how the different frequency components evolve over time across the different um frames that we have in the original signal. And so, and now this is actually the dream that we wanted to come true. So now, not only we have information about the frequency components, which was something that we already had with the spectrum, the magnitude spectrum, but we also have information about the components evolving over time, which is the information that we usually get from the time domain. And this is why a spectrogram is called a time frequency representation. And this is why spectrograms are so important in A I audio. Now, I'm not gonna get into the details of the implementation and all of these things like uh for spectrograms because that's the",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1978s",
        "start_time": "1978.005"
    },
    {
        "id": "6d67cd06",
        "text": "So now, not only we have information about the frequency components, which was something that we already had with the spectrum, the magnitude spectrum, but we also have information about the components evolving over time, which is the information that we usually get from the time domain. And this is why a spectrogram is called a time frequency representation. And this is why spectrograms are so important in A I audio. Now, I'm not gonna get into the details of the implementation and all of these things like uh for spectrograms because that's the um topic of the next video. So in the next video, we're gonna be using Python and Libros specifically for extracting spectrograms. We're gonna be looking into different flavors of spectrograms and understand which ones to use. And then we're gonna be examining like different audio samples and comparing them perhaps like different musical genres and how like they their spectrograms differ. OK.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=1998s",
        "start_time": "1998.64"
    },
    {
        "id": "1247399a",
        "text": "which is the information that we usually get from the time domain. And this is why a spectrogram is called a time frequency representation. And this is why spectrograms are so important in A I audio. Now, I'm not gonna get into the details of the implementation and all of these things like uh for spectrograms because that's the um topic of the next video. So in the next video, we're gonna be using Python and Libros specifically for extracting spectrograms. We're gonna be looking into different flavors of spectrograms and understand which ones to use. And then we're gonna be examining like different audio samples and comparing them perhaps like different musical genres and how like they their spectrograms differ. OK. So I hope like you found uh this video instructive and useful. If that's the case, please consider leaving a like and if you haven't subscribed yet to the channel and you want to see more videos like this, please consider subscribing. So if you have any questions, please leave them in the comments section below. I think that's all for today. I'll see you next time. Cheers.",
        "video": "Short-Time Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "-Yxj3yfvY-4",
        "youtube_link": "https://www.youtube.com/watch?v=-Yxj3yfvY-4&t=2014s",
        "start_time": "2014.969"
    },
    {
        "id": "d495d9f5",
        "text": "Hi, everybody and welcome to new exciting video and the audio signal processing for machine learning series. Last time we looked uh at the theory behind the discrete fourier transform and the fast fourier transform, which is a flavor of discrete fourier transform. This time, I want to actually extract the fourier transform coefficients using Python and specifically Noon P. Beyond that, I'm going to show you also a bunch of uh different um magnitude spectra for instruments which play the same note so that we can at least have a glance into why we perceive their timer uh differently. OK. So let's get started with our Gipson notebook. And here, first thing I want to import a few libraries that are gonna be useful for our purpose today.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=0s",
        "start_time": "0.409"
    },
    {
        "id": "df6fa89c",
        "text": "This time, I want to actually extract the fourier transform coefficients using Python and specifically Noon P. Beyond that, I'm going to show you also a bunch of uh different um magnitude spectra for instruments which play the same note so that we can at least have a glance into why we perceive their timer uh differently. OK. So let's get started with our Gipson notebook. And here, first thing I want to import a few libraries that are gonna be useful for our purpose today. So I'll import nin pi as NP. And as I said, we'll use nin pi to extract the fast fourier transform. Then I'll import mat plot leap dot pi plot SPLT. We'll use this library to plot the magnitude spectrums and then I'll uh import or S",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=16s",
        "start_time": "16.86"
    },
    {
        "id": "36d80c4e",
        "text": "same note so that we can at least have a glance into why we perceive their timer uh differently. OK. So let's get started with our Gipson notebook. And here, first thing I want to import a few libraries that are gonna be useful for our purpose today. So I'll import nin pi as NP. And as I said, we'll use nin pi to extract the fast fourier transform. Then I'll import mat plot leap dot pi plot SPLT. We'll use this library to plot the magnitude spectrums and then I'll uh import or S I'll import libros to load the uh audio files. And finally, I want to import IPYTHON dot uh display as IP D and we'll use, uh, this to, uh, listen to the audio files directly in our Jupiter notebook. Ok. So,",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=34s",
        "start_time": "34.365"
    },
    {
        "id": "1521afc7",
        "text": "So I'll import nin pi as NP. And as I said, we'll use nin pi to extract the fast fourier transform. Then I'll import mat plot leap dot pi plot SPLT. We'll use this library to plot the magnitude spectrums and then I'll uh import or S I'll import libros to load the uh audio files. And finally, I want to import IPYTHON dot uh display as IP D and we'll use, uh, this to, uh, listen to the audio files directly in our Jupiter notebook. Ok. So, uh, now what I want to do is just like unload the audio files and show them and listen to them directly here in the,",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=53s",
        "start_time": "53.069"
    },
    {
        "id": "2deda0bb",
        "text": "I'll import libros to load the uh audio files. And finally, I want to import IPYTHON dot uh display as IP D and we'll use, uh, this to, uh, listen to the audio files directly in our Jupiter notebook. Ok. So, uh, now what I want to do is just like unload the audio files and show them and listen to them directly here in the, um, Jupiter notebook. And so first thing that I want to do is create a base deer and that is gonna point to the directory that, uh, contains, uh, all the different audio files that I want to show you guys. And so I'll start with home and then Valerio, then Pie Charm projects,",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=78s",
        "start_time": "78.099"
    },
    {
        "id": "721c8163",
        "text": "uh, now what I want to do is just like unload the audio files and show them and listen to them directly here in the, um, Jupiter notebook. And so first thing that I want to do is create a base deer and that is gonna point to the directory that, uh, contains, uh, all the different audio files that I want to show you guys. And so I'll start with home and then Valerio, then Pie Charm projects, audio signal processing for ML. And we are at the 14th.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=103s",
        "start_time": "103.62"
    },
    {
        "id": "cb4c3612",
        "text": "um, Jupiter notebook. And so first thing that I want to do is create a base deer and that is gonna point to the directory that, uh, contains, uh, all the different audio files that I want to show you guys. And so I'll start with home and then Valerio, then Pie Charm projects, audio signal processing for ML. And we are at the 14th. Uh, and finally, I'm gonna have this audio directory here. OK. So this is the paster and then I'm gonna have this, uh, yeah, let's call it violin uh file. And",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=112s",
        "start_time": "112.18"
    },
    {
        "id": "6e291394",
        "text": "audio signal processing for ML. And we are at the 14th. Uh, and finally, I'm gonna have this audio directory here. OK. So this is the paster and then I'm gonna have this, uh, yeah, let's call it violin uh file. And so you'll search here. Where is, it should be over here, right? Ok. So here, uh, I'm in the base here and as you can see, we have a bunch of different uh files. So let me just go back to the GPI notebook. So the violin file is called uh violin C",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=137s",
        "start_time": "137.32"
    },
    {
        "id": "10c6dbe7",
        "text": "Uh, and finally, I'm gonna have this audio directory here. OK. So this is the paster and then I'm gonna have this, uh, yeah, let's call it violin uh file. And so you'll search here. Where is, it should be over here, right? Ok. So here, uh, I'm in the base here and as you can see, we have a bunch of different uh files. So let me just go back to the GPI notebook. So the violin file is called uh violin C violin C dot A W.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=143s",
        "start_time": "143.729"
    },
    {
        "id": "c7ab57c7",
        "text": "so you'll search here. Where is, it should be over here, right? Ok. So here, uh, I'm in the base here and as you can see, we have a bunch of different uh files. So let me just go back to the GPI notebook. So the violin file is called uh violin C violin C dot A W. Then we have the sax file and this is called if I remember correctly Sax dot WAV. And then we have this piano file",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=160s",
        "start_time": "160.139"
    },
    {
        "id": "34e5a75f",
        "text": "violin C dot A W. Then we have the sax file and this is called if I remember correctly Sax dot WAV. And then we have this piano file which is equal to",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=184s",
        "start_time": "184.1"
    },
    {
        "id": "6b536818",
        "text": "Then we have the sax file and this is called if I remember correctly Sax dot WAV. And then we have this piano file which is equal to piano dot wav. And finally, we have the nice file. So we're also going to see the magnitude spectrum for some nice and this is called nice dot W. Ok. But the piano one is called piano C dot W. Ok.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=188s",
        "start_time": "188.41"
    },
    {
        "id": "a8e8c2e7",
        "text": "which is equal to piano dot wav. And finally, we have the nice file. So we're also going to see the magnitude spectrum for some nice and this is called nice dot W. Ok. But the piano one is called piano C dot W. Ok. Good. So now let's try to load this audio directly into Jupiter networks so that we can, uh listen to that. And so for doing this, we'll do an IP D dot uh audio",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=203s",
        "start_time": "203.779"
    },
    {
        "id": "93948229",
        "text": "piano dot wav. And finally, we have the nice file. So we're also going to see the magnitude spectrum for some nice and this is called nice dot W. Ok. But the piano one is called piano C dot W. Ok. Good. So now let's try to load this audio directly into Jupiter networks so that we can, uh listen to that. And so for doing this, we'll do an IP D dot uh audio and we need uh to pass the file path to like all of these different files. And so we'll do I OS dot path dot join and we'll pass the base here as well as for the violin, the violin file over here.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=207s",
        "start_time": "207.16"
    },
    {
        "id": "1b55feb2",
        "text": "Good. So now let's try to load this audio directly into Jupiter networks so that we can, uh listen to that. And so for doing this, we'll do an IP D dot uh audio and we need uh to pass the file path to like all of these different files. And so we'll do I OS dot path dot join and we'll pass the base here as well as for the violin, the violin file over here. And yes, as you can see, we loaded that I'm gonna load all of these guys uh first and then we're gonna quickly listen to them, but they're very, very short. OK? So yeah, let me",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=227s",
        "start_time": "227.32"
    },
    {
        "id": "6fe778dc",
        "text": "and we need uh to pass the file path to like all of these different files. And so we'll do I OS dot path dot join and we'll pass the base here as well as for the violin, the violin file over here. And yes, as you can see, we loaded that I'm gonna load all of these guys uh first and then we're gonna quickly listen to them, but they're very, very short. OK? So yeah, let me do this set file. So I'm gonna pass in the, what is it? The piano file and finally I want to pass in the noise file over here.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=239s",
        "start_time": "239.57"
    },
    {
        "id": "69fe8d14",
        "text": "And yes, as you can see, we loaded that I'm gonna load all of these guys uh first and then we're gonna quickly listen to them, but they're very, very short. OK? So yeah, let me do this set file. So I'm gonna pass in the, what is it? The piano file and finally I want to pass in the noise file over here. OK.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=260s",
        "start_time": "260.059"
    },
    {
        "id": "1d99deb8",
        "text": "do this set file. So I'm gonna pass in the, what is it? The piano file and finally I want to pass in the noise file over here. OK. So yeah, let's listen to this first thing. So here is I see four notes for the violin.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=273s",
        "start_time": "273.39"
    },
    {
        "id": "dfc0e40f",
        "text": "OK. So yeah, let's listen to this first thing. So here is I see four notes for the violin. OK. So the C four notes uh So think of like the piano keyboard, the C four is the middle C. So it's the C at the center of the keyboard. Then we have ac for not for the saxophone.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=290s",
        "start_time": "290.75"
    },
    {
        "id": "9bdb37d7",
        "text": "So yeah, let's listen to this first thing. So here is I see four notes for the violin. OK. So the C four notes uh So think of like the piano keyboard, the C four is the middle C. So it's the C at the center of the keyboard. Then we have ac for not for the saxophone. OK? And then I see five notes for uh piano.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=292s",
        "start_time": "292.019"
    },
    {
        "id": "324f1bf9",
        "text": "OK. So the C four notes uh So think of like the piano keyboard, the C four is the middle C. So it's the C at the center of the keyboard. Then we have ac for not for the saxophone. OK? And then I see five notes for uh piano. So this is still a sea, but it's an octave above them like the, the middle sea. And finally, we have some noise.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=303s",
        "start_time": "303.7"
    },
    {
        "id": "a058238d",
        "text": "OK? And then I see five notes for uh piano. So this is still a sea, but it's an octave above them like the, the middle sea. And finally, we have some noise. We are all used to this kind of noise. Next, we want to load this uh, files as an umpire race and for doing that, we are going to be using Libros, ok? So I'll load the violin C for sound and I'm gonna get back also the sample rate",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=319s",
        "start_time": "319.209"
    },
    {
        "id": "16b26539",
        "text": "So this is still a sea, but it's an octave above them like the, the middle sea. And finally, we have some noise. We are all used to this kind of noise. Next, we want to load this uh, files as an umpire race and for doing that, we are going to be using Libros, ok? So I'll load the violin C for sound and I'm gonna get back also the sample rate and I'm gonna use a Li Brosa dot load. And what I want to pass in is the, this guy. So basically the five pa to the violin file.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=327s",
        "start_time": "327.48"
    },
    {
        "id": "9424db01",
        "text": "We are all used to this kind of noise. Next, we want to load this uh, files as an umpire race and for doing that, we are going to be using Libros, ok? So I'll load the violin C for sound and I'm gonna get back also the sample rate and I'm gonna use a Li Brosa dot load. And what I want to pass in is the, this guy. So basically the five pa to the violin file. OK. So this is for violin, but we should do the very same thing for uh Sax and Sax is gonna be Sax C four. Now, the sample rate is gonna be defaulted to 22,050 Hertz and we just mm",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=335s",
        "start_time": "335.059"
    },
    {
        "id": "40478378",
        "text": "and I'm gonna use a Li Brosa dot load. And what I want to pass in is the, this guy. So basically the five pa to the violin file. OK. So this is for violin, but we should do the very same thing for uh Sax and Sax is gonna be Sax C four. Now, the sample rate is gonna be defaulted to 22,050 Hertz and we just mm need like the first one then I mean like here the sample rate is going to be the same. So I'm just gonna be using a underscore because I say, well, I really don't care about that. There we go. OK? So this is the same but here it's not the violin file. It's actually the sax file. Then we have the piano but it's not C four. It's gonna be C five. Once again, I'll put an underscore here",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=358s",
        "start_time": "358.72"
    },
    {
        "id": "704c4332",
        "text": "OK. So this is for violin, but we should do the very same thing for uh Sax and Sax is gonna be Sax C four. Now, the sample rate is gonna be defaulted to 22,050 Hertz and we just mm need like the first one then I mean like here the sample rate is going to be the same. So I'm just gonna be using a underscore because I say, well, I really don't care about that. There we go. OK? So this is the same but here it's not the violin file. It's actually the sax file. Then we have the piano but it's not C four. It's gonna be C five. Once again, I'll put an underscore here and this is going to be a piano file. And finally, here we have our noise",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=372s",
        "start_time": "372.399"
    },
    {
        "id": "d90869e0",
        "text": "need like the first one then I mean like here the sample rate is going to be the same. So I'm just gonna be using a underscore because I say, well, I really don't care about that. There we go. OK? So this is the same but here it's not the violin file. It's actually the sax file. Then we have the piano but it's not C four. It's gonna be C five. Once again, I'll put an underscore here and this is going to be a piano file. And finally, here we have our noise and",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=389s",
        "start_time": "389.299"
    },
    {
        "id": "65b80b53",
        "text": "and this is going to be a piano file. And finally, here we have our noise and let's pass in the no file. So yeah, so now we should have all of these guys uh with us. So let me show you that the sample rate is 22 2050 Hertz and uh you can see it here and that's the default value for libros. So that's fine. Now, let's take a look at the violin C four. So this is a",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=415s",
        "start_time": "415.47"
    },
    {
        "id": "2ff98cee",
        "text": "and let's pass in the no file. So yeah, so now we should have all of these guys uh with us. So let me show you that the sample rate is 22 2050 Hertz and uh you can see it here and that's the default value for libros. So that's fine. Now, let's take a look at the violin C four. So this is a and then pi array now. So, and this is basically like the, the waveform associated with this audio file. And if we take a look at the shape, we're going to see that this is like a uh an array and which has like only like one dimension. And the",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=425s",
        "start_time": "425.51"
    },
    {
        "id": "fcd0f3f3",
        "text": "let's pass in the no file. So yeah, so now we should have all of these guys uh with us. So let me show you that the sample rate is 22 2050 Hertz and uh you can see it here and that's the default value for libros. So that's fine. Now, let's take a look at the violin C four. So this is a and then pi array now. So, and this is basically like the, the waveform associated with this audio file. And if we take a look at the shape, we're going to see that this is like a uh an array and which has like only like one dimension. And the um like, I mean like in, in, in that one axis, we have around 60,000 samples and this is the number of samples that we have like in the audio file which is cool. OK. So now let's try to extract the full transform coefficients and I'll hold it for, let's say the violent sound. So we'll do a",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=427s",
        "start_time": "427.589"
    },
    {
        "id": "0e3d19ed",
        "text": "and then pi array now. So, and this is basically like the, the waveform associated with this audio file. And if we take a look at the shape, we're going to see that this is like a uh an array and which has like only like one dimension. And the um like, I mean like in, in, in that one axis, we have around 60,000 samples and this is the number of samples that we have like in the audio file which is cool. OK. So now let's try to extract the full transform coefficients and I'll hold it for, let's say the violent sound. So we'll do a uh FT or let's do violin FT where FT stands for fourier transform. And so if we're doing that, we'll use a NP dot FFT dot FFT. Now, if you feel that this FFT dot FFT is redundant, I'll tell you that this first FFT is a module that contains a bunch of different implementations of the fast forward a transform one of which is the classical",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=450s",
        "start_time": "450.269"
    },
    {
        "id": "eae142fc",
        "text": "um like, I mean like in, in, in that one axis, we have around 60,000 samples and this is the number of samples that we have like in the audio file which is cool. OK. So now let's try to extract the full transform coefficients and I'll hold it for, let's say the violent sound. So we'll do a uh FT or let's do violin FT where FT stands for fourier transform. And so if we're doing that, we'll use a NP dot FFT dot FFT. Now, if you feel that this FFT dot FFT is redundant, I'll tell you that this first FFT is a module that contains a bunch of different implementations of the fast forward a transform one of which is the classical FFT. So that's why you have like this double FFT thing here. OK. So here we need to pass the actual uh signal, right. So this VC four and now basically by doing so we move from the uh time domain here to the frequency domain over there. OK. So now let's take a look at this uh fourier coefficients. So,",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=468s",
        "start_time": "468.35"
    },
    {
        "id": "6b0f1ff0",
        "text": "uh FT or let's do violin FT where FT stands for fourier transform. And so if we're doing that, we'll use a NP dot FFT dot FFT. Now, if you feel that this FFT dot FFT is redundant, I'll tell you that this first FFT is a module that contains a bunch of different implementations of the fast forward a transform one of which is the classical FFT. So that's why you have like this double FFT thing here. OK. So here we need to pass the actual uh signal, right. So this VC four and now basically by doing so we move from the uh time domain here to the frequency domain over there. OK. So now let's take a look at this uh fourier coefficients. So, uh let's take a look at the shape of this array and we'll take it over here and we'll do a violin ft dot uh shape.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=493s",
        "start_time": "493.6"
    },
    {
        "id": "0e15aeb5",
        "text": "FFT. So that's why you have like this double FFT thing here. OK. So here we need to pass the actual uh signal, right. So this VC four and now basically by doing so we move from the uh time domain here to the frequency domain over there. OK. So now let's take a look at this uh fourier coefficients. So, uh let's take a look at the shape of this array and we'll take it over here and we'll do a violin ft dot uh shape. And uh as you can see here, so this is uh this has the same shape that we had for the signal uh itself. Now, uh if you don't remember why, that's the case, I really suggest you to go check out my previous video, which should be here on the discrete, the theory behind the discrete fourier transform. But basically, the idea in a nutshell is that we are gonna have like as many",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=520s",
        "start_time": "520.21"
    },
    {
        "id": "f370a9d5",
        "text": "uh let's take a look at the shape of this array and we'll take it over here and we'll do a violin ft dot uh shape. And uh as you can see here, so this is uh this has the same shape that we had for the signal uh itself. Now, uh if you don't remember why, that's the case, I really suggest you to go check out my previous video, which should be here on the discrete, the theory behind the discrete fourier transform. But basically, the idea in a nutshell is that we are gonna have like as many frequency beams",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=547s",
        "start_time": "547.719"
    },
    {
        "id": "50cefe40",
        "text": "And uh as you can see here, so this is uh this has the same shape that we had for the signal uh itself. Now, uh if you don't remember why, that's the case, I really suggest you to go check out my previous video, which should be here on the discrete, the theory behind the discrete fourier transform. But basically, the idea in a nutshell is that we are gonna have like as many frequency beams uh uh as the number of samples that we have in the original time domain uh representation. So this checks out. OK. So now let's take a look at what we have uh for one frequency bin. So we're going to take like the, the first fourier transform coefficient fourier coefficient for violin FT.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=558s",
        "start_time": "558.489"
    },
    {
        "id": "c388cc0a",
        "text": "frequency beams uh uh as the number of samples that we have in the original time domain uh representation. So this checks out. OK. So now let's take a look at what we have uh for one frequency bin. So we're going to take like the, the first fourier transform coefficient fourier coefficient for violin FT. And as you can see here, this is a complex number and you have the uh real parts here and here you have the imaginary part of the, of this complex number. Now, once again, if you don't know what I'm talking about uh about like complex numbers, I suggest you like to go check out my video on complex numbers and it should be over here.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=587s",
        "start_time": "587.19"
    },
    {
        "id": "8d6e0744",
        "text": "uh uh as the number of samples that we have in the original time domain uh representation. So this checks out. OK. So now let's take a look at what we have uh for one frequency bin. So we're going to take like the, the first fourier transform coefficient fourier coefficient for violin FT. And as you can see here, this is a complex number and you have the uh real parts here and here you have the imaginary part of the, of this complex number. Now, once again, if you don't know what I'm talking about uh about like complex numbers, I suggest you like to go check out my video on complex numbers and it should be over here. And then if you can't remember why the four A transform uh like produces uh complex numbers. I suggest you to go check out this video that provides you with a lot of um information about the fourier transform itself. OK. So uh here we have like this complex number and we know that this complex number provides information both about a phase as well as uh the magnitude.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=589s",
        "start_time": "589.909"
    },
    {
        "id": "e102eeb4",
        "text": "And as you can see here, this is a complex number and you have the uh real parts here and here you have the imaginary part of the, of this complex number. Now, once again, if you don't know what I'm talking about uh about like complex numbers, I suggest you like to go check out my video on complex numbers and it should be over here. And then if you can't remember why the four A transform uh like produces uh complex numbers. I suggest you to go check out this video that provides you with a lot of um information about the fourier transform itself. OK. So uh here we have like this complex number and we know that this complex number provides information both about a phase as well as uh the magnitude. And now the thing is that for most applications in A IO A, we usually don't care that much about face. So when we analyze signals, what we really care is to understand how much is each frequency is present in a, in an audio signal. And for that, we just need the magnitude. So what we do usually we it's",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=615s",
        "start_time": "615.929"
    },
    {
        "id": "7d97868f",
        "text": "And then if you can't remember why the four A transform uh like produces uh complex numbers. I suggest you to go check out this video that provides you with a lot of um information about the fourier transform itself. OK. So uh here we have like this complex number and we know that this complex number provides information both about a phase as well as uh the magnitude. And now the thing is that for most applications in A IO A, we usually don't care that much about face. So when we analyze signals, what we really care is to understand how much is each frequency is present in a, in an audio signal. And for that, we just need the magnitude. So what we do usually we it's discard the information about phase and we only retain information about the magnitude. And so we move from the fourier coefficients uh to the magnitude spectrum. So how do we do that? Well, that is very simple because we just like take the absolute value of the fourier coefficients. In other words, we can take the magnitude spectrum of the violin",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=644s",
        "start_time": "644.7"
    },
    {
        "id": "377e29fc",
        "text": "And now the thing is that for most applications in A IO A, we usually don't care that much about face. So when we analyze signals, what we really care is to understand how much is each frequency is present in a, in an audio signal. And for that, we just need the magnitude. So what we do usually we it's discard the information about phase and we only retain information about the magnitude. And so we move from the fourier coefficients uh to the magnitude spectrum. So how do we do that? Well, that is very simple because we just like take the absolute value of the fourier coefficients. In other words, we can take the magnitude spectrum of the violin uh sound here by taking NIN P dot apps which stands for absolute value of the violin FT. So we pass the uh free transform coefficients to the absolute value uh function and we get back to the magnitude spectrum. Now this is not the violin, it's just violin like this.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=674s",
        "start_time": "674.15"
    },
    {
        "id": "769fa927",
        "text": "discard the information about phase and we only retain information about the magnitude. And so we move from the fourier coefficients uh to the magnitude spectrum. So how do we do that? Well, that is very simple because we just like take the absolute value of the fourier coefficients. In other words, we can take the magnitude spectrum of the violin uh sound here by taking NIN P dot apps which stands for absolute value of the violin FT. So we pass the uh free transform coefficients to the absolute value uh function and we get back to the magnitude spectrum. Now this is not the violin, it's just violin like this. Oh No, sorry. I just messed up everything. So it's not the violin, but it's the violin. OK. So now uh let's take a look at what happens here if we visualize like the first um coefficient for the magnitude spectrum. Now, as you can see here, we don't have the uh a complex number",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=701s",
        "start_time": "701.869"
    },
    {
        "id": "fc4b05e5",
        "text": "uh sound here by taking NIN P dot apps which stands for absolute value of the violin FT. So we pass the uh free transform coefficients to the absolute value uh function and we get back to the magnitude spectrum. Now this is not the violin, it's just violin like this. Oh No, sorry. I just messed up everything. So it's not the violin, but it's the violin. OK. So now uh let's take a look at what happens here if we visualize like the first um coefficient for the magnitude spectrum. Now, as you can see here, we don't have the uh a complex number anymore. We actually have one single like real number. And that is indeed the magnitude associated with this initial frequency bin over here. OK. So",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=730s",
        "start_time": "730.03"
    },
    {
        "id": "82f493a7",
        "text": "Oh No, sorry. I just messed up everything. So it's not the violin, but it's the violin. OK. So now uh let's take a look at what happens here if we visualize like the first um coefficient for the magnitude spectrum. Now, as you can see here, we don't have the uh a complex number anymore. We actually have one single like real number. And that is indeed the magnitude associated with this initial frequency bin over here. OK. So now you should have an idea of how to extract uh the uh fourier transform and how to calculate the magnitude spectrum. But what we want to do here next is to actually visualize the magnitude uh spectra for all of these different sounds that we have here so that we can compare them.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=755s",
        "start_time": "755.219"
    },
    {
        "id": "a3d3cd8f",
        "text": "anymore. We actually have one single like real number. And that is indeed the magnitude associated with this initial frequency bin over here. OK. So now you should have an idea of how to extract uh the uh fourier transform and how to calculate the magnitude spectrum. But what we want to do here next is to actually visualize the magnitude uh spectra for all of these different sounds that we have here so that we can compare them. OK. So for that, I'm gonna create a little function called a plot magnitude spectrum.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=780s",
        "start_time": "780.83"
    },
    {
        "id": "798519b9",
        "text": "now you should have an idea of how to extract uh the uh fourier transform and how to calculate the magnitude spectrum. But what we want to do here next is to actually visualize the magnitude uh spectra for all of these different sounds that we have here so that we can compare them. OK. So for that, I'm gonna create a little function called a plot magnitude spectrum. So this function is gonna accept a few parameters. So first of all the signal itself in the time domain, then it's gonna take a title so that we can give a title to our uh plots and then it's gonna accept a uh sample rate. OK. Yeah. Why do they do that well?",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=794s",
        "start_time": "794.71"
    },
    {
        "id": "915a9f5e",
        "text": "OK. So for that, I'm gonna create a little function called a plot magnitude spectrum. So this function is gonna accept a few parameters. So first of all the signal itself in the time domain, then it's gonna take a title so that we can give a title to our uh plots and then it's gonna accept a uh sample rate. OK. Yeah. Why do they do that well? OK.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=817s",
        "start_time": "817.109"
    },
    {
        "id": "60f9dd17",
        "text": "So this function is gonna accept a few parameters. So first of all the signal itself in the time domain, then it's gonna take a title so that we can give a title to our uh plots and then it's gonna accept a uh sample rate. OK. Yeah. Why do they do that well? OK. So now the first thing that we want to do is to calculate the fourier transform and we already say how to do that. And we'll do a NIN pi dot FFT dot FFT and we'll pass the uh signal, then we'll calculate the magnitude uh spectrum. And for that, we need to pass, we need to create like this NP dot apps and we'll pass the fourier transformer",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=827s",
        "start_time": "827.979"
    },
    {
        "id": "9e0441fc",
        "text": "OK. So now the first thing that we want to do is to calculate the fourier transform and we already say how to do that. And we'll do a NIN pi dot FFT dot FFT and we'll pass the uh signal, then we'll calculate the magnitude uh spectrum. And for that, we need to pass, we need to create like this NP dot apps and we'll pass the fourier transformer and then we want to plot the magnitude uh spectrum. And so how do we do that? Well, first of all, we need to initialize a figure. And so we'll do a plot dot uh figure and we'll pass in a, a fixed size",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=850s",
        "start_time": "850.13"
    },
    {
        "id": "91842648",
        "text": "So now the first thing that we want to do is to calculate the fourier transform and we already say how to do that. And we'll do a NIN pi dot FFT dot FFT and we'll pass the uh signal, then we'll calculate the magnitude uh spectrum. And for that, we need to pass, we need to create like this NP dot apps and we'll pass the fourier transformer and then we want to plot the magnitude uh spectrum. And so how do we do that? Well, first of all, we need to initialize a figure. And so we'll do a plot dot uh figure and we'll pass in a, a fixed size and this is a keyword argument. And here I'm going to pass in 18 for the width and five for the height.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=851s",
        "start_time": "851.59"
    },
    {
        "id": "28e0d8ac",
        "text": "and then we want to plot the magnitude uh spectrum. And so how do we do that? Well, first of all, we need to initialize a figure. And so we'll do a plot dot uh figure and we'll pass in a, a fixed size and this is a keyword argument. And here I'm going to pass in 18 for the width and five for the height. And then the next thing that we want to do is to um kind of like create the X axis and these are gonna be like our frequency beams. And so we can call this uh like frequency. And so how are we gonna do this? So I'll, we're gonna be using this uh L space um A function from NPI.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=880s",
        "start_time": "880.979"
    },
    {
        "id": "eb3eaa21",
        "text": "and this is a keyword argument. And here I'm going to pass in 18 for the width and five for the height. And then the next thing that we want to do is to um kind of like create the X axis and these are gonna be like our frequency beams. And so we can call this uh like frequency. And so how are we gonna do this? So I'll, we're gonna be using this uh L space um A function from NPI. So let me just like write this and then I'll explain what it does. So we'll take this between uh zero and the simple rate. And then here I'm gonna pass in the length of magnitude spectrum. So what's uh uh I've done here is basically taking a, a range. So we're gonna have like the frequency which is uh between zero and the uh sample rate.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=902s",
        "start_time": "902.59"
    },
    {
        "id": "9494bc75",
        "text": "And then the next thing that we want to do is to um kind of like create the X axis and these are gonna be like our frequency beams. And so we can call this uh like frequency. And so how are we gonna do this? So I'll, we're gonna be using this uh L space um A function from NPI. So let me just like write this and then I'll explain what it does. So we'll take this between uh zero and the simple rate. And then here I'm gonna pass in the length of magnitude spectrum. So what's uh uh I've done here is basically taking a, a range. So we're gonna have like the frequency which is uh between zero and the uh sample rate. And then uh what I'm gonna do is I'm gonna like create as many divisions, as many frequency beams as the num as the length of the magnitude uh spectrum. So this is for frequency, then what we'll do next is just like plotting",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=911s",
        "start_time": "911.599"
    },
    {
        "id": "a482ffe3",
        "text": "So let me just like write this and then I'll explain what it does. So we'll take this between uh zero and the simple rate. And then here I'm gonna pass in the length of magnitude spectrum. So what's uh uh I've done here is basically taking a, a range. So we're gonna have like the frequency which is uh between zero and the uh sample rate. And then uh what I'm gonna do is I'm gonna like create as many divisions, as many frequency beams as the num as the length of the magnitude uh spectrum. So this is for frequency, then what we'll do next is just like plotting So we'll do a plot dot uh plot, we need to pass the X axis and this is gonna be frequency. Our Y axis is obviously the uh magnitude uh spectrum over here. And then uh what we want to do next is to pass the X label.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=939s",
        "start_time": "939.919"
    },
    {
        "id": "6a55712d",
        "text": "And then uh what I'm gonna do is I'm gonna like create as many divisions, as many frequency beams as the num as the length of the magnitude uh spectrum. So this is for frequency, then what we'll do next is just like plotting So we'll do a plot dot uh plot, we need to pass the X axis and this is gonna be frequency. Our Y axis is obviously the uh magnitude uh spectrum over here. And then uh what we want to do next is to pass the X label. And here as a, as the label for our X axis, we'll put in frequency which is measured in Hertz and then we'll do a plot dot uh title and the title is gonna be equal to title.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=969s",
        "start_time": "969.359"
    },
    {
        "id": "d7d8da75",
        "text": "So we'll do a plot dot uh plot, we need to pass the X axis and this is gonna be frequency. Our Y axis is obviously the uh magnitude uh spectrum over here. And then uh what we want to do next is to pass the X label. And here as a, as the label for our X axis, we'll put in frequency which is measured in Hertz and then we'll do a plot dot uh title and the title is gonna be equal to title. And finally, we'll do a plot dot show so that we can show our plots. OK. So now if I haven't made any mistakes, we should be able to visualize the magnitude spectrum here. So now let me pass the uh yeah, let's take the",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=989s",
        "start_time": "989.07"
    },
    {
        "id": "16ec0c87",
        "text": "And here as a, as the label for our X axis, we'll put in frequency which is measured in Hertz and then we'll do a plot dot uh title and the title is gonna be equal to title. And finally, we'll do a plot dot show so that we can show our plots. OK. So now if I haven't made any mistakes, we should be able to visualize the magnitude spectrum here. So now let me pass the uh yeah, let's take the VNC four. Are you here",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1011s",
        "start_time": "1011.82"
    },
    {
        "id": "8fe94da0",
        "text": "And finally, we'll do a plot dot show so that we can show our plots. OK. So now if I haven't made any mistakes, we should be able to visualize the magnitude spectrum here. So now let me pass the uh yeah, let's take the VNC four. Are you here uh pass that signal then as the title I'll just pass in violin and then the sample H it is going to be equal to sample. H Let's see. Oh Here we go. Nice that we have our magnitude spectrum. And as expected on the X axis, we have the frequency",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1033s",
        "start_time": "1033.63"
    },
    {
        "id": "f1445450",
        "text": "VNC four. Are you here uh pass that signal then as the title I'll just pass in violin and then the sample H it is going to be equal to sample. H Let's see. Oh Here we go. Nice that we have our magnitude spectrum. And as expected on the X axis, we have the frequency uh all the different frequency bins uh expressed in Hertz on the Y axis, we have the magnitude. Now, if you remember from my previous video, uh you probably know that uh like what we're going to get out of like this spectrum is some kind of like central cym met. And indeed, you see that",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1056s",
        "start_time": "1056.81"
    },
    {
        "id": "b3ac896c",
        "text": "uh pass that signal then as the title I'll just pass in violin and then the sample H it is going to be equal to sample. H Let's see. Oh Here we go. Nice that we have our magnitude spectrum. And as expected on the X axis, we have the frequency uh all the different frequency bins uh expressed in Hertz on the Y axis, we have the magnitude. Now, if you remember from my previous video, uh you probably know that uh like what we're going to get out of like this spectrum is some kind of like central cym met. And indeed, you see that we have like central symmetry. So basically we can kind of like consider only the uh frequencies up to like the, the center point which is the NRS uh frequency.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1063s",
        "start_time": "1063.449"
    },
    {
        "id": "07a8aa2c",
        "text": "uh all the different frequency bins uh expressed in Hertz on the Y axis, we have the magnitude. Now, if you remember from my previous video, uh you probably know that uh like what we're going to get out of like this spectrum is some kind of like central cym met. And indeed, you see that we have like central symmetry. So basically we can kind of like consider only the uh frequencies up to like the, the center point which is the NRS uh frequency. OK. So uh let's do that. And so I want to like make this like a little bit more uh flexible. So what I'll do is I'll change this function and I'll uh introduce like another uh argument called a frequency ratio. And I'll put like this as default equal to one.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1080s",
        "start_time": "1080.969"
    },
    {
        "id": "95fa2dee",
        "text": "we have like central symmetry. So basically we can kind of like consider only the uh frequencies up to like the, the center point which is the NRS uh frequency. OK. So uh let's do that. And so I want to like make this like a little bit more uh flexible. So what I'll do is I'll change this function and I'll uh introduce like another uh argument called a frequency ratio. And I'll put like this as default equal to one. OK? But basically here uh I want to calculate the number of frequency beans right? Using the frequency uh ratio. Now this is going to be equal to",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1103s",
        "start_time": "1103.739"
    },
    {
        "id": "636c64bd",
        "text": "OK. So uh let's do that. And so I want to like make this like a little bit more uh flexible. So what I'll do is I'll change this function and I'll uh introduce like another uh argument called a frequency ratio. And I'll put like this as default equal to one. OK? But basically here uh I want to calculate the number of frequency beans right? Using the frequency uh ratio. Now this is going to be equal to the frequency that we have. Well, the length of the frequency that we have here. And then we'll multiply that by the frequency ratio uh obviously like this number is gonna be a float. So we need to cast it to uh an integer. So I'll do this like that. OK?",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1116s",
        "start_time": "1116.65"
    },
    {
        "id": "63ddf9db",
        "text": "OK? But basically here uh I want to calculate the number of frequency beans right? Using the frequency uh ratio. Now this is going to be equal to the frequency that we have. Well, the length of the frequency that we have here. And then we'll multiply that by the frequency ratio uh obviously like this number is gonna be a float. So we need to cast it to uh an integer. So I'll do this like that. OK? And so now uh like by doing so we can only, we can take like only a,",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1139s",
        "start_time": "1139.14"
    },
    {
        "id": "1434ba9d",
        "text": "the frequency that we have. Well, the length of the frequency that we have here. And then we'll multiply that by the frequency ratio uh obviously like this number is gonna be a float. So we need to cast it to uh an integer. So I'll do this like that. OK? And so now uh like by doing so we can only, we can take like only a, a part of the original number of like frequency bins. And so now what we can do is just like go in here and slice the frequency and magnitude spectrum um variables here, arrays and only like consider uh from zero up to the number of frequency bins that we want to consider. And I'll do the very same thing for the um magnitude spectrum",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1155s",
        "start_time": "1155.939"
    },
    {
        "id": "e4321ef6",
        "text": "And so now uh like by doing so we can only, we can take like only a, a part of the original number of like frequency bins. And so now what we can do is just like go in here and slice the frequency and magnitude spectrum um variables here, arrays and only like consider uh from zero up to the number of frequency bins that we want to consider. And I'll do the very same thing for the um magnitude spectrum here. OK? So now if I put the frequency ratio equal to one, well we still have the same thing. And as you can see the frequency in Hertz goes up to 22,000 uh 50 Hertz. Yeah. Let me just like uh remove this. Oh By the way, given. Yeah, I said this.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1180s",
        "start_time": "1180.489"
    },
    {
        "id": "81cc2293",
        "text": "a part of the original number of like frequency bins. And so now what we can do is just like go in here and slice the frequency and magnitude spectrum um variables here, arrays and only like consider uh from zero up to the number of frequency bins that we want to consider. And I'll do the very same thing for the um magnitude spectrum here. OK? So now if I put the frequency ratio equal to one, well we still have the same thing. And as you can see the frequency in Hertz goes up to 22,000 uh 50 Hertz. Yeah. Let me just like uh remove this. Oh By the way, given. Yeah, I said this. So this is the sound of A I uh slack uh community. And so if you want to join here, you can get like information about like all things like A I audio and A I music and you can also like network with very, very cool people know things",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1187s",
        "start_time": "1187.26"
    },
    {
        "id": "b64a9425",
        "text": "here. OK? So now if I put the frequency ratio equal to one, well we still have the same thing. And as you can see the frequency in Hertz goes up to 22,000 uh 50 Hertz. Yeah. Let me just like uh remove this. Oh By the way, given. Yeah, I said this. So this is the sound of A I uh slack uh community. And so if you want to join here, you can get like information about like all things like A I audio and A I music and you can also like network with very, very cool people know things about like the uh like community, like updates and new things you can ask for advice or you can just uh yeah, talk about papers and other cool stuff all around like a IO you that if you are interested in joining the sound of A I community, I'll leave you the sign up link in the uh comment in the description below",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1216s",
        "start_time": "1216.979"
    },
    {
        "id": "65608f34",
        "text": "So this is the sound of A I uh slack uh community. And so if you want to join here, you can get like information about like all things like A I audio and A I music and you can also like network with very, very cool people know things about like the uh like community, like updates and new things you can ask for advice or you can just uh yeah, talk about papers and other cool stuff all around like a IO you that if you are interested in joining the sound of A I community, I'll leave you the sign up link in the uh comment in the description below after this little teacher with the sound of A I is like community. Let's go back to like the plot here. And as we said, so when we have the frequency ratio which is equal to one still we get all of our um f the frequency like has all the original frequency bins. And so it's",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1239s",
        "start_time": "1239.42"
    },
    {
        "id": "62372be8",
        "text": "about like the uh like community, like updates and new things you can ask for advice or you can just uh yeah, talk about papers and other cool stuff all around like a IO you that if you are interested in joining the sound of A I community, I'll leave you the sign up link in the uh comment in the description below after this little teacher with the sound of A I is like community. Let's go back to like the plot here. And as we said, so when we have the frequency ratio which is equal to one still we get all of our um f the frequency like has all the original frequency bins. And so it's up to the uh sample rate frequency which is 22,050 Hertz. But if we change date, say to 0.5 we'll just have uh we'll just consider the frequencies up up to the Nicholas frequency which is half the sample rate, which in this case is 11,000 Hertz.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1260s",
        "start_time": "1260.969"
    },
    {
        "id": "6ae446b2",
        "text": "after this little teacher with the sound of A I is like community. Let's go back to like the plot here. And as we said, so when we have the frequency ratio which is equal to one still we get all of our um f the frequency like has all the original frequency bins. And so it's up to the uh sample rate frequency which is 22,050 Hertz. But if we change date, say to 0.5 we'll just have uh we'll just consider the frequencies up up to the Nicholas frequency which is half the sample rate, which in this case is 11,000 Hertz. And uh but still here we see that most of the energy is contained in like lower frequencies. So I to zoom in, I'll just like put a 0.1 here. And as you can see here, like this is like nice because here we can actually see the different frequencies that are like involved in shaping",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1282s",
        "start_time": "1282.979"
    },
    {
        "id": "c5ffa46b",
        "text": "up to the uh sample rate frequency which is 22,050 Hertz. But if we change date, say to 0.5 we'll just have uh we'll just consider the frequencies up up to the Nicholas frequency which is half the sample rate, which in this case is 11,000 Hertz. And uh but still here we see that most of the energy is contained in like lower frequencies. So I to zoom in, I'll just like put a 0.1 here. And as you can see here, like this is like nice because here we can actually see the different frequencies that are like involved in shaping this violin C four sound. And so here at around 2060 Hertz, we have AAA first pick and this is the fundamental frequency of C four and engaged like this is the frequency of AC four note.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1302s",
        "start_time": "1302.579"
    },
    {
        "id": "d082d2ed",
        "text": "And uh but still here we see that most of the energy is contained in like lower frequencies. So I to zoom in, I'll just like put a 0.1 here. And as you can see here, like this is like nice because here we can actually see the different frequencies that are like involved in shaping this violin C four sound. And so here at around 2060 Hertz, we have AAA first pick and this is the fundamental frequency of C four and engaged like this is the frequency of AC four note. Then we have like the first harmonic, which it should be around 520 Hertz uh which is very present. And then we have the third, the 4th 5th harmonic and so forth. The interesting thing here is that the uh first harmonic of this violin T four sound has more energy than the fundamental. So which is kind of like interested in itself.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1323s",
        "start_time": "1323.79"
    },
    {
        "id": "353cdbc3",
        "text": "this violin C four sound. And so here at around 2060 Hertz, we have AAA first pick and this is the fundamental frequency of C four and engaged like this is the frequency of AC four note. Then we have like the first harmonic, which it should be around 520 Hertz uh which is very present. And then we have the third, the 4th 5th harmonic and so forth. The interesting thing here is that the uh first harmonic of this violin T four sound has more energy than the fundamental. So which is kind of like interested in itself. OK. So now let's compare this with the sax saxophone sound. So Sax C four and here, Papa sa",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1343s",
        "start_time": "1343.06"
    },
    {
        "id": "c8fda1f3",
        "text": "Then we have like the first harmonic, which it should be around 520 Hertz uh which is very present. And then we have the third, the 4th 5th harmonic and so forth. The interesting thing here is that the uh first harmonic of this violin T four sound has more energy than the fundamental. So which is kind of like interested in itself. OK. So now let's compare this with the sax saxophone sound. So Sax C four and here, Papa sa and as you can see, we have a quite similar um spectrum uh magnitude spectrum here. And this is to be expected because like both instruments are playing the very same note, which is this C four. And so you see I pick here on uh around like the fundamental frequency and on the first harmonic. But on a closer look, you'll see that the, the relative presence of these harmonics and the",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1363s",
        "start_time": "1363.06"
    },
    {
        "id": "efc963ce",
        "text": "OK. So now let's compare this with the sax saxophone sound. So Sax C four and here, Papa sa and as you can see, we have a quite similar um spectrum uh magnitude spectrum here. And this is to be expected because like both instruments are playing the very same note, which is this C four. And so you see I pick here on uh around like the fundamental frequency and on the first harmonic. But on a closer look, you'll see that the, the relative presence of these harmonics and the fundamental is different for the violin and the saxophone. And so this is one of the reasons why we perceive this sounds as different in timer that they still have like the same frequency uh because they're both playing the same C for note, but they sound different. And one of the reasons why that's the case, it's because they, their energy content, which we can see through this power spectrum is different.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1390s",
        "start_time": "1390.869"
    },
    {
        "id": "754d88d2",
        "text": "and as you can see, we have a quite similar um spectrum uh magnitude spectrum here. And this is to be expected because like both instruments are playing the very same note, which is this C four. And so you see I pick here on uh around like the fundamental frequency and on the first harmonic. But on a closer look, you'll see that the, the relative presence of these harmonics and the fundamental is different for the violin and the saxophone. And so this is one of the reasons why we perceive this sounds as different in timer that they still have like the same frequency uh because they're both playing the same C for note, but they sound different. And one of the reasons why that's the case, it's because they, their energy content, which we can see through this power spectrum is different. OK. So now let's take a look at the C five notes on uh the piano. So I'll do a piano C five and let's call this piano,",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1402s",
        "start_time": "1402.42"
    },
    {
        "id": "96d2dbcb",
        "text": "fundamental is different for the violin and the saxophone. And so this is one of the reasons why we perceive this sounds as different in timer that they still have like the same frequency uh because they're both playing the same C for note, but they sound different. And one of the reasons why that's the case, it's because they, their energy content, which we can see through this power spectrum is different. OK. So now let's take a look at the C five notes on uh the piano. So I'll do a piano C five and let's call this piano, right? And now, obviously like the fundamental here is not 260 but it's rather 520 Hertz because we are a NCTA above C four because we are in C five and that basically doubles the um frequency of the fundamental.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1431s",
        "start_time": "1431.17"
    },
    {
        "id": "687a651e",
        "text": "OK. So now let's take a look at the C five notes on uh the piano. So I'll do a piano C five and let's call this piano, right? And now, obviously like the fundamental here is not 260 but it's rather 520 Hertz because we are a NCTA above C four because we are in C five and that basically doubles the um frequency of the fundamental. And, but the, the cool thing here is that you can see that uh the pick is on the fundamental, then the first harmonic partial uh is kind of like less present than the fundamental",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1460s",
        "start_time": "1460.119"
    },
    {
        "id": "a1a03a4d",
        "text": "right? And now, obviously like the fundamental here is not 260 but it's rather 520 Hertz because we are a NCTA above C four because we are in C five and that basically doubles the um frequency of the fundamental. And, but the, the cool thing here is that you can see that uh the pick is on the fundamental, then the first harmonic partial uh is kind of like less present than the fundamental and then like the other like harmonics just like fade away in their presence. This is different from what happens with the violin and the saxophone where the first harmonic partial is more present than the fundamental.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1472s",
        "start_time": "1472.989"
    },
    {
        "id": "df625f93",
        "text": "And, but the, the cool thing here is that you can see that uh the pick is on the fundamental, then the first harmonic partial uh is kind of like less present than the fundamental and then like the other like harmonics just like fade away in their presence. This is different from what happens with the violin and the saxophone where the first harmonic partial is more present than the fundamental. OK. So now, uh as a final thing, let's take a look at the noise signal.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1491s",
        "start_time": "1491.719"
    },
    {
        "id": "ccb6af32",
        "text": "and then like the other like harmonics just like fade away in their presence. This is different from what happens with the violin and the saxophone where the first harmonic partial is more present than the fundamental. OK. So now, uh as a final thing, let's take a look at the noise signal. So we'll have like noise here and I'll put in here a noise. All right. And as you can see here, what happens is that you have uh a lot of activity, a lot of energy in all the different frequency bins.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1506s",
        "start_time": "1506.859"
    },
    {
        "id": "6bb18498",
        "text": "OK. So now, uh as a final thing, let's take a look at the noise signal. So we'll have like noise here and I'll put in here a noise. All right. And as you can see here, what happens is that you have uh a lot of activity, a lot of energy in all the different frequency bins. For sure. We have some kind of like pick here towards like the lower frequencies. But then as you can see, like all the frequency bins have a little bit of like of energy in them. And that's what I um noisy signal looks like. OK.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1523s",
        "start_time": "1523.9"
    },
    {
        "id": "8f712363",
        "text": "So we'll have like noise here and I'll put in here a noise. All right. And as you can see here, what happens is that you have uh a lot of activity, a lot of energy in all the different frequency bins. For sure. We have some kind of like pick here towards like the lower frequencies. But then as you can see, like all the frequency bins have a little bit of like of energy in them. And that's what I um noisy signal looks like. OK. Cool. So I think like we, we did like some cool stuff like in this video because now you should be able like to extract the um four transform with Python using NP. And then you should also be able to visualize the",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1531s",
        "start_time": "1531.319"
    },
    {
        "id": "765c3489",
        "text": "For sure. We have some kind of like pick here towards like the lower frequencies. But then as you can see, like all the frequency bins have a little bit of like of energy in them. And that's what I um noisy signal looks like. OK. Cool. So I think like we, we did like some cool stuff like in this video because now you should be able like to extract the um four transform with Python using NP. And then you should also be able to visualize the magnitude spectrum using lip. And finally, we also now know the difference between like different instruments and what uh that uh like means in terms of like uh perceptual um characteristics lack of sound. OK. So now one thing that I want to draw your attention to before I dash off and I finish off like this video is that",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1547s",
        "start_time": "1547.625"
    },
    {
        "id": "6cdc6e4d",
        "text": "Cool. So I think like we, we did like some cool stuff like in this video because now you should be able like to extract the um four transform with Python using NP. And then you should also be able to visualize the magnitude spectrum using lip. And finally, we also now know the difference between like different instruments and what uh that uh like means in terms of like uh perceptual um characteristics lack of sound. OK. So now one thing that I want to draw your attention to before I dash off and I finish off like this video is that uh each uh yeah of this magnitude spectra is a kind of like a snapshot that we take and it's a snapshot that provides us information across all the duration of a sound. It doesn't give us, give us any information about what happens",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1565s",
        "start_time": "1565.3"
    },
    {
        "id": "df149867",
        "text": "magnitude spectrum using lip. And finally, we also now know the difference between like different instruments and what uh that uh like means in terms of like uh perceptual um characteristics lack of sound. OK. So now one thing that I want to draw your attention to before I dash off and I finish off like this video is that uh each uh yeah of this magnitude spectra is a kind of like a snapshot that we take and it's a snapshot that provides us information across all the duration of a sound. It doesn't give us, give us any information about what happens uh with relation to the frequency like second by second or like moment by moment, we just lose that information because we average everything across all the duration of a sound. Wouldn't it be fantastic if we could have information about the frequency and uh it as a function of time as well? Well, that would be fantastic indeed. And",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1582s",
        "start_time": "1582.589"
    },
    {
        "id": "f3a84085",
        "text": "uh each uh yeah of this magnitude spectra is a kind of like a snapshot that we take and it's a snapshot that provides us information across all the duration of a sound. It doesn't give us, give us any information about what happens uh with relation to the frequency like second by second or like moment by moment, we just lose that information because we average everything across all the duration of a sound. Wouldn't it be fantastic if we could have information about the frequency and uh it as a function of time as well? Well, that would be fantastic indeed. And it's going to be the topic of my next video, which is going to introduce the theory behind the short time fourier transform. And the short time fourier transform is going to enable us to extract spectra spectrum, the spectrum from an audio signal. And this is the main currency that we use in A I, audio and music information retrieval to make",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1608s",
        "start_time": "1608.739"
    },
    {
        "id": "b37a71bd",
        "text": "uh with relation to the frequency like second by second or like moment by moment, we just lose that information because we average everything across all the duration of a sound. Wouldn't it be fantastic if we could have information about the frequency and uh it as a function of time as well? Well, that would be fantastic indeed. And it's going to be the topic of my next video, which is going to introduce the theory behind the short time fourier transform. And the short time fourier transform is going to enable us to extract spectra spectrum, the spectrum from an audio signal. And this is the main currency that we use in A I, audio and music information retrieval to make sense of uh digital audio signals. So stay tuned for that. That's all for today. I hope you've enjoyed the video. If that's the case, please leave a like and if you haven't subscribed to the channel, please consider doing so. If you have any questions as usual, feel free to leave them in the comments section below. See you next time. Cheers.",
        "video": "How to Extract the Fourier Transform with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "R-5uxKTRjzM",
        "youtube_link": "https://www.youtube.com/watch?v=R-5uxKTRjzM&t=1630s",
        "start_time": "1630.42"
    },
    {
        "id": "e1af0553",
        "text": "Hi, everybody and welcome to a new exciting video in the audio signal processing for machine learning series. Last time we learned about a short time, four year transform and spectrograms in a theoretical way. This time, it's time to actually use Python and the audio processing library lib browser to extract spectrograms from audio files. So let's get started. So I already wrote a Jupiter notebook here. And so I'm gonna just like run through it and I'm gonna tell you like what I'm doing and the different steps like to actually extract uh spectrogram. So the first thing that we want to do is just like import some uh like libraries. So we import OS so that we can uh load audio like our audio files, we import to Li Brosa Libres dot display for uh just like showing visualizing like the spectrograms and we'll import like noon pie and map lib dot plot for um actually doing or just like plotting uh the spectrograms like in other results that we'll have. OK. So let import all of this and then we want to just like load audio files with Li Brosa. So",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "dd82e619",
        "text": "spectrograms from audio files. So let's get started. So I already wrote a Jupiter notebook here. And so I'm gonna just like run through it and I'm gonna tell you like what I'm doing and the different steps like to actually extract uh spectrogram. So the first thing that we want to do is just like import some uh like libraries. So we import OS so that we can uh load audio like our audio files, we import to Li Brosa Libres dot display for uh just like showing visualizing like the spectrograms and we'll import like noon pie and map lib dot plot for um actually doing or just like plotting uh the spectrograms like in other results that we'll have. OK. So let import all of this and then we want to just like load audio files with Li Brosa. So uh we are gonna be working with four different audio files. So the first one is just like a skill and uh yeah, it's just like a reside. It's like at this path. Then we're gonna have um",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=21s",
        "start_time": "21.959"
    },
    {
        "id": "5a2b52c8",
        "text": "uh load audio like our audio files, we import to Li Brosa Libres dot display for uh just like showing visualizing like the spectrograms and we'll import like noon pie and map lib dot plot for um actually doing or just like plotting uh the spectrograms like in other results that we'll have. OK. So let import all of this and then we want to just like load audio files with Li Brosa. So uh we are gonna be working with four different audio files. So the first one is just like a skill and uh yeah, it's just like a reside. It's like at this path. Then we're gonna have um uh kind of like a 32nd snippet from the BC. 30 seconds snippets from red hot chili peppers and 32nd snippet from uh Duke Ellington. So we have like three different musical genres represented some classical music with the BC. rock music, with the red hot chili peppers and jazz with Duke Helling. OK. But first thing, let's try to",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=43s",
        "start_time": "43.919"
    },
    {
        "id": "a957c2a7",
        "text": "uh we are gonna be working with four different audio files. So the first one is just like a skill and uh yeah, it's just like a reside. It's like at this path. Then we're gonna have um uh kind of like a 32nd snippet from the BC. 30 seconds snippets from red hot chili peppers and 32nd snippet from uh Duke Ellington. So we have like three different musical genres represented some classical music with the BC. rock music, with the red hot chili peppers and jazz with Duke Helling. OK. But first thing, let's try to listen to this music. So, or, and so we get like an idea of what we're dealing with. And so here we go. So if you do I IP D dot audio and you pass the uh the file, then you're gonna be able to directly listen to the uh to the audio files in the Jupiter notebook. By the way, this IP D comes from",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=72s",
        "start_time": "72.879"
    },
    {
        "id": "bda6b2e6",
        "text": "uh kind of like a 32nd snippet from the BC. 30 seconds snippets from red hot chili peppers and 32nd snippet from uh Duke Ellington. So we have like three different musical genres represented some classical music with the BC. rock music, with the red hot chili peppers and jazz with Duke Helling. OK. But first thing, let's try to listen to this music. So, or, and so we get like an idea of what we're dealing with. And so here we go. So if you do I IP D dot audio and you pass the uh the file, then you're gonna be able to directly listen to the uh to the audio files in the Jupiter notebook. By the way, this IP D comes from this input over here. So you just like input ipython dot display as IP D and then you can use it. OK? So now let's listen to like this different uh audio files. So the first one is gonna be a uh scale uh played on a piano.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=86s",
        "start_time": "86.069"
    },
    {
        "id": "1ff1fe24",
        "text": "listen to this music. So, or, and so we get like an idea of what we're dealing with. And so here we go. So if you do I IP D dot audio and you pass the uh the file, then you're gonna be able to directly listen to the uh to the audio files in the Jupiter notebook. By the way, this IP D comes from this input over here. So you just like input ipython dot display as IP D and then you can use it. OK? So now let's listen to like this different uh audio files. So the first one is gonna be a uh scale uh played on a piano. OK.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=109s",
        "start_time": "109.569"
    },
    {
        "id": "08aa8aca",
        "text": "this input over here. So you just like input ipython dot display as IP D and then you can use it. OK? So now let's listen to like this different uh audio files. So the first one is gonna be a uh scale uh played on a piano. OK. Yeah, it's just repeated a couple of times. Then we have the music from the busy from the red hot chili peppers and from Duke Ellington. So let's listen.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=132s",
        "start_time": "132.57"
    },
    {
        "id": "a75763f1",
        "text": "OK. Yeah, it's just repeated a couple of times. Then we have the music from the busy from the red hot chili peppers and from Duke Ellington. So let's listen. So if you guys followed along so far with the series you probably already recognize this piece cos we used it in a previous video.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=150s",
        "start_time": "150.25"
    },
    {
        "id": "cee97478",
        "text": "Yeah, it's just repeated a couple of times. Then we have the music from the busy from the red hot chili peppers and from Duke Ellington. So let's listen. So if you guys followed along so far with the series you probably already recognize this piece cos we used it in a previous video. So here you have like a huge crescendo, right? With all of this string instrument,",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=153s",
        "start_time": "153.529"
    },
    {
        "id": "c09de26d",
        "text": "So if you guys followed along so far with the series you probably already recognize this piece cos we used it in a previous video. So here you have like a huge crescendo, right? With all of this string instrument, right? You get the idea. So very nice, smooth uh like string driven uh orchestral piece. And then we have a song from the Red Hot Chili.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=165s",
        "start_time": "165.429"
    },
    {
        "id": "54e93130",
        "text": "So here you have like a huge crescendo, right? With all of this string instrument, right? You get the idea. So very nice, smooth uh like string driven uh orchestral piece. And then we have a song from the Red Hot Chili. OK. You get the idea and probably you are all too familiar with that song. Then moving on to this jazz piece from Duke all by Duke",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=178s",
        "start_time": "178.199"
    },
    {
        "id": "9edc9669",
        "text": "right? You get the idea. So very nice, smooth uh like string driven uh orchestral piece. And then we have a song from the Red Hot Chili. OK. You get the idea and probably you are all too familiar with that song. Then moving on to this jazz piece from Duke all by Duke Mhm",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=186s",
        "start_time": "186.199"
    },
    {
        "id": "3976b521",
        "text": "OK. You get the idea and probably you are all too familiar with that song. Then moving on to this jazz piece from Duke all by Duke Mhm Very smooth, right?",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=210s",
        "start_time": "210.13"
    },
    {
        "id": "7d0d4d82",
        "text": "Mhm Very smooth, right? OK. You get the idea. So what we'll do is try to extract the uh spectrogram from this and visualize them and compare them. OK. So what we want to do first is just let you load the, all your files with libros. So we've done this multiple times in earlier videos. Uh So uh what we do is just like lib rosa dot load and we pass like the name of the uh or the, the path to the file",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=220s",
        "start_time": "220.66"
    },
    {
        "id": "780296a4",
        "text": "Very smooth, right? OK. You get the idea. So what we'll do is try to extract the uh spectrogram from this and visualize them and compare them. OK. So what we want to do first is just let you load the, all your files with libros. So we've done this multiple times in earlier videos. Uh So uh what we do is just like lib rosa dot load and we pass like the name of the uh or the, the path to the file we get back is a signal, a NPI array. And then we can also get back the sample rate which when it's defaulted is gonna be equal to 22,050 Hertz. OK. So let's move on to this next step. What we can do, what we should do is extract the short time fourier transform. And this is very easy with the Liberator because we have a function that does that uh super quickly.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=225s",
        "start_time": "225.33"
    },
    {
        "id": "9638cc6e",
        "text": "OK. You get the idea. So what we'll do is try to extract the uh spectrogram from this and visualize them and compare them. OK. So what we want to do first is just let you load the, all your files with libros. So we've done this multiple times in earlier videos. Uh So uh what we do is just like lib rosa dot load and we pass like the name of the uh or the, the path to the file we get back is a signal, a NPI array. And then we can also get back the sample rate which when it's defaulted is gonna be equal to 22,050 Hertz. OK. So let's move on to this next step. What we can do, what we should do is extract the short time fourier transform. And this is very easy with the Liberator because we have a function that does that uh super quickly. So the first thing we wanna do is just let's just set a couple of parameters. So we'll set the frame size equal to 2048 H uh samples. Sorry. And the H size is gonna be equal to 512 samples. Again, these are quite typical parameters for like the frame size and the H size.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=233s",
        "start_time": "233.35"
    },
    {
        "id": "31b29b19",
        "text": "we get back is a signal, a NPI array. And then we can also get back the sample rate which when it's defaulted is gonna be equal to 22,050 Hertz. OK. So let's move on to this next step. What we can do, what we should do is extract the short time fourier transform. And this is very easy with the Liberator because we have a function that does that uh super quickly. So the first thing we wanna do is just let's just set a couple of parameters. So we'll set the frame size equal to 2048 H uh samples. Sorry. And the H size is gonna be equal to 512 samples. Again, these are quite typical parameters for like the frame size and the H size. Uh If you don't know what I'm talking about, I highly suggest you to go check out my previous video on the theory behind the short term fourier transform. I'm just like taking for granted that you've watched that video or you are familiar with the short term fourier transform. So I'm not gonna get into details what all of these parameters actually mean in this um video.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=257s",
        "start_time": "257.154"
    },
    {
        "id": "50a55b40",
        "text": "So the first thing we wanna do is just let's just set a couple of parameters. So we'll set the frame size equal to 2048 H uh samples. Sorry. And the H size is gonna be equal to 512 samples. Again, these are quite typical parameters for like the frame size and the H size. Uh If you don't know what I'm talking about, I highly suggest you to go check out my previous video on the theory behind the short term fourier transform. I'm just like taking for granted that you've watched that video or you are familiar with the short term fourier transform. So I'm not gonna get into details what all of these parameters actually mean in this um video. OK. So let's move on. Now, I can extract the short time period transform using this function which is Great Libres SDFT and then I should pass in the signal. And the first thing that we'll see here and the spectrogram that we'll analyze this",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=282s",
        "start_time": "282.23"
    },
    {
        "id": "154decac",
        "text": "Uh If you don't know what I'm talking about, I highly suggest you to go check out my previous video on the theory behind the short term fourier transform. I'm just like taking for granted that you've watched that video or you are familiar with the short term fourier transform. So I'm not gonna get into details what all of these parameters actually mean in this um video. OK. So let's move on. Now, I can extract the short time period transform using this function which is Great Libres SDFT and then I should pass in the signal. And the first thing that we'll see here and the spectrogram that we'll analyze this are with regard with the, with the scale, just like to see how that is represented in a spectrogram. And it's gonna be easier to visualize than like all the other music that we've uh listened earlier. Then we should pass the frame size and we should pass it. Uh So with this keyword argument called N dash FFT",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=303s",
        "start_time": "303.619"
    },
    {
        "id": "91747b99",
        "text": "OK. So let's move on. Now, I can extract the short time period transform using this function which is Great Libres SDFT and then I should pass in the signal. And the first thing that we'll see here and the spectrogram that we'll analyze this are with regard with the, with the scale, just like to see how that is represented in a spectrogram. And it's gonna be easier to visualize than like all the other music that we've uh listened earlier. Then we should pass the frame size and we should pass it. Uh So with this keyword argument called N dash FFT uh A and underscore FFT and then we pass the hop uh length um keyword argument and we pass our hop size over here. OK. So we do this and then now let's take a look at the shape of the short time fourier transform. So as we uh so in the previous video, this is, is gonna be like a bidi menal array and specifically like the first dimension is relative to the, the frequency.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=324s",
        "start_time": "324.72"
    },
    {
        "id": "ffbe461e",
        "text": "are with regard with the, with the scale, just like to see how that is represented in a spectrogram. And it's gonna be easier to visualize than like all the other music that we've uh listened earlier. Then we should pass the frame size and we should pass it. Uh So with this keyword argument called N dash FFT uh A and underscore FFT and then we pass the hop uh length um keyword argument and we pass our hop size over here. OK. So we do this and then now let's take a look at the shape of the short time fourier transform. So as we uh so in the previous video, this is, is gonna be like a bidi menal array and specifically like the first dimension is relative to the, the frequency. Uh And so here we have like um all the frequency bins and these, these are equal. So like this number is equal to half the frame size plus",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=345s",
        "start_time": "345.41"
    },
    {
        "id": "a708c905",
        "text": "uh A and underscore FFT and then we pass the hop uh length um keyword argument and we pass our hop size over here. OK. So we do this and then now let's take a look at the shape of the short time fourier transform. So as we uh so in the previous video, this is, is gonna be like a bidi menal array and specifically like the first dimension is relative to the, the frequency. Uh And so here we have like um all the frequency bins and these, these are equal. So like this number is equal to half the frame size plus one. So it's basically 2048 divided by two, which is 1024 plus 1, 1025. OK. So it checks out good. And here uh like on the columns, uh the second dimension of this metrics, we have the um number of frames. It's basically like the temporal bins. And in this case, we have like a 342 temporal bins. And if you want to know how to",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=366s",
        "start_time": "366.519"
    },
    {
        "id": "325cd02a",
        "text": "Uh And so here we have like um all the frequency bins and these, these are equal. So like this number is equal to half the frame size plus one. So it's basically 2048 divided by two, which is 1024 plus 1, 1025. OK. So it checks out good. And here uh like on the columns, uh the second dimension of this metrics, we have the um number of frames. It's basically like the temporal bins. And in this case, we have like a 342 temporal bins. And if you want to know how to get from uh like a signal to like a certain number of frames just like to make that calculation, I have like the formula in the previous video regarding short time fourier transform. OK. So now the next thing that we want to see is the actual type of the different items that we have in the short time four transform result or in the matrix. So here we just like take the",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=394s",
        "start_time": "394.029"
    },
    {
        "id": "abed0f12",
        "text": "one. So it's basically 2048 divided by two, which is 1024 plus 1, 1025. OK. So it checks out good. And here uh like on the columns, uh the second dimension of this metrics, we have the um number of frames. It's basically like the temporal bins. And in this case, we have like a 342 temporal bins. And if you want to know how to get from uh like a signal to like a certain number of frames just like to make that calculation, I have like the formula in the previous video regarding short time fourier transform. OK. So now the next thing that we want to see is the actual type of the different items that we have in the short time four transform result or in the matrix. So here we just like take the item at uh col uh row zero, column zero. And as you can see the type is a complex number. And this doesn't come as a surprise because the output of a short time fourier transform is a series of like complex fourier coefficients. And so, uh yeah, so we expect that each of the items which is a fourier coefficient for a given uh like frequency bin and a given like frame is a complex number.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=406s",
        "start_time": "406.95"
    },
    {
        "id": "bea3d614",
        "text": "get from uh like a signal to like a certain number of frames just like to make that calculation, I have like the formula in the previous video regarding short time fourier transform. OK. So now the next thing that we want to see is the actual type of the different items that we have in the short time four transform result or in the matrix. So here we just like take the item at uh col uh row zero, column zero. And as you can see the type is a complex number. And this doesn't come as a surprise because the output of a short time fourier transform is a series of like complex fourier coefficients. And so, uh yeah, so we expect that each of the items which is a fourier coefficient for a given uh like frequency bin and a given like frame is a complex number. But now what we want to do is actually calculate these spectrograms. So we need to to move from the short term fourier transform to the spectrogram. So how do we do that? Well, that's easily done because we just like take the squared magnitude of the short term fourier transform. So we just like use like this NP dot ABS absolute value and we pass in the uh",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=434s",
        "start_time": "434.822"
    },
    {
        "id": "be8650e1",
        "text": "item at uh col uh row zero, column zero. And as you can see the type is a complex number. And this doesn't come as a surprise because the output of a short time fourier transform is a series of like complex fourier coefficients. And so, uh yeah, so we expect that each of the items which is a fourier coefficient for a given uh like frequency bin and a given like frame is a complex number. But now what we want to do is actually calculate these spectrograms. So we need to to move from the short term fourier transform to the spectrogram. So how do we do that? Well, that's easily done because we just like take the squared magnitude of the short term fourier transform. So we just like use like this NP dot ABS absolute value and we pass in the uh short time free transform results here and then we square the results and this is going to be equal to the spectrogram. OK. Yeah. Not that let me move on. Let's take a look at the shape here. And once again, not surprisingly, we have like this shape. So 1025 number of Beins and 342",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=462s",
        "start_time": "462.696"
    },
    {
        "id": "fb995f01",
        "text": "But now what we want to do is actually calculate these spectrograms. So we need to to move from the short term fourier transform to the spectrogram. So how do we do that? Well, that's easily done because we just like take the squared magnitude of the short term fourier transform. So we just like use like this NP dot ABS absolute value and we pass in the uh short time free transform results here and then we square the results and this is going to be equal to the spectrogram. OK. Yeah. Not that let me move on. Let's take a look at the shape here. And once again, not surprisingly, we have like this shape. So 1025 number of Beins and 342 uh number of frames, which is the same that we used to have like with the short term fourier transform results. And that checks out because all we are doing is just like taking the magnitude the squared magnitude. So the the shape of the matrix uh of the original matrix doesn't change. But what does change",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=490s",
        "start_time": "490.809"
    },
    {
        "id": "4de6ad41",
        "text": "short time free transform results here and then we square the results and this is going to be equal to the spectrogram. OK. Yeah. Not that let me move on. Let's take a look at the shape here. And once again, not surprisingly, we have like this shape. So 1025 number of Beins and 342 uh number of frames, which is the same that we used to have like with the short term fourier transform results. And that checks out because all we are doing is just like taking the magnitude the squared magnitude. So the the shape of the matrix uh of the original matrix doesn't change. But what does change is the type of the items. In this case, we have uh floats. And this makes sense because we are taking like the, the magnitude here. And so basically, we are moving from the complex number to like a real number. And this is the spectrogram and this is like what we can actually visualize um uh on a hit map. And so let's see how we can easily visualize the spectrogram uh with uh Li Brosa.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=514s",
        "start_time": "514.58"
    },
    {
        "id": "94568edd",
        "text": "uh number of frames, which is the same that we used to have like with the short term fourier transform results. And that checks out because all we are doing is just like taking the magnitude the squared magnitude. So the the shape of the matrix uh of the original matrix doesn't change. But what does change is the type of the items. In this case, we have uh floats. And this makes sense because we are taking like the, the magnitude here. And so basically, we are moving from the complex number to like a real number. And this is the spectrogram and this is like what we can actually visualize um uh on a hit map. And so let's see how we can easily visualize the spectrogram uh with uh Li Brosa. And so here I wrote like a little function and here this is the signature. So you see uh Y capital Y is just like the um",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=538s",
        "start_time": "538.52"
    },
    {
        "id": "874fa6cc",
        "text": "is the type of the items. In this case, we have uh floats. And this makes sense because we are taking like the, the magnitude here. And so basically, we are moving from the complex number to like a real number. And this is the spectrogram and this is like what we can actually visualize um uh on a hit map. And so let's see how we can easily visualize the spectrogram uh with uh Li Brosa. And so here I wrote like a little function and here this is the signature. So you see uh Y capital Y is just like the um uh it's the spectrogram, then I pass in the sample rates the H length and the Y axis. And here like a default is, let's say uh linear. We'll see what this means like in a second. But before, let's just uh take a look at what I do here, I just like instantiate um a figure uh specifying like the, the, the figure size here using",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=558s",
        "start_time": "558.554"
    },
    {
        "id": "6bf76896",
        "text": "And so here I wrote like a little function and here this is the signature. So you see uh Y capital Y is just like the um uh it's the spectrogram, then I pass in the sample rates the H length and the Y axis. And here like a default is, let's say uh linear. We'll see what this means like in a second. But before, let's just uh take a look at what I do here, I just like instantiate um a figure uh specifying like the, the, the figure size here using like my lip. And then here comes the magic we can use Lisa dot display dot spec show to uh visualize uh any type of spectrogram like um signals. And so here what uh like this function expects is why is it basically like the um spectrogram?",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=586s",
        "start_time": "586.469"
    },
    {
        "id": "28c2cb29",
        "text": "uh it's the spectrogram, then I pass in the sample rates the H length and the Y axis. And here like a default is, let's say uh linear. We'll see what this means like in a second. But before, let's just uh take a look at what I do here, I just like instantiate um a figure uh specifying like the, the, the figure size here using like my lip. And then here comes the magic we can use Lisa dot display dot spec show to uh visualize uh any type of spectrogram like um signals. And so here what uh like this function expects is why is it basically like the um spectrogram? Then the uh sample rate, the hop length, the X axis which is gonna be equal to time. So we're gonna have like on the X axis, we're gonna have like time and on the y axis, we're gonna have like a type of um representation that's uh linear. And then I'm gonna add a color bar here and you'll see what this does. It's basically like a legend that uh provides us information about like the",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=596s",
        "start_time": "596.789"
    },
    {
        "id": "45aade2b",
        "text": "like my lip. And then here comes the magic we can use Lisa dot display dot spec show to uh visualize uh any type of spectrogram like um signals. And so here what uh like this function expects is why is it basically like the um spectrogram? Then the uh sample rate, the hop length, the X axis which is gonna be equal to time. So we're gonna have like on the X axis, we're gonna have like time and on the y axis, we're gonna have like a type of um representation that's uh linear. And then I'm gonna add a color bar here and you'll see what this does. It's basically like a legend that uh provides us information about like the uh how to map the colors into like the different like intensities of the uh of the signal of the amplitude. OK. So now let me run this and now we can plot the spectrogram and I'll pass in the uh spectrogram for like the scale scale like audio file, I'll pass in the sample rate and the hub size. And so let's see what happens here. And here we go our first visualization of a spectrogram, but this doesn't look great. Does it",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=618s",
        "start_time": "618.155"
    },
    {
        "id": "bacb9534",
        "text": "Then the uh sample rate, the hop length, the X axis which is gonna be equal to time. So we're gonna have like on the X axis, we're gonna have like time and on the y axis, we're gonna have like a type of um representation that's uh linear. And then I'm gonna add a color bar here and you'll see what this does. It's basically like a legend that uh provides us information about like the uh how to map the colors into like the different like intensities of the uh of the signal of the amplitude. OK. So now let me run this and now we can plot the spectrogram and I'll pass in the uh spectrogram for like the scale scale like audio file, I'll pass in the sample rate and the hub size. And so let's see what happens here. And here we go our first visualization of a spectrogram, but this doesn't look great. Does it they",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=639s",
        "start_time": "639.77"
    },
    {
        "id": "f779b441",
        "text": "uh how to map the colors into like the different like intensities of the uh of the signal of the amplitude. OK. So now let me run this and now we can plot the spectrogram and I'll pass in the uh spectrogram for like the scale scale like audio file, I'll pass in the sample rate and the hub size. And so let's see what happens here. And here we go our first visualization of a spectrogram, but this doesn't look great. Does it they obviously like the idea here is that the, the brighter the color and the more energy you have like in that uh frequency bin, right? And so we see a little bit of like activity down here and it's repeated. And so probably you can guess that this is like the two scale like the fundamental frequencies of the scales. And in in indeed you see that these like tend to like run up",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=666s",
        "start_time": "666.409"
    },
    {
        "id": "5c8975d6",
        "text": "they obviously like the idea here is that the, the brighter the color and the more energy you have like in that uh frequency bin, right? And so we see a little bit of like activity down here and it's repeated. And so probably you can guess that this is like the two scale like the fundamental frequencies of the scales. And in in indeed you see that these like tend to like run up uh and it's repeated twice because if you remember like in that audio file, we had the same scale repeated twice, but still like everything is black. So it means that it has like very, very little energy. So why is that the case? Well, it turns out that this is how it sounds like work. So uh but uh it's like the way we actually perceive like these energies and amplitude is not really linear as is the case like in this representation here. But it, it's actually uh logarithmic.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=696s",
        "start_time": "696.28"
    },
    {
        "id": "231a5600",
        "text": "obviously like the idea here is that the, the brighter the color and the more energy you have like in that uh frequency bin, right? And so we see a little bit of like activity down here and it's repeated. And so probably you can guess that this is like the two scale like the fundamental frequencies of the scales. And in in indeed you see that these like tend to like run up uh and it's repeated twice because if you remember like in that audio file, we had the same scale repeated twice, but still like everything is black. So it means that it has like very, very little energy. So why is that the case? Well, it turns out that this is how it sounds like work. So uh but uh it's like the way we actually perceive like these energies and amplitude is not really linear as is the case like in this representation here. But it, it's actually uh logarithmic. And so uh to get like closer to the way we perceive a sound, we need to do a kind of uh transformation of the uh intensities like of all the amplitude here. And so we need to, to move like all of this like amplitudes from",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=697s",
        "start_time": "697.349"
    },
    {
        "id": "32058857",
        "text": "uh and it's repeated twice because if you remember like in that audio file, we had the same scale repeated twice, but still like everything is black. So it means that it has like very, very little energy. So why is that the case? Well, it turns out that this is how it sounds like work. So uh but uh it's like the way we actually perceive like these energies and amplitude is not really linear as is the case like in this representation here. But it, it's actually uh logarithmic. And so uh to get like closer to the way we perceive a sound, we need to do a kind of uh transformation of the uh intensities like of all the amplitude here. And so we need to, to move like all of this like amplitudes from the uh basic like linear representation to a logarithmic representation, which is like more perceptually significant. We can easily move from a linear representation of amplitude to a logarithmic one using uh this uh",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=721s",
        "start_time": "721.96"
    },
    {
        "id": "0245baf5",
        "text": "And so uh to get like closer to the way we perceive a sound, we need to do a kind of uh transformation of the uh intensities like of all the amplitude here. And so we need to, to move like all of this like amplitudes from the uh basic like linear representation to a logarithmic representation, which is like more perceptually significant. We can easily move from a linear representation of amplitude to a logarithmic one using uh this uh the function from Libera called power to DB. And the B stands for uh decibels. Now, if you are not familiar with decibels, I suggest you to go check out this video where I talk about decibels and introduce them and explain like how they work. But basically under the hood, what happens is that decibels are actually applying some kind of like a logarithmic transformation",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=751s",
        "start_time": "751.59"
    },
    {
        "id": "968dcba7",
        "text": "the uh basic like linear representation to a logarithmic representation, which is like more perceptually significant. We can easily move from a linear representation of amplitude to a logarithmic one using uh this uh the function from Libera called power to DB. And the B stands for uh decibels. Now, if you are not familiar with decibels, I suggest you to go check out this video where I talk about decibels and introduce them and explain like how they work. But basically under the hood, what happens is that decibels are actually applying some kind of like a logarithmic transformation that when you use this power to be, you're moving like from the power uh representation like of the intensity like to uh decimals. And so we do this and we get back a a log amplitude uh spectrogram here. And so we can then just like",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=771s",
        "start_time": "771.08"
    },
    {
        "id": "0914b277",
        "text": "the function from Libera called power to DB. And the B stands for uh decibels. Now, if you are not familiar with decibels, I suggest you to go check out this video where I talk about decibels and introduce them and explain like how they work. But basically under the hood, what happens is that decibels are actually applying some kind of like a logarithmic transformation that when you use this power to be, you're moving like from the power uh representation like of the intensity like to uh decimals. And so we do this and we get back a a log amplitude uh spectrogram here. And so we can then just like plot that with our function. So plot spectrogram instead of passing the actual like spectrogram of scale, we pass the log amplitude spectrogram and again, we pass the sample rate and the hop size. OK. So if we do that, we get these results, which is way better than the one that had we had before.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=790s",
        "start_time": "790.14"
    },
    {
        "id": "071a2f21",
        "text": "that when you use this power to be, you're moving like from the power uh representation like of the intensity like to uh decimals. And so we do this and we get back a a log amplitude uh spectrogram here. And so we can then just like plot that with our function. So plot spectrogram instead of passing the actual like spectrogram of scale, we pass the log amplitude spectrogram and again, we pass the sample rate and the hop size. OK. So if we do that, we get these results, which is way better than the one that had we had before. So here we start to see like some uh energy like bursts of energy like down here. And as you can see here, probably like this, this is like a constant uh like notes, then you move up, you move up, up, up up. So this probably is just like the fundamental frequencies for the scale that we played.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=809s",
        "start_time": "809.33"
    },
    {
        "id": "cadf213e",
        "text": "plot that with our function. So plot spectrogram instead of passing the actual like spectrogram of scale, we pass the log amplitude spectrogram and again, we pass the sample rate and the hop size. OK. So if we do that, we get these results, which is way better than the one that had we had before. So here we start to see like some uh energy like bursts of energy like down here. And as you can see here, probably like this, this is like a constant uh like notes, then you move up, you move up, up, up up. So this probably is just like the fundamental frequencies for the scale that we played. And if you're wondering about like all of this other kind of like burst uh like of energies like at a higher frequencies, those are uh the harmonic components of the original of the fundamental frequency for the scale. Now we have like twice the same thing because if you remember",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=827s",
        "start_time": "827.44"
    },
    {
        "id": "2d712c2b",
        "text": "So here we start to see like some uh energy like bursts of energy like down here. And as you can see here, probably like this, this is like a constant uh like notes, then you move up, you move up, up, up up. So this probably is just like the fundamental frequencies for the scale that we played. And if you're wondering about like all of this other kind of like burst uh like of energies like at a higher frequencies, those are uh the harmonic components of the original of the fundamental frequency for the scale. Now we have like twice the same thing because if you remember like, yeah, we had twice the the scale like performed and so the same pattern is repeated twice. I I bet like that just like copy paste it like that the same scale like uh in the same in the audio file. OK. But here there's still something that's like a little bit weird which is like that on the frequency side. Uh",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=845s",
        "start_time": "845.75"
    },
    {
        "id": "be15e427",
        "text": "And if you're wondering about like all of this other kind of like burst uh like of energies like at a higher frequencies, those are uh the harmonic components of the original of the fundamental frequency for the scale. Now we have like twice the same thing because if you remember like, yeah, we had twice the the scale like performed and so the same pattern is repeated twice. I I bet like that just like copy paste it like that the same scale like uh in the same in the audio file. OK. But here there's still something that's like a little bit weird which is like that on the frequency side. Uh I mean everything like it is very squashed. And then that the reason why is that the case is because like we are using some kind of like a linear frequency representation right. But if you followed along with the series in one of like the initial videos that we had in the series, I explained that the way we perceive a frequency is,",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=865s",
        "start_time": "865.94"
    },
    {
        "id": "cfaa91ba",
        "text": "like, yeah, we had twice the the scale like performed and so the same pattern is repeated twice. I I bet like that just like copy paste it like that the same scale like uh in the same in the audio file. OK. But here there's still something that's like a little bit weird which is like that on the frequency side. Uh I mean everything like it is very squashed. And then that the reason why is that the case is because like we are using some kind of like a linear frequency representation right. But if you followed along with the series in one of like the initial videos that we had in the series, I explained that the way we perceive a frequency is, is a logarithmic, it's not a linear. So what we want to do probably uh to have a representation of the spectrogram that's more kind of like in line with the way that we perceive a frequency is to actually apply some logarithmic transformation on the frequency as well as the amplitude.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=886s",
        "start_time": "886.215"
    },
    {
        "id": "a01f6b0c",
        "text": "I mean everything like it is very squashed. And then that the reason why is that the case is because like we are using some kind of like a linear frequency representation right. But if you followed along with the series in one of like the initial videos that we had in the series, I explained that the way we perceive a frequency is, is a logarithmic, it's not a linear. So what we want to do probably uh to have a representation of the spectrogram that's more kind of like in line with the way that we perceive a frequency is to actually apply some logarithmic transformation on the frequency as well as the amplitude. So how do we do that? Well, that's extremely simple uh with Li Brosa. And so what we want to do is to create a log frequency log spectrogram uh um representation. And so what we do like in the function that I wrote, you just like pass this uh keyword argument of Y axis and you put it to log. But what this actually does under the hood, it is",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=909s",
        "start_time": "909.71"
    },
    {
        "id": "7f5e39b0",
        "text": "is a logarithmic, it's not a linear. So what we want to do probably uh to have a representation of the spectrogram that's more kind of like in line with the way that we perceive a frequency is to actually apply some logarithmic transformation on the frequency as well as the amplitude. So how do we do that? Well, that's extremely simple uh with Li Brosa. And so what we want to do is to create a log frequency log spectrogram uh um representation. And so what we do like in the function that I wrote, you just like pass this uh keyword argument of Y axis and you put it to log. But what this actually does under the hood, it is uh we are just like passing that in uh this like y axis in the Lisa dot display dot spec shell uh function and we pass it here. And so uh the default that we were using was like y axis is equal to linear. And in other words, we are using like a linear representation of the frequency. But if we put it equal to log, we're gonna be using like a log representation of frequency. OK. So now let's take a look at this and see how it looks like. OK. Good.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=931s",
        "start_time": "931.234"
    },
    {
        "id": "8820198d",
        "text": "So how do we do that? Well, that's extremely simple uh with Li Brosa. And so what we want to do is to create a log frequency log spectrogram uh um representation. And so what we do like in the function that I wrote, you just like pass this uh keyword argument of Y axis and you put it to log. But what this actually does under the hood, it is uh we are just like passing that in uh this like y axis in the Lisa dot display dot spec shell uh function and we pass it here. And so uh the default that we were using was like y axis is equal to linear. And in other words, we are using like a linear representation of the frequency. But if we put it equal to log, we're gonna be using like a log representation of frequency. OK. So now let's take a look at this and see how it looks like. OK. Good. So as you can see here, we have a uh log uh representation and now like this is like way more like spaced out. And as you can see,",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=953s",
        "start_time": "953.159"
    },
    {
        "id": "40b6ce25",
        "text": "uh we are just like passing that in uh this like y axis in the Lisa dot display dot spec shell uh function and we pass it here. And so uh the default that we were using was like y axis is equal to linear. And in other words, we are using like a linear representation of the frequency. But if we put it equal to log, we're gonna be using like a log representation of frequency. OK. So now let's take a look at this and see how it looks like. OK. Good. So as you can see here, we have a uh log uh representation and now like this is like way more like spaced out. And as you can see, so this makes a lot of sense because like we start with a uh basically like the middle C or like C four, the, the central C on the keyboard, which is like this not here, then we go up to ad so we do a CD then up to EFGAB and then back to C but at the octave above. And so we can",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=978s",
        "start_time": "978.359"
    },
    {
        "id": "7b120519",
        "text": "So as you can see here, we have a uh log uh representation and now like this is like way more like spaced out. And as you can see, so this makes a lot of sense because like we start with a uh basically like the middle C or like C four, the, the central C on the keyboard, which is like this not here, then we go up to ad so we do a CD then up to EFGAB and then back to C but at the octave above. And so we can clearly see all of the scale going up here. And it has also like some slightly like different duration, like each note is played with a different duration. So I'll just like play you back like the, the scale once again. So you can notice that. OK.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1007s",
        "start_time": "1007.469"
    },
    {
        "id": "c781d40a",
        "text": "so this makes a lot of sense because like we start with a uh basically like the middle C or like C four, the, the central C on the keyboard, which is like this not here, then we go up to ad so we do a CD then up to EFGAB and then back to C but at the octave above. And so we can clearly see all of the scale going up here. And it has also like some slightly like different duration, like each note is played with a different duration. So I'll just like play you back like the, the scale once again. So you can notice that. OK. OK.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1017s",
        "start_time": "1017.809"
    },
    {
        "id": "8c232e97",
        "text": "clearly see all of the scale going up here. And it has also like some slightly like different duration, like each note is played with a different duration. So I'll just like play you back like the, the scale once again. So you can notice that. OK. OK. And it's this, right. Right. OK.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1038s",
        "start_time": "1038.88"
    },
    {
        "id": "0cf26dd4",
        "text": "OK. And it's this, right. Right. OK. Uh Good. So the next thing that I want to do is just like visualize all the other songs from, yeah, the different genres. So like the uh classical orchestral piece from the BC, the red hot chili pepper, like rock song and the jazz ballad from uh Duke Ellington.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1059s",
        "start_time": "1059.859"
    },
    {
        "id": "a9c3492a",
        "text": "And it's this, right. Right. OK. Uh Good. So the next thing that I want to do is just like visualize all the other songs from, yeah, the different genres. So like the uh classical orchestral piece from the BC, the red hot chili pepper, like rock song and the jazz ballad from uh Duke Ellington. OK. So I'll quickly go explain what to do here, but it's basically what, what we've already done in a more extended way uh with the, with the scale.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1062s",
        "start_time": "1062.869"
    },
    {
        "id": "6692b32b",
        "text": "Uh Good. So the next thing that I want to do is just like visualize all the other songs from, yeah, the different genres. So like the uh classical orchestral piece from the BC, the red hot chili pepper, like rock song and the jazz ballad from uh Duke Ellington. OK. So I'll quickly go explain what to do here, but it's basically what, what we've already done in a more extended way uh with the, with the scale. Uh But basically what I do here is I extract the short time fourier transform as a first thing. And then uh I just uh get the magnitude, the squared magnitude and then apply this power to decibels. And then we get these signals for the PC for red dot And for Duke. And these are the",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1070s",
        "start_time": "1070.67"
    },
    {
        "id": "ff87ae7f",
        "text": "OK. So I'll quickly go explain what to do here, but it's basically what, what we've already done in a more extended way uh with the, with the scale. Uh But basically what I do here is I extract the short time fourier transform as a first thing. And then uh I just uh get the magnitude, the squared magnitude and then apply this power to decibels. And then we get these signals for the PC for red dot And for Duke. And these are the log spectrograms for this different uh like songs. And then I just like pass those like into like this block spectrogram function. And I asked to have a log representation of the frequency. And so what we're gonna see is a log frequency log amplitude uh spectrogram. OK. So let's take a look at this. OK. So the first one is the spectrogram for the classical music piece. This is the red hot chili peppers one and this is like the jazz piece.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1095s",
        "start_time": "1095.4"
    },
    {
        "id": "2b135a8f",
        "text": "Uh But basically what I do here is I extract the short time fourier transform as a first thing. And then uh I just uh get the magnitude, the squared magnitude and then apply this power to decibels. And then we get these signals for the PC for red dot And for Duke. And these are the log spectrograms for this different uh like songs. And then I just like pass those like into like this block spectrogram function. And I asked to have a log representation of the frequency. And so what we're gonna see is a log frequency log amplitude uh spectrogram. OK. So let's take a look at this. OK. So the first one is the spectrogram for the classical music piece. This is the red hot chili peppers one and this is like the jazz piece. OK. So is there any major difference that yeah, we can see just like straight away. Yes, there is. So in the case of classical music or this orchestral piece with a lot of like smooth string sounds, you see that the kind of like the this",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1104s",
        "start_time": "1104.939"
    },
    {
        "id": "2515a23c",
        "text": "log spectrograms for this different uh like songs. And then I just like pass those like into like this block spectrogram function. And I asked to have a log representation of the frequency. And so what we're gonna see is a log frequency log amplitude uh spectrogram. OK. So let's take a look at this. OK. So the first one is the spectrogram for the classical music piece. This is the red hot chili peppers one and this is like the jazz piece. OK. So is there any major difference that yeah, we can see just like straight away. Yes, there is. So in the case of classical music or this orchestral piece with a lot of like smooth string sounds, you see that the kind of like the this distribution of the energy like in the different frequencies it tends like to change, right? Quite a lot. And obviously the redder in this spectrogram like the, the, the color and the more energy you have like in that frequency of that specific moment in time, right?",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1128s",
        "start_time": "1128.29"
    },
    {
        "id": "6c4670ab",
        "text": "OK. So is there any major difference that yeah, we can see just like straight away. Yes, there is. So in the case of classical music or this orchestral piece with a lot of like smooth string sounds, you see that the kind of like the this distribution of the energy like in the different frequencies it tends like to change, right? Quite a lot. And obviously the redder in this spectrogram like the, the, the color and the more energy you have like in that frequency of that specific moment in time, right? And uh so here, as you see, like if you remember, we had like a huge crescendo a kind of like increasing intensity uh towards like the the center of like the, the, the snippet of that like the BC uh orchestral piece and here you have it down here. So you have like higher frequency",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1157s",
        "start_time": "1157.81"
    },
    {
        "id": "bce10a53",
        "text": "distribution of the energy like in the different frequencies it tends like to change, right? Quite a lot. And obviously the redder in this spectrogram like the, the, the color and the more energy you have like in that frequency of that specific moment in time, right? And uh so here, as you see, like if you remember, we had like a huge crescendo a kind of like increasing intensity uh towards like the the center of like the, the, the snippet of that like the BC uh orchestral piece and here you have it down here. So you have like higher frequency see that get like uh kind of like higher energies like this. And then if you listen to the end of the piece, it tends to kind of fade away and you can kind of visualize that because like the this uh colors tends to like kind of like fade out. It's they are not as red as they used to be like in this central part. For example. Now let's compare this kind of like zoo spectrogram to the red hot chili pepper one,",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1176s",
        "start_time": "1176.229"
    },
    {
        "id": "0d5b3357",
        "text": "And uh so here, as you see, like if you remember, we had like a huge crescendo a kind of like increasing intensity uh towards like the the center of like the, the, the snippet of that like the BC uh orchestral piece and here you have it down here. So you have like higher frequency see that get like uh kind of like higher energies like this. And then if you listen to the end of the piece, it tends to kind of fade away and you can kind of visualize that because like the this uh colors tends to like kind of like fade out. It's they are not as red as they used to be like in this central part. For example. Now let's compare this kind of like zoo spectrogram to the red hot chili pepper one, right? This is, this feels like a way more kind of I I would say like like a pattern like that repeats itself like quite a lot and we have a lot of like activity in the lower uh like frequencies. And this has also to do with the presence of uh like a kick drum. So you have like this base um kind of like they",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1194s",
        "start_time": "1194.81"
    },
    {
        "id": "187f2973",
        "text": "see that get like uh kind of like higher energies like this. And then if you listen to the end of the piece, it tends to kind of fade away and you can kind of visualize that because like the this uh colors tends to like kind of like fade out. It's they are not as red as they used to be like in this central part. For example. Now let's compare this kind of like zoo spectrogram to the red hot chili pepper one, right? This is, this feels like a way more kind of I I would say like like a pattern like that repeats itself like quite a lot and we have a lot of like activity in the lower uh like frequencies. And this has also to do with the presence of uh like a kick drum. So you have like this base um kind of like they snare like with the, with the, with the bass drum uh kind of uh creating like this typical like rock pattern and you can see it here like in with all of this activity here, you have like a lot of repetition. So take a look, for example, at this like patterns like here like in red",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1214s",
        "start_time": "1214.589"
    },
    {
        "id": "e6d8a6f8",
        "text": "right? This is, this feels like a way more kind of I I would say like like a pattern like that repeats itself like quite a lot and we have a lot of like activity in the lower uh like frequencies. And this has also to do with the presence of uh like a kick drum. So you have like this base um kind of like they snare like with the, with the, with the bass drum uh kind of uh creating like this typical like rock pattern and you can see it here like in with all of this activity here, you have like a lot of repetition. So take a look, for example, at this like patterns like here like in red like this OK. So you see that with the rock piece, you have like a lot of energy like in the lower um frequencies and you have like these patterns that you can kind of recognize and that's because the music is based off like patterns rhythmical as well as like um melodic ones. OK?",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1239s",
        "start_time": "1239.329"
    },
    {
        "id": "feb2d9ed",
        "text": "snare like with the, with the, with the bass drum uh kind of uh creating like this typical like rock pattern and you can see it here like in with all of this activity here, you have like a lot of repetition. So take a look, for example, at this like patterns like here like in red like this OK. So you see that with the rock piece, you have like a lot of energy like in the lower um frequencies and you have like these patterns that you can kind of recognize and that's because the music is based off like patterns rhythmical as well as like um melodic ones. OK? And, and then you have like the jazz piece uh by Jake Ellington. And here I could say that's a little bit like of the two worlds, right? So you still have like certain patterns that you can clearly see. And that's because we, we had like some kind of like a basic, like a groove with a drum kit and with bass. Um",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1265s",
        "start_time": "1265.81"
    },
    {
        "id": "d9ec1045",
        "text": "like this OK. So you see that with the rock piece, you have like a lot of energy like in the lower um frequencies and you have like these patterns that you can kind of recognize and that's because the music is based off like patterns rhythmical as well as like um melodic ones. OK? And, and then you have like the jazz piece uh by Jake Ellington. And here I could say that's a little bit like of the two worlds, right? So you still have like certain patterns that you can clearly see. And that's because we, we had like some kind of like a basic, like a groove with a drum kit and with bass. Um but still like, it's kind of like more fluid, right? It's not as strict as the rock piece by the red hot chili peppers, right? OK. So here, like at a glance, you, you can't see that spectrograms can reveal a lot about different musical genres, obviously. Like this is just like i an anecdotal example, but more, yeah, more often than not, these are like certain features that you actually see",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1284s",
        "start_time": "1284.479"
    },
    {
        "id": "28d05f19",
        "text": "And, and then you have like the jazz piece uh by Jake Ellington. And here I could say that's a little bit like of the two worlds, right? So you still have like certain patterns that you can clearly see. And that's because we, we had like some kind of like a basic, like a groove with a drum kit and with bass. Um but still like, it's kind of like more fluid, right? It's not as strict as the rock piece by the red hot chili peppers, right? OK. So here, like at a glance, you, you can't see that spectrograms can reveal a lot about different musical genres, obviously. Like this is just like i an anecdotal example, but more, yeah, more often than not, these are like certain features that you actually see across different genres, like when you like, look at their spectrogram. So you can have like a, a fair guess if you're experienced in uh visual spectrograms, whether like you're dealing with a classical music, kind of like peace or you're dealing with a, a rock uh like ballad or whatever,",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1303s",
        "start_time": "1303.3"
    },
    {
        "id": "18de52fc",
        "text": "but still like, it's kind of like more fluid, right? It's not as strict as the rock piece by the red hot chili peppers, right? OK. So here, like at a glance, you, you can't see that spectrograms can reveal a lot about different musical genres, obviously. Like this is just like i an anecdotal example, but more, yeah, more often than not, these are like certain features that you actually see across different genres, like when you like, look at their spectrogram. So you can have like a, a fair guess if you're experienced in uh visual spectrograms, whether like you're dealing with a classical music, kind of like peace or you're dealing with a, a rock uh like ballad or whatever, that's all for today. I hope you enjoyed the video. Next time we're gonna move on to another flavor of spectrograms called male spectrograms, which are more psychological perceptually relevant than the royal spectrograms that we've seen here. So if you've enjoyed the video and found it useful, please leave a like if you haven't subscribed and you would like to see more videos like this. Well, just subscribe to the channel",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1324s",
        "start_time": "1324.93"
    },
    {
        "id": "28148a7c",
        "text": "across different genres, like when you like, look at their spectrogram. So you can have like a, a fair guess if you're experienced in uh visual spectrograms, whether like you're dealing with a classical music, kind of like peace or you're dealing with a, a rock uh like ballad or whatever, that's all for today. I hope you enjoyed the video. Next time we're gonna move on to another flavor of spectrograms called male spectrograms, which are more psychological perceptually relevant than the royal spectrograms that we've seen here. So if you've enjoyed the video and found it useful, please leave a like if you haven't subscribed and you would like to see more videos like this. Well, just subscribe to the channel and if you have any questions as always, just like, leave them in the comments section below. I hope I'll see you next time. Cheers.",
        "video": "How to Extract Spectrograms from Audio with Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "3gzI4Z2OFgY",
        "youtube_link": "https://www.youtube.com/watch?v=3gzI4Z2OFgY&t=1353s",
        "start_time": "1353.744"
    },
    {
        "id": "cfafad25",
        "text": "Hi, everybody and welcome to a new exciting video in the audio processing for machine learning series. This time we start looking into audio signals and specifically we want to understand how we can take a sound and convert it into a digitalized audio signal that then we can use to manipulate it or to extract features and do whatever we want with it. Really? OK. But first of all, let's understand what's an audio signal. So this is a possible representation of a sound and this representation has all the info that we need in order to reproduce the sound once again to reconstruct it. OK? But we have to understand that here we have a huge problem and the problem is that on the one hand, sound is a mechanical wave that's analog in nature. And on the other, we want to process it with digital technologies like uh our computers, for example.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "b4a59ef6",
        "text": "So this is a possible representation of a sound and this representation has all the info that we need in order to reproduce the sound once again to reconstruct it. OK? But we have to understand that here we have a huge problem and the problem is that on the one hand, sound is a mechanical wave that's analog in nature. And on the other, we want to process it with digital technologies like uh our computers, for example. So how can we convert analog signals into digital signals? Well, that's the topic of today's video. But before we delve into that, I want to just give you a brief overview of what analog and digital signals are. So let's start with analog signals. So here, the intuition is that",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=28s",
        "start_time": "28.549"
    },
    {
        "id": "57e59c96",
        "text": "and the problem is that on the one hand, sound is a mechanical wave that's analog in nature. And on the other, we want to process it with digital technologies like uh our computers, for example. So how can we convert analog signals into digital signals? Well, that's the topic of today's video. But before we delve into that, I want to just give you a brief overview of what analog and digital signals are. So let's start with analog signals. So here, the intuition is that both on the X axis which is time and on the y axis which is uh amplitude or sound or air pressure. For example, we have continuous values. So we have real number values. So here we have an example of an analog signal. So as you can see the curve is continuous,",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=49s",
        "start_time": "49.459"
    },
    {
        "id": "bfa1bdfa",
        "text": "So how can we convert analog signals into digital signals? Well, that's the topic of today's video. But before we delve into that, I want to just give you a brief overview of what analog and digital signals are. So let's start with analog signals. So here, the intuition is that both on the X axis which is time and on the y axis which is uh amplitude or sound or air pressure. For example, we have continuous values. So we have real number values. So here we have an example of an analog signal. So as you can see the curve is continuous, and we have a problem with an analog signal if we want to store it in a digital format. And that's basically we have infinite resolution both on the time and on the amplitude axis. So no matter what, we can always go like at a high resolution, so we can look at uh I don't know seconds, then",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=64s",
        "start_time": "64.069"
    },
    {
        "id": "c32618d3",
        "text": "both on the X axis which is time and on the y axis which is uh amplitude or sound or air pressure. For example, we have continuous values. So we have real number values. So here we have an example of an analog signal. So as you can see the curve is continuous, and we have a problem with an analog signal if we want to store it in a digital format. And that's basically we have infinite resolution both on the time and on the amplitude axis. So no matter what, we can always go like at a high resolution, so we can look at uh I don't know seconds, then microseconds, nanoseconds even peak ads. And we still have a real number of value there. And that's because we have continuous value continuous time, right? And the same problem also appears on the amplitude axis where still we have real numbers. So potentially infinite numbers. And obviously that has the",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=85s",
        "start_time": "85.76"
    },
    {
        "id": "bcfdab25",
        "text": "and we have a problem with an analog signal if we want to store it in a digital format. And that's basically we have infinite resolution both on the time and on the amplitude axis. So no matter what, we can always go like at a high resolution, so we can look at uh I don't know seconds, then microseconds, nanoseconds even peak ads. And we still have a real number of value there. And that's because we have continuous value continuous time, right? And the same problem also appears on the amplitude axis where still we have real numbers. So potentially infinite numbers. And obviously that has the kind of like drawback of requiring infinite memory to storing such a signal in a digital format. And obviously, we can afford that. And that's why we need to switch to digital signal. So digital signal uh basically has a sequence of discrete values. It's as if like we were taking snapshots at different times of a continuous uh signal.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=107s",
        "start_time": "107.449"
    },
    {
        "id": "a79e210c",
        "text": "microseconds, nanoseconds even peak ads. And we still have a real number of value there. And that's because we have continuous value continuous time, right? And the same problem also appears on the amplitude axis where still we have real numbers. So potentially infinite numbers. And obviously that has the kind of like drawback of requiring infinite memory to storing such a signal in a digital format. And obviously, we can afford that. And that's why we need to switch to digital signal. So digital signal uh basically has a sequence of discrete values. It's as if like we were taking snapshots at different times of a continuous uh signal. And these data points can only take on a finite number of buyers. So not all the possible real numbers, but only a tiny subset of that. Now how do we move from analog to digital signal? Well, that's a process called analog to digital conversion or",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=129s",
        "start_time": "129.46"
    },
    {
        "id": "2960aae0",
        "text": "kind of like drawback of requiring infinite memory to storing such a signal in a digital format. And obviously, we can afford that. And that's why we need to switch to digital signal. So digital signal uh basically has a sequence of discrete values. It's as if like we were taking snapshots at different times of a continuous uh signal. And these data points can only take on a finite number of buyers. So not all the possible real numbers, but only a tiny subset of that. Now how do we move from analog to digital signal? Well, that's a process called analog to digital conversion or its acronym A DC. And this process consists of two subst steps. So one is sampling the other one is quantization. Now, before getting into these two things in detail, I just want to tell you that the result of A DC is audio signal, audio digital signal. And we usually",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=152s",
        "start_time": "152.22"
    },
    {
        "id": "482abea6",
        "text": "And these data points can only take on a finite number of buyers. So not all the possible real numbers, but only a tiny subset of that. Now how do we move from analog to digital signal? Well, that's a process called analog to digital conversion or its acronym A DC. And this process consists of two subst steps. So one is sampling the other one is quantization. Now, before getting into these two things in detail, I just want to tell you that the result of A DC is audio signal, audio digital signal. And we usually refer to audio digital signal also with another term that probably you'll hear like if you, if you delve deeper into uh audio digital processing and that's called pulse code modulation. OK. So this is like a term that you want to know because then you know what basically like people are talking about. OK? But now,",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=177s",
        "start_time": "177.38"
    },
    {
        "id": "bda65346",
        "text": "its acronym A DC. And this process consists of two subst steps. So one is sampling the other one is quantization. Now, before getting into these two things in detail, I just want to tell you that the result of A DC is audio signal, audio digital signal. And we usually refer to audio digital signal also with another term that probably you'll hear like if you, if you delve deeper into uh audio digital processing and that's called pulse code modulation. OK. So this is like a term that you want to know because then you know what basically like people are talking about. OK? But now, regardless of like the jargon that we use, let's move on to the real meat here.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=199s",
        "start_time": "199.029"
    },
    {
        "id": "c73efbf1",
        "text": "refer to audio digital signal also with another term that probably you'll hear like if you, if you delve deeper into uh audio digital processing and that's called pulse code modulation. OK. So this is like a term that you want to know because then you know what basically like people are talking about. OK? But now, regardless of like the jargon that we use, let's move on to the real meat here. So sampling. So we said there are two steps in a DC. The first one is sampling. OK. So what's sampling basically? Well, it's kind of like self explanatory. So we just like sample like data points across like a sound wave at specific points in time. And these black dots here are all like sample points. Now, how do we sample? Well, we usually",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=220s",
        "start_time": "220.679"
    },
    {
        "id": "c17c5309",
        "text": "regardless of like the jargon that we use, let's move on to the real meat here. So sampling. So we said there are two steps in a DC. The first one is sampling. OK. So what's sampling basically? Well, it's kind of like self explanatory. So we just like sample like data points across like a sound wave at specific points in time. And these black dots here are all like sample points. Now, how do we sample? Well, we usually the site on a period on a sampling kind of like period and we sample at equidistant intervals in time. And these intervals are just like the period which is indicated with capital T and at each period, we sample a data point. So this is the first one, this is the second one, the third one and so on. And so fourth. OK. Now",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=243s",
        "start_time": "243.11"
    },
    {
        "id": "e933ab0d",
        "text": "So sampling. So we said there are two steps in a DC. The first one is sampling. OK. So what's sampling basically? Well, it's kind of like self explanatory. So we just like sample like data points across like a sound wave at specific points in time. And these black dots here are all like sample points. Now, how do we sample? Well, we usually the site on a period on a sampling kind of like period and we sample at equidistant intervals in time. And these intervals are just like the period which is indicated with capital T and at each period, we sample a data point. So this is the first one, this is the second one, the third one and so on. And so fourth. OK. Now how do we locate samples on the X axis on time? So let's say we want to locate the time at which we have sample number N.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=248s",
        "start_time": "248.52"
    },
    {
        "id": "dd461e0f",
        "text": "the site on a period on a sampling kind of like period and we sample at equidistant intervals in time. And these intervals are just like the period which is indicated with capital T and at each period, we sample a data point. So this is the first one, this is the second one, the third one and so on. And so fourth. OK. Now how do we locate samples on the X axis on time? So let's say we want to locate the time at which we have sample number N. So that's TN and we can use this formula and given, we know that we are sampling points at equidistant time intervals called capital T. We can just multiply N which is the sample that we want to um like find and multiply that by the period. And that will give us like the time at which that sample is appearing in the signal.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=274s",
        "start_time": "274.265"
    },
    {
        "id": "1b8e40a2",
        "text": "how do we locate samples on the X axis on time? So let's say we want to locate the time at which we have sample number N. So that's TN and we can use this formula and given, we know that we are sampling points at equidistant time intervals called capital T. We can just multiply N which is the sample that we want to um like find and multiply that by the period. And that will give us like the time at which that sample is appearing in the signal. OK. So now there's another very interesting characteristic of sampling. This is like a feature that we can play around with to obtain different types of like sampling. And that's called the sampling rate. We can indicate that with SR and that's basically the inverse of the period. OK. And this sampling rate is basically a frequency and it's measured in Hertz. OK.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=302s",
        "start_time": "302.38"
    },
    {
        "id": "356cd26b",
        "text": "So that's TN and we can use this formula and given, we know that we are sampling points at equidistant time intervals called capital T. We can just multiply N which is the sample that we want to um like find and multiply that by the period. And that will give us like the time at which that sample is appearing in the signal. OK. So now there's another very interesting characteristic of sampling. This is like a feature that we can play around with to obtain different types of like sampling. And that's called the sampling rate. We can indicate that with SR and that's basically the inverse of the period. OK. And this sampling rate is basically a frequency and it's measured in Hertz. OK. And this indicates the uh kind of number of samples that we have for each second of our digital signal. OK. So now we can distinguish between like lower sampling rates and higher sampling rates. So why, why do we butter like about like this distinction and like what's the effect on the overall sampling process? OK. So let's take a look at this sampling, low sampling rates here.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=314s",
        "start_time": "314.91"
    },
    {
        "id": "a0624765",
        "text": "OK. So now there's another very interesting characteristic of sampling. This is like a feature that we can play around with to obtain different types of like sampling. And that's called the sampling rate. We can indicate that with SR and that's basically the inverse of the period. OK. And this sampling rate is basically a frequency and it's measured in Hertz. OK. And this indicates the uh kind of number of samples that we have for each second of our digital signal. OK. So now we can distinguish between like lower sampling rates and higher sampling rates. So why, why do we butter like about like this distinction and like what's the effect on the overall sampling process? OK. So let's take a look at this sampling, low sampling rates here. And so here you can see the",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=344s",
        "start_time": "344.1"
    },
    {
        "id": "00eba1c4",
        "text": "And this indicates the uh kind of number of samples that we have for each second of our digital signal. OK. So now we can distinguish between like lower sampling rates and higher sampling rates. So why, why do we butter like about like this distinction and like what's the effect on the overall sampling process? OK. So let's take a look at this sampling, low sampling rates here. And so here you can see the uh this like uh vertical bars and these represent each of these like represents a SAM sample. And these are like wider than the ones that we have here on the right hand side because we have like a lower uh sample rate, which means like the period, uh the sampling period is higher. Now what happens here is that there's a difference between the area below",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=373s",
        "start_time": "373.54"
    },
    {
        "id": "8651daa3",
        "text": "And so here you can see the uh this like uh vertical bars and these represent each of these like represents a SAM sample. And these are like wider than the ones that we have here on the right hand side because we have like a lower uh sample rate, which means like the period, uh the sampling period is higher. Now what happens here is that there's a difference between the area below the continuous curve and the area that is created by this vertical bars. And the difference is the sampling error intuitively, that is like that difference is the amount of information that we necessarily loss lose when we are like applying sampling. Now, if we compare that sampling error",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=403s",
        "start_time": "403.429"
    },
    {
        "id": "b81c74f4",
        "text": "uh this like uh vertical bars and these represent each of these like represents a SAM sample. And these are like wider than the ones that we have here on the right hand side because we have like a lower uh sample rate, which means like the period, uh the sampling period is higher. Now what happens here is that there's a difference between the area below the continuous curve and the area that is created by this vertical bars. And the difference is the sampling error intuitively, that is like that difference is the amount of information that we necessarily loss lose when we are like applying sampling. Now, if we compare that sampling error between the low sampling rate here on the left hand side and the higher sampling rate, you'll notice that obviously having like higher temporal resolution here, we're gonna have less of an error, right. So the the higher the sampling rate and the",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=407s",
        "start_time": "407.14"
    },
    {
        "id": "4d3be58e",
        "text": "the continuous curve and the area that is created by this vertical bars. And the difference is the sampling error intuitively, that is like that difference is the amount of information that we necessarily loss lose when we are like applying sampling. Now, if we compare that sampling error between the low sampling rate here on the left hand side and the higher sampling rate, you'll notice that obviously having like higher temporal resolution here, we're gonna have less of an error, right. So the the higher the sampling rate and the lower the sampling error. Now, why is this interesting? And what sampling rates should we use? Well, this is like an open question and it really depends on what you need to do which like sound",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=433s",
        "start_time": "433.72"
    },
    {
        "id": "ef24dfcf",
        "text": "between the low sampling rate here on the left hand side and the higher sampling rate, you'll notice that obviously having like higher temporal resolution here, we're gonna have less of an error, right. So the the higher the sampling rate and the lower the sampling error. Now, why is this interesting? And what sampling rates should we use? Well, this is like an open question and it really depends on what you need to do which like sound an interesting question we can ask is why do we have certain sampling rates? So for example, for the CD technology, we have a sampling rate which is 44.1 kilohertz. So why people decided on that specific value? Is that arbitrary? Well,",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=460s",
        "start_time": "460.67"
    },
    {
        "id": "c9610288",
        "text": "lower the sampling error. Now, why is this interesting? And what sampling rates should we use? Well, this is like an open question and it really depends on what you need to do which like sound an interesting question we can ask is why do we have certain sampling rates? So for example, for the CD technology, we have a sampling rate which is 44.1 kilohertz. So why people decided on that specific value? Is that arbitrary? Well, obviously there's a certain level uh of like, I mean, it's arbitrary to a certain extent, but then there's like some reasoning behind it. And to understand what the reasoning is, we need to introduce another concept, which is the nest frequency that comes from the NWS theorem. We can um indicate this which are F or then and the NIST frequency is given by half the sampling rates.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=478s",
        "start_time": "478.799"
    },
    {
        "id": "368db107",
        "text": "an interesting question we can ask is why do we have certain sampling rates? So for example, for the CD technology, we have a sampling rate which is 44.1 kilohertz. So why people decided on that specific value? Is that arbitrary? Well, obviously there's a certain level uh of like, I mean, it's arbitrary to a certain extent, but then there's like some reasoning behind it. And to understand what the reasoning is, we need to introduce another concept, which is the nest frequency that comes from the NWS theorem. We can um indicate this which are F or then and the NIST frequency is given by half the sampling rates. What the liquid frequency tells us is basically the upper bound frequency that we can have in a digital signal that's not going to recreate any artifacts, right? So basically up until",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=497s",
        "start_time": "497.2"
    },
    {
        "id": "f1a74489",
        "text": "obviously there's a certain level uh of like, I mean, it's arbitrary to a certain extent, but then there's like some reasoning behind it. And to understand what the reasoning is, we need to introduce another concept, which is the nest frequency that comes from the NWS theorem. We can um indicate this which are F or then and the NIST frequency is given by half the sampling rates. What the liquid frequency tells us is basically the upper bound frequency that we can have in a digital signal that's not going to recreate any artifacts, right? So basically up until the nus frequency, we can reconstruct a signal. OK. But if we go above the nus frequency in our signal, we are gonna start to have artifacts and we'll see what those artifacts are in a second.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=515s",
        "start_time": "515.83"
    },
    {
        "id": "5b94daf1",
        "text": "What the liquid frequency tells us is basically the upper bound frequency that we can have in a digital signal that's not going to recreate any artifacts, right? So basically up until the nus frequency, we can reconstruct a signal. OK. But if we go above the nus frequency in our signal, we are gonna start to have artifacts and we'll see what those artifacts are in a second. Now, if we move back to the uh to the CD example, and we take a look at the nucleus frequency for the CD, that's 44.1 K divided by two, which gives us 22,050 Hertz. This is the nucleus frequency for a CD. Now this number should ring a bell for you.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=544s",
        "start_time": "544.0"
    },
    {
        "id": "8165666e",
        "text": "the nus frequency, we can reconstruct a signal. OK. But if we go above the nus frequency in our signal, we are gonna start to have artifacts and we'll see what those artifacts are in a second. Now, if we move back to the uh to the CD example, and we take a look at the nucleus frequency for the CD, that's 44.1 K divided by two, which gives us 22,050 Hertz. This is the nucleus frequency for a CD. Now this number should ring a bell for you. So if you watch my previous videos, when I, where I was covering the human hearing range, you're probably familiar with the opera",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=561s",
        "start_time": "561.255"
    },
    {
        "id": "1114ede5",
        "text": "Now, if we move back to the uh to the CD example, and we take a look at the nucleus frequency for the CD, that's 44.1 K divided by two, which gives us 22,050 Hertz. This is the nucleus frequency for a CD. Now this number should ring a bell for you. So if you watch my previous videos, when I, where I was covering the human hearing range, you're probably familiar with the opera hearing range for humans, which is around 20 k. So the NUS frequency for CD is kind of like very close to that. And so why is that the case? Well, that's the case because this means that we can go up to 22 K plus in Hertz and still not have any artifacts there.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=578s",
        "start_time": "578.7"
    },
    {
        "id": "aa0a5cb6",
        "text": "So if you watch my previous videos, when I, where I was covering the human hearing range, you're probably familiar with the opera hearing range for humans, which is around 20 k. So the NUS frequency for CD is kind of like very close to that. And so why is that the case? Well, that's the case because this means that we can go up to 22 K plus in Hertz and still not have any artifacts there. And we know that 22 K is slightly above the human hearing range. And so that means that we can basically appreciate the whole uh frequency range in a CD without getting any artifacts. OK. So now we are talking about artifacts. So if we go above the NS frequency, but what are those artifacts? Well,",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=602s",
        "start_time": "602.84"
    },
    {
        "id": "eff87787",
        "text": "hearing range for humans, which is around 20 k. So the NUS frequency for CD is kind of like very close to that. And so why is that the case? Well, that's the case because this means that we can go up to 22 K plus in Hertz and still not have any artifacts there. And we know that 22 K is slightly above the human hearing range. And so that means that we can basically appreciate the whole uh frequency range in a CD without getting any artifacts. OK. So now we are talking about artifacts. So if we go above the NS frequency, but what are those artifacts? Well, the artifacts that we inject in a signal, if we have uh frequencies that are above the liquid frequencies are determined by a liaising. I'm sure you're familiar with the term A liaising. But here I want to show you what a lasing really is. And so for that, we can take a look at this graph. Now, here you have a continuous signal in a red and then we've taken some samples that are like this black dots.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=613s",
        "start_time": "613.26"
    },
    {
        "id": "6b5e0cbe",
        "text": "And we know that 22 K is slightly above the human hearing range. And so that means that we can basically appreciate the whole uh frequency range in a CD without getting any artifacts. OK. So now we are talking about artifacts. So if we go above the NS frequency, but what are those artifacts? Well, the artifacts that we inject in a signal, if we have uh frequencies that are above the liquid frequencies are determined by a liaising. I'm sure you're familiar with the term A liaising. But here I want to show you what a lasing really is. And so for that, we can take a look at this graph. Now, here you have a continuous signal in a red and then we've taken some samples that are like this black dots. Uh The interesting thing here is that if we look at the frequency of the original uh signaling in red, that's higher than the nus frequency that we have for our sampling process, what that entails is a problem is an artifact and that's amazing. So, and how do we uh see that? Well, let's take a look at the reconstruction of the digital curve here and it's like this",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=638s",
        "start_time": "638.7"
    },
    {
        "id": "7d2eded8",
        "text": "the artifacts that we inject in a signal, if we have uh frequencies that are above the liquid frequencies are determined by a liaising. I'm sure you're familiar with the term A liaising. But here I want to show you what a lasing really is. And so for that, we can take a look at this graph. Now, here you have a continuous signal in a red and then we've taken some samples that are like this black dots. Uh The interesting thing here is that if we look at the frequency of the original uh signaling in red, that's higher than the nus frequency that we have for our sampling process, what that entails is a problem is an artifact and that's amazing. So, and how do we uh see that? Well, let's take a look at the reconstruction of the digital curve here and it's like this curve in blue. Now, as you can see here, uh there's a complete difference between the original signal which is, which has quite high frequency and the signal, the reconstructed signal in blue, which has a way lower frequency and that like artifact is amazing. So what it does basically is it just like shift down all the frequencies that are above the",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=664s",
        "start_time": "664.63"
    },
    {
        "id": "0c951760",
        "text": "Uh The interesting thing here is that if we look at the frequency of the original uh signaling in red, that's higher than the nus frequency that we have for our sampling process, what that entails is a problem is an artifact and that's amazing. So, and how do we uh see that? Well, let's take a look at the reconstruction of the digital curve here and it's like this curve in blue. Now, as you can see here, uh there's a complete difference between the original signal which is, which has quite high frequency and the signal, the reconstructed signal in blue, which has a way lower frequency and that like artifact is amazing. So what it does basically is it just like shift down all the frequencies that are above the N frequency. Now, this is kind of like the intuition behind it, but I feel it's a little bit abstract. So let me show you the effect of a li on a piece of music. Now, I'm in my digital audio work session. This is Audacity, the name of the software. It's open source. So you should definitely check that out. And if you are on Linux, it's great because you can run it and it's not the case with most D Aws. Really.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=693s",
        "start_time": "693.979"
    },
    {
        "id": "ab7ce2b5",
        "text": "curve in blue. Now, as you can see here, uh there's a complete difference between the original signal which is, which has quite high frequency and the signal, the reconstructed signal in blue, which has a way lower frequency and that like artifact is amazing. So what it does basically is it just like shift down all the frequencies that are above the N frequency. Now, this is kind of like the intuition behind it, but I feel it's a little bit abstract. So let me show you the effect of a li on a piece of music. Now, I'm in my digital audio work session. This is Audacity, the name of the software. It's open source. So you should definitely check that out. And if you are on Linux, it's great because you can run it and it's not the case with most D Aws. Really. OK. So here we have like a piece of music. So let's listen to it at the, its original sampling rate of 44.1 kilohertz.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=720s",
        "start_time": "720.736"
    },
    {
        "id": "04c441b2",
        "text": "N frequency. Now, this is kind of like the intuition behind it, but I feel it's a little bit abstract. So let me show you the effect of a li on a piece of music. Now, I'm in my digital audio work session. This is Audacity, the name of the software. It's open source. So you should definitely check that out. And if you are on Linux, it's great because you can run it and it's not the case with most D Aws. Really. OK. So here we have like a piece of music. So let's listen to it at the, its original sampling rate of 44.1 kilohertz. OK? So now let me resample that. So",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=747s",
        "start_time": "747.492"
    },
    {
        "id": "aeb810a2",
        "text": "OK. So here we have like a piece of music. So let's listen to it at the, its original sampling rate of 44.1 kilohertz. OK? So now let me resample that. So OK. Yeah, let me select this. So what I want to do here is to resample the audio and put it and use a sampling rate of one kilohertz. It's quite dramatic.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=774s",
        "start_time": "774.559"
    },
    {
        "id": "e5609d87",
        "text": "OK? So now let me resample that. So OK. Yeah, let me select this. So what I want to do here is to resample the audio and put it and use a sampling rate of one kilohertz. It's quite dramatic. The change. Let's listen,",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=798s",
        "start_time": "798.299"
    },
    {
        "id": "2efe2154",
        "text": "OK. Yeah, let me select this. So what I want to do here is to resample the audio and put it and use a sampling rate of one kilohertz. It's quite dramatic. The change. Let's listen, right? The difference is out. It's just incredible, right? And that's because uh all the frequencies that are above the nus frequency uh which in our case is 500 Hertz just like, get a lied and so they just like get",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=805s",
        "start_time": "805.51"
    },
    {
        "id": "6ffe164a",
        "text": "The change. Let's listen, right? The difference is out. It's just incredible, right? And that's because uh all the frequencies that are above the nus frequency uh which in our case is 500 Hertz just like, get a lied and so they just like get moved, get artifacts and get moved towards like the like lower frequencies. OK. So that's the effect of a liaising on sound. Now we are done with sampling. So we should move to the second step in an auto to digital conversion which is quantization.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=820s",
        "start_time": "820.52"
    },
    {
        "id": "8e711202",
        "text": "right? The difference is out. It's just incredible, right? And that's because uh all the frequencies that are above the nus frequency uh which in our case is 500 Hertz just like, get a lied and so they just like get moved, get artifacts and get moved towards like the like lower frequencies. OK. So that's the effect of a liaising on sound. Now we are done with sampling. So we should move to the second step in an auto to digital conversion which is quantization. OK. So now that we are familiar with the process of sampling, we can apply more or less like the same thing to quantization. The only difference really here is that instead of like sampling on the X axis, we are quants on the Y axis on the amplitude.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=834s",
        "start_time": "834.349"
    },
    {
        "id": "6b5a162d",
        "text": "moved, get artifacts and get moved towards like the like lower frequencies. OK. So that's the effect of a liaising on sound. Now we are done with sampling. So we should move to the second step in an auto to digital conversion which is quantization. OK. So now that we are familiar with the process of sampling, we can apply more or less like the same thing to quantization. The only difference really here is that instead of like sampling on the X axis, we are quants on the Y axis on the amplitude. But basically the idea here is that we have a fixed discrete number of amplitude values on the Y axis. And then at each sample, we just quantize the value of amplitude to the closest",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=852s",
        "start_time": "852.76"
    },
    {
        "id": "351d30be",
        "text": "OK. So now that we are familiar with the process of sampling, we can apply more or less like the same thing to quantization. The only difference really here is that instead of like sampling on the X axis, we are quants on the Y axis on the amplitude. But basically the idea here is that we have a fixed discrete number of amplitude values on the Y axis. And then at each sample, we just quantize the value of amplitude to the closest uh value that we have available here on our Y axis. By applying quantization, we create a quantization error just the way we did create a sampling error when we were applying sampling to the analog signals.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=870s",
        "start_time": "870.38"
    },
    {
        "id": "4b707f5e",
        "text": "But basically the idea here is that we have a fixed discrete number of amplitude values on the Y axis. And then at each sample, we just quantize the value of amplitude to the closest uh value that we have available here on our Y axis. By applying quantization, we create a quantization error just the way we did create a sampling error when we were applying sampling to the analog signals. Now it's uh kind of like intuitive that the higher the resolution, the quantization resolution or in other words, the more values we have here on the Y axis and the lower the quantization error is going to be. Now, if we take a look at the",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=886s",
        "start_time": "886.83"
    },
    {
        "id": "c1fd02e8",
        "text": "uh value that we have available here on our Y axis. By applying quantization, we create a quantization error just the way we did create a sampling error when we were applying sampling to the analog signals. Now it's uh kind of like intuitive that the higher the resolution, the quantization resolution or in other words, the more values we have here on the Y axis and the lower the quantization error is going to be. Now, if we take a look at the values that we are using here on the Y axis, you'll see that these are binary values specifically, we are using four bits. Now, you may be wondering but why are we using uh like binary values here? Well, that's because we are dealing with like um",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=904s",
        "start_time": "904.979"
    },
    {
        "id": "f3e0206f",
        "text": "Now it's uh kind of like intuitive that the higher the resolution, the quantization resolution or in other words, the more values we have here on the Y axis and the lower the quantization error is going to be. Now, if we take a look at the values that we are using here on the Y axis, you'll see that these are binary values specifically, we are using four bits. Now, you may be wondering but why are we using uh like binary values here? Well, that's because we are dealing with like um digital computers, right? So we are going to store this audio digital signals in digital computers. And so it only makes sense just to work with binary values. So the resolution of quantization is calculated in number of bits. Now you may hear uh this other like term bit depth and that's kind of like a synonym for resolution.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=923s",
        "start_time": "923.4"
    },
    {
        "id": "433854e4",
        "text": "values that we are using here on the Y axis, you'll see that these are binary values specifically, we are using four bits. Now, you may be wondering but why are we using uh like binary values here? Well, that's because we are dealing with like um digital computers, right? So we are going to store this audio digital signals in digital computers. And so it only makes sense just to work with binary values. So the resolution of quantization is calculated in number of bits. Now you may hear uh this other like term bit depth and that's kind of like a synonym for resolution. So the uh bits app uh resolution for A CD is 16 bits. OK.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=940s",
        "start_time": "940.525"
    },
    {
        "id": "0fe17a1a",
        "text": "digital computers, right? So we are going to store this audio digital signals in digital computers. And so it only makes sense just to work with binary values. So the resolution of quantization is calculated in number of bits. Now you may hear uh this other like term bit depth and that's kind of like a synonym for resolution. So the uh bits app uh resolution for A CD is 16 bits. OK. So if we want to just like have an idea what that is like in decimal decimal numbers, so we'll just uh apply like this conversion here. So we get like we do a two to the power of 16 which is the bit depth and we get this number here which is 16 65.5 k values. So these are all the possible amplitude values that we can use when we quantize an analog",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=958s",
        "start_time": "958.369"
    },
    {
        "id": "7c732f68",
        "text": "So the uh bits app uh resolution for A CD is 16 bits. OK. So if we want to just like have an idea what that is like in decimal decimal numbers, so we'll just uh apply like this conversion here. So we get like we do a two to the power of 16 which is the bit depth and we get this number here which is 16 65.5 k values. So these are all the possible amplitude values that we can use when we quantize an analog signal in a CD. An interesting problem is to check the amount of hard disk memory required for storing one minute worth of sound that obviously depends on the sampling rate and bit depth that we are applying. So now let's assume that we are using like customary values like the CD values. So 44.1 kilohertz for the sampling rate and a bit depth of 16.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=983s",
        "start_time": "983.409"
    },
    {
        "id": "94120587",
        "text": "So if we want to just like have an idea what that is like in decimal decimal numbers, so we'll just uh apply like this conversion here. So we get like we do a two to the power of 16 which is the bit depth and we get this number here which is 16 65.5 k values. So these are all the possible amplitude values that we can use when we quantize an analog signal in a CD. An interesting problem is to check the amount of hard disk memory required for storing one minute worth of sound that obviously depends on the sampling rate and bit depth that we are applying. So now let's assume that we are using like customary values like the CD values. So 44.1 kilohertz for the sampling rate and a bit depth of 16. So this is the like formula that gives us like the amount of memory that we require for one minute of sound expressed in megabytes. Now let's break this down So we start by multiplying the sampling rate by the, well, the bit depth by the sampling rate. And so this gives us the number of beats per second.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=991s",
        "start_time": "991.88"
    },
    {
        "id": "562fd549",
        "text": "signal in a CD. An interesting problem is to check the amount of hard disk memory required for storing one minute worth of sound that obviously depends on the sampling rate and bit depth that we are applying. So now let's assume that we are using like customary values like the CD values. So 44.1 kilohertz for the sampling rate and a bit depth of 16. So this is the like formula that gives us like the amount of memory that we require for one minute of sound expressed in megabytes. Now let's break this down So we start by multiplying the sampling rate by the, well, the bit depth by the sampling rate. And so this gives us the number of beats per second. Now, by dividing that number by 1,048,000 something, we move from number of bits per second to a number of megabits per second. If we divide that by eight, we move from number of uh bits by um number of bits, number of megabits per second to number of megabytes per second. OK. And so this is like what comes out of all these operations.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1019s",
        "start_time": "1019.854"
    },
    {
        "id": "088c136f",
        "text": "So this is the like formula that gives us like the amount of memory that we require for one minute of sound expressed in megabytes. Now let's break this down So we start by multiplying the sampling rate by the, well, the bit depth by the sampling rate. And so this gives us the number of beats per second. Now, by dividing that number by 1,048,000 something, we move from number of bits per second to a number of megabits per second. If we divide that by eight, we move from number of uh bits by um number of bits, number of megabits per second to number of megabytes per second. OK. And so this is like what comes out of all these operations. Now, if we multiply that by 60 we get 5.49 megabytes. And this is like the amount of memory that we need for storing one minute of sound.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1048s",
        "start_time": "1048.198"
    },
    {
        "id": "823df38e",
        "text": "Now, by dividing that number by 1,048,000 something, we move from number of bits per second to a number of megabits per second. If we divide that by eight, we move from number of uh bits by um number of bits, number of megabits per second to number of megabytes per second. OK. And so this is like what comes out of all these operations. Now, if we multiply that by 60 we get 5.49 megabytes. And this is like the amount of memory that we need for storing one minute of sound. As you can see, this is a lot of memory. So this is like a a problem because like for storing audio, like a and as a wave file, we need a lot of memory. That's why you get a lot of lossy formats like the MP3 which shrink the amount of memory that we need for storing audio data. Now,",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1072s",
        "start_time": "1072.16"
    },
    {
        "id": "270e0c2d",
        "text": "Now, if we multiply that by 60 we get 5.49 megabytes. And this is like the amount of memory that we need for storing one minute of sound. As you can see, this is a lot of memory. So this is like a a problem because like for storing audio, like a and as a wave file, we need a lot of memory. That's why you get a lot of lossy formats like the MP3 which shrink the amount of memory that we need for storing audio data. Now, a concept that's connected with the um with the quantization process is that dynamic range. Now, this is a an intuitive quite intuitive concept and basically dynamic range is the difference between the largest and small",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1101s",
        "start_time": "1101.819"
    },
    {
        "id": "73245087",
        "text": "As you can see, this is a lot of memory. So this is like a a problem because like for storing audio, like a and as a wave file, we need a lot of memory. That's why you get a lot of lossy formats like the MP3 which shrink the amount of memory that we need for storing audio data. Now, a concept that's connected with the um with the quantization process is that dynamic range. Now, this is a an intuitive quite intuitive concept and basically dynamic range is the difference between the largest and small the signal that the system can record. In other words, we can think of dynamic range as the uh kind of like the loudness range that we can appreciate from uh uh some digital sound. And the idea here is that the higher the resolution, so the more bits we use and the higher the dynamic range that we have now",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1114s",
        "start_time": "1114.53"
    },
    {
        "id": "9ae64113",
        "text": "a concept that's connected with the um with the quantization process is that dynamic range. Now, this is a an intuitive quite intuitive concept and basically dynamic range is the difference between the largest and small the signal that the system can record. In other words, we can think of dynamic range as the uh kind of like the loudness range that we can appreciate from uh uh some digital sound. And the idea here is that the higher the resolution, so the more bits we use and the higher the dynamic range that we have now the concept of dynamic range is itself connected with this other concept which is that of signal to quantization noise ratio. Now, this signal to noise ratio is the relationship between the max signal strength and the quantization error. And this signal to quantization ratio uh correlates with the with dynamic range.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1137s",
        "start_time": "1137.17"
    },
    {
        "id": "3fd67ec9",
        "text": "the signal that the system can record. In other words, we can think of dynamic range as the uh kind of like the loudness range that we can appreciate from uh uh some digital sound. And the idea here is that the higher the resolution, so the more bits we use and the higher the dynamic range that we have now the concept of dynamic range is itself connected with this other concept which is that of signal to quantization noise ratio. Now, this signal to noise ratio is the relationship between the max signal strength and the quantization error. And this signal to quantization ratio uh correlates with the with dynamic range. So how can we um get an idea of the signal to noise ratio like for like some digital signal?",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1152s",
        "start_time": "1152.574"
    },
    {
        "id": "7d54785c",
        "text": "the concept of dynamic range is itself connected with this other concept which is that of signal to quantization noise ratio. Now, this signal to noise ratio is the relationship between the max signal strength and the quantization error. And this signal to quantization ratio uh correlates with the with dynamic range. So how can we um get an idea of the signal to noise ratio like for like some digital signal? So it's given like by this simple formula here. So uh SQNR it's approximately this constant. So 6.02 by capital Q where capital Q is the bit depth. Now, in the case of 16 bit depth,",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1180s",
        "start_time": "1180.18"
    },
    {
        "id": "801a4a7e",
        "text": "So how can we um get an idea of the signal to noise ratio like for like some digital signal? So it's given like by this simple formula here. So uh SQNR it's approximately this constant. So 6.02 by capital Q where capital Q is the bit depth. Now, in the case of 16 bit depth, we have a signal to quantization noise ratio which is 96 decibels obviously like the this uh signal to noise ratio is measured in decibels. And in this case, it correlates like it's basically like it indicates the",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1207s",
        "start_time": "1207.16"
    },
    {
        "id": "d4a72fd7",
        "text": "So it's given like by this simple formula here. So uh SQNR it's approximately this constant. So 6.02 by capital Q where capital Q is the bit depth. Now, in the case of 16 bit depth, we have a signal to quantization noise ratio which is 96 decibels obviously like the this uh signal to noise ratio is measured in decibels. And in this case, it correlates like it's basically like it indicates the dynamic range itself. In other words, we know that when we are quantis a analog signal applying a bit depth of 16, we are going to end up with a dynamic range of 96 decibels. OK",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1215s",
        "start_time": "1215.93"
    },
    {
        "id": "d245866a",
        "text": "we have a signal to quantization noise ratio which is 96 decibels obviously like the this uh signal to noise ratio is measured in decibels. And in this case, it correlates like it's basically like it indicates the dynamic range itself. In other words, we know that when we are quantis a analog signal applying a bit depth of 16, we are going to end up with a dynamic range of 96 decibels. OK good. So this was it like for the process like of digitization, that kind of like has these two steps. So sampling and quantization we saw both of them. Now, the next question like that we can ask is then how do we record sound starting like from a sound and then arriving like at a um digitalized version of that sound of that audio signal.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1234s",
        "start_time": "1234.599"
    },
    {
        "id": "5aa4d992",
        "text": "dynamic range itself. In other words, we know that when we are quantis a analog signal applying a bit depth of 16, we are going to end up with a dynamic range of 96 decibels. OK good. So this was it like for the process like of digitization, that kind of like has these two steps. So sampling and quantization we saw both of them. Now, the next question like that we can ask is then how do we record sound starting like from a sound and then arriving like at a um digitalized version of that sound of that audio signal. OK. So we start with a mechanical wave which is just sound that hits a microphone. And this uh let's just like a diaphragm, for example, like starts oscillating and this oscillation creates a unlock uh electrical signal and this electrical signal",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1252s",
        "start_time": "1252.31"
    },
    {
        "id": "02b895d3",
        "text": "good. So this was it like for the process like of digitization, that kind of like has these two steps. So sampling and quantization we saw both of them. Now, the next question like that we can ask is then how do we record sound starting like from a sound and then arriving like at a um digitalized version of that sound of that audio signal. OK. So we start with a mechanical wave which is just sound that hits a microphone. And this uh let's just like a diaphragm, for example, like starts oscillating and this oscillation creates a unlock uh electrical signal and this electrical signal gets packed into a sound card which acts as an A DC device or analog to digital converter. And obviously it applies sampling and quantization and it also applies some anti liaising filtering so that it avoids liaising. And so for doing that, what we usually do is we",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1271s",
        "start_time": "1271.9"
    },
    {
        "id": "e0ad5f83",
        "text": "OK. So we start with a mechanical wave which is just sound that hits a microphone. And this uh let's just like a diaphragm, for example, like starts oscillating and this oscillation creates a unlock uh electrical signal and this electrical signal gets packed into a sound card which acts as an A DC device or analog to digital converter. And obviously it applies sampling and quantization and it also applies some anti liaising filtering so that it avoids liaising. And so for doing that, what we usually do is we have a low pass filter that cuts off all the frequencies that are above the NS frequency. So out of like the sound card, we get, we get a digital signal that then we can store on our laptop computer.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1300s",
        "start_time": "1300.069"
    },
    {
        "id": "b25740b9",
        "text": "gets packed into a sound card which acts as an A DC device or analog to digital converter. And obviously it applies sampling and quantization and it also applies some anti liaising filtering so that it avoids liaising. And so for doing that, what we usually do is we have a low pass filter that cuts off all the frequencies that are above the NS frequency. So out of like the sound card, we get, we get a digital signal that then we can store on our laptop computer. Now, the interesting question is also like the reverse of this one. So how do we reproduce sound? So we start obviously with a digital signal. Now we put that into our sound card once again. But this time the sound card applies like the inverse of audio to digital analog to digital conversion. It does digital",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1323s",
        "start_time": "1323.094"
    },
    {
        "id": "9acc7cde",
        "text": "have a low pass filter that cuts off all the frequencies that are above the NS frequency. So out of like the sound card, we get, we get a digital signal that then we can store on our laptop computer. Now, the interesting question is also like the reverse of this one. So how do we reproduce sound? So we start obviously with a digital signal. Now we put that into our sound card once again. But this time the sound card applies like the inverse of audio to digital analog to digital conversion. It does digital to analog conversion and it creates an electrical signal that gets transferred to speakers. And this signal stimulates membranes and these membranes convert the basically the electrical signal into a mechanical wave which is sound which hits our ears and now we hear sound.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1346s",
        "start_time": "1346.579"
    },
    {
        "id": "e8316aef",
        "text": "Now, the interesting question is also like the reverse of this one. So how do we reproduce sound? So we start obviously with a digital signal. Now we put that into our sound card once again. But this time the sound card applies like the inverse of audio to digital analog to digital conversion. It does digital to analog conversion and it creates an electrical signal that gets transferred to speakers. And this signal stimulates membranes and these membranes convert the basically the electrical signal into a mechanical wave which is sound which hits our ears and now we hear sound. Wow, that was intense. But I hope by now you have an idea of how all of this like comes together. So",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1364s",
        "start_time": "1364.339"
    },
    {
        "id": "f0322b52",
        "text": "to analog conversion and it creates an electrical signal that gets transferred to speakers. And this signal stimulates membranes and these membranes convert the basically the electrical signal into a mechanical wave which is sound which hits our ears and now we hear sound. Wow, that was intense. But I hope by now you have an idea of how all of this like comes together. So after like this introductory videos, now you should have like a good understanding of like mechanical waves sound, the different features of sound, like frequency, loudness and tre",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1386s",
        "start_time": "1386.545"
    },
    {
        "id": "f5f54ff6",
        "text": "Wow, that was intense. But I hope by now you have an idea of how all of this like comes together. So after like this introductory videos, now you should have like a good understanding of like mechanical waves sound, the different features of sound, like frequency, loudness and tre and now you should also understand and have a solid background in audio signals. So what's coming next? Well, next we start doing some serious stuff. So we'll start to have an overview of different audio features. And specifically, we're gonna look into",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1409s",
        "start_time": "1409.31"
    },
    {
        "id": "33cc0c05",
        "text": "after like this introductory videos, now you should have like a good understanding of like mechanical waves sound, the different features of sound, like frequency, loudness and tre and now you should also understand and have a solid background in audio signals. So what's coming next? Well, next we start doing some serious stuff. So we'll start to have an overview of different audio features. And specifically, we're gonna look into time domain features in audio and frequency domain features. The interesting thing about these features, those are like the ones that we extract from audio and we can then use them for training our machine learning or deep learning algorithms. OK? So stay tuned for that",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1418s",
        "start_time": "1418.859"
    },
    {
        "id": "2a43cf23",
        "text": "and now you should also understand and have a solid background in audio signals. So what's coming next? Well, next we start doing some serious stuff. So we'll start to have an overview of different audio features. And specifically, we're gonna look into time domain features in audio and frequency domain features. The interesting thing about these features, those are like the ones that we extract from audio and we can then use them for training our machine learning or deep learning algorithms. OK? So stay tuned for that because that's coming before finishing off like this um video, I just want to invite you back to uh the sound of a IL community. So here you have a community of like minded people who are interested in all things A I, audio music and audio, digital signal processing. So I",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1433s",
        "start_time": "1433.839"
    },
    {
        "id": "f0962d46",
        "text": "time domain features in audio and frequency domain features. The interesting thing about these features, those are like the ones that we extract from audio and we can then use them for training our machine learning or deep learning algorithms. OK? So stay tuned for that because that's coming before finishing off like this um video, I just want to invite you back to uh the sound of a IL community. So here you have a community of like minded people who are interested in all things A I, audio music and audio, digital signal processing. So I I invite you like to join this community and I'll leave you a link to sign up to the slack community in the description below. Now, if you have any questions about this uh video, please feel free. Like to ask them in the comments section below. It's all for today. I hope you enjoyed the video. I'll see you next time. Cheers.",
        "video": "Understanding Audio Signals for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "daB9naGBVv4",
        "youtube_link": "https://www.youtube.com/watch?v=daB9naGBVv4&t=1450s",
        "start_time": "1450.26"
    },
    {
        "id": "f1707108",
        "text": "Hi, everybody and welcome to a new exciting video and the audio processing for machine learning series. This time, we're gonna introduce a few audio features, but mainly we're gonna be focusing on strategies that we can use to categorize this different audio features. Before we get into that, we should ask a couple of very important questions. One, what are audio features? Two, why are they important to us? Well, audio features are descriptors of sound. And the basic idea here is that when we have different audio features, they will tell us a different or they will uh provide us information about different aspects of sound. And the catch here is that we can use these audio features for training our intelligent audio systems. In other words, we should identify a few of these audio features we're interested in and then pass it into onto our um machine learning system so that they will hopefully like learn patterns and then solve our task or problems. OK.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=0s",
        "start_time": "0.33"
    },
    {
        "id": "86ca8d4a",
        "text": "Before we get into that, we should ask a couple of very important questions. One, what are audio features? Two, why are they important to us? Well, audio features are descriptors of sound. And the basic idea here is that when we have different audio features, they will tell us a different or they will uh provide us information about different aspects of sound. And the catch here is that we can use these audio features for training our intelligent audio systems. In other words, we should identify a few of these audio features we're interested in and then pass it into onto our um machine learning system so that they will hopefully like learn patterns and then solve our task or problems. OK. So now let's review a few strategies that we can use to categorize this audio features. So here I've listed a five. Now, the basic idea here is that I want for these strategies like to be as general as possible. And by that, I mean that they would be able to deal with any type of sound really. But some of this category of strategies are specific to music type of signal.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=17s",
        "start_time": "17.709"
    },
    {
        "id": "681f3894",
        "text": "And the catch here is that we can use these audio features for training our intelligent audio systems. In other words, we should identify a few of these audio features we're interested in and then pass it into onto our um machine learning system so that they will hopefully like learn patterns and then solve our task or problems. OK. So now let's review a few strategies that we can use to categorize this audio features. So here I've listed a five. Now, the basic idea here is that I want for these strategies like to be as general as possible. And by that, I mean that they would be able to deal with any type of sound really. But some of this category of strategies are specific to music type of signal. So for example, the third one here, so music as aspects and also like the first one here to a certain extent level of obstruction, but let's review this. So we have level of obstruction, temporal scope, music aspects, signal, domain and machine learning approach.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=44s",
        "start_time": "44.389"
    },
    {
        "id": "f512deef",
        "text": "So now let's review a few strategies that we can use to categorize this audio features. So here I've listed a five. Now, the basic idea here is that I want for these strategies like to be as general as possible. And by that, I mean that they would be able to deal with any type of sound really. But some of this category of strategies are specific to music type of signal. So for example, the third one here, so music as aspects and also like the first one here to a certain extent level of obstruction, but let's review this. So we have level of obstruction, temporal scope, music aspects, signal, domain and machine learning approach. For the remaining of the video, I'm gonna be delving diving into each of these strategies for categorization and we'll see what all of this like mean. OK. So let's get started with the first one. So your level of abstraction. Now this is uh one that mainly I should say uh uh kind of like covers like mu music signal more than general sound or audio.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=68s",
        "start_time": "68.9"
    },
    {
        "id": "4248d43c",
        "text": "So for example, the third one here, so music as aspects and also like the first one here to a certain extent level of obstruction, but let's review this. So we have level of obstruction, temporal scope, music aspects, signal, domain and machine learning approach. For the remaining of the video, I'm gonna be delving diving into each of these strategies for categorization and we'll see what all of this like mean. OK. So let's get started with the first one. So your level of abstraction. Now this is uh one that mainly I should say uh uh kind of like covers like mu music signal more than general sound or audio. So, and we can divide this like into three levels. So low level audio features, mid-level audio features and high level audio features. Now, I've taken like this distinction and also like this picture here from this great book called Music Similarity and",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=96s",
        "start_time": "96.599"
    },
    {
        "id": "6bc4efda",
        "text": "For the remaining of the video, I'm gonna be delving diving into each of these strategies for categorization and we'll see what all of this like mean. OK. So let's get started with the first one. So your level of abstraction. Now this is uh one that mainly I should say uh uh kind of like covers like mu music signal more than general sound or audio. So, and we can divide this like into three levels. So low level audio features, mid-level audio features and high level audio features. Now, I've taken like this distinction and also like this picture here from this great book called Music Similarity and Retrieval and Introduction to audio and web based Strategies. So this is like a an incredible book in music information Retrieval. And I highly suggest you to check that out. But let's focus on this uh strategy like for classifying audio features. So",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=112s",
        "start_time": "112.139"
    },
    {
        "id": "6ddc87fb",
        "text": "So, and we can divide this like into three levels. So low level audio features, mid-level audio features and high level audio features. Now, I've taken like this distinction and also like this picture here from this great book called Music Similarity and Retrieval and Introduction to audio and web based Strategies. So this is like a an incredible book in music information Retrieval. And I highly suggest you to check that out. But let's focus on this uh strategy like for classifying audio features. So uh what's low level features? So what are these, well, these are features that make sense for the machine, but don't make that much sense for us human beings. So basically, these are like kind of like statistical features that get extracted directly from audio and we human beings or I would say that people who are not trained in audio processing can't really understand what all of these things are. Right.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=140s",
        "start_time": "140.44"
    },
    {
        "id": "6bb8ccb9",
        "text": "Retrieval and Introduction to audio and web based Strategies. So this is like a an incredible book in music information Retrieval. And I highly suggest you to check that out. But let's focus on this uh strategy like for classifying audio features. So uh what's low level features? So what are these, well, these are features that make sense for the machine, but don't make that much sense for us human beings. So basically, these are like kind of like statistical features that get extracted directly from audio and we human beings or I would say that people who are not trained in audio processing can't really understand what all of these things are. Right. Now, if we go up one level, we are at the mid level. So here we have features that start to make sense from a perceptual perspective, right? And so here we have things that are in features that are like involved with peach and beat uh attributes. So in this category,",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=157s",
        "start_time": "157.86"
    },
    {
        "id": "285dc406",
        "text": "uh what's low level features? So what are these, well, these are features that make sense for the machine, but don't make that much sense for us human beings. So basically, these are like kind of like statistical features that get extracted directly from audio and we human beings or I would say that people who are not trained in audio processing can't really understand what all of these things are. Right. Now, if we go up one level, we are at the mid level. So here we have features that start to make sense from a perceptual perspective, right? And so here we have things that are in features that are like involved with peach and beat uh attributes. So in this category, we have audio features like notes onset, which is basically like when a note like starts, we have fluctuation patterns and MF CCS. Now if we go up another level, we arrive at the high level features and these are very abstract features and these kind of like tend to map to uh",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=175s",
        "start_time": "175.49"
    },
    {
        "id": "c75a08d1",
        "text": "Now, if we go up one level, we are at the mid level. So here we have features that start to make sense from a perceptual perspective, right? And so here we have things that are in features that are like involved with peach and beat uh attributes. So in this category, we have audio features like notes onset, which is basically like when a note like starts, we have fluctuation patterns and MF CCS. Now if we go up another level, we arrive at the high level features and these are very abstract features and these kind of like tend to map to uh musical constructs that uh we can understand and like perceive when we listen to some music. In this case, we can talk about uh instrumentation, key chords, melody, rhythm, tempo, right? So basically the idea is like the higher you go and the more abstract these audio features become",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=202s",
        "start_time": "202.789"
    },
    {
        "id": "8828701e",
        "text": "we have audio features like notes onset, which is basically like when a note like starts, we have fluctuation patterns and MF CCS. Now if we go up another level, we arrive at the high level features and these are very abstract features and these kind of like tend to map to uh musical constructs that uh we can understand and like perceive when we listen to some music. In this case, we can talk about uh instrumentation, key chords, melody, rhythm, tempo, right? So basically the idea is like the higher you go and the more abstract these audio features become OK. So now this is a way a strategy that we can use to classify all your features. But let's review another strategy and this is the temporal scope and this strategy applies to any type of sound music or non music. And so here we can uh kind of like divide the audio features into like three categories. So we have",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=223s",
        "start_time": "223.925"
    },
    {
        "id": "25c4e936",
        "text": "musical constructs that uh we can understand and like perceive when we listen to some music. In this case, we can talk about uh instrumentation, key chords, melody, rhythm, tempo, right? So basically the idea is like the higher you go and the more abstract these audio features become OK. So now this is a way a strategy that we can use to classify all your features. But let's review another strategy and this is the temporal scope and this strategy applies to any type of sound music or non music. And so here we can uh kind of like divide the audio features into like three categories. So we have audio features that are focused that give us informa instantaneous information about the audio signal. And basically like this consider very short chunks of audio signal, which usually is around like 50 to 100 milliseconds or like 20 to 100 milliseconds like something like that. So it's like almost like instantaneous information",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=246s",
        "start_time": "246.27"
    },
    {
        "id": "8c1f4822",
        "text": "OK. So now this is a way a strategy that we can use to classify all your features. But let's review another strategy and this is the temporal scope and this strategy applies to any type of sound music or non music. And so here we can uh kind of like divide the audio features into like three categories. So we have audio features that are focused that give us informa instantaneous information about the audio signal. And basically like this consider very short chunks of audio signal, which usually is around like 50 to 100 milliseconds or like 20 to 100 milliseconds like something like that. So it's like almost like instantaneous information here, it is important to uh like uh remember that the kind of like minimal resolution, temporal resolution that we human beings are capable of appreciating is around 10 milliseconds. So we can really go below that threshold because otherwise, I mean, it, it wouldn't make any sense for us on a perceptual level.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=273s",
        "start_time": "273.19"
    },
    {
        "id": "34aeddb6",
        "text": "audio features that are focused that give us informa instantaneous information about the audio signal. And basically like this consider very short chunks of audio signal, which usually is around like 50 to 100 milliseconds or like 20 to 100 milliseconds like something like that. So it's like almost like instantaneous information here, it is important to uh like uh remember that the kind of like minimal resolution, temporal resolution that we human beings are capable of appreciating is around 10 milliseconds. So we can really go below that threshold because otherwise, I mean, it, it wouldn't make any sense for us on a perceptual level. OK. So then we have uh the segment level features. And so these are audio features that we can calculate on segments of audio, right? And here we're talking about seconds. So anywhere from 2 to 5, 1015, 20 seconds, even something like that. And now I can give you an example",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=298s",
        "start_time": "298.92"
    },
    {
        "id": "689314cc",
        "text": "here, it is important to uh like uh remember that the kind of like minimal resolution, temporal resolution that we human beings are capable of appreciating is around 10 milliseconds. So we can really go below that threshold because otherwise, I mean, it, it wouldn't make any sense for us on a perceptual level. OK. So then we have uh the segment level features. And so these are audio features that we can calculate on segments of audio, right? And here we're talking about seconds. So anywhere from 2 to 5, 1015, 20 seconds, even something like that. And now I can give you an example again in music because it makes a lot of sense. So these are features that give us information about uh for example, a bar or a musical phrase. And so it's something like that provides us information about something that makes sense from a musical standpoint, right? And it tends to yeah, just like aggregate something from instantaneous information from like a longer period.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=323s",
        "start_time": "323.769"
    },
    {
        "id": "f800e058",
        "text": "OK. So then we have uh the segment level features. And so these are audio features that we can calculate on segments of audio, right? And here we're talking about seconds. So anywhere from 2 to 5, 1015, 20 seconds, even something like that. And now I can give you an example again in music because it makes a lot of sense. So these are features that give us information about uh for example, a bar or a musical phrase. And so it's something like that provides us information about something that makes sense from a musical standpoint, right? And it tends to yeah, just like aggregate something from instantaneous information from like a longer period. And finally, we have features that uh provide us information about the whole sound. So these are basically like aggregate features. So we can kind of like aggregate the results from like a lower temporal resolution features like instantaneous or segment level features. And then I don't know we can use",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=345s",
        "start_time": "345.779"
    },
    {
        "id": "16bd2b0c",
        "text": "again in music because it makes a lot of sense. So these are features that give us information about uh for example, a bar or a musical phrase. And so it's something like that provides us information about something that makes sense from a musical standpoint, right? And it tends to yeah, just like aggregate something from instantaneous information from like a longer period. And finally, we have features that uh provide us information about the whole sound. So these are basically like aggregate features. So we can kind of like aggregate the results from like a lower temporal resolution features like instantaneous or segment level features. And then I don't know we can use some kind of average or more sophisticated ways of like aggregating results and then we would get like a number or like a feature vector, whatever that describes the whole sound, the whole signal. So we have only one descriptor for the whole thing, right? OK.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=368s",
        "start_time": "368.994"
    },
    {
        "id": "129f1eda",
        "text": "And finally, we have features that uh provide us information about the whole sound. So these are basically like aggregate features. So we can kind of like aggregate the results from like a lower temporal resolution features like instantaneous or segment level features. And then I don't know we can use some kind of average or more sophisticated ways of like aggregating results and then we would get like a number or like a feature vector, whatever that describes the whole sound, the whole signal. So we have only one descriptor for the whole thing, right? OK. So this is another strategy we can use to uh categorize um audio features. So what about the third one? Now, this is clearly focused on music only. And so here, if you remember like the",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=392s",
        "start_time": "392.859"
    },
    {
        "id": "7cdbb98d",
        "text": "some kind of average or more sophisticated ways of like aggregating results and then we would get like a number or like a feature vector, whatever that describes the whole sound, the whole signal. So we have only one descriptor for the whole thing, right? OK. So this is another strategy we can use to uh categorize um audio features. So what about the third one? Now, this is clearly focused on music only. And so here, if you remember like the level features and high level features, so there they started to like deal with some sort of like musical aspects, like for example, note onset kind of like relates to, to, to beat, to melody a little bit as well. And but something else uh like, for example, I don't know, like key could be related with both like harmony and pitch to a certain extent, right?",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=412s",
        "start_time": "412.07"
    },
    {
        "id": "2ce2fdd4",
        "text": "So this is another strategy we can use to uh categorize um audio features. So what about the third one? Now, this is clearly focused on music only. And so here, if you remember like the level features and high level features, so there they started to like deal with some sort of like musical aspects, like for example, note onset kind of like relates to, to, to beat, to melody a little bit as well. And but something else uh like, for example, I don't know, like key could be related with both like harmony and pitch to a certain extent, right? So basically, like we can kind of like classify these audio features based on the musical aspects that they give us a glimpse on to. OK. And um yeah, so as I said, this is like a strategy categorization strategy that we can use mainly for music for obvious reasons, right? So what about like audio, like more in general? So do we have like also other types of like classifying",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=431s",
        "start_time": "431.5"
    },
    {
        "id": "14a99216",
        "text": "level features and high level features, so there they started to like deal with some sort of like musical aspects, like for example, note onset kind of like relates to, to, to beat, to melody a little bit as well. And but something else uh like, for example, I don't know, like key could be related with both like harmony and pitch to a certain extent, right? So basically, like we can kind of like classify these audio features based on the musical aspects that they give us a glimpse on to. OK. And um yeah, so as I said, this is like a strategy categorization strategy that we can use mainly for music for obvious reasons, right? So what about like audio, like more in general? So do we have like also other types of like classifying uh or categorizing audio features for all sorts of audios? Yes. And this is like probably the most important kind of like strategy that you have for categorizing the different audio features. And uh this is based on like the signal domain that we are in. OK. So I know like this may sound a little bit weird but like with a few examples, it's gonna be super clear to like understand.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=447s",
        "start_time": "447.41"
    },
    {
        "id": "f6677e68",
        "text": "So basically, like we can kind of like classify these audio features based on the musical aspects that they give us a glimpse on to. OK. And um yeah, so as I said, this is like a strategy categorization strategy that we can use mainly for music for obvious reasons, right? So what about like audio, like more in general? So do we have like also other types of like classifying uh or categorizing audio features for all sorts of audios? Yes. And this is like probably the most important kind of like strategy that you have for categorizing the different audio features. And uh this is based on like the signal domain that we are in. OK. So I know like this may sound a little bit weird but like with a few examples, it's gonna be super clear to like understand. OK. So we have certain audio features that are like in the",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=474s",
        "start_time": "474.859"
    },
    {
        "id": "471f4d45",
        "text": "uh or categorizing audio features for all sorts of audios? Yes. And this is like probably the most important kind of like strategy that you have for categorizing the different audio features. And uh this is based on like the signal domain that we are in. OK. So I know like this may sound a little bit weird but like with a few examples, it's gonna be super clear to like understand. OK. So we have certain audio features that are like in the time domain. So some of these are amplitude envelope root mean square energy or zero crossing rate. There are way more than this and we're gonna cover them in future videos. Uh But for now, uh I don't really want to get into the details of this different audio features. I just want to give you an idea why they are time domain features, right? OK. And this is because they are extracted from",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=504s",
        "start_time": "504.88"
    },
    {
        "id": "58b6bd20",
        "text": "OK. So we have certain audio features that are like in the time domain. So some of these are amplitude envelope root mean square energy or zero crossing rate. There are way more than this and we're gonna cover them in future videos. Uh But for now, uh I don't really want to get into the details of this different audio features. I just want to give you an idea why they are time domain features, right? OK. And this is because they are extracted from away from us. So from the basic raw audio. And so",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=532s",
        "start_time": "532.57"
    },
    {
        "id": "f2f589d9",
        "text": "time domain. So some of these are amplitude envelope root mean square energy or zero crossing rate. There are way more than this and we're gonna cover them in future videos. Uh But for now, uh I don't really want to get into the details of this different audio features. I just want to give you an idea why they are time domain features, right? OK. And this is because they are extracted from away from us. So from the basic raw audio. And so uh here like the time domain just like as a definition is captured in a waveform. And you should be familiar with this because like we covered this like extensively in previous videos, basically uh in a waveform what you have on the X axis is time and on the Y axis is amplitude. And so basically we can take a look at all the events which happen in a sound and the time domain audio features,",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=537s",
        "start_time": "537.809"
    },
    {
        "id": "fae3ccf7",
        "text": "away from us. So from the basic raw audio. And so uh here like the time domain just like as a definition is captured in a waveform. And you should be familiar with this because like we covered this like extensively in previous videos, basically uh in a waveform what you have on the X axis is time and on the Y axis is amplitude. And so basically we can take a look at all the events which happen in a sound and the time domain audio features, extract information from this representation. So this is why we call them time domain audio features. OK.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=567s",
        "start_time": "567.979"
    },
    {
        "id": "ddee6f0a",
        "text": "uh here like the time domain just like as a definition is captured in a waveform. And you should be familiar with this because like we covered this like extensively in previous videos, basically uh in a waveform what you have on the X axis is time and on the Y axis is amplitude. And so basically we can take a look at all the events which happen in a sound and the time domain audio features, extract information from this representation. So this is why we call them time domain audio features. OK. So now this is all good and well, but we have a problem and the problem is that uh sound is kind of characterized by frequency. The frequency is an extremely important descriptor of sound as we've seen in previous videos. But with the time domain representation representation, we don't have any of that, right. And so there are a bunch of other features which go",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=572s",
        "start_time": "572.809"
    },
    {
        "id": "62635cee",
        "text": "extract information from this representation. So this is why we call them time domain audio features. OK. So now this is all good and well, but we have a problem and the problem is that uh sound is kind of characterized by frequency. The frequency is an extremely important descriptor of sound as we've seen in previous videos. But with the time domain representation representation, we don't have any of that, right. And so there are a bunch of other features which go under the name of like frequency domain features which actually focus on the frequency components of uh sound. So some of these features are band energy ratio spectral Centroid spectral flux spectral spread. I mean you name it you have a list which is almost endless. Once again, we're gonna cover this in uh future videos. But here I just want to give you like the high level idea, right?",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=600s",
        "start_time": "600.739"
    },
    {
        "id": "1c76d581",
        "text": "So now this is all good and well, but we have a problem and the problem is that uh sound is kind of characterized by frequency. The frequency is an extremely important descriptor of sound as we've seen in previous videos. But with the time domain representation representation, we don't have any of that, right. And so there are a bunch of other features which go under the name of like frequency domain features which actually focus on the frequency components of uh sound. So some of these features are band energy ratio spectral Centroid spectral flux spectral spread. I mean you name it you have a list which is almost endless. Once again, we're gonna cover this in uh future videos. But here I just want to give you like the high level idea, right? So here we're talking about the frequency domain, but what's the frequency domain? And how do we get to a frequency domain? Well, the basic idea here is that we take the row row audio. So like we take a signal from its time domain representation, we apply the fourier transform and we basically translate the signal from the time domain to the frequency domain.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=609s",
        "start_time": "609.02"
    },
    {
        "id": "68be4060",
        "text": "under the name of like frequency domain features which actually focus on the frequency components of uh sound. So some of these features are band energy ratio spectral Centroid spectral flux spectral spread. I mean you name it you have a list which is almost endless. Once again, we're gonna cover this in uh future videos. But here I just want to give you like the high level idea, right? So here we're talking about the frequency domain, but what's the frequency domain? And how do we get to a frequency domain? Well, the basic idea here is that we take the row row audio. So like we take a signal from its time domain representation, we apply the fourier transform and we basically translate the signal from the time domain to the frequency domain. Now don't be scared like if you are not familiar with the um fourier transform because this is again something that will cover quite in detail in future videos. But basically when we apply the fourier transform to uh the time domain representation of signal, we get a spectrum which we can visualize like here.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=637s",
        "start_time": "637.229"
    },
    {
        "id": "438ed9dd",
        "text": "So here we're talking about the frequency domain, but what's the frequency domain? And how do we get to a frequency domain? Well, the basic idea here is that we take the row row audio. So like we take a signal from its time domain representation, we apply the fourier transform and we basically translate the signal from the time domain to the frequency domain. Now don't be scared like if you are not familiar with the um fourier transform because this is again something that will cover quite in detail in future videos. But basically when we apply the fourier transform to uh the time domain representation of signal, we get a spectrum which we can visualize like here. So as you can see here on the X axis, you have frequency and on the y axis you have magnitude. In other words, here we are taking a picture of the sound and analyzing which components, frequency",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=666s",
        "start_time": "666.039"
    },
    {
        "id": "f7b7a8b1",
        "text": "Now don't be scared like if you are not familiar with the um fourier transform because this is again something that will cover quite in detail in future videos. But basically when we apply the fourier transform to uh the time domain representation of signal, we get a spectrum which we can visualize like here. So as you can see here on the X axis, you have frequency and on the y axis you have magnitude. In other words, here we are taking a picture of the sound and analyzing which components, frequency components are present in that sound, right? And so basically here you can see like that there is like a a very like high peak here which should be, I don't know probably around 250 something uh Hertz, right?",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=692s",
        "start_time": "692.82"
    },
    {
        "id": "b824c39a",
        "text": "So as you can see here on the X axis, you have frequency and on the y axis you have magnitude. In other words, here we are taking a picture of the sound and analyzing which components, frequency components are present in that sound, right? And so basically here you can see like that there is like a a very like high peak here which should be, I don't know probably around 250 something uh Hertz, right? And basically like the idea here is that you have information about all the frequencies that make up a sound in this particular example here, like this spectrum is the uh kind of like the the I've obtained this by applying the fourier transform to this waveform here. So as you can see this two",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=715s",
        "start_time": "715.179"
    },
    {
        "id": "570c8bcb",
        "text": "components are present in that sound, right? And so basically here you can see like that there is like a a very like high peak here which should be, I don't know probably around 250 something uh Hertz, right? And basically like the idea here is that you have information about all the frequencies that make up a sound in this particular example here, like this spectrum is the uh kind of like the the I've obtained this by applying the fourier transform to this waveform here. So as you can see this two categories of",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=732s",
        "start_time": "732.159"
    },
    {
        "id": "63b8b271",
        "text": "And basically like the idea here is that you have information about all the frequencies that make up a sound in this particular example here, like this spectrum is the uh kind of like the the I've obtained this by applying the fourier transform to this waveform here. So as you can see this two categories of uh audio features give us information, complementary information. So time domain audio features give us information about like stuff that has have has to do with time. Whereas frequency domain gives us information about stuff that has to do with uh frequency, right. The problem though is that we with this representations, we don't have um both of those things together. So information about time and frequency,",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=749s",
        "start_time": "749.39"
    },
    {
        "id": "ec8d3455",
        "text": "categories of uh audio features give us information, complementary information. So time domain audio features give us information about like stuff that has have has to do with time. Whereas frequency domain gives us information about stuff that has to do with uh frequency, right. The problem though is that we with this representations, we don't have um both of those things together. So information about time and frequency, but wouldn't it be wonderful if we had those type of audio features. Well, indeed, we have those we have time frequency domain uh features and we use the time frequency representation for those uh some of",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=775s",
        "start_time": "775.21"
    },
    {
        "id": "7fa90bc2",
        "text": "uh audio features give us information, complementary information. So time domain audio features give us information about like stuff that has have has to do with time. Whereas frequency domain gives us information about stuff that has to do with uh frequency, right. The problem though is that we with this representations, we don't have um both of those things together. So information about time and frequency, but wouldn't it be wonderful if we had those type of audio features. Well, indeed, we have those we have time frequency domain uh features and we use the time frequency representation for those uh some of uh these um features are spectrograms, male spectrograms or constant que transform. So now let's focus just like on the first one, the spectrogram, which is the most famous one, right? So how do we obtain that? Well, we start once again from the um time to demand representation of signal and then we apply a short time fourier transform and we obtain",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=777s",
        "start_time": "777.539"
    },
    {
        "id": "d5c86873",
        "text": "but wouldn't it be wonderful if we had those type of audio features. Well, indeed, we have those we have time frequency domain uh features and we use the time frequency representation for those uh some of uh these um features are spectrograms, male spectrograms or constant que transform. So now let's focus just like on the first one, the spectrogram, which is the most famous one, right? So how do we obtain that? Well, we start once again from the um time to demand representation of signal and then we apply a short time fourier transform and we obtain in a spectrogram which is this guy down here. Now, once again, we're going to cover the short time fourier transform in future videos quite in detail. So I know I've made a lot of promises and I hope I'll deliver like throughout the series, but let's focus on the spectrogram for a second. Now, uh if I remember correctly, we we already, so you should have already seen spectrograms in previous videos. But let's review this once again.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=806s",
        "start_time": "806.789"
    },
    {
        "id": "2a6f5f8a",
        "text": "uh these um features are spectrograms, male spectrograms or constant que transform. So now let's focus just like on the first one, the spectrogram, which is the most famous one, right? So how do we obtain that? Well, we start once again from the um time to demand representation of signal and then we apply a short time fourier transform and we obtain in a spectrogram which is this guy down here. Now, once again, we're going to cover the short time fourier transform in future videos quite in detail. So I know I've made a lot of promises and I hope I'll deliver like throughout the series, but let's focus on the spectrogram for a second. Now, uh if I remember correctly, we we already, so you should have already seen spectrograms in previous videos. But let's review this once again. Basically the idea here is that in a spectrum, you have information both about time and frequency on the X axis indeed you have time whereas on the y axis you have frequencies and basically here what you see is the different frequency components at different points in time.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=823s",
        "start_time": "823.859"
    },
    {
        "id": "8406bf8b",
        "text": "in a spectrogram which is this guy down here. Now, once again, we're going to cover the short time fourier transform in future videos quite in detail. So I know I've made a lot of promises and I hope I'll deliver like throughout the series, but let's focus on the spectrogram for a second. Now, uh if I remember correctly, we we already, so you should have already seen spectrograms in previous videos. But let's review this once again. Basically the idea here is that in a spectrum, you have information both about time and frequency on the X axis indeed you have time whereas on the y axis you have frequencies and basically here what you see is the different frequency components at different points in time. And the amount of contribution that each frequency band has at this particular time is described through a color. In this case, the brighter the color the more the um contribution that's particular frequency band at that specific time. So, and as you can see here, we have like a lot of contribution from this uh frequency of 256 Hertz, which is A I I believe like middle C,",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=850s",
        "start_time": "850.604"
    },
    {
        "id": "c76e0d4a",
        "text": "Basically the idea here is that in a spectrum, you have information both about time and frequency on the X axis indeed you have time whereas on the y axis you have frequencies and basically here what you see is the different frequency components at different points in time. And the amount of contribution that each frequency band has at this particular time is described through a color. In this case, the brighter the color the more the um contribution that's particular frequency band at that specific time. So, and as you can see here, we have like a lot of contribution from this uh frequency of 256 Hertz, which is A I I believe like middle C, OK. And this like uh computes correctly because if we go back to the spectrum, which is kind of like snapshot for the whole sound, we have a peak here which is most likely around 256 as well. OK.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=877s",
        "start_time": "877.53"
    },
    {
        "id": "19d9e965",
        "text": "And the amount of contribution that each frequency band has at this particular time is described through a color. In this case, the brighter the color the more the um contribution that's particular frequency band at that specific time. So, and as you can see here, we have like a lot of contribution from this uh frequency of 256 Hertz, which is A I I believe like middle C, OK. And this like uh computes correctly because if we go back to the spectrum, which is kind of like snapshot for the whole sound, we have a peak here which is most likely around 256 as well. OK. So now I hope like you have like a good understanding like of this different types of like audio features. So like the time domain audio features, the frequency domain ones and the time frequency uh features.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=897s",
        "start_time": "897.919"
    },
    {
        "id": "796931a4",
        "text": "OK. And this like uh computes correctly because if we go back to the spectrum, which is kind of like snapshot for the whole sound, we have a peak here which is most likely around 256 as well. OK. So now I hope like you have like a good understanding like of this different types of like audio features. So like the time domain audio features, the frequency domain ones and the time frequency uh features. Now uh one last way we can use to classify all your features is based on the machine learning approach that we use obviously. Like here, we can make a quite important distinction between traditional machine learning algorithms like support vector machines, logistic regression or linear regression and deep learning architectures. And we'll see why that's the case in a second. So for a traditional machine learning,",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=928s",
        "start_time": "928.809"
    },
    {
        "id": "d44502f3",
        "text": "So now I hope like you have like a good understanding like of this different types of like audio features. So like the time domain audio features, the frequency domain ones and the time frequency uh features. Now uh one last way we can use to classify all your features is based on the machine learning approach that we use obviously. Like here, we can make a quite important distinction between traditional machine learning algorithms like support vector machines, logistic regression or linear regression and deep learning architectures. And we'll see why that's the case in a second. So for a traditional machine learning, what we usually do is we just look at all the possible audio features both in the audio domain and sorry, both in the time domain and in the frequency domain. And basically what we do is we hand pick the ones that we believe will work the best for the problem that we want to solve. For example, we could say, yeah, we",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=947s",
        "start_time": "947.989"
    },
    {
        "id": "9cc7b839",
        "text": "Now uh one last way we can use to classify all your features is based on the machine learning approach that we use obviously. Like here, we can make a quite important distinction between traditional machine learning algorithms like support vector machines, logistic regression or linear regression and deep learning architectures. And we'll see why that's the case in a second. So for a traditional machine learning, what we usually do is we just look at all the possible audio features both in the audio domain and sorry, both in the time domain and in the frequency domain. And basically what we do is we hand pick the ones that we believe will work the best for the problem that we want to solve. For example, we could say, yeah, we uh we want to solve like audio classification. So we we want to pass to machine learn a support vector machine for example, some like audio files. And then we want to know whether like we have the sound of a car engine of an airplane or or a gunshot for example, right. So what we could do is like identify a few",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=962s",
        "start_time": "962.94"
    },
    {
        "id": "b3f637dc",
        "text": "what we usually do is we just look at all the possible audio features both in the audio domain and sorry, both in the time domain and in the frequency domain. And basically what we do is we hand pick the ones that we believe will work the best for the problem that we want to solve. For example, we could say, yeah, we uh we want to solve like audio classification. So we we want to pass to machine learn a support vector machine for example, some like audio files. And then we want to know whether like we have the sound of a car engine of an airplane or or a gunshot for example, right. So what we could do is like identify a few audio features that make sense. So say for example is this ridge amplitude envelope zero crossing rates and spectral flux, we would isolate them, we would extract them from the audio files and then we would feed them into the traditional machine learning algorithm so that we can train",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=990s",
        "start_time": "990.309"
    },
    {
        "id": "b3dc7612",
        "text": "uh we want to solve like audio classification. So we we want to pass to machine learn a support vector machine for example, some like audio files. And then we want to know whether like we have the sound of a car engine of an airplane or or a gunshot for example, right. So what we could do is like identify a few audio features that make sense. So say for example is this ridge amplitude envelope zero crossing rates and spectral flux, we would isolate them, we would extract them from the audio files and then we would feed them into the traditional machine learning algorithm so that we can train like our support vector machine for example. And then at inference time, we would just like feed the these three audio features for the audio that we want to analyze and hopefully we'll get a result which in this case appears to be car engine",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1013s",
        "start_time": "1013.739"
    },
    {
        "id": "357e0abd",
        "text": "audio features that make sense. So say for example is this ridge amplitude envelope zero crossing rates and spectral flux, we would isolate them, we would extract them from the audio files and then we would feed them into the traditional machine learning algorithm so that we can train like our support vector machine for example. And then at inference time, we would just like feed the these three audio features for the audio that we want to analyze and hopefully we'll get a result which in this case appears to be car engine good. So how does this differ from deep learning? Well, we know that in deep learning and not just in the audio space, we tend to use unstructured data. For example, in image processing, we don't pass any specifically handcrafted like feature like image feature, we just pass the whole uh image as a as a pixel, right, as A as a as a",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1034s",
        "start_time": "1034.18"
    },
    {
        "id": "31ade826",
        "text": "like our support vector machine for example. And then at inference time, we would just like feed the these three audio features for the audio that we want to analyze and hopefully we'll get a result which in this case appears to be car engine good. So how does this differ from deep learning? Well, we know that in deep learning and not just in the audio space, we tend to use unstructured data. For example, in image processing, we don't pass any specifically handcrafted like feature like image feature, we just pass the whole uh image as a as a pixel, right, as A as a as a kind of like collection of pixels. So in a sense, we do a similar thing here in um audio processing. So what we do is we pass uh a bunch of like unstructured audio representations. And by that, I mean, we could just pass the whole row audio",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1050s",
        "start_time": "1050.864"
    },
    {
        "id": "a0251b72",
        "text": "good. So how does this differ from deep learning? Well, we know that in deep learning and not just in the audio space, we tend to use unstructured data. For example, in image processing, we don't pass any specifically handcrafted like feature like image feature, we just pass the whole uh image as a as a pixel, right, as A as a as a kind of like collection of pixels. So in a sense, we do a similar thing here in um audio processing. So what we do is we pass uh a bunch of like unstructured audio representations. And by that, I mean, we could just pass the whole row audio in its basic like time a domain representation or we could pass spectrogram like audio features. So spectrograms males, spectrograms, constant Q transforms,",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1067s",
        "start_time": "1067.91"
    },
    {
        "id": "93673d5a",
        "text": "kind of like collection of pixels. So in a sense, we do a similar thing here in um audio processing. So what we do is we pass uh a bunch of like unstructured audio representations. And by that, I mean, we could just pass the whole row audio in its basic like time a domain representation or we could pass spectrogram like audio features. So spectrograms males, spectrograms, constant Q transforms, sorry or we could pass even MF CCS but right now like uh I don't see like many papers uh coming out uh like with MFCC anymore, right? So basically, the idea is that we get like something like this, the whole whole spectrogram, we feed that to a deep neural network and then we get back our prediction. OK. So now let's uh review,",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1096s",
        "start_time": "1096.13"
    },
    {
        "id": "05562a82",
        "text": "in its basic like time a domain representation or we could pass spectrogram like audio features. So spectrograms males, spectrograms, constant Q transforms, sorry or we could pass even MF CCS but right now like uh I don't see like many papers uh coming out uh like with MFCC anymore, right? So basically, the idea is that we get like something like this, the whole whole spectrogram, we feed that to a deep neural network and then we get back our prediction. OK. So now let's uh review, I think it's important to like to review like the different types of like audio uh intelligent systems that we can build traditionally before the advent of machine learning, we used to use basic digital processing techniques. And so we would use like this audio DS P techniques and",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1117s",
        "start_time": "1117.479"
    },
    {
        "id": "a37274e9",
        "text": "sorry or we could pass even MF CCS but right now like uh I don't see like many papers uh coming out uh like with MFCC anymore, right? So basically, the idea is that we get like something like this, the whole whole spectrogram, we feed that to a deep neural network and then we get back our prediction. OK. So now let's uh review, I think it's important to like to review like the different types of like audio uh intelligent systems that we can build traditionally before the advent of machine learning, we used to use basic digital processing techniques. And so we would use like this audio DS P techniques and we would put them together and use whatever like audio features we wanted and use some kind of rules to uh for example, like classify sounds or to identify notes onsets or I don't know, like to uh recognize like the, the beat in a, in a music piece. OK. Then with the advent of the machine",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1133s",
        "start_time": "1133.51"
    },
    {
        "id": "12cef12e",
        "text": "I think it's important to like to review like the different types of like audio uh intelligent systems that we can build traditionally before the advent of machine learning, we used to use basic digital processing techniques. And so we would use like this audio DS P techniques and we would put them together and use whatever like audio features we wanted and use some kind of rules to uh for example, like classify sounds or to identify notes onsets or I don't know, like to uh recognize like the, the beat in a, in a music piece. OK. Then with the advent of the machine learning and data sets, we started to work with a traditional machine learning approaches. And here we've, we've seen it already, right? So we do some kind of like feature engineering, we start with the whole audio features in the time frequency domain and we just decide which ones of those like are worth experimenting with and using in our application.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1162s",
        "start_time": "1162.67"
    },
    {
        "id": "9d3f7cb2",
        "text": "we would put them together and use whatever like audio features we wanted and use some kind of rules to uh for example, like classify sounds or to identify notes onsets or I don't know, like to uh recognize like the, the beat in a, in a music piece. OK. Then with the advent of the machine learning and data sets, we started to work with a traditional machine learning approaches. And here we've, we've seen it already, right? So we do some kind of like feature engineering, we start with the whole audio features in the time frequency domain and we just decide which ones of those like are worth experimenting with and using in our application. Then with the advent and the revolution of deep learning, all of this changed. And now we just pass in unstructured spectrograms or even raw audio. And the promise of deep learning is that the algorithms will be able to extract",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1183s",
        "start_time": "1183.119"
    },
    {
        "id": "bc848bcf",
        "text": "learning and data sets, we started to work with a traditional machine learning approaches. And here we've, we've seen it already, right? So we do some kind of like feature engineering, we start with the whole audio features in the time frequency domain and we just decide which ones of those like are worth experimenting with and using in our application. Then with the advent and the revolution of deep learning, all of this changed. And now we just pass in unstructured spectrograms or even raw audio. And the promise of deep learning is that the algorithms will be able to extract the features automatically by themselves. Without us, the human engineers doing like human hand ping of features and things like that. OK.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1205s",
        "start_time": "1205.385"
    },
    {
        "id": "9121bb3e",
        "text": "Then with the advent and the revolution of deep learning, all of this changed. And now we just pass in unstructured spectrograms or even raw audio. And the promise of deep learning is that the algorithms will be able to extract the features automatically by themselves. Without us, the human engineers doing like human hand ping of features and things like that. OK. That's it. So now you should have more or less like an idea of the different types of audio features we'll be dealing with. So uh if there's one thing that I hope you'll take away from this uh video is that's like the main categorization that we can use instead of like the signal domain. So we, and that's like what we'll be doing in future videos, we'll be reviewing uh time domain audio features,",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1228s",
        "start_time": "1228.209"
    },
    {
        "id": "d6c2920c",
        "text": "the features automatically by themselves. Without us, the human engineers doing like human hand ping of features and things like that. OK. That's it. So now you should have more or less like an idea of the different types of audio features we'll be dealing with. So uh if there's one thing that I hope you'll take away from this uh video is that's like the main categorization that we can use instead of like the signal domain. So we, and that's like what we'll be doing in future videos, we'll be reviewing uh time domain audio features, frequency domain audio features and time frequency uh features as well. OK. So what's up next then?",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1248s",
        "start_time": "1248.15"
    },
    {
        "id": "ef92d0b6",
        "text": "That's it. So now you should have more or less like an idea of the different types of audio features we'll be dealing with. So uh if there's one thing that I hope you'll take away from this uh video is that's like the main categorization that we can use instead of like the signal domain. So we, and that's like what we'll be doing in future videos, we'll be reviewing uh time domain audio features, frequency domain audio features and time frequency uh features as well. OK. So what's up next then? So now you have a good picture like of the different ingredients that we are going to play around with. So namely the audio features, the next step is to understand how we extract them directly from audio. So in the next video, we're gonna look at the feature",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1260s",
        "start_time": "1260.739"
    },
    {
        "id": "cf97fe9c",
        "text": "frequency domain audio features and time frequency uh features as well. OK. So what's up next then? So now you have a good picture like of the different ingredients that we are going to play around with. So namely the audio features, the next step is to understand how we extract them directly from audio. So in the next video, we're gonna look at the feature extraction pipeline both for time and frequency domain features. OK. So yeah, I hope you enjoyed this video. But before dashing off, I just want to remind you about the Sound of the Ice LA community, which is a community of people",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1290s",
        "start_time": "1290.64"
    },
    {
        "id": "e51fdc44",
        "text": "So now you have a good picture like of the different ingredients that we are going to play around with. So namely the audio features, the next step is to understand how we extract them directly from audio. So in the next video, we're gonna look at the feature extraction pipeline both for time and frequency domain features. OK. So yeah, I hope you enjoyed this video. But before dashing off, I just want to remind you about the Sound of the Ice LA community, which is a community of people who are interested in all your processing A I music A I audio. So I really suggest you to join that if you haven't done so already. And I'll leave you the uh sign up link to the Slack community in the description below. OK? So that's it for today. I really hope you enjoyed this video and I'll see you next time. Cheers.",
        "video": "Types of Audio Features for Machine Learning",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZZ9u1vUtcIA",
        "youtube_link": "https://www.youtube.com/watch?v=ZZ9u1vUtcIA&t=1300s",
        "start_time": "1300.54"
    },
    {
        "id": "ae5d74a0",
        "text": "Hi, everybody and welcome to a new video in the audio signal processing for machine learning series. This time we'll implement the amplitude envelope feature from scratch. And in the process, we'll also get familiar with Libros, which is the audio processing library that we'll use throughout this course. And we also plot waveforms and the amplitude envelope itself. Before getting, I wanted to show you the Libres documentation here. So you can check this out. I'll leave you the link in the description below. Interesting thing is that Libres doesn't have an extractor for amplitude envelope and so we'll build one from scratch. OK. So now let's get started. And so the first thing that we want to do is to, yeah, let's start by importing uh Li Broza. And we want to also import Li Brusa dot display that has utilities for plots for plotting stuff. OK. So now what we want to do is to load",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "7c013e7e",
        "text": "I wanted to show you the Libres documentation here. So you can check this out. I'll leave you the link in the description below. Interesting thing is that Libres doesn't have an extractor for amplitude envelope and so we'll build one from scratch. OK. So now let's get started. And so the first thing that we want to do is to, yeah, let's start by importing uh Li Broza. And we want to also import Li Brusa dot display that has utilities for plots for plotting stuff. OK. So now what we want to do is to load audio files.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=26s",
        "start_time": "26.229"
    },
    {
        "id": "aa47b9f7",
        "text": "And we want to also import Li Brusa dot display that has utilities for plots for plotting stuff. OK. So now what we want to do is to load audio files. OK? And so what audio files are we using today? Well, I have here in the audio signal processing for machine learning uh code base here at uh in the folder eight, which is the one for this current video, have a sub folder called audio. Here we have three audio files. So one is called ABC and it's a 32nd passage of an orchestral piece by",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=52s",
        "start_time": "52.74"
    },
    {
        "id": "9afa1e01",
        "text": "audio files. OK? And so what audio files are we using today? Well, I have here in the audio signal processing for machine learning uh code base here at uh in the folder eight, which is the one for this current video, have a sub folder called audio. Here we have three audio files. So one is called ABC and it's a 32nd passage of an orchestral piece by uh Claude the Bey. And we have 30 seconds by Duke Ellington, the jazz musician and 30 seconds from a song by the red hot chili pepper. So we have a little bit of rock, uh classical music and jazz music in there. OK. So now let's uh create uh let's um get the paths. So we'll have the Bey um we'll call this the Bey file and",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=68s",
        "start_time": "68.94"
    },
    {
        "id": "0f15d62c",
        "text": "OK? And so what audio files are we using today? Well, I have here in the audio signal processing for machine learning uh code base here at uh in the folder eight, which is the one for this current video, have a sub folder called audio. Here we have three audio files. So one is called ABC and it's a 32nd passage of an orchestral piece by uh Claude the Bey. And we have 30 seconds by Duke Ellington, the jazz musician and 30 seconds from a song by the red hot chili pepper. So we have a little bit of rock, uh classical music and jazz music in there. OK. So now let's uh create uh let's um get the paths. So we'll have the Bey um we'll call this the Bey file and this I can get from audio and then over here",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=70s",
        "start_time": "70.51"
    },
    {
        "id": "96184823",
        "text": "uh Claude the Bey. And we have 30 seconds by Duke Ellington, the jazz musician and 30 seconds from a song by the red hot chili pepper. So we have a little bit of rock, uh classical music and jazz music in there. OK. So now let's uh create uh let's um get the paths. So we'll have the Bey um we'll call this the Bey file and this I can get from audio and then over here BC W and then I can do the same thing for the red hot chili peppers. So I'll do audio and then I'll say red hot uh W and same thing for the Duke Ellington uh file audio and then I'll say uh Duke.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=99s",
        "start_time": "99.319"
    },
    {
        "id": "30782789",
        "text": "this I can get from audio and then over here BC W and then I can do the same thing for the red hot chili peppers. So I'll do audio and then I'll say red hot uh W and same thing for the Duke Ellington uh file audio and then I'll say uh Duke. OK, perfect. So these are the, the paths now I can do this because I started Jupiter a notebook uh in",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=127s",
        "start_time": "127.62"
    },
    {
        "id": "16526ee7",
        "text": "BC W and then I can do the same thing for the red hot chili peppers. So I'll do audio and then I'll say red hot uh W and same thing for the Duke Ellington uh file audio and then I'll say uh Duke. OK, perfect. So these are the, the paths now I can do this because I started Jupiter a notebook uh in here. So at this level here, so basically I then can access uh the audio uh folder and then obviously the file itself. OK. So the first thing that we want to do here is to import",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=135s",
        "start_time": "135.63"
    },
    {
        "id": "277c1b06",
        "text": "OK, perfect. So these are the, the paths now I can do this because I started Jupiter a notebook uh in here. So at this level here, so basically I then can access uh the audio uh folder and then obviously the file itself. OK. So the first thing that we want to do here is to import from ipython display",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=160s",
        "start_time": "160.21"
    },
    {
        "id": "e5526596",
        "text": "here. So at this level here, so basically I then can access uh the audio uh folder and then obviously the file itself. OK. So the first thing that we want to do here is to import from ipython display as IP D. And so this has this package has some interesting uh utilities that we can use to just like listen to some music here in a Jupiter no book. So lets take a look at this So we'll do IP D dot uh audio and then we'll pass in the, the BC file over here. And all of a sudden, we have our amazing",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=171s",
        "start_time": "171.869"
    },
    {
        "id": "6b8cb39f",
        "text": "from ipython display as IP D. And so this has this package has some interesting uh utilities that we can use to just like listen to some music here in a Jupiter no book. So lets take a look at this So we'll do IP D dot uh audio and then we'll pass in the, the BC file over here. And all of a sudden, we have our amazing uh audio player here. We can listen to uh like this music, but let's load the other music first. So we'll load a red dot file over here",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=190s",
        "start_time": "190.94"
    },
    {
        "id": "69beb6bc",
        "text": "as IP D. And so this has this package has some interesting uh utilities that we can use to just like listen to some music here in a Jupiter no book. So lets take a look at this So we'll do IP D dot uh audio and then we'll pass in the, the BC file over here. And all of a sudden, we have our amazing uh audio player here. We can listen to uh like this music, but let's load the other music first. So we'll load a red dot file over here and then we load the",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=197s",
        "start_time": "197.25"
    },
    {
        "id": "7320ce12",
        "text": "uh audio player here. We can listen to uh like this music, but let's load the other music first. So we'll load a red dot file over here and then we load the Duke Ellington five. OK. So let's listen to this music.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=226s",
        "start_time": "226.27"
    },
    {
        "id": "28cda218",
        "text": "and then we load the Duke Ellington five. OK. So let's listen to this music. OK. So this is a, an orchestral piece by Claude Du Bei.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=239s",
        "start_time": "239.5"
    },
    {
        "id": "4c10d531",
        "text": "Duke Ellington five. OK. So let's listen to this music. OK. So this is a, an orchestral piece by Claude Du Bei. So loads of strings here and as you will see there is a climax here, the intensity rises up",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=244s",
        "start_time": "244.339"
    },
    {
        "id": "395200ea",
        "text": "OK. So this is a, an orchestral piece by Claude Du Bei. So loads of strings here and as you will see there is a climax here, the intensity rises up and then it just fades away.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=253s",
        "start_time": "253.41"
    },
    {
        "id": "e9c49ddc",
        "text": "So loads of strings here and as you will see there is a climax here, the intensity rises up and then it just fades away. OK.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=258s",
        "start_time": "258.869"
    },
    {
        "id": "872ceeec",
        "text": "and then it just fades away. OK. Nice. So,",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=268s",
        "start_time": "268.019"
    },
    {
        "id": "b061e3ef",
        "text": "OK. Nice. So, OK, you get the idea and then we have some music by Duke Ellington. Very bouncy. Jazzy.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=274s",
        "start_time": "274.279"
    },
    {
        "id": "4be54883",
        "text": "Nice. So, OK, you get the idea and then we have some music by Duke Ellington. Very bouncy. Jazzy. OK? Cool. So you have the, an idea of what we are dealing with in terms of like music or like audio cues over here. OK. So now we actually want to load these wave files using a libros. And for doing that, we can use a method, basic method uh from Li Brosa that's called load, not surprisingly. OK. So",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=275s",
        "start_time": "275.64"
    },
    {
        "id": "f21ffa5c",
        "text": "OK, you get the idea and then we have some music by Duke Ellington. Very bouncy. Jazzy. OK? Cool. So you have the, an idea of what we are dealing with in terms of like music or like audio cues over here. OK. So now we actually want to load these wave files using a libros. And for doing that, we can use a method, basic method uh from Li Brosa that's called load, not surprisingly. OK. So um here this method gives us back uh the signal itself which in this case, we'll call just like the BC and then it gives us back the sample rate. And so we'll do a libres dot load and then we here we should pass the, the file. OK?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=290s",
        "start_time": "290.63"
    },
    {
        "id": "d183a7a0",
        "text": "OK? Cool. So you have the, an idea of what we are dealing with in terms of like music or like audio cues over here. OK. So now we actually want to load these wave files using a libros. And for doing that, we can use a method, basic method uh from Li Brosa that's called load, not surprisingly. OK. So um here this method gives us back uh the signal itself which in this case, we'll call just like the BC and then it gives us back the sample rate. And so we'll do a libres dot load and then we here we should pass the, the file. OK? So now there are a few uh a bunch actually of parameters optional parameters that we have. One is for example, sample rate, we could uh specify a sample rate that we want uh Libres to load our uh audio file uh with.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=304s",
        "start_time": "304.119"
    },
    {
        "id": "1654c826",
        "text": "um here this method gives us back uh the signal itself which in this case, we'll call just like the BC and then it gives us back the sample rate. And so we'll do a libres dot load and then we here we should pass the, the file. OK? So now there are a few uh a bunch actually of parameters optional parameters that we have. One is for example, sample rate, we could uh specify a sample rate that we want uh Libres to load our uh audio file uh with. And the, the basic uh case that we have here or I mean the default value that Li Brosa uses is 22,050 which is a totally fine sampling rate for uh like our needs. And so I'm not going to change that. And then we have also like another option which is called mono, which is a ball parameter. So this mono could be equal to true or false and basically deals tells Liberator of whether we want to load the",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=330s",
        "start_time": "330.119"
    },
    {
        "id": "8621ad56",
        "text": "So now there are a few uh a bunch actually of parameters optional parameters that we have. One is for example, sample rate, we could uh specify a sample rate that we want uh Libres to load our uh audio file uh with. And the, the basic uh case that we have here or I mean the default value that Li Brosa uses is 22,050 which is a totally fine sampling rate for uh like our needs. And so I'm not going to change that. And then we have also like another option which is called mono, which is a ball parameter. So this mono could be equal to true or false and basically deals tells Liberator of whether we want to load the audio file as as is, for example, it could be a stereo file. So with two channels or like as man, the default behavior of load is to uh load the audio file as mono. And this is like totally fine and we usually work with mo mono signals because we, you really don't lose that much information when you are dealing uh like with mono a audio compared to like uh yeah, stereo audio. For example,",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=352s",
        "start_time": "352.72"
    },
    {
        "id": "6872ba1b",
        "text": "And the, the basic uh case that we have here or I mean the default value that Li Brosa uses is 22,050 which is a totally fine sampling rate for uh like our needs. And so I'm not going to change that. And then we have also like another option which is called mono, which is a ball parameter. So this mono could be equal to true or false and basically deals tells Liberator of whether we want to load the audio file as as is, for example, it could be a stereo file. So with two channels or like as man, the default behavior of load is to uh load the audio file as mono. And this is like totally fine and we usually work with mo mono signals because we, you really don't lose that much information when you are dealing uh like with mono a audio compared to like uh yeah, stereo audio. For example, obviously, this can change from problem to problem. But uh for today, we're just gonna use the default or MO OK. So now let's uh load also the uh red hot. And here I use the underscore because we'll have like the sample ridge here and it's gonna be the same. So we don't want, we don't care about that variable now. OK.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=370s",
        "start_time": "370.45"
    },
    {
        "id": "3d49b08a",
        "text": "audio file as as is, for example, it could be a stereo file. So with two channels or like as man, the default behavior of load is to uh load the audio file as mono. And this is like totally fine and we usually work with mo mono signals because we, you really don't lose that much information when you are dealing uh like with mono a audio compared to like uh yeah, stereo audio. For example, obviously, this can change from problem to problem. But uh for today, we're just gonna use the default or MO OK. So now let's uh load also the uh red hot. And here I use the underscore because we'll have like the sample ridge here and it's gonna be the same. So we don't want, we don't care about that variable now. OK. So here this, the BC file becomes the uh red hot file and then we'll do a duke and we'll do once again Li Brosa dot uh load and here we'll pass in the uh duke file. Ok.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=399s",
        "start_time": "399.19"
    },
    {
        "id": "8e542102",
        "text": "obviously, this can change from problem to problem. But uh for today, we're just gonna use the default or MO OK. So now let's uh load also the uh red hot. And here I use the underscore because we'll have like the sample ridge here and it's gonna be the same. So we don't want, we don't care about that variable now. OK. So here this, the BC file becomes the uh red hot file and then we'll do a duke and we'll do once again Li Brosa dot uh load and here we'll pass in the uh duke file. Ok. Good. OK. So now we've loaded all of these guys for timing. I'll just be working with the BC, one of these to show you like a few like things about like what we've loaded. But then when I want to compare this, obviously, I use all of these three different signals.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=428s",
        "start_time": "428.2"
    },
    {
        "id": "6f5e879d",
        "text": "So here this, the BC file becomes the uh red hot file and then we'll do a duke and we'll do once again Li Brosa dot uh load and here we'll pass in the uh duke file. Ok. Good. OK. So now we've loaded all of these guys for timing. I'll just be working with the BC, one of these to show you like a few like things about like what we've loaded. But then when I want to compare this, obviously, I use all of these three different signals. If you're wondering how many samples we have in the uh signal. So we can just do ABC dot size. And you'll see that we have this many samples, almost 700,000 samples. OK? So now let's take a look at the uh duration of one sample.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=451s",
        "start_time": "451.44"
    },
    {
        "id": "aba917e9",
        "text": "Good. OK. So now we've loaded all of these guys for timing. I'll just be working with the BC, one of these to show you like a few like things about like what we've loaded. But then when I want to compare this, obviously, I use all of these three different signals. If you're wondering how many samples we have in the uh signal. So we can just do ABC dot size. And you'll see that we have this many samples, almost 700,000 samples. OK? So now let's take a look at the uh duration of one sample. OK? So this uh du Samp let's call this sample duration and this is equal to the inverse of the sampling range. And let's print this so that it's nicer to see and we'll say that um duration of one sample is and here we pass in the sample duration.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=473s",
        "start_time": "473.57"
    },
    {
        "id": "71138e28",
        "text": "If you're wondering how many samples we have in the uh signal. So we can just do ABC dot size. And you'll see that we have this many samples, almost 700,000 samples. OK? So now let's take a look at the uh duration of one sample. OK? So this uh du Samp let's call this sample duration and this is equal to the inverse of the sampling range. And let's print this so that it's nicer to see and we'll say that um duration of one sample is and here we pass in the sample duration. And then we want to specify that we only want six decimal values and this is in seconds. OK? So let's take a look at this. And so the duration of one sample is 0.0000 45 seconds. So this is like a very, very short duration.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=489s",
        "start_time": "489.619"
    },
    {
        "id": "9e9e594f",
        "text": "OK? So this uh du Samp let's call this sample duration and this is equal to the inverse of the sampling range. And let's print this so that it's nicer to see and we'll say that um duration of one sample is and here we pass in the sample duration. And then we want to specify that we only want six decimal values and this is in seconds. OK? So let's take a look at this. And so the duration of one sample is 0.0000 45 seconds. So this is like a very, very short duration. So now why do we want this sample duration. Well, because I want to uh calculate the uh duration of the audio signal",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=515s",
        "start_time": "515.69"
    },
    {
        "id": "93e4ebed",
        "text": "And then we want to specify that we only want six decimal values and this is in seconds. OK? So let's take a look at this. And so the duration of one sample is 0.0000 45 seconds. So this is like a very, very short duration. So now why do we want this sample duration. Well, because I want to uh calculate the uh duration of the audio signal in seconds. Ok. And so for that, we should do a duration",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=544s",
        "start_time": "544.07"
    },
    {
        "id": "6346bb26",
        "text": "So now why do we want this sample duration. Well, because I want to uh calculate the uh duration of the audio signal in seconds. Ok. And so for that, we should do a duration and here we should do duration is equal to the sample duration. So the duration for one sample and multiply that by the total number of samples that we have in",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=564s",
        "start_time": "564.469"
    },
    {
        "id": "b286f16c",
        "text": "in seconds. Ok. And so for that, we should do a duration and here we should do duration is equal to the sample duration. So the duration for one sample and multiply that by the total number of samples that we have in the signal, right?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=576s",
        "start_time": "576.01"
    },
    {
        "id": "cd7f847d",
        "text": "and here we should do duration is equal to the sample duration. So the duration for one sample and multiply that by the total number of samples that we have in the signal, right? And let me grab this. So the duration of signal",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=585s",
        "start_time": "585.359"
    },
    {
        "id": "c5f4935e",
        "text": "the signal, right? And let me grab this. So the duration of signal is and he will pass in the duration.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=604s",
        "start_time": "604.059"
    },
    {
        "id": "db40ab99",
        "text": "And let me grab this. So the duration of signal is and he will pass in the duration. Oh, I had a type over here. OK?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=606s",
        "start_time": "606.34"
    },
    {
        "id": "96fc3273",
        "text": "is and he will pass in the duration. Oh, I had a type over here. OK? And the duration of the signal as expected is 30 seconds. So this is the length of the uh audio files that we are dealing with right now. OK. So let me just take out a few of this",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=615s",
        "start_time": "615.78"
    },
    {
        "id": "b864bcac",
        "text": "Oh, I had a type over here. OK? And the duration of the signal as expected is 30 seconds. So this is the length of the uh audio files that we are dealing with right now. OK. So let me just take out a few of this boxes here because otherwise a little bit too cluttered the whole thing. OK? So now we know the duration of the signal. So what, what, what I want to do really here is just like to visualize the waveforms. OK. So how do we do that? Well, given we want to do something a little bit elaborate, I want to import uh much uh plot of leap",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=623s",
        "start_time": "623.809"
    },
    {
        "id": "cb889694",
        "text": "And the duration of the signal as expected is 30 seconds. So this is the length of the uh audio files that we are dealing with right now. OK. So let me just take out a few of this boxes here because otherwise a little bit too cluttered the whole thing. OK? So now we know the duration of the signal. So what, what, what I want to do really here is just like to visualize the waveforms. OK. So how do we do that? Well, given we want to do something a little bit elaborate, I want to import uh much uh plot of leap dot plot PLT. OK. So",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=628s",
        "start_time": "628.39"
    },
    {
        "id": "438e063f",
        "text": "boxes here because otherwise a little bit too cluttered the whole thing. OK? So now we know the duration of the signal. So what, what, what I want to do really here is just like to visualize the waveforms. OK. So how do we do that? Well, given we want to do something a little bit elaborate, I want to import uh much uh plot of leap dot plot PLT. OK. So yeah, I have another type in here. OK. So we'll create a figure and so we'll do plot dot uh figure and we'll pass in a parameter called thick size and this is uh gonna be equal. Yeah, we'll put 15 for the width and 17 for the height.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=641s",
        "start_time": "641.57"
    },
    {
        "id": "b521abee",
        "text": "dot plot PLT. OK. So yeah, I have another type in here. OK. So we'll create a figure and so we'll do plot dot uh figure and we'll pass in a parameter called thick size and this is uh gonna be equal. Yeah, we'll put 15 for the width and 17 for the height. And now we're gonna have three different subplots. And we want to stack the waveforms for all the different audio signals uh vertically. OK. So we'll do a plots dot soup plots. And here we'll say that we want uh three rows, one column and this plot is at index uh one.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=670s",
        "start_time": "670.859"
    },
    {
        "id": "ce063469",
        "text": "yeah, I have another type in here. OK. So we'll create a figure and so we'll do plot dot uh figure and we'll pass in a parameter called thick size and this is uh gonna be equal. Yeah, we'll put 15 for the width and 17 for the height. And now we're gonna have three different subplots. And we want to stack the waveforms for all the different audio signals uh vertically. OK. So we'll do a plots dot soup plots. And here we'll say that we want uh three rows, one column and this plot is at index uh one. And so here we'll do a uh so here, what we can do is do a uh li browser dot display and we'll do a wave plots. OK? And so here we need to pass the uh signal",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=677s",
        "start_time": "677.479"
    },
    {
        "id": "dfd560d8",
        "text": "And now we're gonna have three different subplots. And we want to stack the waveforms for all the different audio signals uh vertically. OK. So we'll do a plots dot soup plots. And here we'll say that we want uh three rows, one column and this plot is at index uh one. And so here we'll do a uh so here, what we can do is do a uh li browser dot display and we'll do a wave plots. OK? And so here we need to pass the uh signal and the signal in this case is the BC. So we'll have the BC first, then we'll do a plot dot uh title and the title in this case is gonna be uh the BC and given, I know that this mm waveforms are gonna be normalized between minus one and one. I want to set a range on the um",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=699s",
        "start_time": "699.179"
    },
    {
        "id": "9a84f05b",
        "text": "And so here we'll do a uh so here, what we can do is do a uh li browser dot display and we'll do a wave plots. OK? And so here we need to pass the uh signal and the signal in this case is the BC. So we'll have the BC first, then we'll do a plot dot uh title and the title in this case is gonna be uh the BC and given, I know that this mm waveforms are gonna be normalized between minus one and one. I want to set a range on the um uh on the uh Y axis and I'll set that to",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=722s",
        "start_time": "722.95"
    },
    {
        "id": "f47c7542",
        "text": "and the signal in this case is the BC. So we'll have the BC first, then we'll do a plot dot uh title and the title in this case is gonna be uh the BC and given, I know that this mm waveforms are gonna be normalized between minus one and one. I want to set a range on the um uh on the uh Y axis and I'll set that to uh should be minus one and one. OK.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=743s",
        "start_time": "743.39"
    },
    {
        "id": "9029ca39",
        "text": "uh on the uh Y axis and I'll set that to uh should be minus one and one. OK. So we repeat this a couple of times, but now this is gonna be for",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=769s",
        "start_time": "769.409"
    },
    {
        "id": "09c4a4c4",
        "text": "uh should be minus one and one. OK. So we repeat this a couple of times, but now this is gonna be for uh red hot. And so we'll call this red chili peppers",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=777s",
        "start_time": "777.71"
    },
    {
        "id": "40b306ee",
        "text": "So we repeat this a couple of times, but now this is gonna be for uh red hot. And so we'll call this red chili peppers and then we'll do the same thing for uh Duke Ellington and we'll call this Du uh Duke Ellington, ok. And finally, here we'll do a plot dot show.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=784s",
        "start_time": "784.719"
    },
    {
        "id": "b6478096",
        "text": "uh red hot. And so we'll call this red chili peppers and then we'll do the same thing for uh Duke Ellington and we'll call this Du uh Duke Ellington, ok. And finally, here we'll do a plot dot show. Now, let's take a look at this. If I haven't messed up, we should see the results. And indeed, I've messed up a little bit. Uh So here you have like all of the uh different um wave plots uh like all in the same uh uh graph,",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=795s",
        "start_time": "795.08"
    },
    {
        "id": "69f3f6c9",
        "text": "and then we'll do the same thing for uh Duke Ellington and we'll call this Du uh Duke Ellington, ok. And finally, here we'll do a plot dot show. Now, let's take a look at this. If I haven't messed up, we should see the results. And indeed, I've messed up a little bit. Uh So here you have like all of the uh different um wave plots uh like all in the same uh uh graph, right? OK. So we want to change that and, and that's why we had this subplot. So here we put this at index two and here at index three. Also another thing that I want to do is add a little bit of transparency here because it's gonna just like look nicer. So in this web plot, so we'll do this,",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=802s",
        "start_time": "802.469"
    },
    {
        "id": "0a47c41a",
        "text": "Now, let's take a look at this. If I haven't messed up, we should see the results. And indeed, I've messed up a little bit. Uh So here you have like all of the uh different um wave plots uh like all in the same uh uh graph, right? OK. So we want to change that and, and that's why we had this subplot. So here we put this at index two and here at index three. Also another thing that I want to do is add a little bit of transparency here because it's gonna just like look nicer. So in this web plot, so we'll do this, do this. OK? Let's go. Let's see. OK. Here we go. OK. So here we have the different wave plots for the beauty, the red hot chili peppers song and Duke Ellington's piece. OK. So we can draw conclusions like already like by comparing these guys here. So as you can see over all the uh the BC wave plot",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=817s",
        "start_time": "817.929"
    },
    {
        "id": "4d380460",
        "text": "right? OK. So we want to change that and, and that's why we had this subplot. So here we put this at index two and here at index three. Also another thing that I want to do is add a little bit of transparency here because it's gonna just like look nicer. So in this web plot, so we'll do this, do this. OK? Let's go. Let's see. OK. Here we go. OK. So here we have the different wave plots for the beauty, the red hot chili peppers song and Duke Ellington's piece. OK. So we can draw conclusions like already like by comparing these guys here. So as you can see over all the uh the BC wave plot is kind of like is very fluid in its uh like envelope, right? So it is like fairly like stable down here. Then you have like a huge rise of tension and, and, and rise of energy right over here. Here you have the climax and then it goes back down.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=836s",
        "start_time": "836.155"
    },
    {
        "id": "35643dad",
        "text": "do this. OK? Let's go. Let's see. OK. Here we go. OK. So here we have the different wave plots for the beauty, the red hot chili peppers song and Duke Ellington's piece. OK. So we can draw conclusions like already like by comparing these guys here. So as you can see over all the uh the BC wave plot is kind of like is very fluid in its uh like envelope, right? So it is like fairly like stable down here. Then you have like a huge rise of tension and, and, and rise of energy right over here. Here you have the climax and then it goes back down. If we compare that with the red hot chili pepper song, it's quite different, right? Because here the the overall envelope tends to remain the same, right? And that's a feature that you'll always, you'll often find in popular music. And here we can talk about like rock music, pop music or EDM for example. And",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=859s",
        "start_time": "859.479"
    },
    {
        "id": "61915444",
        "text": "is kind of like is very fluid in its uh like envelope, right? So it is like fairly like stable down here. Then you have like a huge rise of tension and, and, and rise of energy right over here. Here you have the climax and then it goes back down. If we compare that with the red hot chili pepper song, it's quite different, right? Because here the the overall envelope tends to remain the same, right? And that's a feature that you'll always, you'll often find in popular music. And here we can talk about like rock music, pop music or EDM for example. And here you'll notice also certain spikes in uh the uh waveform and these spikes as you can see are like at regular points in time. And as you guess it, these are like the kick snare. Uh So the drum k like coming in, right?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=883s",
        "start_time": "883.34"
    },
    {
        "id": "48a7e62e",
        "text": "If we compare that with the red hot chili pepper song, it's quite different, right? Because here the the overall envelope tends to remain the same, right? And that's a feature that you'll always, you'll often find in popular music. And here we can talk about like rock music, pop music or EDM for example. And here you'll notice also certain spikes in uh the uh waveform and these spikes as you can see are like at regular points in time. And as you guess it, these are like the kick snare. Uh So the drum k like coming in, right? And uh if we compare this, like with uh Duke Ellington, you have like a little bit like of the two worlds here, right? And the classical music and the the rock music. So there's quite a lot of variability like in, in intensity, but it's more like micro is not like at the macro level.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=903s",
        "start_time": "903.5"
    },
    {
        "id": "c7fe774d",
        "text": "here you'll notice also certain spikes in uh the uh waveform and these spikes as you can see are like at regular points in time. And as you guess it, these are like the kick snare. Uh So the drum k like coming in, right? And uh if we compare this, like with uh Duke Ellington, you have like a little bit like of the two worlds here, right? And the classical music and the the rock music. So there's quite a lot of variability like in, in intensity, but it's more like micro is not like at the macro level. And so, so basically, the point, the takeaway point here is that when you're dealing with uh classical music, which uses uh obviously a lot of like acoustic, mainly acoustic instruments, then you'll have like a lot of variability in the waveform. Whereas",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=925s",
        "start_time": "925.77"
    },
    {
        "id": "24478f33",
        "text": "And uh if we compare this, like with uh Duke Ellington, you have like a little bit like of the two worlds here, right? And the classical music and the the rock music. So there's quite a lot of variability like in, in intensity, but it's more like micro is not like at the macro level. And so, so basically, the point, the takeaway point here is that when you're dealing with uh classical music, which uses uh obviously a lot of like acoustic, mainly acoustic instruments, then you'll have like a lot of variability in the waveform. Whereas like with popular music, which more kind of like electronic electric uh uh instruments then like the, the waveform tends to be a little bit like more stable, obviously, like this can change a lot depending on the song. But like this is like a general rule of thumb. OK. So now the next step that we want to do is to actually calculate the",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=942s",
        "start_time": "942.539"
    },
    {
        "id": "0eef40b0",
        "text": "And so, so basically, the point, the takeaway point here is that when you're dealing with uh classical music, which uses uh obviously a lot of like acoustic, mainly acoustic instruments, then you'll have like a lot of variability in the waveform. Whereas like with popular music, which more kind of like electronic electric uh uh instruments then like the, the waveform tends to be a little bit like more stable, obviously, like this can change a lot depending on the song. But like this is like a general rule of thumb. OK. So now the next step that we want to do is to actually calculate the amplitude",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=961s",
        "start_time": "961.669"
    },
    {
        "id": "f73687d9",
        "text": "like with popular music, which more kind of like electronic electric uh uh instruments then like the, the waveform tends to be a little bit like more stable, obviously, like this can change a lot depending on the song. But like this is like a general rule of thumb. OK. So now the next step that we want to do is to actually calculate the amplitude envelope. OK. And so I'm gonna create a function for doing that. Actually, I'm gonna create two functions. So one like is gonna be like a little bit easier to understand in some sort of like Python code the other one is a little bit fancier but the algorithm is the same. The only thing that changes is the, the, the Python code. OK. So let's start with the, with the simple one, the most uh the more intuitive the amplitude envelope.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=978s",
        "start_time": "978.5"
    },
    {
        "id": "afddbdc9",
        "text": "amplitude envelope. OK. And so I'm gonna create a function for doing that. Actually, I'm gonna create two functions. So one like is gonna be like a little bit easier to understand in some sort of like Python code the other one is a little bit fancier but the algorithm is the same. The only thing that changes is the, the, the Python code. OK. So let's start with the, with the simple one, the most uh the more intuitive the amplitude envelope. And here we sh uh this uh function takes the uh a signal and it takes a frame size. OK.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1001s",
        "start_time": "1001.849"
    },
    {
        "id": "5bebb81a",
        "text": "envelope. OK. And so I'm gonna create a function for doing that. Actually, I'm gonna create two functions. So one like is gonna be like a little bit easier to understand in some sort of like Python code the other one is a little bit fancier but the algorithm is the same. The only thing that changes is the, the, the Python code. OK. So let's start with the, with the simple one, the most uh the more intuitive the amplitude envelope. And here we sh uh this uh function takes the uh a signal and it takes a frame size. OK. So",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1004s",
        "start_time": "1004.099"
    },
    {
        "id": "45f704d1",
        "text": "And here we sh uh this uh function takes the uh a signal and it takes a frame size. OK. So uh amplitude envelope and we're gonna set this equal to a list. If you guys remember from the previous video uh the for calculating the amplitude envelope at a specific frame, what we want to do is just like take the maximum value of the amplitude calculated across all the samples in that given frame. And for getting the amplitude envelope for the whole",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1033s",
        "start_time": "1033.75"
    },
    {
        "id": "0c15250b",
        "text": "So uh amplitude envelope and we're gonna set this equal to a list. If you guys remember from the previous video uh the for calculating the amplitude envelope at a specific frame, what we want to do is just like take the maximum value of the amplitude calculated across all the samples in that given frame. And for getting the amplitude envelope for the whole signal, we take the uh the max uh amplitude for uh each frame. And yeah, we just like uh a pen that like to get like a value for each uh frame right now. If you don't remember that I went into quite details about like the mathematics of the amplitude envelope in the previous video. So you should definitely check that out and it should be over here just like go check that out. OK. So",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1045s",
        "start_time": "1045.3"
    },
    {
        "id": "0db331ab",
        "text": "uh amplitude envelope and we're gonna set this equal to a list. If you guys remember from the previous video uh the for calculating the amplitude envelope at a specific frame, what we want to do is just like take the maximum value of the amplitude calculated across all the samples in that given frame. And for getting the amplitude envelope for the whole signal, we take the uh the max uh amplitude for uh each frame. And yeah, we just like uh a pen that like to get like a value for each uh frame right now. If you don't remember that I went into quite details about like the mathematics of the amplitude envelope in the previous video. So you should definitely check that out and it should be over here just like go check that out. OK. So here we want to calculate, calculate the amplitude envelope for each frame. So, and for doing that, we'll do a four",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1046s",
        "start_time": "1046.93"
    },
    {
        "id": "f5ad4980",
        "text": "signal, we take the uh the max uh amplitude for uh each frame. And yeah, we just like uh a pen that like to get like a value for each uh frame right now. If you don't remember that I went into quite details about like the mathematics of the amplitude envelope in the previous video. So you should definitely check that out and it should be over here just like go check that out. OK. So here we want to calculate, calculate the amplitude envelope for each frame. So, and for doing that, we'll do a four uh I in range",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1075s",
        "start_time": "1075.52"
    },
    {
        "id": "a6e0e871",
        "text": "here we want to calculate, calculate the amplitude envelope for each frame. So, and for doing that, we'll do a four uh I in range and here we'll start from zero, then we'll stop at the length of the signal. But here we don't want to just uh have this eye that goes like 123 at each iteration. No, we wanted to jump and we wanted to jump by the, the frame size. So like this third uh like argument here in the range function is the step size in this case. So let's assume the frame size",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1105s",
        "start_time": "1105.709"
    },
    {
        "id": "e674a377",
        "text": "uh I in range and here we'll start from zero, then we'll stop at the length of the signal. But here we don't want to just uh have this eye that goes like 123 at each iteration. No, we wanted to jump and we wanted to jump by the, the frame size. So like this third uh like argument here in the range function is the step size in this case. So let's assume the frame size is equal to 100. So we'll start at the first iteration I is going to be equal to zero, then it's going to be second iteration is going to be equal to 100 then 203 100 you get the idea why do we do that? Well, we do that because in this way, I is always going to be the first sample in a frame. OK?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1120s",
        "start_time": "1120.579"
    },
    {
        "id": "401efbc4",
        "text": "and here we'll start from zero, then we'll stop at the length of the signal. But here we don't want to just uh have this eye that goes like 123 at each iteration. No, we wanted to jump and we wanted to jump by the, the frame size. So like this third uh like argument here in the range function is the step size in this case. So let's assume the frame size is equal to 100. So we'll start at the first iteration I is going to be equal to zero, then it's going to be second iteration is going to be equal to 100 then 203 100 you get the idea why do we do that? Well, we do that because in this way, I is always going to be the first sample in a frame. OK? And that's something that we definitely",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1125s",
        "start_time": "1125.439"
    },
    {
        "id": "3667b807",
        "text": "is equal to 100. So we'll start at the first iteration I is going to be equal to zero, then it's going to be second iteration is going to be equal to 100 then 203 100 you get the idea why do we do that? Well, we do that because in this way, I is always going to be the first sample in a frame. OK? And that's something that we definitely need. And so now we could say current frame",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1151s",
        "start_time": "1151.814"
    },
    {
        "id": "72df8389",
        "text": "And that's something that we definitely need. And so now we could say current frame unplayed ch",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1178s",
        "start_time": "1178.069"
    },
    {
        "id": "3ac17db6",
        "text": "need. And so now we could say current frame unplayed ch envelope",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1182s",
        "start_time": "1182.069"
    },
    {
        "id": "de9f0f87",
        "text": "unplayed ch envelope and they look and so this is equal to the max of the slice of signal that just considers the all the frames in the current, uh all the samples in the current frame. And so this is given by uh the slice of signal calculated between I and I plus",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1190s",
        "start_time": "1190.56"
    },
    {
        "id": "e25cbaf5",
        "text": "envelope and they look and so this is equal to the max of the slice of signal that just considers the all the frames in the current, uh all the samples in the current frame. And so this is given by uh the slice of signal calculated between I and I plus frame size. OK. So we are slicing the signal considering only the samples uh for a given frame.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1192s",
        "start_time": "1192.869"
    },
    {
        "id": "e369f551",
        "text": "and they look and so this is equal to the max of the slice of signal that just considers the all the frames in the current, uh all the samples in the current frame. And so this is given by uh the slice of signal calculated between I and I plus frame size. OK. So we are slicing the signal considering only the samples uh for a given frame. OK. So, and here we take the um amplitude envelope and we want to append the current frame amplitude envelope. OK.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1195s",
        "start_time": "1195.959"
    },
    {
        "id": "8088d81b",
        "text": "frame size. OK. So we are slicing the signal considering only the samples uh for a given frame. OK. So, and here we take the um amplitude envelope and we want to append the current frame amplitude envelope. OK. So we'll do this and here we can uh return the amplitude envelope but we'll return that as a N pi array. So we'll do this. OK. So this should work. But uh let me",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1218s",
        "start_time": "1218.989"
    },
    {
        "id": "9853c296",
        "text": "OK. So, and here we take the um amplitude envelope and we want to append the current frame amplitude envelope. OK. So we'll do this and here we can uh return the amplitude envelope but we'll return that as a N pi array. So we'll do this. OK. So this should work. But uh let me just create a constant up here.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1229s",
        "start_time": "1229.51"
    },
    {
        "id": "f5420157",
        "text": "So we'll do this and here we can uh return the amplitude envelope but we'll return that as a N pi array. So we'll do this. OK. So this should work. But uh let me just create a constant up here. So we'll put the frame size equal to 1000 and 24 which is totally legit number for frame size.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1244s",
        "start_time": "1244.9"
    },
    {
        "id": "c498a2fe",
        "text": "just create a constant up here. So we'll put the frame size equal to 1000 and 24 which is totally legit number for frame size. OK. So now let's try to calculate the amplitude envelope for the BC signal. And so we'll do a",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1265s",
        "start_time": "1265.089"
    },
    {
        "id": "96e76f2e",
        "text": "So we'll put the frame size equal to 1000 and 24 which is totally legit number for frame size. OK. So now let's try to calculate the amplitude envelope for the BC signal. And so we'll do a um pude envelope, we'll pass in the BC signal and we'll pass in the frame size. Now, what I want to do is show you the length of this guy. And obviously, we have an error here, which is NP because I didn't import NP. And so let me import that. So we'll do import NPI as MP.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1269s",
        "start_time": "1269.28"
    },
    {
        "id": "0c9e1e0c",
        "text": "OK. So now let's try to calculate the amplitude envelope for the BC signal. And so we'll do a um pude envelope, we'll pass in the BC signal and we'll pass in the frame size. Now, what I want to do is show you the length of this guy. And obviously, we have an error here, which is NP because I didn't import NP. And so let me import that. So we'll do import NPI as MP. OK? Now let's go back down here and hopefully this should work. OK? Here we go.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1277s",
        "start_time": "1277.209"
    },
    {
        "id": "4086d92b",
        "text": "um pude envelope, we'll pass in the BC signal and we'll pass in the frame size. Now, what I want to do is show you the length of this guy. And obviously, we have an error here, which is NP because I didn't import NP. And so let me import that. So we'll do import NPI as MP. OK? Now let's go back down here and hopefully this should work. OK? Here we go. So here we have uh the, the size of this array is 646. So this is the number of frames that we have in the, the BC uh signal. Now, if I change the frame size, uh let's say I have it. So I put it at 512. So now the number of frames should like double or the number of uh like",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1289s",
        "start_time": "1289.42"
    },
    {
        "id": "319583b6",
        "text": "OK? Now let's go back down here and hopefully this should work. OK? Here we go. So here we have uh the, the size of this array is 646. So this is the number of frames that we have in the, the BC uh signal. Now, if I change the frame size, uh let's say I have it. So I put it at 512. So now the number of frames should like double or the number of uh like uh um the, the, the size of the ali envelope also should double, right. Let's see.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1314s",
        "start_time": "1314.839"
    },
    {
        "id": "8ee027ba",
        "text": "So here we have uh the, the size of this array is 646. So this is the number of frames that we have in the, the BC uh signal. Now, if I change the frame size, uh let's say I have it. So I put it at 512. So now the number of frames should like double or the number of uh like uh um the, the, the size of the ali envelope also should double, right. Let's see. Yeah, but I should rerun this and here we go. So we have double the uh the number of frames over here. OK?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1320s",
        "start_time": "1320.729"
    },
    {
        "id": "d6442b80",
        "text": "uh um the, the, the size of the ali envelope also should double, right. Let's see. Yeah, but I should rerun this and here we go. So we have double the uh the number of frames over here. OK? Now, this is all good and well, but it only consider the, the case, this amplitude envelope function where we have uh non overlapping frames. If we had uh overlapping uh frames, we would have, we would need to uh change like this um function. And",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1347s",
        "start_time": "1347.569"
    },
    {
        "id": "7f941b30",
        "text": "Yeah, but I should rerun this and here we go. So we have double the uh the number of frames over here. OK? Now, this is all good and well, but it only consider the, the case, this amplitude envelope function where we have uh non overlapping frames. If we had uh overlapping uh frames, we would have, we would need to uh change like this um function. And now if you don't remember what like overlapping and non overlapping frames are, I suggest you to go check out this video to just like refresh or like learn that. But basically, when we have over uh overlapping frames, we'll still have like the, the frame size, which in this case will put equal to 1000 24. But then we have another",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1353s",
        "start_time": "1353.66"
    },
    {
        "id": "2f9f5a35",
        "text": "Now, this is all good and well, but it only consider the, the case, this amplitude envelope function where we have uh non overlapping frames. If we had uh overlapping uh frames, we would have, we would need to uh change like this um function. And now if you don't remember what like overlapping and non overlapping frames are, I suggest you to go check out this video to just like refresh or like learn that. But basically, when we have over uh overlapping frames, we'll still have like the, the frame size, which in this case will put equal to 1000 24. But then we have another um",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1361s",
        "start_time": "1361.839"
    },
    {
        "id": "856cfe19",
        "text": "now if you don't remember what like overlapping and non overlapping frames are, I suggest you to go check out this video to just like refresh or like learn that. But basically, when we have over uh overlapping frames, we'll still have like the, the frame size, which in this case will put equal to 1000 24. But then we have another um important like parameter that's called the hop length that tells us. So given like the current frame, how many sample samples we shift to the right for calculating the next frame. And in this case, we'll put 512 which again is a totally legit uh H length.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1383s",
        "start_time": "1383.76"
    },
    {
        "id": "93132655",
        "text": "um important like parameter that's called the hop length that tells us. So given like the current frame, how many sample samples we shift to the right for calculating the next frame. And in this case, we'll put 512 which again is a totally legit uh H length. And here we want to change this function so that it accepts also the a new parameter called hop length. So how do we change this? Well, what the only thing that we need to change here is that instead of like jumping at each iteration by the frame size, we want to jump by the uh hop length, right?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1406s",
        "start_time": "1406.579"
    },
    {
        "id": "78d38949",
        "text": "important like parameter that's called the hop length that tells us. So given like the current frame, how many sample samples we shift to the right for calculating the next frame. And in this case, we'll put 512 which again is a totally legit uh H length. And here we want to change this function so that it accepts also the a new parameter called hop length. So how do we change this? Well, what the only thing that we need to change here is that instead of like jumping at each iteration by the frame size, we want to jump by the uh hop length, right? So if we, if we change that now we should have like the whole thing",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1407s",
        "start_time": "1407.89"
    },
    {
        "id": "1458af8a",
        "text": "And here we want to change this function so that it accepts also the a new parameter called hop length. So how do we change this? Well, what the only thing that we need to change here is that instead of like jumping at each iteration by the frame size, we want to jump by the uh hop length, right? So if we, if we change that now we should have like the whole thing working. OK. So let's retry this.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1427s",
        "start_time": "1427.43"
    },
    {
        "id": "1499a92c",
        "text": "So if we, if we change that now we should have like the whole thing working. OK. So let's retry this. OK. So here I'll have to pass the H length, I'll do that And as you can see here, we have 1000 292 samples over here and which uh which is expected because we have a H length of 512",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1453s",
        "start_time": "1453.979"
    },
    {
        "id": "5a2d6f99",
        "text": "working. OK. So let's retry this. OK. So here I'll have to pass the H length, I'll do that And as you can see here, we have 1000 292 samples over here and which uh which is expected because we have a H length of 512 which is like the same like jumping to the right at each frame that we used before when we had the frame size at 512. There's also a nicer way to calculate the amplitude envelope. I mean, the algorithm is the same. It's just like the, the Python code that's a little bit fancier. And I guess they, which showed up to you. OK. So let's call this fancy Ted",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1459s",
        "start_time": "1459.78"
    },
    {
        "id": "8357ab16",
        "text": "OK. So here I'll have to pass the H length, I'll do that And as you can see here, we have 1000 292 samples over here and which uh which is expected because we have a H length of 512 which is like the same like jumping to the right at each frame that we used before when we had the frame size at 512. There's also a nicer way to calculate the amplitude envelope. I mean, the algorithm is the same. It's just like the, the Python code that's a little bit fancier. And I guess they, which showed up to you. OK. So let's call this fancy Ted envelope and we'll pass in the very same uh parameters over here.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1464s",
        "start_time": "1464.01"
    },
    {
        "id": "f717ed5e",
        "text": "which is like the same like jumping to the right at each frame that we used before when we had the frame size at 512. There's also a nicer way to calculate the amplitude envelope. I mean, the algorithm is the same. It's just like the, the Python code that's a little bit fancier. And I guess they, which showed up to you. OK. So let's call this fancy Ted envelope and we'll pass in the very same uh parameters over here. And this is gonna be just a one liner. So we'll do a NP array and here we'll have a list of comprehension and here we'll take the maximum of the signal. Uh Once again, this is gonna be between I and I plus hop uh length.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1483s",
        "start_time": "1483.05"
    },
    {
        "id": "73009285",
        "text": "envelope and we'll pass in the very same uh parameters over here. And this is gonna be just a one liner. So we'll do a NP array and here we'll have a list of comprehension and here we'll take the maximum of the signal. Uh Once again, this is gonna be between I and I plus hop uh length. And then we'll take this for I in",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1511s",
        "start_time": "1511.13"
    },
    {
        "id": "be2345dc",
        "text": "And this is gonna be just a one liner. So we'll do a NP array and here we'll have a list of comprehension and here we'll take the maximum of the signal. Uh Once again, this is gonna be between I and I plus hop uh length. And then we'll take this for I in uh well, this is not H length, this is frame size and then we'll take the four I in uh range.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1518s",
        "start_time": "1518.989"
    },
    {
        "id": "b286961c",
        "text": "And then we'll take this for I in uh well, this is not H length, this is frame size and then we'll take the four I in uh range. And here is gonna be between zero of the",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1541s",
        "start_time": "1541.03"
    },
    {
        "id": "010e73c9",
        "text": "uh well, this is not H length, this is frame size and then we'll take the four I in uh range. And here is gonna be between zero of the signal",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1546s",
        "start_time": "1546.14"
    },
    {
        "id": "739b70da",
        "text": "And here is gonna be between zero of the signal dot size and the step that we'll use. Now this is the hop length once again",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1556s",
        "start_time": "1556.939"
    },
    {
        "id": "ed71f74a",
        "text": "signal dot size and the step that we'll use. Now this is the hop length once again uh up length. OK? So, yep, this should work. OK? So now let me calculate. Uh yeah, let's put it down here. So we'll do fancy, we'll call this Fancy amplitude envelope for the BC. And we'll do a fancy oops fancy",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1562s",
        "start_time": "1562.849"
    },
    {
        "id": "4a777450",
        "text": "dot size and the step that we'll use. Now this is the hop length once again uh up length. OK? So, yep, this should work. OK? So now let me calculate. Uh yeah, let's put it down here. So we'll do fancy, we'll call this Fancy amplitude envelope for the BC. And we'll do a fancy oops fancy amplitude envelope and we pass in these parameters.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1565s",
        "start_time": "1565.31"
    },
    {
        "id": "55924e21",
        "text": "uh up length. OK? So, yep, this should work. OK? So now let me calculate. Uh yeah, let's put it down here. So we'll do fancy, we'll call this Fancy amplitude envelope for the BC. And we'll do a fancy oops fancy amplitude envelope and we pass in these parameters. OK.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1572s",
        "start_time": "1572.589"
    },
    {
        "id": "320b5329",
        "text": "amplitude envelope and we pass in these parameters. OK. Here we go attempts to work fine. Now, what we want to show is that the uh",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1598s",
        "start_time": "1598.739"
    },
    {
        "id": "19b54256",
        "text": "OK. Here we go attempts to work fine. Now, what we want to show is that the uh the amplitude envelope calculated with this function, the amplitude envelope function with the fancy amplitude envelope function are the same. So if we're doing that, we could do I A",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1605s",
        "start_time": "1605.859"
    },
    {
        "id": "08879520",
        "text": "Here we go attempts to work fine. Now, what we want to show is that the uh the amplitude envelope calculated with this function, the amplitude envelope function with the fancy amplitude envelope function are the same. So if we're doing that, we could do I A A E amplitude envelope. Well, let's do this. We'll do a EW PC is equal to fancy a amplitude envelope to PC for all the values.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1608s",
        "start_time": "1608.369"
    },
    {
        "id": "376ae133",
        "text": "the amplitude envelope calculated with this function, the amplitude envelope function with the fancy amplitude envelope function are the same. So if we're doing that, we could do I A A E amplitude envelope. Well, let's do this. We'll do a EW PC is equal to fancy a amplitude envelope to PC for all the values. So let's see if this works. OK. Yeah, that's true. So basically what I've done here is compare",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1615s",
        "start_time": "1615.569"
    },
    {
        "id": "504d3052",
        "text": "A E amplitude envelope. Well, let's do this. We'll do a EW PC is equal to fancy a amplitude envelope to PC for all the values. So let's see if this works. OK. Yeah, that's true. So basically what I've done here is compare uh like all the items in the array, one by one with the current sponge like item in the other array. And then I, I want like I wanted to have like that all of these values must be equal for all of them. And if that's the case, then we would get a tree which is like what we got. And so basically the two values are the same. In other words, we wrote like the same algorithms like in, in two different versions with Python code.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1628s",
        "start_time": "1628.04"
    },
    {
        "id": "c647dcae",
        "text": "So let's see if this works. OK. Yeah, that's true. So basically what I've done here is compare uh like all the items in the array, one by one with the current sponge like item in the other array. And then I, I want like I wanted to have like that all of these values must be equal for all of them. And if that's the case, then we would get a tree which is like what we got. And so basically the two values are the same. In other words, we wrote like the same algorithms like in, in two different versions with Python code. OK. So this is nice. Now, the final thing that I want to do is to visualize,",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1644s",
        "start_time": "1644.489"
    },
    {
        "id": "2d678056",
        "text": "uh like all the items in the array, one by one with the current sponge like item in the other array. And then I, I want like I wanted to have like that all of these values must be equal for all of them. And if that's the case, then we would get a tree which is like what we got. And so basically the two values are the same. In other words, we wrote like the same algorithms like in, in two different versions with Python code. OK. So this is nice. Now, the final thing that I want to do is to visualize, I want to visualize the ample envelope for uh all the audio files. OK. So how do we do that? Well, it's quite simple and, and it really relies on the, the code that we've already",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1650s",
        "start_time": "1650.89"
    },
    {
        "id": "3e2eb530",
        "text": "OK. So this is nice. Now, the final thing that I want to do is to visualize, I want to visualize the ample envelope for uh all the audio files. OK. So how do we do that? Well, it's quite simple and, and it really relies on the, the code that we've already uh written",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1679s",
        "start_time": "1679.56"
    },
    {
        "id": "06a44174",
        "text": "I want to visualize the ample envelope for uh all the audio files. OK. So how do we do that? Well, it's quite simple and, and it really relies on the, the code that we've already uh written over here for like visualizing like the, the, the wave form. So now the only difference here that we need is to add another plot uh like in each of these like subplots. And so we'll do a plot dot PLT dot uh plot.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1689s",
        "start_time": "1689.15"
    },
    {
        "id": "52beabeb",
        "text": "uh written over here for like visualizing like the, the, the wave form. So now the only difference here that we need is to add another plot uh like in each of these like subplots. And so we'll do a plot dot PLT dot uh plot. Here, we'll do T and here we'll pass the A, the BC and then the color is gonna be equal to red. Now, if you're wondering about this, t uh we haven't defined it and that yet. And so I'll define that in a second. But before doing that, I just want to repeat this",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1712s",
        "start_time": "1712.209"
    },
    {
        "id": "31ed702a",
        "text": "over here for like visualizing like the, the, the wave form. So now the only difference here that we need is to add another plot uh like in each of these like subplots. And so we'll do a plot dot PLT dot uh plot. Here, we'll do T and here we'll pass the A, the BC and then the color is gonna be equal to red. Now, if you're wondering about this, t uh we haven't defined it and that yet. And so I'll define that in a second. But before doing that, I just want to repeat this down here for uh red hots",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1714s",
        "start_time": "1714.14"
    },
    {
        "id": "a30657e1",
        "text": "Here, we'll do T and here we'll pass the A, the BC and then the color is gonna be equal to red. Now, if you're wondering about this, t uh we haven't defined it and that yet. And so I'll define that in a second. But before doing that, I just want to repeat this down here for uh red hots and down here for Duke Ellington.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1733s",
        "start_time": "1733.28"
    },
    {
        "id": "354d5a59",
        "text": "down here for uh red hots and down here for Duke Ellington. Oh, let's take this. OK. Now, as you know, as you probably noticed, I haven't defined the amplitude envelope",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1754s",
        "start_time": "1754.26"
    },
    {
        "id": "5e1b8188",
        "text": "and down here for Duke Ellington. Oh, let's take this. OK. Now, as you know, as you probably noticed, I haven't defined the amplitude envelope for the red hots yet. And so we'll do that. We're here and so here I'll do a amplitude envelope and here I'll pass uh red, the red hot uh signal and then the frame size and H length and I'll do the same for Duke",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1762s",
        "start_time": "1762.15"
    },
    {
        "id": "62df4d7f",
        "text": "Oh, let's take this. OK. Now, as you know, as you probably noticed, I haven't defined the amplitude envelope for the red hots yet. And so we'll do that. We're here and so here I'll do a amplitude envelope and here I'll pass uh red, the red hot uh signal and then the frame size and H length and I'll do the same for Duke Ellington. Yeah, let me just grab this and pass in here, Duke. OK. This should work. Now, the one thing that it's still missing here that I said is the, this key, right?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1768s",
        "start_time": "1768.189"
    },
    {
        "id": "2732db06",
        "text": "for the red hots yet. And so we'll do that. We're here and so here I'll do a amplitude envelope and here I'll pass uh red, the red hot uh signal and then the frame size and H length and I'll do the same for Duke Ellington. Yeah, let me just grab this and pass in here, Duke. OK. This should work. Now, the one thing that it's still missing here that I said is the, this key, right? OK. So this is basically like the X axis for the plot. And here we have like the Y axis. So, and here we, we want uh time",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1779s",
        "start_time": "1779.319"
    },
    {
        "id": "73629a78",
        "text": "Ellington. Yeah, let me just grab this and pass in here, Duke. OK. This should work. Now, the one thing that it's still missing here that I said is the, this key, right? OK. So this is basically like the X axis for the plot. And here we have like the Y axis. So, and here we, we want uh time and so for doing that, what we need to do is first of all get the frames and the frames is gonna be uh equal to a range between a zero and uh the size of the amplitude envelope.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1802s",
        "start_time": "1802.91"
    },
    {
        "id": "5444af94",
        "text": "OK. So this is basically like the X axis for the plot. And here we have like the Y axis. So, and here we, we want uh time and so for doing that, what we need to do is first of all get the frames and the frames is gonna be uh equal to a range between a zero and uh the size of the amplitude envelope. And so this is equal to a WC dot size I could have used also a amplitude envelope uh for like Duke Ellington or red hot chili peppers because it's like the same in terms of like the its length its size. OK. So now I want to calculate tea and here for passing from",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1820s",
        "start_time": "1820.01"
    },
    {
        "id": "9ed91ad5",
        "text": "and so for doing that, what we need to do is first of all get the frames and the frames is gonna be uh equal to a range between a zero and uh the size of the amplitude envelope. And so this is equal to a WC dot size I could have used also a amplitude envelope uh for like Duke Ellington or red hot chili peppers because it's like the same in terms of like the its length its size. OK. So now I want to calculate tea and here for passing from frames to time, we can use a, a very nice uh function by lib browser and it's called frames to time over here. And so here",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1830s",
        "start_time": "1830.819"
    },
    {
        "id": "eba35196",
        "text": "And so this is equal to a WC dot size I could have used also a amplitude envelope uh for like Duke Ellington or red hot chili peppers because it's like the same in terms of like the its length its size. OK. So now I want to calculate tea and here for passing from frames to time, we can use a, a very nice uh function by lib browser and it's called frames to time over here. And so here we need to specify the frames and uh we uh need to also specify the, the hop length and the hop length in this case is equal to hop length over here. OK? So here we've basically got the, the time",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1850s",
        "start_time": "1850.849"
    },
    {
        "id": "1a0070a9",
        "text": "frames to time, we can use a, a very nice uh function by lib browser and it's called frames to time over here. And so here we need to specify the frames and uh we uh need to also specify the, the hop length and the hop length in this case is equal to hop length over here. OK? So here we've basically got the, the time and uh along with the time we, we've also we also plot the amplitude envelope for um each of the different audio files. OK? So if I haven't made any mistakes, so this should work. Let's see. And here it goes and we have it in red",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1873s",
        "start_time": "1873.77"
    },
    {
        "id": "e44c09d5",
        "text": "we need to specify the frames and uh we uh need to also specify the, the hop length and the hop length in this case is equal to hop length over here. OK? So here we've basically got the, the time and uh along with the time we, we've also we also plot the amplitude envelope for um each of the different audio files. OK? So if I haven't made any mistakes, so this should work. Let's see. And here it goes and we have it in red and yeah, not surprisingly, the amplitude envelope just like full is like the the envelope like of the wave firm. And you can see it here like in red for the BC, for the red hot chili pepper song and for uh Duke Ellington. OK?",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1888s",
        "start_time": "1888.935"
    },
    {
        "id": "a7d02144",
        "text": "and uh along with the time we, we've also we also plot the amplitude envelope for um each of the different audio files. OK? So if I haven't made any mistakes, so this should work. Let's see. And here it goes and we have it in red and yeah, not surprisingly, the amplitude envelope just like full is like the the envelope like of the wave firm. And you can see it here like in red for the BC, for the red hot chili pepper song and for uh Duke Ellington. OK? So now one thing like that's uh obvious once again, is that like, they usually uh you have like this spikes like in rock music or like popular music more in general, which uh are determined by like the drum kit, like kicking in.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1911s",
        "start_time": "1911.719"
    },
    {
        "id": "22c0a342",
        "text": "and yeah, not surprisingly, the amplitude envelope just like full is like the the envelope like of the wave firm. And you can see it here like in red for the BC, for the red hot chili pepper song and for uh Duke Ellington. OK? So now one thing like that's uh obvious once again, is that like, they usually uh you have like this spikes like in rock music or like popular music more in general, which uh are determined by like the drum kit, like kicking in. And then, uh usually you also have like the amplitude envelope, the mean amplitude envelope for music that's in a rock genre, pop, popular genre that's usually higher than that for like classical music or jazz music.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1931s",
        "start_time": "1931.93"
    },
    {
        "id": "104777e5",
        "text": "So now one thing like that's uh obvious once again, is that like, they usually uh you have like this spikes like in rock music or like popular music more in general, which uh are determined by like the drum kit, like kicking in. And then, uh usually you also have like the amplitude envelope, the mean amplitude envelope for music that's in a rock genre, pop, popular genre that's usually higher than that for like classical music or jazz music. OK? But here you have it. I hope like you've enjoyed this video. Now, you know how to create uh a, how to calculate the amplitude envelope, how to visualize waveforms the amplitude envelope and play around with like basic processing features in the browser. So that's all for today.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1947s",
        "start_time": "1947.579"
    },
    {
        "id": "3697c51b",
        "text": "And then, uh usually you also have like the amplitude envelope, the mean amplitude envelope for music that's in a rock genre, pop, popular genre that's usually higher than that for like classical music or jazz music. OK? But here you have it. I hope like you've enjoyed this video. Now, you know how to create uh a, how to calculate the amplitude envelope, how to visualize waveforms the amplitude envelope and play around with like basic processing features in the browser. So that's all for today. So if you like the video, please consider like living alike. If you haven't subscribed yet, please consider subscribing if you have any questions, leave them in the comment section below and I guess I'll see you next time. Cheers.",
        "video": "Extracting the amplitude envelope feature from scratch in Python",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "rlypsap6Wow",
        "youtube_link": "https://www.youtube.com/watch?v=rlypsap6Wow&t=1965s",
        "start_time": "1965.989"
    },
    {
        "id": "2997ff99",
        "text": "Hi, everybody and welcome to yet another video in the audio signal processing from machine learning series. This time we continue our quest, our journey into the wonderful world of the fourier transform. Specifically, we'll look at the discrete fourier transform. But before getting like our heads down into the DM T or Discord fourier transform, I want to remind you once again about the sound of A I Slack community, which is a community where you can just learn a lot of stuff about audio processing and A I music in general and you can network with very cool people. So if you're interested in joining, I'll leave you the sign up link in the description below. But now let's move on to the cool stuff. OK. Last time, if you guys remember we defined this beast of an equation here, which in other words is simply the fourier transform expressed through complex numbers. Now, I'm not gonna get into the details here and I take for granted that you already know that. But if that's not the case, I suggest you to check out this video, which by the way is my last video. OK?",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "5dac8dc2",
        "text": "T or Discord fourier transform, I want to remind you once again about the sound of A I Slack community, which is a community where you can just learn a lot of stuff about audio processing and A I music in general and you can network with very cool people. So if you're interested in joining, I'll leave you the sign up link in the description below. But now let's move on to the cool stuff. OK. Last time, if you guys remember we defined this beast of an equation here, which in other words is simply the fourier transform expressed through complex numbers. Now, I'm not gonna get into the details here and I take for granted that you already know that. But if that's not the case, I suggest you to check out this video, which by the way is my last video. OK? So uh this formula works for analog signals or in other words, signals that are continuous in time. But the reality is that when we deal like with audio signals or just like any type of signal in general in today's world, we actually deal with a discrete",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=24s",
        "start_time": "24.139"
    },
    {
        "id": "6b2e9898",
        "text": "Last time, if you guys remember we defined this beast of an equation here, which in other words is simply the fourier transform expressed through complex numbers. Now, I'm not gonna get into the details here and I take for granted that you already know that. But if that's not the case, I suggest you to check out this video, which by the way is my last video. OK? So uh this formula works for analog signals or in other words, signals that are continuous in time. But the reality is that when we deal like with audio signals or just like any type of signal in general in today's world, we actually deal with a discrete signals. And this discrete signals are the ones that we use and we manipulate uh with digital machines like our computers or like edge devices, whatever you want. Really? So how do we move from our GFT our continuous signal to a uh digital signal? Well, we go through the process of digital,",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=49s",
        "start_time": "49.43"
    },
    {
        "id": "12a3d217",
        "text": "So uh this formula works for analog signals or in other words, signals that are continuous in time. But the reality is that when we deal like with audio signals or just like any type of signal in general in today's world, we actually deal with a discrete signals. And this discrete signals are the ones that we use and we manipulate uh with digital machines like our computers or like edge devices, whatever you want. Really? So how do we move from our GFT our continuous signal to a uh digital signal? Well, we go through the process of digital, in other words, like we sample an analog signal which is like the one that you see here in red and we just like take certain points which we can, which we can call sample points. And so here we take like the first sample points and we use like capital T which is a sampling period which basically tells us",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=74s",
        "start_time": "74.48"
    },
    {
        "id": "15ff98c4",
        "text": "signals. And this discrete signals are the ones that we use and we manipulate uh with digital machines like our computers or like edge devices, whatever you want. Really? So how do we move from our GFT our continuous signal to a uh digital signal? Well, we go through the process of digital, in other words, like we sample an analog signal which is like the one that you see here in red and we just like take certain points which we can, which we can call sample points. And so here we take like the first sample points and we use like capital T which is a sampling period which basically tells us uh after how much time we actually take a sample. So here we have like the first um sample point, then the second and the third, you get the idea. Now if you want to know more about the digitalization process which has both some",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=95s",
        "start_time": "95.949"
    },
    {
        "id": "56e81356",
        "text": "in other words, like we sample an analog signal which is like the one that you see here in red and we just like take certain points which we can, which we can call sample points. And so here we take like the first sample points and we use like capital T which is a sampling period which basically tells us uh after how much time we actually take a sample. So here we have like the first um sample point, then the second and the third, you get the idea. Now if you want to know more about the digitalization process which has both some and quantization, I suggest you to go check out a previous video that I had on this topic and it should be like up here. But I hope like you've already watched that and because like we know some of those ideas for moving forward, OK. So",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=117s",
        "start_time": "117.419"
    },
    {
        "id": "1bb4a63d",
        "text": "uh after how much time we actually take a sample. So here we have like the first um sample point, then the second and the third, you get the idea. Now if you want to know more about the digitalization process which has both some and quantization, I suggest you to go check out a previous video that I had on this topic and it should be like up here. But I hope like you've already watched that and because like we know some of those ideas for moving forward, OK. So um when we digitalize the signal, so what happens is basically that uh like our original like analog signal GFT which is continuous in time. It's kind of like transforms into this X of N an X of even the convention for saying that we are taking a sample. The point here is that we have like a discrete. So this MS differently like from this T like are these",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=139s",
        "start_time": "139.059"
    },
    {
        "id": "71a83657",
        "text": "and quantization, I suggest you to go check out a previous video that I had on this topic and it should be like up here. But I hope like you've already watched that and because like we know some of those ideas for moving forward, OK. So um when we digitalize the signal, so what happens is basically that uh like our original like analog signal GFT which is continuous in time. It's kind of like transforms into this X of N an X of even the convention for saying that we are taking a sample. The point here is that we have like a discrete. So this MS differently like from this T like are these crete, so we have like N zero or N one N two and three and X then is a sample taken at a particular time. But what's that particular time? Well, if we want to know like the, the points, so the time T at which we've taken the sample, we have to multiply and which is the current sample we are in could be 123 by the sampling period.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=155s",
        "start_time": "155.544"
    },
    {
        "id": "afdf8c18",
        "text": "um when we digitalize the signal, so what happens is basically that uh like our original like analog signal GFT which is continuous in time. It's kind of like transforms into this X of N an X of even the convention for saying that we are taking a sample. The point here is that we have like a discrete. So this MS differently like from this T like are these crete, so we have like N zero or N one N two and three and X then is a sample taken at a particular time. But what's that particular time? Well, if we want to know like the, the points, so the time T at which we've taken the sample, we have to multiply and which is the current sample we are in could be 123 by the sampling period. OK. So this gives you like an idea of the type of like transformation that we will need for moving from continuous fourier transform to discrete fourier transform.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=173s",
        "start_time": "173.38"
    },
    {
        "id": "e294b765",
        "text": "crete, so we have like N zero or N one N two and three and X then is a sample taken at a particular time. But what's that particular time? Well, if we want to know like the, the points, so the time T at which we've taken the sample, we have to multiply and which is the current sample we are in could be 123 by the sampling period. OK. So this gives you like an idea of the type of like transformation that we will need for moving from continuous fourier transform to discrete fourier transform. But now let's try to build the DFT or discrete fourier transform starting from the continuous fourier transform. And so we here we'll see how each of the elements in the fourier transform get mapped onto their like discrete counterparts. OK. We start from the",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=201s",
        "start_time": "201.86"
    },
    {
        "id": "079718cb",
        "text": "OK. So this gives you like an idea of the type of like transformation that we will need for moving from continuous fourier transform to discrete fourier transform. But now let's try to build the DFT or discrete fourier transform starting from the continuous fourier transform. And so we here we'll see how each of the elements in the fourier transform get mapped onto their like discrete counterparts. OK. We start from the definition of the four transform itself. And so in the continuous case, we call this like G hat of F and in the discrete case, we call it X hat of F and obviously this is the convention that we were using before to say that here we are dealing with samples, not with an analog signal. OK. So let's move on to the juicy stuff. OK. So in the continuous case, in the top",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=230s",
        "start_time": "230.8"
    },
    {
        "id": "bece6288",
        "text": "But now let's try to build the DFT or discrete fourier transform starting from the continuous fourier transform. And so we here we'll see how each of the elements in the fourier transform get mapped onto their like discrete counterparts. OK. We start from the definition of the four transform itself. And so in the continuous case, we call this like G hat of F and in the discrete case, we call it X hat of F and obviously this is the convention that we were using before to say that here we are dealing with samples, not with an analog signal. OK. So let's move on to the juicy stuff. OK. So in the continuous case, in the top equation, you see that we are using integral here, so we are integrating uh over time. And that's because we have like a time is continuous. So we have like infinite points in time And when you sum something like that's infinite, like this continues, you use like a, an integral, right? But in our case, the idea is that we have like discrete points,",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=244s",
        "start_time": "244.07"
    },
    {
        "id": "74d1842c",
        "text": "definition of the four transform itself. And so in the continuous case, we call this like G hat of F and in the discrete case, we call it X hat of F and obviously this is the convention that we were using before to say that here we are dealing with samples, not with an analog signal. OK. So let's move on to the juicy stuff. OK. So in the continuous case, in the top equation, you see that we are using integral here, so we are integrating uh over time. And that's because we have like a time is continuous. So we have like infinite points in time And when you sum something like that's infinite, like this continues, you use like a, an integral, right? But in our case, the idea is that we have like discrete points, discrete samples. And so what we do is we use a sum which basically tells us that we shoot sum over N which are like the, the different points where we take the, the samples. OK. So let's move on. Now, this one is quite easy and we already saw it, right? So we don't want to deal with GFT anymore, but rather we want to deal with the sample. So X them and the next one",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=265s",
        "start_time": "265.204"
    },
    {
        "id": "8f71ff4f",
        "text": "equation, you see that we are using integral here, so we are integrating uh over time. And that's because we have like a time is continuous. So we have like infinite points in time And when you sum something like that's infinite, like this continues, you use like a, an integral, right? But in our case, the idea is that we have like discrete points, discrete samples. And so what we do is we use a sum which basically tells us that we shoot sum over N which are like the, the different points where we take the, the samples. OK. So let's move on. Now, this one is quite easy and we already saw it, right? So we don't want to deal with GFT anymore, but rather we want to deal with the sample. So X them and the next one which is like this um exponential here, more or less stays the same, right? You can see. And this is like the, if you remember from my previous video, this is like the uh the kind of like pure turn we decompose uh our original signal into uh And now the point is that we only have like one little modification instead of having T we have N over here.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=293s",
        "start_time": "293.38"
    },
    {
        "id": "528b10cd",
        "text": "discrete samples. And so what we do is we use a sum which basically tells us that we shoot sum over N which are like the, the different points where we take the, the samples. OK. So let's move on. Now, this one is quite easy and we already saw it, right? So we don't want to deal with GFT anymore, but rather we want to deal with the sample. So X them and the next one which is like this um exponential here, more or less stays the same, right? You can see. And this is like the, if you remember from my previous video, this is like the uh the kind of like pure turn we decompose uh our original signal into uh And now the point is that we only have like one little modification instead of having T we have N over here. Uh So this like should uh just like, let you understand that from a very high level like conceptual point, what we are doing here is that instead of dealing with T time as a continuous variable, we are taking these snapshots at N, right. And these are like snapshots that take like discrete points in time.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=320s",
        "start_time": "320.519"
    },
    {
        "id": "4aa50928",
        "text": "which is like this um exponential here, more or less stays the same, right? You can see. And this is like the, if you remember from my previous video, this is like the uh the kind of like pure turn we decompose uh our original signal into uh And now the point is that we only have like one little modification instead of having T we have N over here. Uh So this like should uh just like, let you understand that from a very high level like conceptual point, what we are doing here is that instead of dealing with T time as a continuous variable, we are taking these snapshots at N, right. And these are like snapshots that take like discrete points in time. So all in all what we do instead of using T we use like this snapshot. And OK, this is great because we have like a first version of the discrete fourier transform. But we need to take that around a little bit. OK. Now let's take a look at the visual interpretation of this first version of the um discrete fourier transform. If you",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=349s",
        "start_time": "349.85"
    },
    {
        "id": "bd913138",
        "text": "Uh So this like should uh just like, let you understand that from a very high level like conceptual point, what we are doing here is that instead of dealing with T time as a continuous variable, we are taking these snapshots at N, right. And these are like snapshots that take like discrete points in time. So all in all what we do instead of using T we use like this snapshot. And OK, this is great because we have like a first version of the discrete fourier transform. But we need to take that around a little bit. OK. Now let's take a look at the visual interpretation of this first version of the um discrete fourier transform. If you remember what we said uh back in the like in our previous videos. Uh is that like when we visualize the fourier transform, when we, we take the visual interpretation of that, what we do actually is we are just like taking the area that's below a uh our like a signal, our analog signal in that case, right?",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=379s",
        "start_time": "379.57"
    },
    {
        "id": "8eaeb3a8",
        "text": "So all in all what we do instead of using T we use like this snapshot. And OK, this is great because we have like a first version of the discrete fourier transform. But we need to take that around a little bit. OK. Now let's take a look at the visual interpretation of this first version of the um discrete fourier transform. If you remember what we said uh back in the like in our previous videos. Uh is that like when we visualize the fourier transform, when we, we take the visual interpretation of that, what we do actually is we are just like taking the area that's below a uh our like a signal, our analog signal in that case, right? And now what we do like with the disc four transform is something like conceptually similar and you can see it here, right? So here like on the Y axis, you have like amplitude uh on the X axis, you have like time and here like in blue, you have like this abstract, let's call it like it's our original analog signal. And now like we sample that at certain points in time.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=402s",
        "start_time": "402.29"
    },
    {
        "id": "963c1756",
        "text": "remember what we said uh back in the like in our previous videos. Uh is that like when we visualize the fourier transform, when we, we take the visual interpretation of that, what we do actually is we are just like taking the area that's below a uh our like a signal, our analog signal in that case, right? And now what we do like with the disc four transform is something like conceptually similar and you can see it here, right? So here like on the Y axis, you have like amplitude uh on the X axis, you have like time and here like in blue, you have like this abstract, let's call it like it's our original analog signal. And now like we sample that at certain points in time. And what we do with the fourier transform, basically, it's just like we take uh like the areas like of these rectangles and the rectangles approximate the overall area that stays below the blue curve, which is our original analog signal. And as you can see here, when we take these rectangles here, we have like certain errors because we are only approximating the",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=428s",
        "start_time": "428.44"
    },
    {
        "id": "d5ce1afe",
        "text": "And now what we do like with the disc four transform is something like conceptually similar and you can see it here, right? So here like on the Y axis, you have like amplitude uh on the X axis, you have like time and here like in blue, you have like this abstract, let's call it like it's our original analog signal. And now like we sample that at certain points in time. And what we do with the fourier transform, basically, it's just like we take uh like the areas like of these rectangles and the rectangles approximate the overall area that stays below the blue curve, which is our original analog signal. And as you can see here, when we take these rectangles here, we have like certain errors because we are only approximating the actual area below the signal. But the point is like we still have like errors obviously. Uh If you make like this mental exercise and you think that we take like a smaller sampling period, we would have like way more like vertical bars like this rectangles like this and",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=454s",
        "start_time": "454.779"
    },
    {
        "id": "9af7daf2",
        "text": "And what we do with the fourier transform, basically, it's just like we take uh like the areas like of these rectangles and the rectangles approximate the overall area that stays below the blue curve, which is our original analog signal. And as you can see here, when we take these rectangles here, we have like certain errors because we are only approximating the actual area below the signal. But the point is like we still have like errors obviously. Uh If you make like this mental exercise and you think that we take like a smaller sampling period, we would have like way more like vertical bars like this rectangles like this and which basically would lead to uh like a smaller error overall. OK. But that's basically like the idea. So we take this, the area that's given by something like all of these rectangles, each of this rectangle obviously like represents like a a sample, right.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=480s",
        "start_time": "480.309"
    },
    {
        "id": "53f650a0",
        "text": "actual area below the signal. But the point is like we still have like errors obviously. Uh If you make like this mental exercise and you think that we take like a smaller sampling period, we would have like way more like vertical bars like this rectangles like this and which basically would lead to uh like a smaller error overall. OK. But that's basically like the idea. So we take this, the area that's given by something like all of these rectangles, each of this rectangle obviously like represents like a a sample, right. OK. So now let's move on and see that we still have a problem with our definition of the discrete fourier transform. And that's why I said this is like the, the first definition that needs to be tweaked a little bit to be perfect for our purposes. And so we have a couple of problems here. So",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=506s",
        "start_time": "506.406"
    },
    {
        "id": "51a3f732",
        "text": "which basically would lead to uh like a smaller error overall. OK. But that's basically like the idea. So we take this, the area that's given by something like all of these rectangles, each of this rectangle obviously like represents like a a sample, right. OK. So now let's move on and see that we still have a problem with our definition of the discrete fourier transform. And that's why I said this is like the, the first definition that needs to be tweaked a little bit to be perfect for our purposes. And so we have a couple of problems here. So uh problem number one is that we are still dealing with a continuous frequency. And problem number two is that we are dealing potentially like with an infinite number of samples or at least like this is what the fourier transform is supposed to be. It",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=527s",
        "start_time": "527.671"
    },
    {
        "id": "3d88e444",
        "text": "OK. So now let's move on and see that we still have a problem with our definition of the discrete fourier transform. And that's why I said this is like the, the first definition that needs to be tweaked a little bit to be perfect for our purposes. And so we have a couple of problems here. So uh problem number one is that we are still dealing with a continuous frequency. And problem number two is that we are dealing potentially like with an infinite number of samples or at least like this is what the fourier transform is supposed to be. It supposed to uh sum over like infinite time. In in our case, this translates into an infinite number of samples. So yeah, let's take a look at the um formula for the discrete fourier transform and try to explain what I mean by those two issues. So regarding the frequency, I'm basically saying that this frequency is in a fourier transform is supposed to be like infinite or like actually continuous, right?",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=549s",
        "start_time": "549.179"
    },
    {
        "id": "9d30fc9e",
        "text": "uh problem number one is that we are still dealing with a continuous frequency. And problem number two is that we are dealing potentially like with an infinite number of samples or at least like this is what the fourier transform is supposed to be. It supposed to uh sum over like infinite time. In in our case, this translates into an infinite number of samples. So yeah, let's take a look at the um formula for the discrete fourier transform and try to explain what I mean by those two issues. So regarding the frequency, I'm basically saying that this frequency is in a fourier transform is supposed to be like infinite or like actually continuous, right? So and that's just like something that we can't do like with our digital machines because like everything that resounds uh or sounds similar to like infinite doesn't work with our machines for computational and memory problems, right?",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=570s",
        "start_time": "570.26"
    },
    {
        "id": "7876796d",
        "text": "supposed to uh sum over like infinite time. In in our case, this translates into an infinite number of samples. So yeah, let's take a look at the um formula for the discrete fourier transform and try to explain what I mean by those two issues. So regarding the frequency, I'm basically saying that this frequency is in a fourier transform is supposed to be like infinite or like actually continuous, right? So and that's just like something that we can't do like with our digital machines because like everything that resounds uh or sounds similar to like infinite doesn't work with our machines for computational and memory problems, right? OK. So we have to sort that out but first we have to sort out this other issue which is basically that with the original continuous where we transform, we want to integrate across infinite time possible like time that goes like the age of the universe and beyond. Because it's in,",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=588s",
        "start_time": "588.715"
    },
    {
        "id": "9bc4c727",
        "text": "So and that's just like something that we can't do like with our digital machines because like everything that resounds uh or sounds similar to like infinite doesn't work with our machines for computational and memory problems, right? OK. So we have to sort that out but first we have to sort out this other issue which is basically that with the original continuous where we transform, we want to integrate across infinite time possible like time that goes like the age of the universe and beyond. Because it's in, in other words, we would need to create like an infinite number of samples for that. But that's not really like what we can do either like with our digital machines. So how do we solve these two big problems? Well, we hack around them, right? So hack number one is for time.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=618s",
        "start_time": "618.739"
    },
    {
        "id": "cded3eb9",
        "text": "OK. So we have to sort that out but first we have to sort out this other issue which is basically that with the original continuous where we transform, we want to integrate across infinite time possible like time that goes like the age of the universe and beyond. Because it's in, in other words, we would need to create like an infinite number of samples for that. But that's not really like what we can do either like with our digital machines. So how do we solve these two big problems? Well, we hack around them, right? So hack number one is for time. So what we do here is quite sensible and it makes sense. Um Basically what we do is we consider the frequencies we want to decompose our original signal into to be nonzero only in a finite time interval.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=634s",
        "start_time": "634.669"
    },
    {
        "id": "46bfb0d3",
        "text": "in other words, we would need to create like an infinite number of samples for that. But that's not really like what we can do either like with our digital machines. So how do we solve these two big problems? Well, we hack around them, right? So hack number one is for time. So what we do here is quite sensible and it makes sense. Um Basically what we do is we consider the frequencies we want to decompose our original signal into to be nonzero only in a finite time interval. Does this make sense? Yes. Why is that? Well, let's take like as an example, like a song. So we have a song that's three minutes long. Well, it makes sense to only take into consideration there three minutes worth of samples, not the infinite number of samples that comes like with infinite time, right? And that's because we assume that this three minutes of song are the ones",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=654s",
        "start_time": "654.445"
    },
    {
        "id": "cc6ff318",
        "text": "So what we do here is quite sensible and it makes sense. Um Basically what we do is we consider the frequencies we want to decompose our original signal into to be nonzero only in a finite time interval. Does this make sense? Yes. Why is that? Well, let's take like as an example, like a song. So we have a song that's three minutes long. Well, it makes sense to only take into consideration there three minutes worth of samples, not the infinite number of samples that comes like with infinite time, right? And that's because we assume that this three minutes of song are the ones that will provide us the uh information about like the uh the frequency we want to decompose that song into. OK. And so what that basically means is that instead of uh considering an infinite number of samples, we can only focus on a finite number of samples. And this is like definitely something that we can then handle with our machines. OK? All good for time. Let's move on to frequency.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=674s",
        "start_time": "674.4"
    },
    {
        "id": "8151b330",
        "text": "Does this make sense? Yes. Why is that? Well, let's take like as an example, like a song. So we have a song that's three minutes long. Well, it makes sense to only take into consideration there three minutes worth of samples, not the infinite number of samples that comes like with infinite time, right? And that's because we assume that this three minutes of song are the ones that will provide us the uh information about like the uh the frequency we want to decompose that song into. OK. And so what that basically means is that instead of uh considering an infinite number of samples, we can only focus on a finite number of samples. And this is like definitely something that we can then handle with our machines. OK? All good for time. Let's move on to frequency. Well, here there's hack number two. So instead of like taking uh all possible frequencies, an infinite number of frequencies because we treat frequency as a continuous variable. What we do is we compute the uh transform for only a finite number of frequencies. And now you may be wondering but uh how many frequencies we want to uh",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=693s",
        "start_time": "693.719"
    },
    {
        "id": "150dc5e0",
        "text": "that will provide us the uh information about like the uh the frequency we want to decompose that song into. OK. And so what that basically means is that instead of uh considering an infinite number of samples, we can only focus on a finite number of samples. And this is like definitely something that we can then handle with our machines. OK? All good for time. Let's move on to frequency. Well, here there's hack number two. So instead of like taking uh all possible frequencies, an infinite number of frequencies because we treat frequency as a continuous variable. What we do is we compute the uh transform for only a finite number of frequencies. And now you may be wondering but uh how many frequencies we want to uh consider when we actually calculate the discrete fourier transform? Well, we have like a nice little hack here and basically the number of frequencies and that we consider is equal to the number of samples and that we have in the signal that we want to take the discrete fourier transform of",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=720s",
        "start_time": "720.969"
    },
    {
        "id": "7e831b5e",
        "text": "Well, here there's hack number two. So instead of like taking uh all possible frequencies, an infinite number of frequencies because we treat frequency as a continuous variable. What we do is we compute the uh transform for only a finite number of frequencies. And now you may be wondering but uh how many frequencies we want to uh consider when we actually calculate the discrete fourier transform? Well, we have like a nice little hack here and basically the number of frequencies and that we consider is equal to the number of samples and that we have in the signal that we want to take the discrete fourier transform of OK. But why N should be equal to M for a couple of reasons. Reason number one is very important. And it's the, it's something that we've seen like in a previous video. And it's basically this idea that we can do, we can have like a round trip from the frequency domain to the time domain.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=748s",
        "start_time": "748.539"
    },
    {
        "id": "90d2613d",
        "text": "consider when we actually calculate the discrete fourier transform? Well, we have like a nice little hack here and basically the number of frequencies and that we consider is equal to the number of samples and that we have in the signal that we want to take the discrete fourier transform of OK. But why N should be equal to M for a couple of reasons. Reason number one is very important. And it's the, it's something that we've seen like in a previous video. And it's basically this idea that we can do, we can have like a round trip from the frequency domain to the time domain. And or I should say we start from the time domain, we move to the frequency domain. But then using uh an inverse for transform, we can go back to the um a time domain. And if we have like M which is equal to N, we basically can move like quite easily from the frequency from the time domain to the frequency domain. And then from the frequency domain back to the time domain.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=773s",
        "start_time": "773.099"
    },
    {
        "id": "fd49d443",
        "text": "OK. But why N should be equal to M for a couple of reasons. Reason number one is very important. And it's the, it's something that we've seen like in a previous video. And it's basically this idea that we can do, we can have like a round trip from the frequency domain to the time domain. And or I should say we start from the time domain, we move to the frequency domain. But then using uh an inverse for transform, we can go back to the um a time domain. And if we have like M which is equal to N, we basically can move like quite easily from the frequency from the time domain to the frequency domain. And then from the frequency domain back to the time domain. A second reason is because having M equal to N is very, is um efficient from a computational standpoint. So these are the two reasons why we use the same number of frequencies as the number of samples that we have. Now let's put into context what we've just said uh with words and take a look at the math at what happens at the actual",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=798s",
        "start_time": "798.01"
    },
    {
        "id": "abe2b429",
        "text": "And or I should say we start from the time domain, we move to the frequency domain. But then using uh an inverse for transform, we can go back to the um a time domain. And if we have like M which is equal to N, we basically can move like quite easily from the frequency from the time domain to the frequency domain. And then from the frequency domain back to the time domain. A second reason is because having M equal to N is very, is um efficient from a computational standpoint. So these are the two reasons why we use the same number of frequencies as the number of samples that we have. Now let's put into context what we've just said uh with words and take a look at the math at what happens at the actual uh with the actual uh formulas here. So we start with our original definition of the discrete fourier transform. And here we have like these two problems, right. Problem number one, we have like an infinite number of",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=817s",
        "start_time": "817.809"
    },
    {
        "id": "abba8ff1",
        "text": "A second reason is because having M equal to N is very, is um efficient from a computational standpoint. So these are the two reasons why we use the same number of frequencies as the number of samples that we have. Now let's put into context what we've just said uh with words and take a look at the math at what happens at the actual uh with the actual uh formulas here. So we start with our original definition of the discrete fourier transform. And here we have like these two problems, right. Problem number one, we have like an infinite number of in this sum because we have an infinite number of samples. Problem number two, we have like frequency which is continues. So we work our first hack. And what happens is that we get this guy over here.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=843s",
        "start_time": "843.919"
    },
    {
        "id": "297e980b",
        "text": "uh with the actual uh formulas here. So we start with our original definition of the discrete fourier transform. And here we have like these two problems, right. Problem number one, we have like an infinite number of in this sum because we have an infinite number of samples. Problem number two, we have like frequency which is continues. So we work our first hack. And what happens is that we get this guy over here. So now we're not summing an infinite number of samples, but rather we uh constrain we consider only an N number of samples and this is great. Now, we wave our magic when once, once again, and we finally get to the",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=868s",
        "start_time": "868.109"
    },
    {
        "id": "2bcd25ca",
        "text": "in this sum because we have an infinite number of samples. Problem number two, we have like frequency which is continues. So we work our first hack. And what happens is that we get this guy over here. So now we're not summing an infinite number of samples, but rather we uh constrain we consider only an N number of samples and this is great. Now, we wave our magic when once, once again, and we finally get to the definition of the discrete fourier transform as is used like throughout the world and perhaps the universe and it's this one over here. And as you can see like the last bit that we changed was this, instead of having the frequency F, we have this K divided",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=883s",
        "start_time": "883.135"
    },
    {
        "id": "880d1e9a",
        "text": "So now we're not summing an infinite number of samples, but rather we uh constrain we consider only an N number of samples and this is great. Now, we wave our magic when once, once again, and we finally get to the definition of the discrete fourier transform as is used like throughout the world and perhaps the universe and it's this one over here. And as you can see like the last bit that we changed was this, instead of having the frequency F, we have this K divided by N or capital N. Now we changed it here and we obviously changed it here as well because we had F here previously. OK. But what is this K thing? Well, to understand what is K,",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=898s",
        "start_time": "898.429"
    },
    {
        "id": "cccc5ec2",
        "text": "definition of the discrete fourier transform as is used like throughout the world and perhaps the universe and it's this one over here. And as you can see like the last bit that we changed was this, instead of having the frequency F, we have this K divided by N or capital N. Now we changed it here and we obviously changed it here as well because we had F here previously. OK. But what is this K thing? Well, to understand what is K, we should understand like where like K, the range of K and K uh ranges from zero to N minus one or in other words, from zero to N minus one. So we have N capital N uh numbers, capital N values, right? And this is in other words, like K has the same uh uh the range as N has, right? Uh",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=920s",
        "start_time": "920.489"
    },
    {
        "id": "74e12ae2",
        "text": "by N or capital N. Now we changed it here and we obviously changed it here as well because we had F here previously. OK. But what is this K thing? Well, to understand what is K, we should understand like where like K, the range of K and K uh ranges from zero to N minus one or in other words, from zero to N minus one. So we have N capital N uh numbers, capital N values, right? And this is in other words, like K has the same uh uh the range as N has, right? Uh So, but let's try to understand what this means. Like frequency wise. So",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=937s",
        "start_time": "937.729"
    },
    {
        "id": "31915cc5",
        "text": "we should understand like where like K, the range of K and K uh ranges from zero to N minus one or in other words, from zero to N minus one. So we have N capital N uh numbers, capital N values, right? And this is in other words, like K has the same uh uh the range as N has, right? Uh So, but let's try to understand what this means. Like frequency wise. So the frequency of a given value K and this frequency obviously is expressed in Hertz is given by this formula here. So it's K divided by N times T capital T where T is the sampling period.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=955s",
        "start_time": "955.39"
    },
    {
        "id": "57bb4016",
        "text": "So, but let's try to understand what this means. Like frequency wise. So the frequency of a given value K and this frequency obviously is expressed in Hertz is given by this formula here. So it's K divided by N times T capital T where T is the sampling period. If you guys remember from a previous video in this series, the sampling period is the inverse of the sampling rate. In other words, what we can do is a rewrite this formula like this. So K divided by capital N and all of these guys multiplied by the sample",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=985s",
        "start_time": "985.69"
    },
    {
        "id": "03779aa0",
        "text": "the frequency of a given value K and this frequency obviously is expressed in Hertz is given by this formula here. So it's K divided by N times T capital T where T is the sampling period. If you guys remember from a previous video in this series, the sampling period is the inverse of the sampling rate. In other words, what we can do is a rewrite this formula like this. So K divided by capital N and all of these guys multiplied by the sample rate. Now, what does this tell us? Well, this tell us that in other words, like the range of the uh frequency that we have as an output from the fourier transform is between zero when K is zero. And basically the sampling sampling rate.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=991s",
        "start_time": "991.869"
    },
    {
        "id": "ab1a50d6",
        "text": "If you guys remember from a previous video in this series, the sampling period is the inverse of the sampling rate. In other words, what we can do is a rewrite this formula like this. So K divided by capital N and all of these guys multiplied by the sample rate. Now, what does this tell us? Well, this tell us that in other words, like the range of the uh frequency that we have as an output from the fourier transform is between zero when K is zero. And basically the sampling sampling rate. So this is like the kind of like uh frequency range that we consider when we apply a fourier transform. And at the same time,",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1013s",
        "start_time": "1013.01"
    },
    {
        "id": "5b5c6280",
        "text": "rate. Now, what does this tell us? Well, this tell us that in other words, like the range of the uh frequency that we have as an output from the fourier transform is between zero when K is zero. And basically the sampling sampling rate. So this is like the kind of like uh frequency range that we consider when we apply a fourier transform. And at the same time, uh we take uh a number of divisions like of this range between zero and sampling rate that's equal to the number of samples that we have in the original signal. And that's given by capital N.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1032s",
        "start_time": "1032.65"
    },
    {
        "id": "9debb3a1",
        "text": "So this is like the kind of like uh frequency range that we consider when we apply a fourier transform. And at the same time, uh we take uh a number of divisions like of this range between zero and sampling rate that's equal to the number of samples that we have in the original signal. And that's given by capital N. OK. So now I want to show you uh something really, really cool. And so we're gonna take a look at the uh magnitude spectrum that comes out from a digital fourier transform. So if you guys remember, so the output of a fourier transform is the fourier coefficients which provides",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1052s",
        "start_time": "1052.5"
    },
    {
        "id": "07363dd3",
        "text": "uh we take uh a number of divisions like of this range between zero and sampling rate that's equal to the number of samples that we have in the original signal. And that's given by capital N. OK. So now I want to show you uh something really, really cool. And so we're gonna take a look at the uh magnitude spectrum that comes out from a digital fourier transform. So if you guys remember, so the output of a fourier transform is the fourier coefficients which provides information for each, at each frequency, they give us two parameters. One is the phase, the other one is the magnitude and the magnitude is the one that we are the most interested in when we are analyzing audio uh data. And that's because it tells us how much present a certain frequency is in an original signal, right? OK. So",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1065s",
        "start_time": "1065.64"
    },
    {
        "id": "85f628a6",
        "text": "OK. So now I want to show you uh something really, really cool. And so we're gonna take a look at the uh magnitude spectrum that comes out from a digital fourier transform. So if you guys remember, so the output of a fourier transform is the fourier coefficients which provides information for each, at each frequency, they give us two parameters. One is the phase, the other one is the magnitude and the magnitude is the one that we are the most interested in when we are analyzing audio uh data. And that's because it tells us how much present a certain frequency is in an original signal, right? OK. So uh what, what I, what I've done here is basically, I've just plotted the magnitude uh coefficient that comes out from a digital fourier discrete fourier transform. And as you can see here, we have like something that's really, really interesting.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1082s",
        "start_time": "1082.619"
    },
    {
        "id": "37639ba7",
        "text": "information for each, at each frequency, they give us two parameters. One is the phase, the other one is the magnitude and the magnitude is the one that we are the most interested in when we are analyzing audio uh data. And that's because it tells us how much present a certain frequency is in an original signal, right? OK. So uh what, what I, what I've done here is basically, I've just plotted the magnitude uh coefficient that comes out from a digital fourier discrete fourier transform. And as you can see here, we have like something that's really, really interesting. And on the X axis, obviously, we have like the frequency as expressed in Hertz. And here you can see that the original sampling rate was probably around 22,000 Hertz. I know what that was and it was like 22,050 Hertz to be precise. But what's happening here,",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1106s",
        "start_time": "1106.68"
    },
    {
        "id": "6a4f93a4",
        "text": "uh what, what I, what I've done here is basically, I've just plotted the magnitude uh coefficient that comes out from a digital fourier discrete fourier transform. And as you can see here, we have like something that's really, really interesting. And on the X axis, obviously, we have like the frequency as expressed in Hertz. And here you can see that the original sampling rate was probably around 22,000 Hertz. I know what that was and it was like 22,050 Hertz to be precise. But what's happening here, it seems like there's something crazy. So we have like a nice, like little like distribution of energy like in the lower frequencies. And then all of a sudden in the higher frequencies, we have again, like a lot of energy here in these higher frequencies. Is this something real or is this some kind of artifact? Well, if we inspect this like more closely we see that like these two",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1132s",
        "start_time": "1132.849"
    },
    {
        "id": "62443689",
        "text": "And on the X axis, obviously, we have like the frequency as expressed in Hertz. And here you can see that the original sampling rate was probably around 22,000 Hertz. I know what that was and it was like 22,050 Hertz to be precise. But what's happening here, it seems like there's something crazy. So we have like a nice, like little like distribution of energy like in the lower frequencies. And then all of a sudden in the higher frequencies, we have again, like a lot of energy here in these higher frequencies. Is this something real or is this some kind of artifact? Well, if we inspect this like more closely we see that like these two like energy like distributions are like the same, but they are mirrored. In other words, there's a central point of symmetry and what's happening like on the right of this symmetry is the mirrored version of the what what's happening like on the left hand side. In other words, we have redundancies. And",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1151s",
        "start_time": "1151.92"
    },
    {
        "id": "d1a46b49",
        "text": "it seems like there's something crazy. So we have like a nice, like little like distribution of energy like in the lower frequencies. And then all of a sudden in the higher frequencies, we have again, like a lot of energy here in these higher frequencies. Is this something real or is this some kind of artifact? Well, if we inspect this like more closely we see that like these two like energy like distributions are like the same, but they are mirrored. In other words, there's a central point of symmetry and what's happening like on the right of this symmetry is the mirrored version of the what what's happening like on the left hand side. In other words, we have redundancies. And uh if we take a look at this, like very careful, we discover that this happens when K is equal to N divided by two. In other words, at the central frequency, which is basically the sampling rate divided by two. So this is like kind of like a central symmetry.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1170s",
        "start_time": "1170.459"
    },
    {
        "id": "ed08aa94",
        "text": "like energy like distributions are like the same, but they are mirrored. In other words, there's a central point of symmetry and what's happening like on the right of this symmetry is the mirrored version of the what what's happening like on the left hand side. In other words, we have redundancies. And uh if we take a look at this, like very careful, we discover that this happens when K is equal to N divided by two. In other words, at the central frequency, which is basically the sampling rate divided by two. So this is like kind of like a central symmetry. And what this means like for our purposes is that we only need to consider the left hand side of this plot. So in other words, we are only interested like in up to sampling rate divided by two",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1198s",
        "start_time": "1198.17"
    },
    {
        "id": "1c40fb74",
        "text": "uh if we take a look at this, like very careful, we discover that this happens when K is equal to N divided by two. In other words, at the central frequency, which is basically the sampling rate divided by two. So this is like kind of like a central symmetry. And what this means like for our purposes is that we only need to consider the left hand side of this plot. So in other words, we are only interested like in up to sampling rate divided by two uh frequency, right? What happens like above that is just like a repetition of what we saw below that frequency. So this is like something to keep in mind when we are dealing with like fourier transforms or even like spectrograms as we'll see coming in the f in next videos. OK. But we've seen this particular frequency, that's the sampling ra uh rate divided by two",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1220s",
        "start_time": "1220.719"
    },
    {
        "id": "a0b2e3ad",
        "text": "And what this means like for our purposes is that we only need to consider the left hand side of this plot. So in other words, we are only interested like in up to sampling rate divided by two uh frequency, right? What happens like above that is just like a repetition of what we saw below that frequency. So this is like something to keep in mind when we are dealing with like fourier transforms or even like spectrograms as we'll see coming in the f in next videos. OK. But we've seen this particular frequency, that's the sampling ra uh rate divided by two already in a previous video. And that's called the Nyquist frequency. And that's the frequency, the kind of like a threshold above which we're not capable of reconstructing a digital signal into like its original analog signal without injecting some form of like a liaising. Now, if you want to like",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1242s",
        "start_time": "1242.4"
    },
    {
        "id": "b85a77ec",
        "text": "uh frequency, right? What happens like above that is just like a repetition of what we saw below that frequency. So this is like something to keep in mind when we are dealing with like fourier transforms or even like spectrograms as we'll see coming in the f in next videos. OK. But we've seen this particular frequency, that's the sampling ra uh rate divided by two already in a previous video. And that's called the Nyquist frequency. And that's the frequency, the kind of like a threshold above which we're not capable of reconstructing a digital signal into like its original analog signal without injecting some form of like a liaising. Now, if you want to like just like brush up a little bit like what like a liaising is and the N frequency I suggest you to go check out this video up here. One last thing that I want to cover is a particular algorithm that's very handy for calculating the discrete fourier transform. And not surprisingly because of how it works, it's called the fast fourier transform.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1259s",
        "start_time": "1259.01"
    },
    {
        "id": "2e11edd4",
        "text": "already in a previous video. And that's called the Nyquist frequency. And that's the frequency, the kind of like a threshold above which we're not capable of reconstructing a digital signal into like its original analog signal without injecting some form of like a liaising. Now, if you want to like just like brush up a little bit like what like a liaising is and the N frequency I suggest you to go check out this video up here. One last thing that I want to cover is a particular algorithm that's very handy for calculating the discrete fourier transform. And not surprisingly because of how it works, it's called the fast fourier transform. And this algorithm was discovered by um Gauss and fourier almost like at the same time. So the problem is that with the discrete fourier transform, we have like a way of like computing like this fourier coefficients that's quite expensive. The number of operations that we need to carry out is",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1284s",
        "start_time": "1284.609"
    },
    {
        "id": "4c70dbe3",
        "text": "just like brush up a little bit like what like a liaising is and the N frequency I suggest you to go check out this video up here. One last thing that I want to cover is a particular algorithm that's very handy for calculating the discrete fourier transform. And not surprisingly because of how it works, it's called the fast fourier transform. And this algorithm was discovered by um Gauss and fourier almost like at the same time. So the problem is that with the discrete fourier transform, we have like a way of like computing like this fourier coefficients that's quite expensive. The number of operations that we need to carry out is capital N squared, which is quite a lot if you have like a lot of um samples. So the great thing about the fast four A transform is that it, it is way more efficient. And the number of operations that we need to calculate here is given by N multiplied by log two of N.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1310s",
        "start_time": "1310.135"
    },
    {
        "id": "76501874",
        "text": "And this algorithm was discovered by um Gauss and fourier almost like at the same time. So the problem is that with the discrete fourier transform, we have like a way of like computing like this fourier coefficients that's quite expensive. The number of operations that we need to carry out is capital N squared, which is quite a lot if you have like a lot of um samples. So the great thing about the fast four A transform is that it, it is way more efficient. And the number of operations that we need to calculate here is given by N multiplied by log two of N. So how do we have like this uh the fast fourier transform? Well, I'm not going to get into the details here, but the kind of like high level intuition is that the fast fourier transform exploits like redundancies which happen",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1336s",
        "start_time": "1336.219"
    },
    {
        "id": "49be9a72",
        "text": "capital N squared, which is quite a lot if you have like a lot of um samples. So the great thing about the fast four A transform is that it, it is way more efficient. And the number of operations that we need to calculate here is given by N multiplied by log two of N. So how do we have like this uh the fast fourier transform? Well, I'm not going to get into the details here, but the kind of like high level intuition is that the fast fourier transform exploits like redundancies which happen across sinusoids. So in other words, you calculate like certain things only like once or like just like a few times because then you can reapply them uh in other like situations. And by doing so you can save a lot of computation, but",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1360s",
        "start_time": "1360.14"
    },
    {
        "id": "22471208",
        "text": "So how do we have like this uh the fast fourier transform? Well, I'm not going to get into the details here, but the kind of like high level intuition is that the fast fourier transform exploits like redundancies which happen across sinusoids. So in other words, you calculate like certain things only like once or like just like a few times because then you can reapply them uh in other like situations. And by doing so you can save a lot of computation, but there's no free launch like in this universe. Right. And so because of that, we have like certain constraints. And so if we want for the FFT to work, then we should choose like an N wisely and N should be a power of two. That's when we have N as a power of two. That's where like the, the, the kind of like the stars align. And",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1383s",
        "start_time": "1383.18"
    },
    {
        "id": "903879cb",
        "text": "across sinusoids. So in other words, you calculate like certain things only like once or like just like a few times because then you can reapply them uh in other like situations. And by doing so you can save a lot of computation, but there's no free launch like in this universe. Right. And so because of that, we have like certain constraints. And so if we want for the FFT to work, then we should choose like an N wisely and N should be a power of two. That's when we have N as a power of two. That's where like the, the, the kind of like the stars align. And then we can uh kind of unleash the power of the FFT algorithm. Now this is a very important algorithm and all the stuff that we see an implementation that we use. For example, the um fourier transform when we calculate dash language nun P or Saipi or even Li Breza uh actually rely on the fast fourier transform algorithm.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1399s",
        "start_time": "1399.454"
    },
    {
        "id": "adeea31e",
        "text": "there's no free launch like in this universe. Right. And so because of that, we have like certain constraints. And so if we want for the FFT to work, then we should choose like an N wisely and N should be a power of two. That's when we have N as a power of two. That's where like the, the, the kind of like the stars align. And then we can uh kind of unleash the power of the FFT algorithm. Now this is a very important algorithm and all the stuff that we see an implementation that we use. For example, the um fourier transform when we calculate dash language nun P or Saipi or even Li Breza uh actually rely on the fast fourier transform algorithm. OK. So that's it uh like for this video. Now you should have like a very good understanding of the fourier transform. So we started with intuition of the fourier transform. We moved to like com",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1415s",
        "start_time": "1415.939"
    },
    {
        "id": "45114763",
        "text": "then we can uh kind of unleash the power of the FFT algorithm. Now this is a very important algorithm and all the stuff that we see an implementation that we use. For example, the um fourier transform when we calculate dash language nun P or Saipi or even Li Breza uh actually rely on the fast fourier transform algorithm. OK. So that's it uh like for this video. Now you should have like a very good understanding of the fourier transform. So we started with intuition of the fourier transform. We moved to like com complex numbers and understood how we can actually use complex numbers for defining the fourier transform in a continuous context. Now, you know about the discrete version of the fourier transform. So you may be wondering",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1441s",
        "start_time": "1441.099"
    },
    {
        "id": "36ef570d",
        "text": "OK. So that's it uh like for this video. Now you should have like a very good understanding of the fourier transform. So we started with intuition of the fourier transform. We moved to like com complex numbers and understood how we can actually use complex numbers for defining the fourier transform in a continuous context. Now, you know about the discrete version of the fourier transform. So you may be wondering should we still be and hear and talk more about the fourier transform? And the answer is yes, definitely. So next time we'll leave like all of this theory behind us and we'll just like do a little bit of like of implementation and play around and extract the magnitude spectrum from a bunch of sounds and kind of like, I",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1466s",
        "start_time": "1466.63"
    },
    {
        "id": "6cded66f",
        "text": "complex numbers and understood how we can actually use complex numbers for defining the fourier transform in a continuous context. Now, you know about the discrete version of the fourier transform. So you may be wondering should we still be and hear and talk more about the fourier transform? And the answer is yes, definitely. So next time we'll leave like all of this theory behind us and we'll just like do a little bit of like of implementation and play around and extract the magnitude spectrum from a bunch of sounds and kind of like, I understand what they mean from a uh semantic viewpoint and understand how they differ among themselves. OK, so I hope you enjoyed this video. If that's the case, please leave a like if you haven't subscribed to the channel, please consider doing so. If you have any questions as usual, leave them in the comment section below. That's all for today. I'll see you next time. Cheers.",
        "video": "Discrete Fourier Transform Explained Easily",
        "playlist": "Audio Signal Processing for ML",
        "youtube_video_id": "ZUi_jdOyxIQ",
        "youtube_link": "https://www.youtube.com/watch?v=ZUi_jdOyxIQ&t=1482s",
        "start_time": "1482.334"
    },
    {
        "id": "a47fbdaf",
        "text": "Hi, everybody and welcome to another video in the Deep learning for audio with Python series. This time, we're gonna introduce a super exciting type of network recurrent network called a long short term memory network, right? But before getting into that, let's remember like what we've done uh last time and why we need this LST MS, right? So we la in the last video, we looked into simple R and MS and we saw that they're really good for time series type of data. But we also found out that they have a few issues mainly that they really don't have a long term memory and that's uh the network can't really uh use like context from the past. And that's because like it can't learn patterns with a long dependencies. And this is like a huge issue as we've seen last time because I mean, a lot of like audio music data uh depends like on long patterns. And so we need to find a way to overcome this issue",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=0s",
        "start_time": "0.129"
    },
    {
        "id": "a920811c",
        "text": "last time and why we need this LST MS, right? So we la in the last video, we looked into simple R and MS and we saw that they're really good for time series type of data. But we also found out that they have a few issues mainly that they really don't have a long term memory and that's uh the network can't really uh use like context from the past. And that's because like it can't learn patterns with a long dependencies. And this is like a huge issue as we've seen last time because I mean, a lot of like audio music data uh depends like on long patterns. And so we need to find a way to overcome this issue introducing long short term memory networks or LSD MS. So LSD, MS are a special type of recurrent neural networks. And the idea is that we have a memory cell that will enable us to learn longer term uh patterns.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=19s",
        "start_time": "19.895"
    },
    {
        "id": "c6462a13",
        "text": "and that's uh the network can't really uh use like context from the past. And that's because like it can't learn patterns with a long dependencies. And this is like a huge issue as we've seen last time because I mean, a lot of like audio music data uh depends like on long patterns. And so we need to find a way to overcome this issue introducing long short term memory networks or LSD MS. So LSD, MS are a special type of recurrent neural networks. And the idea is that we have a memory cell that will enable us to learn longer term uh patterns. Now don't get super excited here because uh LSC MS performed really well and they've helped us so much, for example, in music generation and a bunch of different o other tasks that have to do with audio. But the point being is that they detect pattern which",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=39s",
        "start_time": "39.799"
    },
    {
        "id": "893da4d3",
        "text": "introducing long short term memory networks or LSD MS. So LSD, MS are a special type of recurrent neural networks. And the idea is that we have a memory cell that will enable us to learn longer term uh patterns. Now don't get super excited here because uh LSC MS performed really well and they've helped us so much, for example, in music generation and a bunch of different o other tasks that have to do with audio. But the point being is that they detect pattern which up to like 100 steps, but they start to struggle when we have hundreds and hundreds here, let alone thousands of steps. So let's get started and understand how these LSDM networks uh work. But for doing that, we want to create a comparison between simple R and NS and LSD MS. Now, here we have a, a nice diagram",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=65s",
        "start_time": "65.069"
    },
    {
        "id": "12ef64e6",
        "text": "Now don't get super excited here because uh LSC MS performed really well and they've helped us so much, for example, in music generation and a bunch of different o other tasks that have to do with audio. But the point being is that they detect pattern which up to like 100 steps, but they start to struggle when we have hundreds and hundreds here, let alone thousands of steps. So let's get started and understand how these LSDM networks uh work. But for doing that, we want to create a comparison between simple R and NS and LSD MS. Now, here we have a, a nice diagram where we have a, a simple R and N unrolled. Now, this is not my diagram and I'm using like this super cool graphics uh that come from a, an article that's like super important like in the community. And it's a blog post that's called Understanding uh LSDM networks. And I linked uh the article below in the description",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=84s",
        "start_time": "84.489"
    },
    {
        "id": "3db17b30",
        "text": "up to like 100 steps, but they start to struggle when we have hundreds and hundreds here, let alone thousands of steps. So let's get started and understand how these LSDM networks uh work. But for doing that, we want to create a comparison between simple R and NS and LSD MS. Now, here we have a, a nice diagram where we have a, a simple R and N unrolled. Now, this is not my diagram and I'm using like this super cool graphics uh that come from a, an article that's like super important like in the community. And it's a blog post that's called Understanding uh LSDM networks. And I linked uh the article below in the description uh section of this video and I urge you to just like go there and check that out cause like it has, oh it's really, really good to understand uh like LSC MS and it's a bit like complementary to what I'm presenting here.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=104s",
        "start_time": "104.23"
    },
    {
        "id": "9d34eee1",
        "text": "where we have a, a simple R and N unrolled. Now, this is not my diagram and I'm using like this super cool graphics uh that come from a, an article that's like super important like in the community. And it's a blog post that's called Understanding uh LSDM networks. And I linked uh the article below in the description uh section of this video and I urge you to just like go there and check that out cause like it has, oh it's really, really good to understand uh like LSC MS and it's a bit like complementary to what I'm presenting here. Now, getting back right to the good stuff. So here we have this diagram where we have an unrolled uh like recurrent layer. Uh but with a simple RNM. And if you guys remember, remember from last time, so here the memory cell itself is a simple dense layer with a tan H hyperbolic tangent activation function. And",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=130s",
        "start_time": "130.449"
    },
    {
        "id": "81eff7b6",
        "text": "uh section of this video and I urge you to just like go there and check that out cause like it has, oh it's really, really good to understand uh like LSC MS and it's a bit like complementary to what I'm presenting here. Now, getting back right to the good stuff. So here we have this diagram where we have an unrolled uh like recurrent layer. Uh but with a simple RNM. And if you guys remember, remember from last time, so here the memory cell itself is a simple dense layer with a tan H hyperbolic tangent activation function. And uh we have both the inputs at time T so XT and the states uh vector from the previous time step that contribute and now con concatenate it together and fed into the TH and then we get like an output out there and the output is like is twofold, right? So we have like the output",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=155s",
        "start_time": "155.009"
    },
    {
        "id": "e9f5cab2",
        "text": "Now, getting back right to the good stuff. So here we have this diagram where we have an unrolled uh like recurrent layer. Uh but with a simple RNM. And if you guys remember, remember from last time, so here the memory cell itself is a simple dense layer with a tan H hyperbolic tangent activation function. And uh we have both the inputs at time T so XT and the states uh vector from the previous time step that contribute and now con concatenate it together and fed into the TH and then we get like an output out there and the output is like is twofold, right? So we have like the output and then we have the new uh current state which is like the, the state vector, but like output and state vector are the same. Now,",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=167s",
        "start_time": "167.889"
    },
    {
        "id": "1752e240",
        "text": "uh we have both the inputs at time T so XT and the states uh vector from the previous time step that contribute and now con concatenate it together and fed into the TH and then we get like an output out there and the output is like is twofold, right? So we have like the output and then we have the new uh current state which is like the, the state vector, but like output and state vector are the same. Now, let's take a look at LSD MS and here we go,",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=192s",
        "start_time": "192.169"
    },
    {
        "id": "3844dc7f",
        "text": "and then we have the new uh current state which is like the, the state vector, but like output and state vector are the same. Now, let's take a look at LSD MS and here we go, it's a little bit different, right? So, I mean, the whole architecture is basically the very same. So we can unroll an LSTM like in the same way that we do with a simple RNN unit. What really changes? It's this guy here, which is basically the cell itself. So there are a bunch of different things that go on uh like in the cell that like the, the simple R and N cell. Uh yeah, doesn't do at all.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=216s",
        "start_time": "216.679"
    },
    {
        "id": "690114b8",
        "text": "let's take a look at LSD MS and here we go, it's a little bit different, right? So, I mean, the whole architecture is basically the very same. So we can unroll an LSTM like in the same way that we do with a simple RNN unit. What really changes? It's this guy here, which is basically the cell itself. So there are a bunch of different things that go on uh like in the cell that like the, the simple R and N cell. Uh yeah, doesn't do at all. And it's all of these things that will enable us with an LSDM to learn longer term uh patterns.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=227s",
        "start_time": "227.679"
    },
    {
        "id": "a4db61de",
        "text": "it's a little bit different, right? So, I mean, the whole architecture is basically the very same. So we can unroll an LSTM like in the same way that we do with a simple RNN unit. What really changes? It's this guy here, which is basically the cell itself. So there are a bunch of different things that go on uh like in the cell that like the, the simple R and N cell. Uh yeah, doesn't do at all. And it's all of these things that will enable us with an LSDM to learn longer term uh patterns. Cool. OK. So now let's take a look at an LLSDM cell from a very high uh point of view. So you may have noticed this but uh an LSDM cell contains a simple R and M cell. It's a tan h um dense layer and we'll see this in a while. But I mean,",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=233s",
        "start_time": "233.47"
    },
    {
        "id": "f9193add",
        "text": "And it's all of these things that will enable us with an LSDM to learn longer term uh patterns. Cool. OK. So now let's take a look at an LLSDM cell from a very high uh point of view. So you may have noticed this but uh an LSDM cell contains a simple R and M cell. It's a tan h um dense layer and we'll see this in a while. But I mean, so the, the idea is that you can think of like this LSTM as a uh as a kind of like augmentation of a simple RNM cell where we have like these RNM cell and then a bunch of other components. So",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=263s",
        "start_time": "263.45"
    },
    {
        "id": "574dd76b",
        "text": "Cool. OK. So now let's take a look at an LLSDM cell from a very high uh point of view. So you may have noticed this but uh an LSDM cell contains a simple R and M cell. It's a tan h um dense layer and we'll see this in a while. But I mean, so the, the idea is that you can think of like this LSTM as a uh as a kind of like augmentation of a simple RNM cell where we have like these RNM cell and then a bunch of other components. So one of the most important components, there is a second state vector that we can call the cell state. And basically this is the one that uh will uh have like information about long term memory. So it's kinds of stores uh patterns like that are longer term.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=271s",
        "start_time": "271.85"
    },
    {
        "id": "5c275227",
        "text": "so the, the idea is that you can think of like this LSTM as a uh as a kind of like augmentation of a simple RNM cell where we have like these RNM cell and then a bunch of other components. So one of the most important components, there is a second state vector that we can call the cell state. And basically this is the one that uh will uh have like information about long term memory. So it's kinds of stores uh patterns like that are longer term. And then we have a bunch of like gates, uh the four gate gates, the input gate and the output gates and all of these gates which are basically connected with a, with a sigmoid dense layer uh acts as a filter. So they filter uh the information and then decide like what to forget what to input and what to output.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=294s",
        "start_time": "294.549"
    },
    {
        "id": "9186dd61",
        "text": "one of the most important components, there is a second state vector that we can call the cell state. And basically this is the one that uh will uh have like information about long term memory. So it's kinds of stores uh patterns like that are longer term. And then we have a bunch of like gates, uh the four gate gates, the input gate and the output gates and all of these gates which are basically connected with a, with a sigmoid dense layer uh acts as a filter. So they filter uh the information and then decide like what to forget what to input and what to output. Now don't be scared about all of this complexity because we're gonna break down all of these things one by one. So and here we have it in all of its beauty, the LSDM cell. So there are a bunch of things here. So let's start like from the simple ones. So just like the things that more or less we already know. So it is X uh T it's basically just like the input. And so this is like a data point.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=307s",
        "start_time": "307.66"
    },
    {
        "id": "dcf18489",
        "text": "And then we have a bunch of like gates, uh the four gate gates, the input gate and the output gates and all of these gates which are basically connected with a, with a sigmoid dense layer uh acts as a filter. So they filter uh the information and then decide like what to forget what to input and what to output. Now don't be scared about all of this complexity because we're gonna break down all of these things one by one. So and here we have it in all of its beauty, the LSDM cell. So there are a bunch of things here. So let's start like from the simple ones. So just like the things that more or less we already know. So it is X uh T it's basically just like the input. And so this is like a data point. And let's remember here, we are analyzing like the, the, the behavior of this cell like at each time step. And so XD is just like a point in a sequence, right, a sample",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=325s",
        "start_time": "325.26"
    },
    {
        "id": "1285691b",
        "text": "Now don't be scared about all of this complexity because we're gonna break down all of these things one by one. So and here we have it in all of its beauty, the LSDM cell. So there are a bunch of things here. So let's start like from the simple ones. So just like the things that more or less we already know. So it is X uh T it's basically just like the input. And so this is like a data point. And let's remember here, we are analyzing like the, the, the behavior of this cell like at each time step. And so XD is just like a point in a sequence, right, a sample good. Then we have the output over here which is HT and it's just like the output of this cell, which is somehow connected with the hidden state because it's basically the same thing.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=349s",
        "start_time": "349.549"
    },
    {
        "id": "4dc44f62",
        "text": "And let's remember here, we are analyzing like the, the, the behavior of this cell like at each time step. And so XD is just like a point in a sequence, right, a sample good. Then we have the output over here which is HT and it's just like the output of this cell, which is somehow connected with the hidden state because it's basically the same thing. So the output and the hidden states are the same thing. And then we have this secondary um state vector that's called uh the cell state over here which we call uh CT. So it's the cell state at time step T good. So now let's move on. So here you might have seen this but this is our simple RNN cell which is already in the LSTM here.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=377s",
        "start_time": "377.47"
    },
    {
        "id": "f333ed1e",
        "text": "good. Then we have the output over here which is HT and it's just like the output of this cell, which is somehow connected with the hidden state because it's basically the same thing. So the output and the hidden states are the same thing. And then we have this secondary um state vector that's called uh the cell state over here which we call uh CT. So it's the cell state at time step T good. So now let's move on. So here you might have seen this but this is our simple RNN cell which is already in the LSTM here. And as you can see, so here we have this tan H and this is a dense layer now with like this um all of these units here with this sigmoid thing, Sigma tan H uh which like the yellow box that represents dense layers with uh an activation function that in this case is 10 tan H. And in this other case is is like a sigmoid function but going back to the simple RNM cell.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=391s",
        "start_time": "391.109"
    },
    {
        "id": "a5ce0390",
        "text": "So the output and the hidden states are the same thing. And then we have this secondary um state vector that's called uh the cell state over here which we call uh CT. So it's the cell state at time step T good. So now let's move on. So here you might have seen this but this is our simple RNN cell which is already in the LSTM here. And as you can see, so here we have this tan H and this is a dense layer now with like this um all of these units here with this sigmoid thing, Sigma tan H uh which like the yellow box that represents dense layers with uh an activation function that in this case is 10 tan H. And in this other case is is like a sigmoid function but going back to the simple RNM cell. So this is a simple R and M cell. So we have a tonic um a dense layer and the input is a uh is XT. So the basic like the, the, the data as well as the HT minus one, which is the state vector or a hidden state from the previous time step good.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=407s",
        "start_time": "407.64"
    },
    {
        "id": "bbed989c",
        "text": "And as you can see, so here we have this tan H and this is a dense layer now with like this um all of these units here with this sigmoid thing, Sigma tan H uh which like the yellow box that represents dense layers with uh an activation function that in this case is 10 tan H. And in this other case is is like a sigmoid function but going back to the simple RNM cell. So this is a simple R and M cell. So we have a tonic um a dense layer and the input is a uh is XT. So the basic like the, the, the data as well as the HT minus one, which is the state vector or a hidden state from the previous time step good. OK. So let's take a look at this hidden state. So we can think of this hidden state as the short term memory of the LSTM. So keeps information uh kind of like the, the stuff like that's happening like in the, in the last uh like in the current state. And it kind of like uh yeah stores that kind of information. Then",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=435s",
        "start_time": "435.7"
    },
    {
        "id": "8cfba769",
        "text": "So this is a simple R and M cell. So we have a tonic um a dense layer and the input is a uh is XT. So the basic like the, the, the data as well as the HT minus one, which is the state vector or a hidden state from the previous time step good. OK. So let's take a look at this hidden state. So we can think of this hidden state as the short term memory of the LSTM. So keeps information uh kind of like the, the stuff like that's happening like in the, in the last uh like in the current state. And it kind of like uh yeah stores that kind of information. Then uh we also have like this secondary state vector which we can call the cell state. And this cell state is the one responsible for the long term memory. So for storing uh longer term uh dependencies and patterns. And as you can see here, uh the the cell state flows quite nicely",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=465s",
        "start_time": "465.095"
    },
    {
        "id": "4f8030c7",
        "text": "OK. So let's take a look at this hidden state. So we can think of this hidden state as the short term memory of the LSTM. So keeps information uh kind of like the, the stuff like that's happening like in the, in the last uh like in the current state. And it kind of like uh yeah stores that kind of information. Then uh we also have like this secondary state vector which we can call the cell state. And this cell state is the one responsible for the long term memory. So for storing uh longer term uh dependencies and patterns. And as you can see here, uh the the cell state flows quite nicely through the LSTM and it's only updated in two points. So here where we have like this multiplication which is a element wise multiplication, we'll see this in a while. But so here we have like this multiplication and here we have like this sum element wise sum between like two matrices that we'll see. So all of this to say that uh for the cell state, we have very few computations, just two.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=495s",
        "start_time": "495.679"
    },
    {
        "id": "7cd89f4a",
        "text": "uh we also have like this secondary state vector which we can call the cell state. And this cell state is the one responsible for the long term memory. So for storing uh longer term uh dependencies and patterns. And as you can see here, uh the the cell state flows quite nicely through the LSTM and it's only updated in two points. So here where we have like this multiplication which is a element wise multiplication, we'll see this in a while. But so here we have like this multiplication and here we have like this sum element wise sum between like two matrices that we'll see. So all of this to say that uh for the cell state, we have very few computations, just two. And when you have few competitions, the result is that you can stabilize the gradients. And so you are kind of like a better suited to avoid vanishing gradients, which is like the main issue with training rnns cool. OK. So let's take a look at this two updates that we have like a little bit like more um specifically. So the first one",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=521s",
        "start_time": "521.2"
    },
    {
        "id": "4023853b",
        "text": "through the LSTM and it's only updated in two points. So here where we have like this multiplication which is a element wise multiplication, we'll see this in a while. But so here we have like this multiplication and here we have like this sum element wise sum between like two matrices that we'll see. So all of this to say that uh for the cell state, we have very few computations, just two. And when you have few competitions, the result is that you can stabilize the gradients. And so you are kind of like a better suited to avoid vanishing gradients, which is like the main issue with training rnns cool. OK. So let's take a look at this two updates that we have like a little bit like more um specifically. So the first one uh decides what to forget. So when we get like to that point, the cell state uh gets uh updated and we decide what to forget from this long term memory state factor. The second one decides what new info to add to the cell state. And so if you think about this, it's basically,",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=544s",
        "start_time": "544.219"
    },
    {
        "id": "00a698f2",
        "text": "And when you have few competitions, the result is that you can stabilize the gradients. And so you are kind of like a better suited to avoid vanishing gradients, which is like the main issue with training rnns cool. OK. So let's take a look at this two updates that we have like a little bit like more um specifically. So the first one uh decides what to forget. So when we get like to that point, the cell state uh gets uh updated and we decide what to forget from this long term memory state factor. The second one decides what new info to add to the cell state. And so if you think about this, it's basically, so the C state uh gives us like uh tries to keep track of the most important information. It drops the things that are like less important and it adds things that are very important. And so like when we train the network,",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=574s",
        "start_time": "574.049"
    },
    {
        "id": "827944d1",
        "text": "uh decides what to forget. So when we get like to that point, the cell state uh gets uh updated and we decide what to forget from this long term memory state factor. The second one decides what new info to add to the cell state. And so if you think about this, it's basically, so the C state uh gives us like uh tries to keep track of the most important information. It drops the things that are like less important and it adds things that are very important. And so like when we train the network, we basically train an LSTM to be very effective at understanding which patterns to forget because they are not that important all in all and which patterns to uh to remember because they are super important for our uh tasks. Good. OK. So now we're gonna look into the full get segment. I would call it like this wave for get component of the LSTM.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=599s",
        "start_time": "599.799"
    },
    {
        "id": "dce1930c",
        "text": "so the C state uh gives us like uh tries to keep track of the most important information. It drops the things that are like less important and it adds things that are very important. And so like when we train the network, we basically train an LSTM to be very effective at understanding which patterns to forget because they are not that important all in all and which patterns to uh to remember because they are super important for our uh tasks. Good. OK. So now we're gonna look into the full get segment. I would call it like this wave for get component of the LSTM. And so here, as we said, like this component uh yeah, it's uh kind of responsible for forgetting stuff from the cell state. OK.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=623s",
        "start_time": "623.239"
    },
    {
        "id": "7a0446d7",
        "text": "we basically train an LSTM to be very effective at understanding which patterns to forget because they are not that important all in all and which patterns to uh to remember because they are super important for our uh tasks. Good. OK. So now we're gonna look into the full get segment. I would call it like this wave for get component of the LSTM. And so here, as we said, like this component uh yeah, it's uh kind of responsible for forgetting stuff from the cell state. OK. So I'm gonna drop some math here",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=640s",
        "start_time": "640.659"
    },
    {
        "id": "b1cdecbd",
        "text": "And so here, as we said, like this component uh yeah, it's uh kind of responsible for forgetting stuff from the cell state. OK. So I'm gonna drop some math here and I don't be scared about that because like it's quite intuitive. And if you've followed so far, like the, the series, uh it's not really that different from the stuff that I've uh covered when we started looking into uh computation and neural networks. And uh yeah, and all of that stuff. But if you don't remember that I have the uh video for that should be over here just like click that and check that out.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=669s",
        "start_time": "669.909"
    },
    {
        "id": "c1e1acfe",
        "text": "So I'm gonna drop some math here and I don't be scared about that because like it's quite intuitive. And if you've followed so far, like the, the series, uh it's not really that different from the stuff that I've uh covered when we started looking into uh computation and neural networks. And uh yeah, and all of that stuff. But if you don't remember that I have the uh video for that should be over here just like click that and check that out. OK. So back to the important stuff now. So we have this FT which is a forget uh matrix uh for T and uh it is like the results of the forget gate and the forget gate being like this guy here, like this sigma um dense layer over here.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=681s",
        "start_time": "681.39"
    },
    {
        "id": "17075438",
        "text": "and I don't be scared about that because like it's quite intuitive. And if you've followed so far, like the, the series, uh it's not really that different from the stuff that I've uh covered when we started looking into uh computation and neural networks. And uh yeah, and all of that stuff. But if you don't remember that I have the uh video for that should be over here just like click that and check that out. OK. So back to the important stuff now. So we have this FT which is a forget uh matrix uh for T and uh it is like the results of the forget gate and the forget gate being like this guy here, like this sigma um dense layer over here. So what happens here? It's very simple. So we concatenate the input at the current time step with the uh state vector, the hidden vector from the previous time step over a year.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=684s",
        "start_time": "684.989"
    },
    {
        "id": "a997e192",
        "text": "OK. So back to the important stuff now. So we have this FT which is a forget uh matrix uh for T and uh it is like the results of the forget gate and the forget gate being like this guy here, like this sigma um dense layer over here. So what happens here? It's very simple. So we concatenate the input at the current time step with the uh state vector, the hidden vector from the previous time step over a year. And uh we concatenate that and then we apply uh a segment function to like this guy here. And then here we have this BF it's just like a biased term. And this WF is the weight matrix uh for like this dense layer for the forget layer good.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=712s",
        "start_time": "712.789"
    },
    {
        "id": "02a919bb",
        "text": "So what happens here? It's very simple. So we concatenate the input at the current time step with the uh state vector, the hidden vector from the previous time step over a year. And uh we concatenate that and then we apply uh a segment function to like this guy here. And then here we have this BF it's just like a biased term. And this WF is the weight matrix uh for like this dense layer for the forget layer good. And so once we do this, we get a matrix uh that's uh that's like the filter for what we should forget. And we'll get to that in a second. But basically, when we calculate that we are at this point here, so we've just gone through the forget gate now.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=736s",
        "start_time": "736.51"
    },
    {
        "id": "7030b582",
        "text": "And uh we concatenate that and then we apply uh a segment function to like this guy here. And then here we have this BF it's just like a biased term. And this WF is the weight matrix uh for like this dense layer for the forget layer good. And so once we do this, we get a matrix uh that's uh that's like the filter for what we should forget. And we'll get to that in a second. But basically, when we calculate that we are at this point here, so we've just gone through the forget gate now. Uh we are using a sigmoid function. And uh if you guys remember the sigmoid function like shrinks the output between zero and one. And this is great to use as a filter because basically, we're gonna have like for all of the values of this FT matrix, a value between zero and one.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=751s",
        "start_time": "751.02"
    },
    {
        "id": "a3c07f0c",
        "text": "And so once we do this, we get a matrix uh that's uh that's like the filter for what we should forget. And we'll get to that in a second. But basically, when we calculate that we are at this point here, so we've just gone through the forget gate now. Uh we are using a sigmoid function. And uh if you guys remember the sigmoid function like shrinks the output between zero and one. And this is great to use as a filter because basically, we're gonna have like for all of the values of this FT matrix, a value between zero and one. So the things that are the uh the indexes that are closer to zeros are the relative indexes that we're gonna forget in the cell state. Whereas like when we have indexes with values that are closer to one, we're gonna keep those values. So zero, forget one is remember. But now how do we forget stuff or how do we remember stuff? Yeah, that's when",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=771s",
        "start_time": "771.19"
    },
    {
        "id": "edf40499",
        "text": "Uh we are using a sigmoid function. And uh if you guys remember the sigmoid function like shrinks the output between zero and one. And this is great to use as a filter because basically, we're gonna have like for all of the values of this FT matrix, a value between zero and one. So the things that are the uh the indexes that are closer to zeros are the relative indexes that we're gonna forget in the cell state. Whereas like when we have indexes with values that are closer to one, we're gonna keep those values. So zero, forget one is remember. But now how do we forget stuff or how do we remember stuff? Yeah, that's when these element wise multiplication kicks in. So what we do basically is we take the cell state at T minus one. So the the cell state like from the previous um time step and then we perform an element wise multiplication with the FT matrix. But obviously to perform element wise multiplication, you need to have like these two matrices that have the same dimension.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=790s",
        "start_time": "790.15"
    },
    {
        "id": "de9fc3bb",
        "text": "So the things that are the uh the indexes that are closer to zeros are the relative indexes that we're gonna forget in the cell state. Whereas like when we have indexes with values that are closer to one, we're gonna keep those values. So zero, forget one is remember. But now how do we forget stuff or how do we remember stuff? Yeah, that's when these element wise multiplication kicks in. So what we do basically is we take the cell state at T minus one. So the the cell state like from the previous um time step and then we perform an element wise multiplication with the FT matrix. But obviously to perform element wise multiplication, you need to have like these two matrices that have the same dimension. And so we can just like multiply element by element there. And the result is this CTF which uh is a very heavy kind of like uh yeah convention to say this is like the the cell state from uh the previous time step where we decide what to forget at this time step, right? OK.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=814s",
        "start_time": "814.39"
    },
    {
        "id": "7735c863",
        "text": "these element wise multiplication kicks in. So what we do basically is we take the cell state at T minus one. So the the cell state like from the previous um time step and then we perform an element wise multiplication with the FT matrix. But obviously to perform element wise multiplication, you need to have like these two matrices that have the same dimension. And so we can just like multiply element by element there. And the result is this CTF which uh is a very heavy kind of like uh yeah convention to say this is like the the cell state from uh the previous time step where we decide what to forget at this time step, right? OK. So this could feel a little bit abstract. So I'm gonna provide you with an example. OK. So we have like our nice equation over here and here, let's say we have like the cell states uh that's given by three values, right? Uh OK. So it's 124. So, and this is the cell state at T minus one. And then",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=841s",
        "start_time": "841.84"
    },
    {
        "id": "18660c0e",
        "text": "And so we can just like multiply element by element there. And the result is this CTF which uh is a very heavy kind of like uh yeah convention to say this is like the the cell state from uh the previous time step where we decide what to forget at this time step, right? OK. So this could feel a little bit abstract. So I'm gonna provide you with an example. OK. So we have like our nice equation over here and here, let's say we have like the cell states uh that's given by three values, right? Uh OK. So it's 124. So, and this is the cell state at T minus one. And then uh we've uh we've like calculated the input gates and we have this uh value over here for FT which is 101. Now let's try to get to CTF. So",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=870s",
        "start_time": "870.969"
    },
    {
        "id": "6ff6a7ef",
        "text": "So this could feel a little bit abstract. So I'm gonna provide you with an example. OK. So we have like our nice equation over here and here, let's say we have like the cell states uh that's given by three values, right? Uh OK. So it's 124. So, and this is the cell state at T minus one. And then uh we've uh we've like calculated the input gates and we have this uh value over here for FT which is 101. Now let's try to get to CTF. So how should we do that? Well, that's super simple. So we take uh CT minus one and we element wise multiply with FT. So we have 124 multiplied by 101. And if you do multiplication element index by index, so one by one is 12 by zero is 04 by one is four. So the result is 104. So here we've decided what to retain and what to forget.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=897s",
        "start_time": "897.89"
    },
    {
        "id": "5ac400a6",
        "text": "uh we've uh we've like calculated the input gates and we have this uh value over here for FT which is 101. Now let's try to get to CTF. So how should we do that? Well, that's super simple. So we take uh CT minus one and we element wise multiply with FT. So we have 124 multiplied by 101. And if you do multiplication element index by index, so one by one is 12 by zero is 04 by one is four. So the result is 104. So here we've decided what to retain and what to forget. So take a look at the first item and the third item. So index zero and two like in this list of CT minus one.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=922s",
        "start_time": "922.26"
    },
    {
        "id": "1f43dca0",
        "text": "how should we do that? Well, that's super simple. So we take uh CT minus one and we element wise multiply with FT. So we have 124 multiplied by 101. And if you do multiplication element index by index, so one by one is 12 by zero is 04 by one is four. So the result is 104. So here we've decided what to retain and what to forget. So take a look at the first item and the third item. So index zero and two like in this list of CT minus one. And the result is that we are keeping that information in and why is that? Well, that's because the FT is equal to one for those two indexes. So the filter that we are using is just like telling us. Yeah, I want to keep that information because I believe it's important.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=940s",
        "start_time": "940.03"
    },
    {
        "id": "1dab9228",
        "text": "So take a look at the first item and the third item. So index zero and two like in this list of CT minus one. And the result is that we are keeping that information in and why is that? Well, that's because the FT is equal to one for those two indexes. So the filter that we are using is just like telling us. Yeah, I want to keep that information because I believe it's important. What about the poor second index in CTF there? Well, unfortunately, we are dropping that as you can see here. So it becomes zero. And the reason why it's very easy to understand is because the in the, the forgets uh matrix there, we have like for the correspondence um uh index zero which acts like as a, as a filter that drops that value,",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=968s",
        "start_time": "968.44"
    },
    {
        "id": "3a7fbf0c",
        "text": "And the result is that we are keeping that information in and why is that? Well, that's because the FT is equal to one for those two indexes. So the filter that we are using is just like telling us. Yeah, I want to keep that information because I believe it's important. What about the poor second index in CTF there? Well, unfortunately, we are dropping that as you can see here. So it becomes zero. And the reason why it's very easy to understand is because the in the, the forgets uh matrix there, we have like for the correspondence um uh index zero which acts like as a, as a filter that drops that value, right? So now we have an understanding of how like this forget thing uh works on the cell state. OK. So now let's move on to uh the next step. So we said that we have forget input and output as like the main components of a an LSTM. So now let's focus an input. So the input here",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=979s",
        "start_time": "979.059"
    },
    {
        "id": "1d92688e",
        "text": "What about the poor second index in CTF there? Well, unfortunately, we are dropping that as you can see here. So it becomes zero. And the reason why it's very easy to understand is because the in the, the forgets uh matrix there, we have like for the correspondence um uh index zero which acts like as a, as a filter that drops that value, right? So now we have an understanding of how like this forget thing uh works on the cell state. OK. So now let's move on to uh the next step. So we said that we have forget input and output as like the main components of a an LSTM. So now let's focus an input. So the input here um it's kind of like it's made by two parts, right? So we have like our uh simple R and N module which is this tan h uh dense layer and then we have the uh input gate which is this sigmoid uh dense layer over here.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=999s",
        "start_time": "999.77"
    },
    {
        "id": "4eec98be",
        "text": "right? So now we have an understanding of how like this forget thing uh works on the cell state. OK. So now let's move on to uh the next step. So we said that we have forget input and output as like the main components of a an LSTM. So now let's focus an input. So the input here um it's kind of like it's made by two parts, right? So we have like our uh simple R and N module which is this tan h uh dense layer and then we have the uh input gate which is this sigmoid uh dense layer over here. OK. So for the time being, let's calculate the input, let's process the input gate and, and get like a, a matrix out of it, right? And this is gonna act as a filter on the simple R and M component, right? Let's see how this works. So for the, this it, which is the, yeah, as, as I said, like the, the results of the input gate, we we're gonna get like a",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1027s",
        "start_time": "1027.81"
    },
    {
        "id": "bda59033",
        "text": "um it's kind of like it's made by two parts, right? So we have like our uh simple R and N module which is this tan h uh dense layer and then we have the uh input gate which is this sigmoid uh dense layer over here. OK. So for the time being, let's calculate the input, let's process the input gate and, and get like a, a matrix out of it, right? And this is gonna act as a filter on the simple R and M component, right? Let's see how this works. So for the, this it, which is the, yeah, as, as I said, like the, the results of the input gate, we we're gonna get like a um again a matrix that has like the same uh dimensionality uh as the, as what comes out of like the tonic uh layer over here. And we get it by just using a sigmoid function that we apply to the",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1050s",
        "start_time": "1050.76"
    },
    {
        "id": "924f63ce",
        "text": "OK. So for the time being, let's calculate the input, let's process the input gate and, and get like a, a matrix out of it, right? And this is gonna act as a filter on the simple R and M component, right? Let's see how this works. So for the, this it, which is the, yeah, as, as I said, like the, the results of the input gate, we we're gonna get like a um again a matrix that has like the same uh dimensionality uh as the, as what comes out of like the tonic uh layer over here. And we get it by just using a sigmoid function that we apply to the concatenation of hat minus one and XT and to which like we apply or we multiply this W I uh metrics. And obviously, as always, we have like a bias term here, but let's not bother about that. Cool. So here,",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1071s",
        "start_time": "1071.06"
    },
    {
        "id": "0fca7c9a",
        "text": "um again a matrix that has like the same uh dimensionality uh as the, as what comes out of like the tonic uh layer over here. And we get it by just using a sigmoid function that we apply to the concatenation of hat minus one and XT and to which like we apply or we multiply this W I uh metrics. And obviously, as always, we have like a bias term here, but let's not bother about that. Cool. So here, uh with this, it, we have like this value this matrix that comes like at this point. Now let's see what happens like with at, at the other point over here. So we're basically",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1099s",
        "start_time": "1099.67"
    },
    {
        "id": "c0457b25",
        "text": "concatenation of hat minus one and XT and to which like we apply or we multiply this W I uh metrics. And obviously, as always, we have like a bias term here, but let's not bother about that. Cool. So here, uh with this, it, we have like this value this matrix that comes like at this point. Now let's see what happens like with at, at the other point over here. So we're basically uh gonna build a uh CT prime. So which is basically like the new cell state. Uh That is a function obviously of the um uh hidden state at the previous time step as well as the um input data at the current time uh time step. And the two again are combined concatenation together",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1119s",
        "start_time": "1119.079"
    },
    {
        "id": "06212909",
        "text": "uh with this, it, we have like this value this matrix that comes like at this point. Now let's see what happens like with at, at the other point over here. So we're basically uh gonna build a uh CT prime. So which is basically like the new cell state. Uh That is a function obviously of the um uh hidden state at the previous time step as well as the um input data at the current time uh time step. And the two again are combined concatenation together cool. But this time we are using a tan H",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1137s",
        "start_time": "1137.099"
    },
    {
        "id": "c2220e2f",
        "text": "uh gonna build a uh CT prime. So which is basically like the new cell state. Uh That is a function obviously of the um uh hidden state at the previous time step as well as the um input data at the current time uh time step. And the two again are combined concatenation together cool. But this time we are using a tan H to a hyperbolic tangent uh as the nonlinearity. And now this is like where we are at at this point. So this value is C prime CT prime is over here good. So after the tan each layer",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1151s",
        "start_time": "1151.04"
    },
    {
        "id": "50c89097",
        "text": "cool. But this time we are using a tan H to a hyperbolic tangent uh as the nonlinearity. And now this is like where we are at at this point. So this value is C prime CT prime is over here good. So after the tan each layer good, OK. So now we should uh calculate the um the element wise multiplication between CT prime and it. So basically what we are doing here is we are taking the, the new cells state, the information that we want to pass in the new cell state here, CCT prime",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1180s",
        "start_time": "1180.17"
    },
    {
        "id": "281932d6",
        "text": "to a hyperbolic tangent uh as the nonlinearity. And now this is like where we are at at this point. So this value is C prime CT prime is over here good. So after the tan each layer good, OK. So now we should uh calculate the um the element wise multiplication between CT prime and it. So basically what we are doing here is we are taking the, the new cells state, the information that we want to pass in the new cell state here, CCT prime and we are modulating that we are filtering that with it the same way we've done with the four G segment, right? And so here these two matrices are gonna have the same uh dimensionality, we're gonna multiply them and it is gonna decide what's important to keep in the input. So what's relevant and what's just like some garbage that I shouldn't care at all.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1184s",
        "start_time": "1184.42"
    },
    {
        "id": "4a0220e4",
        "text": "good, OK. So now we should uh calculate the um the element wise multiplication between CT prime and it. So basically what we are doing here is we are taking the, the new cells state, the information that we want to pass in the new cell state here, CCT prime and we are modulating that we are filtering that with it the same way we've done with the four G segment, right? And so here these two matrices are gonna have the same uh dimensionality, we're gonna multiply them and it is gonna decide what's important to keep in the input. So what's relevant and what's just like some garbage that I shouldn't care at all. And the result is this CT I, so it's basically the uh the cell state at times C but the inputs, right? So the new stuff that we want to add to the cell state basically, right? OK. So now the next step is to arrive at CT. So which is basically this guy here. So the, the cell state",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1202s",
        "start_time": "1202.319"
    },
    {
        "id": "672e2fe7",
        "text": "and we are modulating that we are filtering that with it the same way we've done with the four G segment, right? And so here these two matrices are gonna have the same uh dimensionality, we're gonna multiply them and it is gonna decide what's important to keep in the input. So what's relevant and what's just like some garbage that I shouldn't care at all. And the result is this CT I, so it's basically the uh the cell state at times C but the inputs, right? So the new stuff that we want to add to the cell state basically, right? OK. So now the next step is to arrive at CT. So which is basically this guy here. So the, the cell state at the current time step and in order to do that, what we do is",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1229s",
        "start_time": "1229.819"
    },
    {
        "id": "c5fe4ab2",
        "text": "And the result is this CT I, so it's basically the uh the cell state at times C but the inputs, right? So the new stuff that we want to add to the cell state basically, right? OK. So now the next step is to arrive at CT. So which is basically this guy here. So the, the cell state at the current time step and in order to do that, what we do is at this point, we do this uh element wise um uh sum and so we sum CTF to CT I",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1257s",
        "start_time": "1257.89"
    },
    {
        "id": "edf9290f",
        "text": "at the current time step and in order to do that, what we do is at this point, we do this uh element wise um uh sum and so we sum CTF to CT I what? So uh let's just like, remember like uh all of these like different elements. So CTF is basically told us like what to forget from the previous state. And so now we want to use that um matrix and add the new stuff to it, which is this CT I over here that came out of, out of like",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1286s",
        "start_time": "1286.239"
    },
    {
        "id": "7b1ab6fb",
        "text": "at this point, we do this uh element wise um uh sum and so we sum CTF to CT I what? So uh let's just like, remember like uh all of these like different elements. So CTF is basically told us like what to forget from the previous state. And so now we want to use that um matrix and add the new stuff to it, which is this CT I over here that came out of, out of like this purple square, right? And we are adding them up over here. And so first part tells us like, what to forget about uh the uh in, in the long term memory, the second element CT I tells us like what it's important to add as new information, right? And the result is CT the South state.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1293s",
        "start_time": "1293.589"
    },
    {
        "id": "c04648da",
        "text": "what? So uh let's just like, remember like uh all of these like different elements. So CTF is basically told us like what to forget from the previous state. And so now we want to use that um matrix and add the new stuff to it, which is this CT I over here that came out of, out of like this purple square, right? And we are adding them up over here. And so first part tells us like, what to forget about uh the uh in, in the long term memory, the second element CT I tells us like what it's important to add as new information, right? And the result is CT the South state. Whoa this is some cool stuff guys. OK. So we now need to understand only like the last component of the LSTM which is the output, which is I would say like really, really important, right? OK. So once again, we have another gate which is this sigmoid uh layer here",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1307s",
        "start_time": "1307.29"
    },
    {
        "id": "4b7c12bc",
        "text": "this purple square, right? And we are adding them up over here. And so first part tells us like, what to forget about uh the uh in, in the long term memory, the second element CT I tells us like what it's important to add as new information, right? And the result is CT the South state. Whoa this is some cool stuff guys. OK. So we now need to understand only like the last component of the LSTM which is the output, which is I would say like really, really important, right? OK. So once again, we have another gate which is this sigmoid uh layer here and uh we calculate this output filter. So this is a matrix and we call it OT which is calculated which is gonna be like over here. So once we've applied the sigmoid function once again to the concatenation of the hidden state at the previous time step with the input at the current time ST time step, right?",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1331s",
        "start_time": "1331.234"
    },
    {
        "id": "d421a8b5",
        "text": "Whoa this is some cool stuff guys. OK. So we now need to understand only like the last component of the LSTM which is the output, which is I would say like really, really important, right? OK. So once again, we have another gate which is this sigmoid uh layer here and uh we calculate this output filter. So this is a matrix and we call it OT which is calculated which is gonna be like over here. So once we've applied the sigmoid function once again to the concatenation of the hidden state at the previous time step with the input at the current time ST time step, right? Good. So now what remains to do is arrive at HT which is the hidden state for the current time step as well as the output that will hopefully feed into the dense layer over here. So",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1356s",
        "start_time": "1356.239"
    },
    {
        "id": "895c80c9",
        "text": "and uh we calculate this output filter. So this is a matrix and we call it OT which is calculated which is gonna be like over here. So once we've applied the sigmoid function once again to the concatenation of the hidden state at the previous time step with the input at the current time ST time step, right? Good. So now what remains to do is arrive at HT which is the hidden state for the current time step as well as the output that will hopefully feed into the dense layer over here. So let's see how we get to HD. It's quite straightforward because once again,",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1382s",
        "start_time": "1382.599"
    },
    {
        "id": "1c3db0e4",
        "text": "Good. So now what remains to do is arrive at HT which is the hidden state for the current time step as well as the output that will hopefully feed into the dense layer over here. So let's see how we get to HD. It's quite straightforward because once again, this is like something that happens like at this point over here, we have uh HT that's given by the uh element wise multiplication between the filter, the output filter ot with",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1410s",
        "start_time": "1410.65"
    },
    {
        "id": "cfe455e0",
        "text": "let's see how we get to HD. It's quite straightforward because once again, this is like something that happens like at this point over here, we have uh HT that's given by the uh element wise multiplication between the filter, the output filter ot with the um with the cell states passed through tan H. Now, this tan H over here is not a dense layer. It's just like the, the function itself. And you may be wondering, but why are we using that? Can we just use CT uh which is like the, the, the C, well, the great thing about tan H once again is that it",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1428s",
        "start_time": "1428.939"
    },
    {
        "id": "b7b39bc1",
        "text": "this is like something that happens like at this point over here, we have uh HT that's given by the uh element wise multiplication between the filter, the output filter ot with the um with the cell states passed through tan H. Now, this tan H over here is not a dense layer. It's just like the, the function itself. And you may be wondering, but why are we using that? Can we just use CT uh which is like the, the, the C, well, the great thing about tan H once again is that it squeezes the, the uh the values between minus one and one. And so, I mean, the value is like constrained and it can explode at that point, which is great, good. OK. So HT is given like by these things",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1435s",
        "start_time": "1435.41"
    },
    {
        "id": "0c3252de",
        "text": "the um with the cell states passed through tan H. Now, this tan H over here is not a dense layer. It's just like the, the function itself. And you may be wondering, but why are we using that? Can we just use CT uh which is like the, the, the C, well, the great thing about tan H once again is that it squeezes the, the uh the values between minus one and one. And so, I mean, the value is like constrained and it can explode at that point, which is great, good. OK. So HT is given like by these things that's good. And yeah, so once we've uh done like all of this, we are like at this point here. So just after like this uh multiplication",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1452s",
        "start_time": "1452.41"
    },
    {
        "id": "3bf88c30",
        "text": "squeezes the, the uh the values between minus one and one. And so, I mean, the value is like constrained and it can explode at that point, which is great, good. OK. So HT is given like by these things that's good. And yeah, so once we've uh done like all of this, we are like at this point here. So just after like this uh multiplication uh operator over there. And at that point, as I said, we take HT and we use it like for two reasons. So one is like we, we use it as the hidden state for the current for the current time step. And we also output it over here and this HT is gonna be fed into the dense layer for arriving hopefully at a good uh prediction",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1476s",
        "start_time": "1476.14"
    },
    {
        "id": "acdbc717",
        "text": "that's good. And yeah, so once we've uh done like all of this, we are like at this point here. So just after like this uh multiplication uh operator over there. And at that point, as I said, we take HT and we use it like for two reasons. So one is like we, we use it as the hidden state for the current for the current time step. And we also output it over here and this HT is gonna be fed into the dense layer for arriving hopefully at a good uh prediction good.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1492s",
        "start_time": "1492.68"
    },
    {
        "id": "dcbd2082",
        "text": "uh operator over there. And at that point, as I said, we take HT and we use it like for two reasons. So one is like we, we use it as the hidden state for the current for the current time step. And we also output it over here and this HT is gonna be fed into the dense layer for arriving hopefully at a good uh prediction good. This was uh quite uh intense, but this is like the LSTM. So now you know about uh long, short term memory cell states, but you should also understand that the one that we've uh seen, it's kind of like the, the basic form of it, but there are a bunch of different variants there. And one of the most important ones I would say is the gated recurrent unit or GR eu or group.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1505s",
        "start_time": "1505.91"
    },
    {
        "id": "2584e7e6",
        "text": "good. This was uh quite uh intense, but this is like the LSTM. So now you know about uh long, short term memory cell states, but you should also understand that the one that we've uh seen, it's kind of like the, the basic form of it, but there are a bunch of different variants there. And one of the most important ones I would say is the gated recurrent unit or GR eu or group. And So here you have like the diagram for it again. Uh So if you want to learn more about gros, I have uh linked uh an article which I, which is like, really good and you should go like, check that out because I'm not gonna, I'm not gonna get into groove like right now because I think like LSDM are already like, quite interesting and then are quite like something good.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1529s",
        "start_time": "1529.81"
    },
    {
        "id": "24be8012",
        "text": "This was uh quite uh intense, but this is like the LSTM. So now you know about uh long, short term memory cell states, but you should also understand that the one that we've uh seen, it's kind of like the, the basic form of it, but there are a bunch of different variants there. And one of the most important ones I would say is the gated recurrent unit or GR eu or group. And So here you have like the diagram for it again. Uh So if you want to learn more about gros, I have uh linked uh an article which I, which is like, really good and you should go like, check that out because I'm not gonna, I'm not gonna get into groove like right now because I think like LSDM are already like, quite interesting and then are quite like something good. Uh But uh main point being that like grew is a variation of an, an LST MA basic LSTM. And but again, you still have like a bunch of like gates and more or less like the principles that we use like in Groos are somehow like similar to LSD MS good.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1532s",
        "start_time": "1532.349"
    },
    {
        "id": "11b070b9",
        "text": "And So here you have like the diagram for it again. Uh So if you want to learn more about gros, I have uh linked uh an article which I, which is like, really good and you should go like, check that out because I'm not gonna, I'm not gonna get into groove like right now because I think like LSDM are already like, quite interesting and then are quite like something good. Uh But uh main point being that like grew is a variation of an, an LST MA basic LSTM. And but again, you still have like a bunch of like gates and more or less like the principles that we use like in Groos are somehow like similar to LSD MS good. So",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1562s",
        "start_time": "1562.16"
    },
    {
        "id": "dfecefb6",
        "text": "Uh But uh main point being that like grew is a variation of an, an LST MA basic LSTM. And but again, you still have like a bunch of like gates and more or less like the principles that we use like in Groos are somehow like similar to LSD MS good. So that's great. So we, we've basically gone through like all LST MS theory and now you should have like a very good and deep understanding of how LST MS work and this idea of like retaining long term memory as well as like short term memory and using like this long term uh state vectors to do like better predictions to have like better because we have better context from the past. Now.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1590s",
        "start_time": "1590.29"
    },
    {
        "id": "493792ee",
        "text": "So that's great. So we, we've basically gone through like all LST MS theory and now you should have like a very good and deep understanding of how LST MS work and this idea of like retaining long term memory as well as like short term memory and using like this long term uh state vectors to do like better predictions to have like better because we have better context from the past. Now. What are we gonna do next? Well, it's time for us to move from uh theory to implementation. So the first step that we'll do is gonna be like preprocess some uh data for and getting it ready for uh using it into R and M. So this is gonna be the topic for the next video. I hope you've enjoyed this video.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1612s",
        "start_time": "1612.31"
    },
    {
        "id": "06adf664",
        "text": "that's great. So we, we've basically gone through like all LST MS theory and now you should have like a very good and deep understanding of how LST MS work and this idea of like retaining long term memory as well as like short term memory and using like this long term uh state vectors to do like better predictions to have like better because we have better context from the past. Now. What are we gonna do next? Well, it's time for us to move from uh theory to implementation. So the first step that we'll do is gonna be like preprocess some uh data for and getting it ready for uh using it into R and M. So this is gonna be the topic for the next video. I hope you've enjoyed this video. If that's the case again, just like, subscribe if you want to have like more videos like this and remember to hit the notification bell if you have any questions. As always, please leave them in the comments section below. I'll try to answer as many as many questions as I can and I guess I'll see you next time. Cheers.",
        "video": "18- Long Short Term Memory (LSTM) Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "eCvz-kB4yko",
        "youtube_link": "https://www.youtube.com/watch?v=eCvz-kB4yko&t=1613s",
        "start_time": "1613.939"
    },
    {
        "id": "68180525",
        "text": "Hi, everybody and welcome to deep learning for audio with Python. In this course, we're gonna learn a lot about deep learning. So what about the learning goals? So first of all, I want you to understand the capabilities and limits of deep learning. So what's possible and what's not possible? Then after that, we're gonna learn a lot about the fundamental theory behind neural networks. We're gonna learn a little bit about the math, for example, that powers this very powerful algorithms. And then we're gonna move on to more practice based stuff and we're gonna learn how to code deep learning uh networks using industry standards, deep learning libraries like tensorflow. And then obviously we're going to play around with a bunch of different types of neural networks. So like R and MS CNN s and we're gonna learn what all of these acronyms like really stand for. Cool. The one thing that you should understand about this course is that its focus is",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=7s",
        "start_time": "7.639"
    },
    {
        "id": "43b3160c",
        "text": "So what about the learning goals? So first of all, I want you to understand the capabilities and limits of deep learning. So what's possible and what's not possible? Then after that, we're gonna learn a lot about the fundamental theory behind neural networks. We're gonna learn a little bit about the math, for example, that powers this very powerful algorithms. And then we're gonna move on to more practice based stuff and we're gonna learn how to code deep learning uh networks using industry standards, deep learning libraries like tensorflow. And then obviously we're going to play around with a bunch of different types of neural networks. So like R and MS CNN s and we're gonna learn what all of these acronyms like really stand for. Cool. The one thing that you should understand about this course is that its focus is on audio and music. Now, can you follow this course? Even if you're not interested in audio at all? Yes, you can because at the end of the day, this is a deep learning course. And so you're gonna learn all the theory and implementation about deep learning, but bear in mind that all the examples are good or most of the examples I should say are gonna be using audio data or music.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=16s",
        "start_time": "16.01"
    },
    {
        "id": "5346597b",
        "text": "learn how to code deep learning uh networks using industry standards, deep learning libraries like tensorflow. And then obviously we're going to play around with a bunch of different types of neural networks. So like R and MS CNN s and we're gonna learn what all of these acronyms like really stand for. Cool. The one thing that you should understand about this course is that its focus is on audio and music. Now, can you follow this course? Even if you're not interested in audio at all? Yes, you can because at the end of the day, this is a deep learning course. And so you're gonna learn all the theory and implementation about deep learning, but bear in mind that all the examples are good or most of the examples I should say are gonna be using audio data or music. Now, what about the technologies that we're gonna use? So obviously, we're gonna use Python and on top of that, we're gonna use tensorflow. So why did they choose both technologies? Right.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=42s",
        "start_time": "42.233"
    },
    {
        "id": "15a6e208",
        "text": "on audio and music. Now, can you follow this course? Even if you're not interested in audio at all? Yes, you can because at the end of the day, this is a deep learning course. And so you're gonna learn all the theory and implementation about deep learning, but bear in mind that all the examples are good or most of the examples I should say are gonna be using audio data or music. Now, what about the technologies that we're gonna use? So obviously, we're gonna use Python and on top of that, we're gonna use tensorflow. So why did they choose both technologies? Right. So Python and tensorflow are both industry standards for artificial intelligence. So if you're trying to like pick up a job in A I or machine learning, obviously, you'll already know that Python like is the way to go. You're gonna be required to to to to know or learn Python.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=68s",
        "start_time": "68.456"
    },
    {
        "id": "dd7fc5ba",
        "text": "Now, what about the technologies that we're gonna use? So obviously, we're gonna use Python and on top of that, we're gonna use tensorflow. So why did they choose both technologies? Right. So Python and tensorflow are both industry standards for artificial intelligence. So if you're trying to like pick up a job in A I or machine learning, obviously, you'll already know that Python like is the way to go. You're gonna be required to to to to know or learn Python. And then obviously on top of that, there's this super nice deep learning library called tensorflow, which is used almost everywhere in start ups at corporations and even in academia and for doing research. Now, the great thing about",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=95s",
        "start_time": "95.669"
    },
    {
        "id": "0e49428b",
        "text": "So Python and tensorflow are both industry standards for artificial intelligence. So if you're trying to like pick up a job in A I or machine learning, obviously, you'll already know that Python like is the way to go. You're gonna be required to to to to know or learn Python. And then obviously on top of that, there's this super nice deep learning library called tensorflow, which is used almost everywhere in start ups at corporations and even in academia and for doing research. Now, the great thing about tensorflow is that on top of tensorflow, you have kind of like high level interface that's called carers that enables you to create very complex networks using very little code. So that's fantastic and that's very nice, like just like to get started with deep learning. And finally, another reason why we are going to use tensorflow is is because it is open source. And so if you want to tweak things around you actually can",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=106s",
        "start_time": "106.41"
    },
    {
        "id": "fd33f39a",
        "text": "And then obviously on top of that, there's this super nice deep learning library called tensorflow, which is used almost everywhere in start ups at corporations and even in academia and for doing research. Now, the great thing about tensorflow is that on top of tensorflow, you have kind of like high level interface that's called carers that enables you to create very complex networks using very little code. So that's fantastic and that's very nice, like just like to get started with deep learning. And finally, another reason why we are going to use tensorflow is is because it is open source. And so if you want to tweak things around you actually can now what about the content? So what are we gonna actually learn So you're gonna get an intro to artificial intelligence, machine learning and deep learning. So you, we're gonna kind of like learn the differences and the overlap. So there's different fields and self.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=126s",
        "start_time": "126.55"
    },
    {
        "id": "49068847",
        "text": "tensorflow is that on top of tensorflow, you have kind of like high level interface that's called carers that enables you to create very complex networks using very little code. So that's fantastic and that's very nice, like just like to get started with deep learning. And finally, another reason why we are going to use tensorflow is is because it is open source. And so if you want to tweak things around you actually can now what about the content? So what are we gonna actually learn So you're gonna get an intro to artificial intelligence, machine learning and deep learning. So you, we're gonna kind of like learn the differences and the overlap. So there's different fields and self. But then after that, we're going to move on and jump into the different flavors of neural networks that are out there. So we'll start with something that's been like historically, the initial network that has been widely adopted and that's the multi layer perception.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=144s",
        "start_time": "144.119"
    },
    {
        "id": "a2fd3026",
        "text": "now what about the content? So what are we gonna actually learn So you're gonna get an intro to artificial intelligence, machine learning and deep learning. So you, we're gonna kind of like learn the differences and the overlap. So there's different fields and self. But then after that, we're going to move on and jump into the different flavors of neural networks that are out there. So we'll start with something that's been like historically, the initial network that has been widely adopted and that's the multi layer perception. Then after that, we're gonna get into convolutional neural networks or CNN S. And you may be familiar at least like with this acronym and these networks are super useful for doing processing with images or and also like",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=174s",
        "start_time": "174.97"
    },
    {
        "id": "eca663c9",
        "text": "But then after that, we're going to move on and jump into the different flavors of neural networks that are out there. So we'll start with something that's been like historically, the initial network that has been widely adopted and that's the multi layer perception. Then after that, we're gonna get into convolutional neural networks or CNN S. And you may be familiar at least like with this acronym and these networks are super useful for doing processing with images or and also like audio. And then we're gonna jump on to uh recurrent neural networks or Aron NS. And these are fantastic algorithms that you want to use for predicting like time series and for handling uh time series types of data. And then finally, we're gonna look into guns or generative adversarial networks that are super fashionable these days.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=191s",
        "start_time": "191.85"
    },
    {
        "id": "37efdcb6",
        "text": "Then after that, we're gonna get into convolutional neural networks or CNN S. And you may be familiar at least like with this acronym and these networks are super useful for doing processing with images or and also like audio. And then we're gonna jump on to uh recurrent neural networks or Aron NS. And these are fantastic algorithms that you want to use for predicting like time series and for handling uh time series types of data. And then finally, we're gonna look into guns or generative adversarial networks that are super fashionable these days. So what should you expect from this course? What type of style in terms of like the learning? Well, we're gonna have three different blocks I would say, well, we're gonna learn quite a lot about the theory. Now, I'm not gonna go super deep into math because at the end of the day, this is not a math course, but you're gonna learn quite a lot about",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=209s",
        "start_time": "209.16"
    },
    {
        "id": "9f4751fe",
        "text": "audio. And then we're gonna jump on to uh recurrent neural networks or Aron NS. And these are fantastic algorithms that you want to use for predicting like time series and for handling uh time series types of data. And then finally, we're gonna look into guns or generative adversarial networks that are super fashionable these days. So what should you expect from this course? What type of style in terms of like the learning? Well, we're gonna have three different blocks I would say, well, we're gonna learn quite a lot about the theory. Now, I'm not gonna go super deep into math because at the end of the day, this is not a math course, but you're gonna learn quite a lot about basic linear algebra and derivatives and these kind of things because we need them to understand how neural networks work and how to tweak them in order to have like very effective uh like uh algorithms for solving our problems. Now, we're going to use all of these",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=225s",
        "start_time": "225.059"
    },
    {
        "id": "7430bece",
        "text": "So what should you expect from this course? What type of style in terms of like the learning? Well, we're gonna have three different blocks I would say, well, we're gonna learn quite a lot about the theory. Now, I'm not gonna go super deep into math because at the end of the day, this is not a math course, but you're gonna learn quite a lot about basic linear algebra and derivatives and these kind of things because we need them to understand how neural networks work and how to tweak them in order to have like very effective uh like uh algorithms for solving our problems. Now, we're going to use all of these theory and we are going to implement that. And uh so basically, we're going to have a bunch of different coding tutorials where we're going to use both Python for coding neural networks from scratch. But then on top of that, we're gonna have tensorflow code where we're going to create very complex neural networks.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=249s",
        "start_time": "249.229"
    },
    {
        "id": "0160e00e",
        "text": "basic linear algebra and derivatives and these kind of things because we need them to understand how neural networks work and how to tweak them in order to have like very effective uh like uh algorithms for solving our problems. Now, we're going to use all of these theory and we are going to implement that. And uh so basically, we're going to have a bunch of different coding tutorials where we're going to use both Python for coding neural networks from scratch. But then on top of that, we're gonna have tensorflow code where we're going to create very complex neural networks. Now, the third part of this is we're gonna have a bunch of different applications, kind of real world applications. I would say where we're gonna test all of the knowledge that we've acquired from theory and basic uh tensorflow code.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=269s",
        "start_time": "269.692"
    },
    {
        "id": "5b4e9027",
        "text": "theory and we are going to implement that. And uh so basically, we're going to have a bunch of different coding tutorials where we're going to use both Python for coding neural networks from scratch. But then on top of that, we're gonna have tensorflow code where we're going to create very complex neural networks. Now, the third part of this is we're gonna have a bunch of different applications, kind of real world applications. I would say where we're gonna test all of the knowledge that we've acquired from theory and basic uh tensorflow code. So obviously, this is like a very important question. So where do I get code and slides? So I'm gonna have a github page like my github page and I'm gonna post all of this uh lessons online. And so you can just like browse them and download what you need. And obviously all of this information is gonna be below in the description of each video in the series.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=290s",
        "start_time": "290.156"
    },
    {
        "id": "846d18bc",
        "text": "Now, the third part of this is we're gonna have a bunch of different applications, kind of real world applications. I would say where we're gonna test all of the knowledge that we've acquired from theory and basic uh tensorflow code. So obviously, this is like a very important question. So where do I get code and slides? So I'm gonna have a github page like my github page and I'm gonna post all of this uh lessons online. And so you can just like browse them and download what you need. And obviously all of this information is gonna be below in the description of each video in the series. Cool. So who's this course for now? Uh when I designed this course I had in mind, Python developers who want to pick up deep learning skills.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=310s",
        "start_time": "310.92"
    },
    {
        "id": "85b33861",
        "text": "So obviously, this is like a very important question. So where do I get code and slides? So I'm gonna have a github page like my github page and I'm gonna post all of this uh lessons online. And so you can just like browse them and download what you need. And obviously all of this information is gonna be below in the description of each video in the series. Cool. So who's this course for now? Uh when I designed this course I had in mind, Python developers who want to pick up deep learning skills. And so this is not a course for um beginners rather like for actual developers and also this, if you are like a DEA who's already playing around with a bunch of this deep learning libraries like tensorflow, for example. But you want to learn more about how you can. So how like neural networks really work like under the hood. So this is really perfect for you because you're gonna get like a an understanding like at a deeper level.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=326s",
        "start_time": "326.45"
    },
    {
        "id": "1a26f0e2",
        "text": "Cool. So who's this course for now? Uh when I designed this course I had in mind, Python developers who want to pick up deep learning skills. And so this is not a course for um beginners rather like for actual developers and also this, if you are like a DEA who's already playing around with a bunch of this deep learning libraries like tensorflow, for example. But you want to learn more about how you can. So how like neural networks really work like under the hood. So this is really perfect for you because you're gonna get like a an understanding like at a deeper level. Now, obviously, this course is also very useful for devs who have an interest in audio and music. Because at the end of the day, you're going to be introduced to A I music and A I audio. And if you are a practitioner who's got some experience in audio, digital signal processing or DS P and you want to step up your game even more and get into A I. Again, this is like the right uh course uh for you.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=349s",
        "start_time": "349.98"
    },
    {
        "id": "2e958f4c",
        "text": "And so this is not a course for um beginners rather like for actual developers and also this, if you are like a DEA who's already playing around with a bunch of this deep learning libraries like tensorflow, for example. But you want to learn more about how you can. So how like neural networks really work like under the hood. So this is really perfect for you because you're gonna get like a an understanding like at a deeper level. Now, obviously, this course is also very useful for devs who have an interest in audio and music. Because at the end of the day, you're going to be introduced to A I music and A I audio. And if you are a practitioner who's got some experience in audio, digital signal processing or DS P and you want to step up your game even more and get into A I. Again, this is like the right uh course uh for you. And finally, I think like another category who would benefit quite a lot from this course are data analysts who want to learn more about machine learning and who want to learn more about uh A I as well and how to cut things out.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=362s",
        "start_time": "362.5"
    },
    {
        "id": "015fad74",
        "text": "Now, obviously, this course is also very useful for devs who have an interest in audio and music. Because at the end of the day, you're going to be introduced to A I music and A I audio. And if you are a practitioner who's got some experience in audio, digital signal processing or DS P and you want to step up your game even more and get into A I. Again, this is like the right uh course uh for you. And finally, I think like another category who would benefit quite a lot from this course are data analysts who want to learn more about machine learning and who want to learn more about uh A I as well and how to cut things out. As I mentioned. This is not a course for Python beginners rather, you should have some intermediate coding skills. Because at the end of the day, I'm not gonna teach you how to code. The focus of this course is on A I not coding itself. Now, if you know uh quite a lot about basic linear algebra, that's fantastic, but it's definitely not necessary because I'm going to cover all the math. We'll need to understand neural networks",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=390s",
        "start_time": "390.809"
    },
    {
        "id": "5d28026a",
        "text": "And finally, I think like another category who would benefit quite a lot from this course are data analysts who want to learn more about machine learning and who want to learn more about uh A I as well and how to cut things out. As I mentioned. This is not a course for Python beginners rather, you should have some intermediate coding skills. Because at the end of the day, I'm not gonna teach you how to code. The focus of this course is on A I not coding itself. Now, if you know uh quite a lot about basic linear algebra, that's fantastic, but it's definitely not necessary because I'm going to cover all the math. We'll need to understand neural networks at the same time. If you know about audio digital signal processing, that's fantastic. But it's not really necessary because again, I'm going to cover all the DS P stuff that we really need.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=416s",
        "start_time": "416.88"
    },
    {
        "id": "9d15588b",
        "text": "As I mentioned. This is not a course for Python beginners rather, you should have some intermediate coding skills. Because at the end of the day, I'm not gonna teach you how to code. The focus of this course is on A I not coding itself. Now, if you know uh quite a lot about basic linear algebra, that's fantastic, but it's definitely not necessary because I'm going to cover all the math. We'll need to understand neural networks at the same time. If you know about audio digital signal processing, that's fantastic. But it's not really necessary because again, I'm going to cover all the DS P stuff that we really need. Cool. So this was it for the course overview. So now just brace yourself, deep learning is coming. Bye.",
        "video": "1- Deep Learning (for Audio) with Python: Course Overview",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "fMqL5vckiU0",
        "youtube_link": "https://www.youtube.com/watch?v=fMqL5vckiU0&t=431s",
        "start_time": "431.679"
    },
    {
        "id": "8ea5d29f",
        "text": "Hi everybody and welcome to yet another video in the deep learning for audio with Python series. This time, it's super exciting because we are looking into convolutional neural networks and we'll try to explain how they work on a theoretical level, right? So what are CNN S? Well CNN S are quite advanced type of um neural network. And they've been mainly used for processing images. And over time, we've found out that they are way better at performing liquid images than the equivalent, for example, like multi-layered perception uh architecture. And the the great thing about CNN S is that they have at the same time way less parameters than dense layers. So when analyzing images, we have this double advantage, so they perform better and they have less parameters than the multi layer perception. But now you may be wondering, but why is that the case? Well, it turns out that image data is somehow uh structured.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "b67b6d48",
        "text": "right? So what are CNN S? Well CNN S are quite advanced type of um neural network. And they've been mainly used for processing images. And over time, we've found out that they are way better at performing liquid images than the equivalent, for example, like multi-layered perception uh architecture. And the the great thing about CNN S is that they have at the same time way less parameters than dense layers. So when analyzing images, we have this double advantage, so they perform better and they have less parameters than the multi layer perception. But now you may be wondering, but why is that the case? Well, it turns out that image data is somehow uh structured. So the pixels are not just like randomly like uh positions like in an image, but usually there are certain emergent structures. So for example, you have structures like edges shapes, you have invariants to translation, you have scale invariants So for example, a square remains a square, square re regardless of how big the square is, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=15s",
        "start_time": "15.729"
    },
    {
        "id": "2a0e2668",
        "text": "And the the great thing about CNN S is that they have at the same time way less parameters than dense layers. So when analyzing images, we have this double advantage, so they perform better and they have less parameters than the multi layer perception. But now you may be wondering, but why is that the case? Well, it turns out that image data is somehow uh structured. So the pixels are not just like randomly like uh positions like in an image, but usually there are certain emergent structures. So for example, you have structures like edges shapes, you have invariants to translation, you have scale invariants So for example, a square remains a square, square re regardless of how big the square is, right? And in a sense like what we do with CNN S is we try to emulate the way we see stuff and we perceive like images, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=39s",
        "start_time": "39.54"
    },
    {
        "id": "5c7a7f09",
        "text": "So the pixels are not just like randomly like uh positions like in an image, but usually there are certain emergent structures. So for example, you have structures like edges shapes, you have invariants to translation, you have scale invariants So for example, a square remains a square, square re regardless of how big the square is, right? And in a sense like what we do with CNN S is we try to emulate the way we see stuff and we perceive like images, right? And uh by doing so what we do when we, when we see like stuff, it's basically we extract uh basic features. So for example, we we are able to extract when we see something like vertical bars or horizontal bars. And in a sense, all the different components of A CNN try to uh learn to extract different uh types of features. So",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=65s",
        "start_time": "65.58"
    },
    {
        "id": "8a686d69",
        "text": "And in a sense like what we do with CNN S is we try to emulate the way we see stuff and we perceive like images, right? And uh by doing so what we do when we, when we see like stuff, it's basically we extract uh basic features. So for example, we we are able to extract when we see something like vertical bars or horizontal bars. And in a sense, all the different components of A CNN try to uh learn to extract different uh types of features. So uh all of this process uh in A CNN is done relying on a, on a couple of components mainly. So one is the one that provides the name to CNN. So it's called convolution and the other one is called uh pooling. So uh in the remaining part of this video, we'll take a look at this two processes, these two components like in some somewhat in detail. OK. So let's start with convolution.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=91s",
        "start_time": "91.51"
    },
    {
        "id": "ab407bfd",
        "text": "And uh by doing so what we do when we, when we see like stuff, it's basically we extract uh basic features. So for example, we we are able to extract when we see something like vertical bars or horizontal bars. And in a sense, all the different components of A CNN try to uh learn to extract different uh types of features. So uh all of this process uh in A CNN is done relying on a, on a couple of components mainly. So one is the one that provides the name to CNN. So it's called convolution and the other one is called uh pooling. So uh in the remaining part of this video, we'll take a look at this two processes, these two components like in some somewhat in detail. OK. So let's start with convolution. So at the center of convolution, we have the idea of a kernel, uh we can call it a kernel or you can also call it a filter. And now uh like over the next few slides, you'll understand why that's the case. But in its simplest um wave like of understanding it, a kernel is no more than a grid, a weights like this over here, you have a three by three",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=103s",
        "start_time": "103.3"
    },
    {
        "id": "44af33ef",
        "text": "uh all of this process uh in A CNN is done relying on a, on a couple of components mainly. So one is the one that provides the name to CNN. So it's called convolution and the other one is called uh pooling. So uh in the remaining part of this video, we'll take a look at this two processes, these two components like in some somewhat in detail. OK. So let's start with convolution. So at the center of convolution, we have the idea of a kernel, uh we can call it a kernel or you can also call it a filter. And now uh like over the next few slides, you'll understand why that's the case. But in its simplest um wave like of understanding it, a kernel is no more than a grid, a weights like this over here, you have a three by three kernel with a bunch of weights. And the idea is that we apply the kernel to the image, right. So we have an image and then we apply the kernel. Now I'm gonna specify how, what this means like mathematically like in a second. But before get getting that, I want to give you like an overview like of these kernels and convolutions, right? So",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=130s",
        "start_time": "130.44"
    },
    {
        "id": "3f8d341b",
        "text": "So at the center of convolution, we have the idea of a kernel, uh we can call it a kernel or you can also call it a filter. And now uh like over the next few slides, you'll understand why that's the case. But in its simplest um wave like of understanding it, a kernel is no more than a grid, a weights like this over here, you have a three by three kernel with a bunch of weights. And the idea is that we apply the kernel to the image, right. So we have an image and then we apply the kernel. Now I'm gonna specify how, what this means like mathematically like in a second. But before get getting that, I want to give you like an overview like of these kernels and convolutions, right? So uh the uh other ideas that's uh kernels like in filters have been traditionally used in uh image processing. So like for detecting edges, for example, for or for creating effects like a blur or things like that, right? Uh But now they, they, we found out that in deep learning, they are very, very useful for processing images as well.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=159s",
        "start_time": "159.169"
    },
    {
        "id": "0b0505eb",
        "text": "kernel with a bunch of weights. And the idea is that we apply the kernel to the image, right. So we have an image and then we apply the kernel. Now I'm gonna specify how, what this means like mathematically like in a second. But before get getting that, I want to give you like an overview like of these kernels and convolutions, right? So uh the uh other ideas that's uh kernels like in filters have been traditionally used in uh image processing. So like for detecting edges, for example, for or for creating effects like a blur or things like that, right? Uh But now they, they, we found out that in deep learning, they are very, very useful for processing images as well. Cool. OK. So now let's uh try to understand how convolutions work starting from an image. Now this is a gray scale image, which basically means that we have a bunch of pixels that um can vary between zero and 255 where zero is basically black and 255 is white. And so",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=185s",
        "start_time": "185.274"
    },
    {
        "id": "e4320f0d",
        "text": "uh the uh other ideas that's uh kernels like in filters have been traditionally used in uh image processing. So like for detecting edges, for example, for or for creating effects like a blur or things like that, right? Uh But now they, they, we found out that in deep learning, they are very, very useful for processing images as well. Cool. OK. So now let's uh try to understand how convolutions work starting from an image. Now this is a gray scale image, which basically means that we have a bunch of pixels that um can vary between zero and 255 where zero is basically black and 255 is white. And so we can take this uh the image of this cat and we can translate it into like this greed uh of pixels and these values which are supposed to be between zero and 253 55 represents like the values for each pixel, right? So I shouldn't notice here. I've used quite like small values just like, yeah, because it's easier",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=207s",
        "start_time": "207.759"
    },
    {
        "id": "f7a9a69c",
        "text": "Cool. OK. So now let's uh try to understand how convolutions work starting from an image. Now this is a gray scale image, which basically means that we have a bunch of pixels that um can vary between zero and 255 where zero is basically black and 255 is white. And so we can take this uh the image of this cat and we can translate it into like this greed uh of pixels and these values which are supposed to be between zero and 253 55 represents like the values for each pixel, right? So I shouldn't notice here. I've used quite like small values just like, yeah, because it's easier uh right. OK. So now let's move on. So we have this translation from an image to its pixel representation. Cool. So we said that we should apply a kernel to the image, right. So we have the pixel representation of the image, we have a kernel here. And the results of the convolution",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=232s",
        "start_time": "232.259"
    },
    {
        "id": "9ffbabef",
        "text": "we can take this uh the image of this cat and we can translate it into like this greed uh of pixels and these values which are supposed to be between zero and 253 55 represents like the values for each pixel, right? So I shouldn't notice here. I've used quite like small values just like, yeah, because it's easier uh right. OK. So now let's move on. So we have this translation from an image to its pixel representation. Cool. So we said that we should apply a kernel to the image, right. So we have the pixel representation of the image, we have a kernel here. And the results of the convolution is a uh itself like a an output uh which is agreed. And in the case of like the, the settings that I'm gonna use, we're gonna get an output that basically has the same size in terms of width and height of the original image over here. Now let's try to understand how convolution works. So",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=256s",
        "start_time": "256.225"
    },
    {
        "id": "454d47c0",
        "text": "uh right. OK. So now let's move on. So we have this translation from an image to its pixel representation. Cool. So we said that we should apply a kernel to the image, right. So we have the pixel representation of the image, we have a kernel here. And the results of the convolution is a uh itself like a an output uh which is agreed. And in the case of like the, the settings that I'm gonna use, we're gonna get an output that basically has the same size in terms of width and height of the original image over here. Now let's try to understand how convolution works. So what we do basically is we overlay the kernel on top of the image. And so, for example, here you'll see that we are overlaying the kernel on top of like this initial uh red square here on the image and we center it around uh like the center like of the kernel. And here it corresponds like to these values to this four that's highlighted in green.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=280s",
        "start_time": "280.579"
    },
    {
        "id": "b70a0d3f",
        "text": "is a uh itself like a an output uh which is agreed. And in the case of like the, the settings that I'm gonna use, we're gonna get an output that basically has the same size in terms of width and height of the original image over here. Now let's try to understand how convolution works. So what we do basically is we overlay the kernel on top of the image. And so, for example, here you'll see that we are overlaying the kernel on top of like this initial uh red square here on the image and we center it around uh like the center like of the kernel. And here it corresponds like to these values to this four that's highlighted in green. Now, uh when we do that, uh we get a, a value and the value of the convolution is then input in the, in this like output grid over here, which is basically like at the same index of the original uh image uh where we've centered the kernel on, right. So how do we get to a value there? Well, what we do basically is we just apply the dot product",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=302s",
        "start_time": "302.709"
    },
    {
        "id": "adee7446",
        "text": "what we do basically is we overlay the kernel on top of the image. And so, for example, here you'll see that we are overlaying the kernel on top of like this initial uh red square here on the image and we center it around uh like the center like of the kernel. And here it corresponds like to these values to this four that's highlighted in green. Now, uh when we do that, uh we get a, a value and the value of the convolution is then input in the, in this like output grid over here, which is basically like at the same index of the original uh image uh where we've centered the kernel on, right. So how do we get to a value there? Well, what we do basically is we just apply the dot product uh having like the two like vectors like the uh the image and the kernel itself. And for the image we just like consider all of these values here in the red square",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=326s",
        "start_time": "326.6"
    },
    {
        "id": "25b4897f",
        "text": "Now, uh when we do that, uh we get a, a value and the value of the convolution is then input in the, in this like output grid over here, which is basically like at the same index of the original uh image uh where we've centered the kernel on, right. So how do we get to a value there? Well, what we do basically is we just apply the dot product uh having like the two like vectors like the uh the image and the kernel itself. And for the image we just like consider all of these values here in the red square uh which are like correspondence uh kind of like to the kernel. So we have like the same number of bodies there, right? And so what we do and we should know how to do A dot Products. Because if, if you don't remember, you should go just and watch back like my video on linear algebra introduction to linear algebra.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=355s",
        "start_time": "355.07"
    },
    {
        "id": "febd6a69",
        "text": "uh having like the two like vectors like the uh the image and the kernel itself. And for the image we just like consider all of these values here in the red square uh which are like correspondence uh kind of like to the kernel. So we have like the same number of bodies there, right? And so what we do and we should know how to do A dot Products. Because if, if you don't remember, you should go just and watch back like my video on linear algebra introduction to linear algebra. But basically what we do here is we take each value and from the image and we multiply that for the correspondence uh value at the, at the same index of the kernel. So like in this case, we have five by one and then we uh add to that to multiplied by zero, then",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=385s",
        "start_time": "385.029"
    },
    {
        "id": "cb90f92a",
        "text": "uh which are like correspondence uh kind of like to the kernel. So we have like the same number of bodies there, right? And so what we do and we should know how to do A dot Products. Because if, if you don't remember, you should go just and watch back like my video on linear algebra introduction to linear algebra. But basically what we do here is we take each value and from the image and we multiply that for the correspondence uh value at the, at the same index of the kernel. So like in this case, we have five by one and then we uh add to that to multiplied by zero, then we add to that three by zero, then we add two plus two. Well, I mean, you get the gist and the last number like the last value that we have is this zero multiplied by minus one over here. Now, uh when we uh take all of these like uh different like expressions and we run the math, we end up with",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=400s",
        "start_time": "400.359"
    },
    {
        "id": "d3cc15fb",
        "text": "But basically what we do here is we take each value and from the image and we multiply that for the correspondence uh value at the, at the same index of the kernel. So like in this case, we have five by one and then we uh add to that to multiplied by zero, then we add to that three by zero, then we add two plus two. Well, I mean, you get the gist and the last number like the last value that we have is this zero multiplied by minus one over here. Now, uh when we uh take all of these like uh different like expressions and we run the math, we end up with 18, which is the value for this convolution. Now not 100% sure that 18 is right? Because I was like, I did things like quite quickly but yeah, you can try that out and see like if 18 is actually like the, the right results for this product. But regardless, let's assume 18 is correct. OK. So we'll take that 18 and we'll just",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=417s",
        "start_time": "417.619"
    },
    {
        "id": "192e4930",
        "text": "we add to that three by zero, then we add two plus two. Well, I mean, you get the gist and the last number like the last value that we have is this zero multiplied by minus one over here. Now, uh when we uh take all of these like uh different like expressions and we run the math, we end up with 18, which is the value for this convolution. Now not 100% sure that 18 is right? Because I was like, I did things like quite quickly but yeah, you can try that out and see like if 18 is actually like the, the right results for this product. But regardless, let's assume 18 is correct. OK. So we'll take that 18 and we'll just it in here in the output grid over here, right? So how do we continue here? Well, it's quite simple because now we slide the kernel on the image here and we slide to the right and now we center the uh the kernel on this like index here with the the one highlighted in green in the image, right? And we redo the the dot products and we get 10, we move on,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=440s",
        "start_time": "440.98"
    },
    {
        "id": "42c465ad",
        "text": "18, which is the value for this convolution. Now not 100% sure that 18 is right? Because I was like, I did things like quite quickly but yeah, you can try that out and see like if 18 is actually like the, the right results for this product. But regardless, let's assume 18 is correct. OK. So we'll take that 18 and we'll just it in here in the output grid over here, right? So how do we continue here? Well, it's quite simple because now we slide the kernel on the image here and we slide to the right and now we center the uh the kernel on this like index here with the the one highlighted in green in the image, right? And we redo the the dot products and we get 10, we move on, we slide and we get like another value and it's minus three, we slide again and we get five. Now we go",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=464s",
        "start_time": "464.066"
    },
    {
        "id": "8eb1ca85",
        "text": "it in here in the output grid over here, right? So how do we continue here? Well, it's quite simple because now we slide the kernel on the image here and we slide to the right and now we center the uh the kernel on this like index here with the the one highlighted in green in the image, right? And we redo the the dot products and we get 10, we move on, we slide and we get like another value and it's minus three, we slide again and we get five. Now we go down, right? We go down one and we go back to the start and we have like another value and we continue like this until basically we can uh arrive at all the numbers for like this square here like highlighted in green. Now I've left uh question marks because I didn't want to do like the calculation. But this is a great",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=487s",
        "start_time": "487.152"
    },
    {
        "id": "c1076649",
        "text": "we slide and we get like another value and it's minus three, we slide again and we get five. Now we go down, right? We go down one and we go back to the start and we have like another value and we continue like this until basically we can uh arrive at all the numbers for like this square here like highlighted in green. Now I've left uh question marks because I didn't want to do like the calculation. But this is a great uh way for you to practice with convolution. So you can just like run the numbers there and uh yeah and write perhaps like the results like in the comments, right? But you may see that there's an issue here, right? Because we have the values of the convolutions just like for this internal square but the edges over here don't get a number.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=516s",
        "start_time": "516.33"
    },
    {
        "id": "2f3bf73a",
        "text": "down, right? We go down one and we go back to the start and we have like another value and we continue like this until basically we can uh arrive at all the numbers for like this square here like highlighted in green. Now I've left uh question marks because I didn't want to do like the calculation. But this is a great uh way for you to practice with convolution. So you can just like run the numbers there and uh yeah and write perhaps like the results like in the comments, right? But you may see that there's an issue here, right? Because we have the values of the convolutions just like for this internal square but the edges over here don't get a number. And why is that an issue? Well, it's why, why, why is it an issue like to run that uh map on the, on the edges? Right? And the reason is like uh quickly explained by uh just opposing like the the kernel on the edges here. So for example, like if we center like the kernel on this 00 like index over here, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=523s",
        "start_time": "523.859"
    },
    {
        "id": "9abb7c0a",
        "text": "uh way for you to practice with convolution. So you can just like run the numbers there and uh yeah and write perhaps like the results like in the comments, right? But you may see that there's an issue here, right? Because we have the values of the convolutions just like for this internal square but the edges over here don't get a number. And why is that an issue? Well, it's why, why, why is it an issue like to run that uh map on the, on the edges? Right? And the reason is like uh quickly explained by uh just opposing like the the kernel on the edges here. So for example, like if we center like the kernel on this 00 like index over here, right? Uh We see that uh yeah, we have a part like of this uh kernel that can't be like applied to like any anything really because we don't have values over there. So what, what, what can we do there? Well, there are a couple of solutions.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=550s",
        "start_time": "550.679"
    },
    {
        "id": "ce23efff",
        "text": "And why is that an issue? Well, it's why, why, why is it an issue like to run that uh map on the, on the edges? Right? And the reason is like uh quickly explained by uh just opposing like the the kernel on the edges here. So for example, like if we center like the kernel on this 00 like index over here, right? Uh We see that uh yeah, we have a part like of this uh kernel that can't be like applied to like any anything really because we don't have values over there. So what, what, what can we do there? Well, there are a couple of solutions. So one it's kind of super straightforward, we say wait, well, wait. So we, we can't apply the kernel there. So we, we'll just ignore it. So we'll just ignore the edges, right? Yeah, this is a solution in itself. But the problem with that is that we are missing, we are losing some information, right. So all the edges on the image, the other solution uh which is the one that we usually use in deep learning is applying",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=573s",
        "start_time": "573.969"
    },
    {
        "id": "3793bbe6",
        "text": "Uh We see that uh yeah, we have a part like of this uh kernel that can't be like applied to like any anything really because we don't have values over there. So what, what, what can we do there? Well, there are a couple of solutions. So one it's kind of super straightforward, we say wait, well, wait. So we, we can't apply the kernel there. So we, we'll just ignore it. So we'll just ignore the edges, right? Yeah, this is a solution in itself. But the problem with that is that we are missing, we are losing some information, right. So all the edges on the image, the other solution uh which is the one that we usually use in deep learning is applying a type of padding called zero padding. So we come up with an edge with an artificial edge that has all zeros for the purpose of calculation, right? And so if we do that, then all of a sudden we can calculate the, we can perform the convolution also like on the edges of the image, right.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=596s",
        "start_time": "596.489"
    },
    {
        "id": "a7259f8d",
        "text": "So one it's kind of super straightforward, we say wait, well, wait. So we, we can't apply the kernel there. So we, we'll just ignore it. So we'll just ignore the edges, right? Yeah, this is a solution in itself. But the problem with that is that we are missing, we are losing some information, right. So all the edges on the image, the other solution uh which is the one that we usually use in deep learning is applying a type of padding called zero padding. So we come up with an edge with an artificial edge that has all zeros for the purpose of calculation, right? And so if we do that, then all of a sudden we can calculate the, we can perform the convolution also like on the edges of the image, right. And so if we uh run the calculations also like on the edges, we end up with an output like this where we basically have a uh the output convolution which is this uh grid uh that where like the k the kernel has been like applied on the image. And we have like all the different results. Again, question marks where I haven't done like the, the the calculation cool.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=613s",
        "start_time": "613.719"
    },
    {
        "id": "86736519",
        "text": "a type of padding called zero padding. So we come up with an edge with an artificial edge that has all zeros for the purpose of calculation, right? And so if we do that, then all of a sudden we can calculate the, we can perform the convolution also like on the edges of the image, right. And so if we uh run the calculations also like on the edges, we end up with an output like this where we basically have a uh the output convolution which is this uh grid uh that where like the k the kernel has been like applied on the image. And we have like all the different results. Again, question marks where I haven't done like the, the the calculation cool. OK. So this is like the basic idea of a convolution. So, but let's think of a kernel. And so, and what that is uh on a, on a semantic level. Well, we can think of a kernel or a filter as a feature detector.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=642s",
        "start_time": "642.03"
    },
    {
        "id": "1eb47334",
        "text": "And so if we uh run the calculations also like on the edges, we end up with an output like this where we basically have a uh the output convolution which is this uh grid uh that where like the k the kernel has been like applied on the image. And we have like all the different results. Again, question marks where I haven't done like the, the the calculation cool. OK. So this is like the basic idea of a convolution. So, but let's think of a kernel. And so, and what that is uh on a, on a semantic level. Well, we can think of a kernel or a filter as a feature detector. So basically, we can have a bunch of like uh different kernels that detects different uh features. So for example, this kernel here",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=665s",
        "start_time": "665.549"
    },
    {
        "id": "6ec515dc",
        "text": "OK. So this is like the basic idea of a convolution. So, but let's think of a kernel. And so, and what that is uh on a, on a semantic level. Well, we can think of a kernel or a filter as a feature detector. So basically, we can have a bunch of like uh different kernels that detects different uh features. So for example, this kernel here uh which has like these ones like on this uh diagonal here, uh it's able to detect oblique lines like diagonals, right lines, right. Whereas this type of kernel here could be used to uh identify vertical lines. And so this is a vertical line uh detector, right? So now when we have a convolutional neural network,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=692s",
        "start_time": "692.609"
    },
    {
        "id": "5f1bc4b9",
        "text": "So basically, we can have a bunch of like uh different kernels that detects different uh features. So for example, this kernel here uh which has like these ones like on this uh diagonal here, uh it's able to detect oblique lines like diagonals, right lines, right. Whereas this type of kernel here could be used to uh identify vertical lines. And so this is a vertical line uh detector, right? So now when we have a convolutional neural network, as I mentioned earlier, what we do is we, we kind of like extract features uh using uh these kernels. But now the great thing about CNN is that we don't uh hard wire those, those kernels",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=714s",
        "start_time": "714.349"
    },
    {
        "id": "8232d6e9",
        "text": "uh which has like these ones like on this uh diagonal here, uh it's able to detect oblique lines like diagonals, right lines, right. Whereas this type of kernel here could be used to uh identify vertical lines. And so this is a vertical line uh detector, right? So now when we have a convolutional neural network, as I mentioned earlier, what we do is we, we kind of like extract features uh using uh these kernels. But now the great thing about CNN is that we don't uh hard wire those, those kernels rather we learn them in the process so we learn the, so the the network itself learns the kernels that it needs to extract in order to perform well, in some classification of whatever task we may think of. Right? And what does it mean to learn a kernel? Well, we, what the uh what we do basically is we learn the values, the weights of the kernel, all of these numbers here, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=724s",
        "start_time": "724.26"
    },
    {
        "id": "0aa852f4",
        "text": "as I mentioned earlier, what we do is we, we kind of like extract features uh using uh these kernels. But now the great thing about CNN is that we don't uh hard wire those, those kernels rather we learn them in the process so we learn the, so the the network itself learns the kernels that it needs to extract in order to perform well, in some classification of whatever task we may think of. Right? And what does it mean to learn a kernel? Well, we, what the uh what we do basically is we learn the values, the weights of the kernel, all of these numbers here, right? And so when we train a CNN, basically, we are training uh these values in the kernels cool.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=751s",
        "start_time": "751.739"
    },
    {
        "id": "b7828e7a",
        "text": "rather we learn them in the process so we learn the, so the the network itself learns the kernels that it needs to extract in order to perform well, in some classification of whatever task we may think of. Right? And what does it mean to learn a kernel? Well, we, what the uh what we do basically is we learn the values, the weights of the kernel, all of these numbers here, right? And so when we train a CNN, basically, we are training uh these values in the kernels cool. OK. So uh when we um handle like convolution, we have a bunch of like architectural decisions that we can take to decide which type of convolutions uh like to use, right? And so uh here I have like a few uh things, a few settings that we should specify uh when we build our architecture.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=767s",
        "start_time": "767.2"
    },
    {
        "id": "78b240eb",
        "text": "And so when we train a CNN, basically, we are training uh these values in the kernels cool. OK. So uh when we um handle like convolution, we have a bunch of like architectural decisions that we can take to decide which type of convolutions uh like to use, right? And so uh here I have like a few uh things, a few settings that we should specify uh when we build our architecture. So we should specify the grid size of the convolution, the stride, the depth and the number of kernels. So let's look into this one by one. Let's start from grid size. Well, this is like very intuitive, right. So uh the grid size is just the number of pixels for the height and the width of the grid. So in this case, for example, we have a three by three kernel",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=797s",
        "start_time": "797.39"
    },
    {
        "id": "3ca12296",
        "text": "OK. So uh when we um handle like convolution, we have a bunch of like architectural decisions that we can take to decide which type of convolutions uh like to use, right? And so uh here I have like a few uh things, a few settings that we should specify uh when we build our architecture. So we should specify the grid size of the convolution, the stride, the depth and the number of kernels. So let's look into this one by one. Let's start from grid size. Well, this is like very intuitive, right. So uh the grid size is just the number of pixels for the height and the width of the grid. So in this case, for example, we have a three by three kernel and in this case, we have a five by five kernel because we have just like 55 values like for, for the width and five values like for, for each height, right. So, and obviously like, let's remember that uh each uh like of these guys basically, like it's equivalent like to, to a pixel and it analyzes just like one pixel when it performs uh when we perform convolutions,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=806s",
        "start_time": "806.15"
    },
    {
        "id": "30daf1a7",
        "text": "So we should specify the grid size of the convolution, the stride, the depth and the number of kernels. So let's look into this one by one. Let's start from grid size. Well, this is like very intuitive, right. So uh the grid size is just the number of pixels for the height and the width of the grid. So in this case, for example, we have a three by three kernel and in this case, we have a five by five kernel because we have just like 55 values like for, for the width and five values like for, for each height, right. So, and obviously like, let's remember that uh each uh like of these guys basically, like it's equivalent like to, to a pixel and it analyzes just like one pixel when it performs uh when we perform convolutions, right? So you'll notice that I'm using um grid sizes with odd numbers. So why is that the case? Well, that's the case. Uh because when we have odd numbers, we have a central",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=831s",
        "start_time": "831.799"
    },
    {
        "id": "c6669cdc",
        "text": "and in this case, we have a five by five kernel because we have just like 55 values like for, for the width and five values like for, for each height, right. So, and obviously like, let's remember that uh each uh like of these guys basically, like it's equivalent like to, to a pixel and it analyzes just like one pixel when it performs uh when we perform convolutions, right? So you'll notice that I'm using um grid sizes with odd numbers. So why is that the case? Well, that's the case. Uh because when we have odd numbers, we have a central um a value that we can use like as a, as a center as a reference uh on the image when we start and run the uh the, the convolution, right? And so it's, it's usually like you, you'll see odd numbers. Now um there, there's usually you'll also see um square",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=859s",
        "start_time": "859.099"
    },
    {
        "id": "1697afb4",
        "text": "right? So you'll notice that I'm using um grid sizes with odd numbers. So why is that the case? Well, that's the case. Uh because when we have odd numbers, we have a central um a value that we can use like as a, as a center as a reference uh on the image when we start and run the uh the, the convolution, right? And so it's, it's usually like you, you'll see odd numbers. Now um there, there's usually you'll also see um square kind of like uh kernels. But you can potentially use also like nouns square like rectangular uh uh grids, right? For example, a one by three or a three by one kernel,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=886s",
        "start_time": "886.869"
    },
    {
        "id": "8b8a4051",
        "text": "um a value that we can use like as a, as a center as a reference uh on the image when we start and run the uh the, the convolution, right? And so it's, it's usually like you, you'll see odd numbers. Now um there, there's usually you'll also see um square kind of like uh kernels. But you can potentially use also like nouns square like rectangular uh uh grids, right? For example, a one by three or a three by one kernel, right? So this was about great size. Now, let's move on to another parameter stride. So what's the stride? Well, the stride is quite simple. It's the step size that we use for sliding the kernel on the image,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=902s",
        "start_time": "902.84"
    },
    {
        "id": "0acb3e65",
        "text": "kind of like uh kernels. But you can potentially use also like nouns square like rectangular uh uh grids, right? For example, a one by three or a three by one kernel, right? So this was about great size. Now, let's move on to another parameter stride. So what's the stride? Well, the stride is quite simple. It's the step size that we use for sliding the kernel on the image, right? So, and the stride itself again is indicated in pixels. So let's try to look at a stride. Uh So here we have um our image and we have a three by three kernel that we are applying. Now, we let's say we have a stride, a one.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=926s",
        "start_time": "926.169"
    },
    {
        "id": "8b878068",
        "text": "right? So this was about great size. Now, let's move on to another parameter stride. So what's the stride? Well, the stride is quite simple. It's the step size that we use for sliding the kernel on the image, right? So, and the stride itself again is indicated in pixels. So let's try to look at a stride. Uh So here we have um our image and we have a three by three kernel that we are applying. Now, we let's say we have a stride, a one. So we'll just move like this. So we start here, then we'll move this by one pixel and then we'll keep moving by another pixel and so on and so forth. And so as you can see, we are sliding just by one pixel and this is a stride of one. Now, let's take, let's try a stride of two. This again, like is also like quite simple to understand because like we start in this position and then we jump by two pixels right. Here we go.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=939s",
        "start_time": "939.88"
    },
    {
        "id": "191da03c",
        "text": "right? So, and the stride itself again is indicated in pixels. So let's try to look at a stride. Uh So here we have um our image and we have a three by three kernel that we are applying. Now, we let's say we have a stride, a one. So we'll just move like this. So we start here, then we'll move this by one pixel and then we'll keep moving by another pixel and so on and so forth. And so as you can see, we are sliding just by one pixel and this is a stride of one. Now, let's take, let's try a stride of two. This again, like is also like quite simple to understand because like we start in this position and then we jump by two pixels right. Here we go. So this is a stride of two. Now, uh this you can specify both the horizontal stride and the vertical stride. So uh when you arrive like at the end of the image, like how much you want to go down in the image say like by one pixel or two pixels. And the important thing to understand is that again, the stride",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=958s",
        "start_time": "958.13"
    },
    {
        "id": "4cc1f20f",
        "text": "So we'll just move like this. So we start here, then we'll move this by one pixel and then we'll keep moving by another pixel and so on and so forth. And so as you can see, we are sliding just by one pixel and this is a stride of one. Now, let's take, let's try a stride of two. This again, like is also like quite simple to understand because like we start in this position and then we jump by two pixels right. Here we go. So this is a stride of two. Now, uh this you can specify both the horizontal stride and the vertical stride. So uh when you arrive like at the end of the image, like how much you want to go down in the image say like by one pixel or two pixels. And the important thing to understand is that again, the stride uh doesn't necessarily need to be uh like the same for like the horizontal value and the vertical value, we can have a stride of two horizontally and a stride of one vertically, for example. But usually you tend to use like uh the same uh stride",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=976s",
        "start_time": "976.75"
    },
    {
        "id": "f97e38fe",
        "text": "So this is a stride of two. Now, uh this you can specify both the horizontal stride and the vertical stride. So uh when you arrive like at the end of the image, like how much you want to go down in the image say like by one pixel or two pixels. And the important thing to understand is that again, the stride uh doesn't necessarily need to be uh like the same for like the horizontal value and the vertical value, we can have a stride of two horizontally and a stride of one vertically, for example. But usually you tend to use like uh the same uh stride uh for like the vertical part and the horizontal cool. So these are so grid size and straight are very important uh like settings for uh the kernel.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1005s",
        "start_time": "1005.94"
    },
    {
        "id": "9ac85c28",
        "text": "uh doesn't necessarily need to be uh like the same for like the horizontal value and the vertical value, we can have a stride of two horizontally and a stride of one vertically, for example. But usually you tend to use like uh the same uh stride uh for like the vertical part and the horizontal cool. So these are so grid size and straight are very important uh like settings for uh the kernel. But then we have another one which is a depth even though I would say like this is like this comes like uh it's more constrained. You don't have like much like leverage like there and you can decide what you want to do. But the basic idea is that if you have a gray scale image, the depth is equal to one. But if you have a color image that's like for example, represented in, in our",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1026s",
        "start_time": "1026.68"
    },
    {
        "id": "a33b78c8",
        "text": "uh for like the vertical part and the horizontal cool. So these are so grid size and straight are very important uh like settings for uh the kernel. But then we have another one which is a depth even though I would say like this is like this comes like uh it's more constrained. You don't have like much like leverage like there and you can decide what you want to do. But the basic idea is that if you have a gray scale image, the depth is equal to one. But if you have a color image that's like for example, represented in, in our R GB, what that basically means is that that image that each pixel for uh a color image has three values, one for the red, one for the green and one for uh the blue collar, right? And so what we do in terms of kernels is we have a kernel. Uh So the the kernel like is divided",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1042s",
        "start_time": "1042.469"
    },
    {
        "id": "a5ea1135",
        "text": "But then we have another one which is a depth even though I would say like this is like this comes like uh it's more constrained. You don't have like much like leverage like there and you can decide what you want to do. But the basic idea is that if you have a gray scale image, the depth is equal to one. But if you have a color image that's like for example, represented in, in our R GB, what that basically means is that that image that each pixel for uh a color image has three values, one for the red, one for the green and one for uh the blue collar, right? And so what we do in terms of kernels is we have a kernel. Uh So the the kernel like is divided like into three like parts, three grids. And so we have a grid for the red channel, we have a grid for the green channel and the third grid for the blue channel. And so, and these are independent, right? So a kernel in the case of R GB uh image data,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1053s",
        "start_time": "1053.43"
    },
    {
        "id": "d4f5df25",
        "text": "R GB, what that basically means is that that image that each pixel for uh a color image has three values, one for the red, one for the green and one for uh the blue collar, right? And so what we do in terms of kernels is we have a kernel. Uh So the the kernel like is divided like into three like parts, three grids. And so we have a grid for the red channel, we have a grid for the green channel and the third grid for the blue channel. And so, and these are independent, right? So a kernel in the case of R GB uh image data, it's gonna have uh a three dimensional uh it's, it's a three dimensional array, right. So where like the, the first two dimensions uh represent the, for example, the width and the height of the, of the kernel and the",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1077s",
        "start_time": "1077.786"
    },
    {
        "id": "48aa1dee",
        "text": "like into three like parts, three grids. And so we have a grid for the red channel, we have a grid for the green channel and the third grid for the blue channel. And so, and these are independent, right? So a kernel in the case of R GB uh image data, it's gonna have uh a three dimensional uh it's, it's a three dimensional array, right. So where like the, the first two dimensions uh represent the, for example, the width and the height of the, of the kernel and the third dimension here it's the depth. So in this case, in this example, we have a three by three kernel which again, we should multiply by three which because that's the depth basically it's like 32 dimensional uh grids. So one for red, one for green and one for blue. And so the total number of weights in this case is equal to 27 because it's basically three by three",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1102s",
        "start_time": "1102.141"
    },
    {
        "id": "6b22eb67",
        "text": "it's gonna have uh a three dimensional uh it's, it's a three dimensional array, right. So where like the, the first two dimensions uh represent the, for example, the width and the height of the, of the kernel and the third dimension here it's the depth. So in this case, in this example, we have a three by three kernel which again, we should multiply by three which because that's the depth basically it's like 32 dimensional uh grids. So one for red, one for green and one for blue. And so the total number of weights in this case is equal to 27 because it's basically three by three uh by three, right? OK. So let's remember guys. So if we are dealing with uh R GB data, we're gonna have a three dimensional array as our kernel where the first two dimensions are the width and the height. And the third dimension is the depth or it's also called the channel, right? Cos in R GB we have three channels",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1126s",
        "start_time": "1126.77"
    },
    {
        "id": "7d1a4541",
        "text": "third dimension here it's the depth. So in this case, in this example, we have a three by three kernel which again, we should multiply by three which because that's the depth basically it's like 32 dimensional uh grids. So one for red, one for green and one for blue. And so the total number of weights in this case is equal to 27 because it's basically three by three uh by three, right? OK. So let's remember guys. So if we are dealing with uh R GB data, we're gonna have a three dimensional array as our kernel where the first two dimensions are the width and the height. And the third dimension is the depth or it's also called the channel, right? Cos in R GB we have three channels great. So now on to the last uh setting that we want to look into, so this is the number of kernels, but this is really not that related to like the the kernel itself, but it's related to the to the convolutional layer. Now,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1142s",
        "start_time": "1142.3"
    },
    {
        "id": "f313db38",
        "text": "uh by three, right? OK. So let's remember guys. So if we are dealing with uh R GB data, we're gonna have a three dimensional array as our kernel where the first two dimensions are the width and the height. And the third dimension is the depth or it's also called the channel, right? Cos in R GB we have three channels great. So now on to the last uh setting that we want to look into, so this is the number of kernels, but this is really not that related to like the the kernel itself, but it's related to the to the convolutional layer. Now, convolutional layer can have and usually has multiple kernels. But let's remember that each kernel outputs a single two dimensional array and it's that output convolution that we've calculated before, right? And so",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1172s",
        "start_time": "1172.56"
    },
    {
        "id": "cddf656b",
        "text": "great. So now on to the last uh setting that we want to look into, so this is the number of kernels, but this is really not that related to like the the kernel itself, but it's related to the to the convolutional layer. Now, convolutional layer can have and usually has multiple kernels. But let's remember that each kernel outputs a single two dimensional array and it's that output convolution that we've calculated before, right? And so um one question could be, so how many outputs do we have from a convolutional layer? Well, we have as many two D arrays as the number of kernels, right? So if we have for example, five kernels",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1198s",
        "start_time": "1198.5"
    },
    {
        "id": "9e4f43bc",
        "text": "convolutional layer can have and usually has multiple kernels. But let's remember that each kernel outputs a single two dimensional array and it's that output convolution that we've calculated before, right? And so um one question could be, so how many outputs do we have from a convolutional layer? Well, we have as many two D arrays as the number of kernels, right? So if we have for example, five kernels in a convolutional layer, so we're gonna have 52 D arrays because we're gonna apply each kernel to the input image and we're gonna get five separate output uh to the to the arrays, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1216s",
        "start_time": "1216.589"
    },
    {
        "id": "ed939a85",
        "text": "um one question could be, so how many outputs do we have from a convolutional layer? Well, we have as many two D arrays as the number of kernels, right? So if we have for example, five kernels in a convolutional layer, so we're gonna have 52 D arrays because we're gonna apply each kernel to the input image and we're gonna get five separate output uh to the to the arrays, right? OK. So this is the number of kernels. Now we've learned quite a lot about uh convolution. Now we need to look at the other side of the coin. Well, I should say like the other part that's usually used in CNN that's called pooling. Now, the, the the the most difficult part is behind this because pooling is quite intuitive.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1235s",
        "start_time": "1235.05"
    },
    {
        "id": "151d1b4b",
        "text": "in a convolutional layer, so we're gonna have 52 D arrays because we're gonna apply each kernel to the input image and we're gonna get five separate output uh to the to the arrays, right? OK. So this is the number of kernels. Now we've learned quite a lot about uh convolution. Now we need to look at the other side of the coin. Well, I should say like the other part that's usually used in CNN that's called pooling. Now, the, the the the most difficult part is behind this because pooling is quite intuitive. So what pooling does like at the end of the day is just like down sampling an image. So it's basically shrinks an image and we do that in a sense in a similar manner to convolution in the sense that we overlay a grid on top of an image.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1251s",
        "start_time": "1251.555"
    },
    {
        "id": "b1eb5f9d",
        "text": "OK. So this is the number of kernels. Now we've learned quite a lot about uh convolution. Now we need to look at the other side of the coin. Well, I should say like the other part that's usually used in CNN that's called pooling. Now, the, the the the most difficult part is behind this because pooling is quite intuitive. So what pooling does like at the end of the day is just like down sampling an image. So it's basically shrinks an image and we do that in a sense in a similar manner to convolution in the sense that we overlay a grid on top of an image. And then in terms of pooling, we have like different options. And the, the, the two that are like the most used are like max pooling and average pooling even though I should say that. Um Yeah, I'd say like in deep learning, like max pooling like is the, is the main one right",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1268s",
        "start_time": "1268.719"
    },
    {
        "id": "7e1ec461",
        "text": "So what pooling does like at the end of the day is just like down sampling an image. So it's basically shrinks an image and we do that in a sense in a similar manner to convolution in the sense that we overlay a grid on top of an image. And then in terms of pooling, we have like different options. And the, the, the two that are like the most used are like max pooling and average pooling even though I should say that. Um Yeah, I'd say like in deep learning, like max pooling like is the, is the main one right now, as I said, like pulling is quite uh intrusive and simple and it doesn't have like any parameters. So we don't learn anything in terms of, of pooling. It's a simple like mathematical operation that we perform. OK. So let's uh take a look at the different pooling settings that we'll have. And so",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1293s",
        "start_time": "1293.05"
    },
    {
        "id": "2d9bbcf9",
        "text": "And then in terms of pooling, we have like different options. And the, the, the two that are like the most used are like max pooling and average pooling even though I should say that. Um Yeah, I'd say like in deep learning, like max pooling like is the, is the main one right now, as I said, like pulling is quite uh intrusive and simple and it doesn't have like any parameters. So we don't learn anything in terms of, of pooling. It's a simple like mathematical operation that we perform. OK. So let's uh take a look at the different pooling settings that we'll have. And so we have like very simple ones again, like there's the grid size that determines like how big like the uh the pooling grid is gonna be, we have the stride again, which is like the, the, the step uh the step size used for sliding the grid on top of an image and then we have the type of pooling. So max pooling or average poling. OK. So now we know like we have like an overview of pooling.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1310s",
        "start_time": "1310.93"
    },
    {
        "id": "9e5cc576",
        "text": "now, as I said, like pulling is quite uh intrusive and simple and it doesn't have like any parameters. So we don't learn anything in terms of, of pooling. It's a simple like mathematical operation that we perform. OK. So let's uh take a look at the different pooling settings that we'll have. And so we have like very simple ones again, like there's the grid size that determines like how big like the uh the pooling grid is gonna be, we have the stride again, which is like the, the, the step uh the step size used for sliding the grid on top of an image and then we have the type of pooling. So max pooling or average poling. OK. So now we know like we have like an overview of pooling. Uh Now let's take a look at the at how pooling really works. OK. So here we'll take a look at a max pooling where we have a two by two grid and we are gonna be using a stride of two, both horizontally and vertically, right? OK. So here we have our two by two",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1327s",
        "start_time": "1327.15"
    },
    {
        "id": "48fa1923",
        "text": "we have like very simple ones again, like there's the grid size that determines like how big like the uh the pooling grid is gonna be, we have the stride again, which is like the, the, the step uh the step size used for sliding the grid on top of an image and then we have the type of pooling. So max pooling or average poling. OK. So now we know like we have like an overview of pooling. Uh Now let's take a look at the at how pooling really works. OK. So here we'll take a look at a max pooling where we have a two by two grid and we are gonna be using a stride of two, both horizontally and vertically, right? OK. So here we have our two by two um pulling greed. And so, and, and we just like go like on top of like the input and so you can image like you can image that this is an image, right? And, and so here uh we have like these values, right? And given we are doing max pooling, we just pick the, the greatest value",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1346s",
        "start_time": "1346.439"
    },
    {
        "id": "50b9c82b",
        "text": "Uh Now let's take a look at the at how pooling really works. OK. So here we'll take a look at a max pooling where we have a two by two grid and we are gonna be using a stride of two, both horizontally and vertically, right? OK. So here we have our two by two um pulling greed. And so, and, and we just like go like on top of like the input and so you can image like you can image that this is an image, right? And, and so here uh we have like these values, right? And given we are doing max pooling, we just pick the, the greatest value and we log it in the output grid over here, right? OK. And so in by doing so we are down sampling uh the original input because basically we are just like getting one parameter out of like these four parameters. Now, how are we gonna continue doing this? Well, if you've like followed uh like attentively like up until now. So you should know that. Now, I'm just gonna slide",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1373s",
        "start_time": "1373.53"
    },
    {
        "id": "3f40b1df",
        "text": "um pulling greed. And so, and, and we just like go like on top of like the input and so you can image like you can image that this is an image, right? And, and so here uh we have like these values, right? And given we are doing max pooling, we just pick the, the greatest value and we log it in the output grid over here, right? OK. And so in by doing so we are down sampling uh the original input because basically we are just like getting one parameter out of like these four parameters. Now, how are we gonna continue doing this? Well, if you've like followed uh like attentively like up until now. So you should know that. Now, I'm just gonna slide the grid with a stride of two. So I'm moving two pixels like on the right. And then again, here I'm gonna do the same thing. I'm gonna pick the highest number, which in this case is 10 and I'm gonna log it over here. Right. We continue and we pick 12 and finally we pick seven.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1396s",
        "start_time": "1396.8"
    },
    {
        "id": "5554a821",
        "text": "and we log it in the output grid over here, right? OK. And so in by doing so we are down sampling uh the original input because basically we are just like getting one parameter out of like these four parameters. Now, how are we gonna continue doing this? Well, if you've like followed uh like attentively like up until now. So you should know that. Now, I'm just gonna slide the grid with a stride of two. So I'm moving two pixels like on the right. And then again, here I'm gonna do the same thing. I'm gonna pick the highest number, which in this case is 10 and I'm gonna log it over here. Right. We continue and we pick 12 and finally we pick seven. Now, um, there's obviously a mathematical relationship between the size of the output, the size of the input and the, the stride, right? And the grade size, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1421s",
        "start_time": "1421.51"
    },
    {
        "id": "24195cec",
        "text": "the grid with a stride of two. So I'm moving two pixels like on the right. And then again, here I'm gonna do the same thing. I'm gonna pick the highest number, which in this case is 10 and I'm gonna log it over here. Right. We continue and we pick 12 and finally we pick seven. Now, um, there's obviously a mathematical relationship between the size of the output, the size of the input and the, the stride, right? And the grade size, right? Ok. So I'm not gonna give you like the general rule but, uh that you can, and you should definitely like uh understand by yourself. But like in this case where we have a uh two by two max pooling reed uh which try to, you'll see that we are basically uh hal like the width and the height of the input. So here we have a four by four like input",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1449s",
        "start_time": "1449.479"
    },
    {
        "id": "215202ae",
        "text": "Now, um, there's obviously a mathematical relationship between the size of the output, the size of the input and the, the stride, right? And the grade size, right? Ok. So I'm not gonna give you like the general rule but, uh that you can, and you should definitely like uh understand by yourself. But like in this case where we have a uh two by two max pooling reed uh which try to, you'll see that we are basically uh hal like the width and the height of the input. So here we have a four by four like input um grid and the output grid in this case is a two by two, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1468s",
        "start_time": "1468.089"
    },
    {
        "id": "ce2d71f0",
        "text": "Ok. So I'm not gonna give you like the general rule but, uh that you can, and you should definitely like uh understand by yourself. But like in this case where we have a uh two by two max pooling reed uh which try to, you'll see that we are basically uh hal like the width and the height of the input. So here we have a four by four like input um grid and the output grid in this case is a two by two, right? OK. So we're basically done. So we know what pooling is. So now we we should uh put together like everything we've learned and understand that a CNN uh just like uses like yeah, pooling",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1482s",
        "start_time": "1482.54"
    },
    {
        "id": "fd1b1953",
        "text": "um grid and the output grid in this case is a two by two, right? OK. So we're basically done. So we know what pooling is. So now we we should uh put together like everything we've learned and understand that a CNN uh just like uses like yeah, pooling and uh convolutions, right? So convolutional layers and pooling layers uh and uh and it uses it like by putting more of this like uh together. So let's take a look at at a typical CNN architecture. So here we have the input, which is kind it's a simple image, right? And then here we have a feature learning um",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1510s",
        "start_time": "1510.599"
    },
    {
        "id": "730fe33e",
        "text": "OK. So we're basically done. So we know what pooling is. So now we we should uh put together like everything we've learned and understand that a CNN uh just like uses like yeah, pooling and uh convolutions, right? So convolutional layers and pooling layers uh and uh and it uses it like by putting more of this like uh together. So let's take a look at at a typical CNN architecture. So here we have the input, which is kind it's a simple image, right? And then here we have a feature learning um I would say like face like in the in the CNN where we have a bunch of like convolutional layers followed by pooling layers. And so now the um the the network can be like as deep as you want really. So you can have up to like 50 100 layers, if not more, right? And after like the feature learning like phase, usually you get a",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1516s",
        "start_time": "1516.29"
    },
    {
        "id": "db40d9b7",
        "text": "and uh convolutions, right? So convolutional layers and pooling layers uh and uh and it uses it like by putting more of this like uh together. So let's take a look at at a typical CNN architecture. So here we have the input, which is kind it's a simple image, right? And then here we have a feature learning um I would say like face like in the in the CNN where we have a bunch of like convolutional layers followed by pooling layers. And so now the um the the network can be like as deep as you want really. So you can have up to like 50 100 layers, if not more, right? And after like the feature learning like phase, usually you get a uh fully connected uh like layer. So here at the end of like this uh the feature learning like section of the CNN, we usually like flatten the results. So we, we move it like from two D to like a one D uh vector. And",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1535s",
        "start_time": "1535.199"
    },
    {
        "id": "6bfc8501",
        "text": "I would say like face like in the in the CNN where we have a bunch of like convolutional layers followed by pooling layers. And so now the um the the network can be like as deep as you want really. So you can have up to like 50 100 layers, if not more, right? And after like the feature learning like phase, usually you get a uh fully connected uh like layer. So here at the end of like this uh the feature learning like section of the CNN, we usually like flatten the results. So we, we move it like from two D to like a one D uh vector. And then we, we pass that information into like one or more fully connected layers. And in the end, we have a soft max classifier which provides us with a uh dis probability distribution on top of a number of like different categories. So like",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1560s",
        "start_time": "1560.3"
    },
    {
        "id": "ff038d92",
        "text": "uh fully connected uh like layer. So here at the end of like this uh the feature learning like section of the CNN, we usually like flatten the results. So we, we move it like from two D to like a one D uh vector. And then we, we pass that information into like one or more fully connected layers. And in the end, we have a soft max classifier which provides us with a uh dis probability distribution on top of a number of like different categories. So like uh like in this case, we are trying to like in this particular example, we are trying to classify different types of like uh transportation vehicles. And so here soft max is gonna give us like values for car track, van, bicycle train, right? Or airplane, for example, right? But let's take a look at this feature learning uh uh like segment of the CNN, which is like the most interesting one for our purposes, right.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1586s",
        "start_time": "1586.489"
    },
    {
        "id": "57aa37a1",
        "text": "then we, we pass that information into like one or more fully connected layers. And in the end, we have a soft max classifier which provides us with a uh dis probability distribution on top of a number of like different categories. So like uh like in this case, we are trying to like in this particular example, we are trying to classify different types of like uh transportation vehicles. And so here soft max is gonna give us like values for car track, van, bicycle train, right? Or airplane, for example, right? But let's take a look at this feature learning uh uh like segment of the CNN, which is like the most interesting one for our purposes, right. So what happens here is basically that at each uh convolution um uh a convolutional layer, basically what we're doing is we're trying to extract features, right? And basically, the idea is that at the beginning towards like the beginning of this feature learning segment, we extract uh very uh like low level features. So it could be like edges. Uh but then",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1602s",
        "start_time": "1602.449"
    },
    {
        "id": "adb223c7",
        "text": "uh like in this case, we are trying to like in this particular example, we are trying to classify different types of like uh transportation vehicles. And so here soft max is gonna give us like values for car track, van, bicycle train, right? Or airplane, for example, right? But let's take a look at this feature learning uh uh like segment of the CNN, which is like the most interesting one for our purposes, right. So what happens here is basically that at each uh convolution um uh a convolutional layer, basically what we're doing is we're trying to extract features, right? And basically, the idea is that at the beginning towards like the beginning of this feature learning segment, we extract uh very uh like low level features. So it could be like edges. Uh but then uh moving forward uh like from edges, we can use, we can leverage them, for example, to uh to arrive at shapes, then going deeper in the network from shapes, we can arrive at objects. So for example, like a car could be,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1618s",
        "start_time": "1618.63"
    },
    {
        "id": "907f6855",
        "text": "So what happens here is basically that at each uh convolution um uh a convolutional layer, basically what we're doing is we're trying to extract features, right? And basically, the idea is that at the beginning towards like the beginning of this feature learning segment, we extract uh very uh like low level features. So it could be like edges. Uh but then uh moving forward uh like from edges, we can use, we can leverage them, for example, to uh to arrive at shapes, then going deeper in the network from shapes, we can arrive at objects. So for example, like a car could be, I don't know, like uh could be represented say by like a few circles uh which could be like the um the tires uh and a and a rectangle, for example, that could be like, I mean the whole uh like space like of the of the car uh like itself, right? So",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1646s",
        "start_time": "1646.369"
    },
    {
        "id": "38a500ca",
        "text": "uh moving forward uh like from edges, we can use, we can leverage them, for example, to uh to arrive at shapes, then going deeper in the network from shapes, we can arrive at objects. So for example, like a car could be, I don't know, like uh could be represented say by like a few circles uh which could be like the um the tires uh and a and a rectangle, for example, that could be like, I mean the whole uh like space like of the of the car uh like itself, right? So and but this is like important to understand. So we we start with low level features and then while we move from one convolution layer to the next we abstract higher level features. Cool. And so this is like an intuition how a CNN architecture uh works? Cool.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1672s",
        "start_time": "1672.05"
    },
    {
        "id": "1278ef2f",
        "text": "I don't know, like uh could be represented say by like a few circles uh which could be like the um the tires uh and a and a rectangle, for example, that could be like, I mean the whole uh like space like of the of the car uh like itself, right? So and but this is like important to understand. So we we start with low level features and then while we move from one convolution layer to the next we abstract higher level features. Cool. And so this is like an intuition how a CNN architecture uh works? Cool. OK. So now you may be wondering, OK, so we've spent so much time on talking about how wonderful CNN S are with images. So what about audio? After all, we are talking about deep learning for audio like in this uh uh series, right? Yeah. But uh the great thing is that uh we can think of audio in a sense itself as a, as an image, right? So if you guys remember,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1689s",
        "start_time": "1689.18"
    },
    {
        "id": "266b7a51",
        "text": "and but this is like important to understand. So we we start with low level features and then while we move from one convolution layer to the next we abstract higher level features. Cool. And so this is like an intuition how a CNN architecture uh works? Cool. OK. So now you may be wondering, OK, so we've spent so much time on talking about how wonderful CNN S are with images. So what about audio? After all, we are talking about deep learning for audio like in this uh uh series, right? Yeah. But uh the great thing is that uh we can think of audio in a sense itself as a, as an image, right? So if you guys remember, so what we usually use when we um use deep learning like audio. Uh we we use like spectrograms, we use MF CCS all of this type of like features, but now we've already like visualize them like as an image. And at the end of the day, these are big, we can be interpreted as images, right? Both the spectrogram and the MF CCS, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1707s",
        "start_time": "1707.459"
    },
    {
        "id": "48cc6d19",
        "text": "OK. So now you may be wondering, OK, so we've spent so much time on talking about how wonderful CNN S are with images. So what about audio? After all, we are talking about deep learning for audio like in this uh uh series, right? Yeah. But uh the great thing is that uh we can think of audio in a sense itself as a, as an image, right? So if you guys remember, so what we usually use when we um use deep learning like audio. Uh we we use like spectrograms, we use MF CCS all of this type of like features, but now we've already like visualize them like as an image. And at the end of the day, these are big, we can be interpreted as images, right? Both the spectrogram and the MF CCS, right? So uh in the case of a spectrogram, for example, time, the different time beings and the frequencies that we have here can be folded like the X and Y indexes for the the pixels of of an image",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1729s",
        "start_time": "1729.939"
    },
    {
        "id": "2a78d05f",
        "text": "so what we usually use when we um use deep learning like audio. Uh we we use like spectrograms, we use MF CCS all of this type of like features, but now we've already like visualize them like as an image. And at the end of the day, these are big, we can be interpreted as images, right? Both the spectrogram and the MF CCS, right? So uh in the case of a spectrogram, for example, time, the different time beings and the frequencies that we have here can be folded like the X and Y indexes for the the pixels of of an image and the amplitude can be thought of the the the value associated to each pixel, right. So in a sense, a spectrogram is a two dimensional array which is really comparable to to an image. And the great thing is that a spectrogram and audio in general has",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1759s",
        "start_time": "1759.02"
    },
    {
        "id": "ef9ffd34",
        "text": "So uh in the case of a spectrogram, for example, time, the different time beings and the frequencies that we have here can be folded like the X and Y indexes for the the pixels of of an image and the amplitude can be thought of the the the value associated to each pixel, right. So in a sense, a spectrogram is a two dimensional array which is really comparable to to an image. And the great thing is that a spectrogram and audio in general has I would say like structures in them",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1784s",
        "start_time": "1784.54"
    },
    {
        "id": "a9320ba9",
        "text": "and the amplitude can be thought of the the the value associated to each pixel, right. So in a sense, a spectrogram is a two dimensional array which is really comparable to to an image. And the great thing is that a spectrogram and audio in general has I would say like structures in them that are like in a sense like similar to like images. So like the the uh the data itself like is somehow like correlated. It's it's not like completely like random. And so because of like those structures that we can identify in spectrogram and MS CCS CNN work really really well because they are able to extract features while we uh like apply them when we apply like convolutions like on the uh on the audio data,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1800s",
        "start_time": "1800.969"
    },
    {
        "id": "dc7f6a61",
        "text": "I would say like structures in them that are like in a sense like similar to like images. So like the the uh the data itself like is somehow like correlated. It's it's not like completely like random. And so because of like those structures that we can identify in spectrogram and MS CCS CNN work really really well because they are able to extract features while we uh like apply them when we apply like convolutions like on the uh on the audio data, right? So now that we have an intuition of how like all your data can be used in CN MS. Let's try to look at a specific example where we look at MF CCS for uh data, right? And so now the question",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1823s",
        "start_time": "1823.199"
    },
    {
        "id": "abc33132",
        "text": "that are like in a sense like similar to like images. So like the the uh the data itself like is somehow like correlated. It's it's not like completely like random. And so because of like those structures that we can identify in spectrogram and MS CCS CNN work really really well because they are able to extract features while we uh like apply them when we apply like convolutions like on the uh on the audio data, right? So now that we have an intuition of how like all your data can be used in CN MS. Let's try to look at a specific example where we look at MF CCS for uh data, right? And so now the question uh that I want to ask you is like given like these uh settings for uh these like MF CCS like and for all of these audio data, what's gonna be like the, the data shape, right? So we have 13 MFCC,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1826s",
        "start_time": "1826.979"
    },
    {
        "id": "a43eeba0",
        "text": "right? So now that we have an intuition of how like all your data can be used in CN MS. Let's try to look at a specific example where we look at MF CCS for uh data, right? And so now the question uh that I want to ask you is like given like these uh settings for uh these like MF CCS like and for all of these audio data, what's gonna be like the, the data shape, right? So we have 13 MFCC, we have a hop length of 512 samples. Now, if you don't re remember what a hop length is or an MFCC uh like is just like go back to my previous videos where I introduced like all of these audio features.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1854s",
        "start_time": "1854.859"
    },
    {
        "id": "921ed818",
        "text": "uh that I want to ask you is like given like these uh settings for uh these like MF CCS like and for all of these audio data, what's gonna be like the, the data shape, right? So we have 13 MFCC, we have a hop length of 512 samples. Now, if you don't re remember what a hop length is or an MFCC uh like is just like go back to my previous videos where I introduced like all of these audio features. Cool. So we said hop length 512 samples and then we have the total number of samples in a audio file that we are analyzing. That's conveniently 51,200. Now, the question is what's the data shape that we, we, we're gonna expect our CNN to be fed with",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1870s",
        "start_time": "1870.13"
    },
    {
        "id": "a99778c2",
        "text": "we have a hop length of 512 samples. Now, if you don't re remember what a hop length is or an MFCC uh like is just like go back to my previous videos where I introduced like all of these audio features. Cool. So we said hop length 512 samples and then we have the total number of samples in a audio file that we are analyzing. That's conveniently 51,200. Now, the question is what's the data shape that we, we, we're gonna expect our CNN to be fed with and right. So the data shape is 100 by 13 by one. So let's analyze why that's the case. So here we have uh 100 different like time windows at which we take 13 values which are the, the 13 MF CCS that we are extracting, right?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1886s",
        "start_time": "1886.444"
    },
    {
        "id": "66210f52",
        "text": "Cool. So we said hop length 512 samples and then we have the total number of samples in a audio file that we are analyzing. That's conveniently 51,200. Now, the question is what's the data shape that we, we, we're gonna expect our CNN to be fed with and right. So the data shape is 100 by 13 by one. So let's analyze why that's the case. So here we have uh 100 different like time windows at which we take 13 values which are the, the 13 MF CCS that we are extracting, right? And 100 comes by dividing the overall number of samples in the audio file by the hop length which is like the, the sliding window that we use uh like to calculate the MF CCS. And so again, like the two, these two values are like quite understandable. Uh because we have, we, we understand them saying yeah, we have 100 time windows at which we've taken the 13 coefficients",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1903s",
        "start_time": "1903.069"
    },
    {
        "id": "97476439",
        "text": "and right. So the data shape is 100 by 13 by one. So let's analyze why that's the case. So here we have uh 100 different like time windows at which we take 13 values which are the, the 13 MF CCS that we are extracting, right? And 100 comes by dividing the overall number of samples in the audio file by the hop length which is like the, the sliding window that we use uh like to calculate the MF CCS. And so again, like the two, these two values are like quite understandable. Uh because we have, we, we understand them saying yeah, we have 100 time windows at which we've taken the 13 coefficients uh uh the first in MF CCS. Um But why do we have a third dimension here? So by one, do you guys remember depth?",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1926s",
        "start_time": "1926.359"
    },
    {
        "id": "d2a77109",
        "text": "And 100 comes by dividing the overall number of samples in the audio file by the hop length which is like the, the sliding window that we use uh like to calculate the MF CCS. And so again, like the two, these two values are like quite understandable. Uh because we have, we, we understand them saying yeah, we have 100 time windows at which we've taken the 13 coefficients uh uh the first in MF CCS. Um But why do we have a third dimension here? So by one, do you guys remember depth? Yeah. So in this case, we have a depth which is equal to one. So basically all your data can be um kind of like compared to like gray scale images where the depth is equal to one. So we don't have like R GB like representation of audio data. It's just like gray scale, we just have depth, just one channel,",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1949s",
        "start_time": "1949.56"
    },
    {
        "id": "b501b3bf",
        "text": "uh uh the first in MF CCS. Um But why do we have a third dimension here? So by one, do you guys remember depth? Yeah. So in this case, we have a depth which is equal to one. So basically all your data can be um kind of like compared to like gray scale images where the depth is equal to one. So we don't have like R GB like representation of audio data. It's just like gray scale, we just have depth, just one channel, right? And so you may be wondering well, but why do we need to like give like this third dimension? Isn't that redundant? Well, it could be but again, like CNN S are like supposed to work like with uh images and usually images, right are color images most often time. And so they have like channels. So the depth is very important",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1978s",
        "start_time": "1978.29"
    },
    {
        "id": "02c97aff",
        "text": "Yeah. So in this case, we have a depth which is equal to one. So basically all your data can be um kind of like compared to like gray scale images where the depth is equal to one. So we don't have like R GB like representation of audio data. It's just like gray scale, we just have depth, just one channel, right? And so you may be wondering well, but why do we need to like give like this third dimension? Isn't that redundant? Well, it could be but again, like CNN S are like supposed to work like with uh images and usually images, right are color images most often time. And so they have like channels. So the depth is very important and right? And uh in this case, I'm giving you like this data shape here because this is like what tensorflow is gonna like accept uh as yeah for learning purposes. And so like it's good to get like into that like frame of mind.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=1987s",
        "start_time": "1987.77"
    },
    {
        "id": "5087bbfa",
        "text": "right? And so you may be wondering well, but why do we need to like give like this third dimension? Isn't that redundant? Well, it could be but again, like CNN S are like supposed to work like with uh images and usually images, right are color images most often time. And so they have like channels. So the depth is very important and right? And uh in this case, I'm giving you like this data shape here because this is like what tensorflow is gonna like accept uh as yeah for learning purposes. And so like it's good to get like into that like frame of mind. Wow. So this was intense. But at the same time now, you should have a quite clear understanding of a convolutional neural network, what its components are, what's like uh the the processes that come into place, how uh like convolutional pooling layers like work and how the overall architecture like is built together. So",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=2011s",
        "start_time": "2011.199"
    },
    {
        "id": "54f0970a",
        "text": "and right? And uh in this case, I'm giving you like this data shape here because this is like what tensorflow is gonna like accept uh as yeah for learning purposes. And so like it's good to get like into that like frame of mind. Wow. So this was intense. But at the same time now, you should have a quite clear understanding of a convolutional neural network, what its components are, what's like uh the the processes that come into place, how uh like convolutional pooling layers like work and how the overall architecture like is built together. So we should just ask ourselves as usual at the end of these videos and say what's up next? Well, as usual, we'll take all of this theoretical uh information and in the next video we're gonna uh put that into practice. So what we'll do is implementing a music genre classifier. But this time we are gonna be using a CNN.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=2032s",
        "start_time": "2032.209"
    },
    {
        "id": "f48593af",
        "text": "Wow. So this was intense. But at the same time now, you should have a quite clear understanding of a convolutional neural network, what its components are, what's like uh the the processes that come into place, how uh like convolutional pooling layers like work and how the overall architecture like is built together. So we should just ask ourselves as usual at the end of these videos and say what's up next? Well, as usual, we'll take all of this theoretical uh information and in the next video we're gonna uh put that into practice. So what we'll do is implementing a music genre classifier. But this time we are gonna be using a CNN. So stay tuned for that. I hope you really enjoyed the uh the this video. If that's the case, just like the video and if you want to know more and have like more videos like this and never miss one, please consider subscribing and I guess I'll see you next time. Cheers.",
        "video": "15- Convolutional Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "t3qWfUYJEYU",
        "youtube_link": "https://www.youtube.com/watch?v=t3qWfUYJEYU&t=2049s",
        "start_time": "2049.678"
    },
    {
        "id": "84c9802b",
        "text": "Hi, everybody and welcome to another video in the Deep Learning for audio with Python series. This time, we're gonna build a convolutional neural network for performing music genre classification. So basically, we're building on top of the previous video where we reviewed. Uh Well, we analyze what a CNN is and how it works as well as like the work that we've done on previous videos on uh music genre classification using a multi layer perception. Now, this is gonna be like a quite intense video. So just like take it and uh relax uh but this is gonna be fun for real. OK. So what I want to do first is just like provide all the different steps, high level steps that we need to go through to build uh like this um CNM. And so uh let me just like start by doing if name is equal to main, right? OK. And so here I'll just like jot down all the different steps. So let's start from the first one. So we want to create a train validation and test sets. Now we've already seen the train and test set. Uh We don't really know that much about validation set or cross validation. So, and we'll see that like in a second. But before let's go through like all the different steps here.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "d167313f",
        "text": "Now, this is gonna be like a quite intense video. So just like take it and uh relax uh but this is gonna be fun for real. OK. So what I want to do first is just like provide all the different steps, high level steps that we need to go through to build uh like this um CNM. And so uh let me just like start by doing if name is equal to main, right? OK. And so here I'll just like jot down all the different steps. So let's start from the first one. So we want to create a train validation and test sets. Now we've already seen the train and test set. Uh We don't really know that much about validation set or cross validation. So, and we'll see that like in a second. But before let's go through like all the different steps here. OK? So before anything else, we build like this different like train validation and test sets, then we want to actually uh build uh the, build the CNN net,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=27s",
        "start_time": "27.52"
    },
    {
        "id": "e983d809",
        "text": "OK. And so here I'll just like jot down all the different steps. So let's start from the first one. So we want to create a train validation and test sets. Now we've already seen the train and test set. Uh We don't really know that much about validation set or cross validation. So, and we'll see that like in a second. But before let's go through like all the different steps here. OK? So before anything else, we build like this different like train validation and test sets, then we want to actually uh build uh the, build the CNN net, right? So once we've built the CNN network, we need to compile, compile the network",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=57s",
        "start_time": "57.38"
    },
    {
        "id": "a584ecd6",
        "text": "OK? So before anything else, we build like this different like train validation and test sets, then we want to actually uh build uh the, build the CNN net, right? So once we've built the CNN network, we need to compile, compile the network and then train the CNN.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=85s",
        "start_time": "85.309"
    },
    {
        "id": "bda6346e",
        "text": "right? So once we've built the CNN network, we need to compile, compile the network and then train the CNN. And once we've trained it, we are gonna evaluate the uh CNN on the test set.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=101s",
        "start_time": "101.66"
    },
    {
        "id": "ac384230",
        "text": "and then train the CNN. And once we've trained it, we are gonna evaluate the uh CNN on the test set. And finally, this is something like that. Many of you guys have asked me uh in the previous video. So you'd like to show you how to do inference with the model uh that we've trained. So we're gonna uh make a prediction, right? So we're gonna learn how to make predictions, make prediction. Um I'd say like on a sample, right? OK. So we have a lot on our hands here. So let's get started from this uh first thing.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=110s",
        "start_time": "110.19"
    },
    {
        "id": "2690a2a3",
        "text": "And once we've trained it, we are gonna evaluate the uh CNN on the test set. And finally, this is something like that. Many of you guys have asked me uh in the previous video. So you'd like to show you how to do inference with the model uh that we've trained. So we're gonna uh make a prediction, right? So we're gonna learn how to make predictions, make prediction. Um I'd say like on a sample, right? OK. So we have a lot on our hands here. So let's get started from this uh first thing. So um what I want to um let you guys understand here is that so far we focused on only like a couple of like uh sets when we were doing like training on RDL and deep learning like models. So then we had like the train set that we use for training purposes and the test set that we use for evaluation.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=115s",
        "start_time": "115.22"
    },
    {
        "id": "624b6392",
        "text": "And finally, this is something like that. Many of you guys have asked me uh in the previous video. So you'd like to show you how to do inference with the model uh that we've trained. So we're gonna uh make a prediction, right? So we're gonna learn how to make predictions, make prediction. Um I'd say like on a sample, right? OK. So we have a lot on our hands here. So let's get started from this uh first thing. So um what I want to um let you guys understand here is that so far we focused on only like a couple of like uh sets when we were doing like training on RDL and deep learning like models. So then we had like the train set that we use for training purposes and the test set that we use for evaluation. But the evaluation that we were doing uh was basically, so we just had like the test set. And so that's all good. Like if you're not gonna do like, I mean a lot of stuff, a lot of like hyper parameter tricking. So if you are changing like the number of epochs that we use, the number uh of like yeah, the batch size, the number of layers in the architecture or the number of neurons per layers. But if you are changing all of these things to get better results,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=127s",
        "start_time": "127.449"
    },
    {
        "id": "8e006e03",
        "text": "So um what I want to um let you guys understand here is that so far we focused on only like a couple of like uh sets when we were doing like training on RDL and deep learning like models. So then we had like the train set that we use for training purposes and the test set that we use for evaluation. But the evaluation that we were doing uh was basically, so we just had like the test set. And so that's all good. Like if you're not gonna do like, I mean a lot of stuff, a lot of like hyper parameter tricking. So if you are changing like the number of epochs that we use, the number uh of like yeah, the batch size, the number of layers in the architecture or the number of neurons per layers. But if you are changing all of these things to get better results, you can just use like the test set because in a sense by doing that, you are um kind of like optimizing uh the the the results like of the model to uh the test data to this test set as well.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=157s",
        "start_time": "157.07"
    },
    {
        "id": "e57b6360",
        "text": "But the evaluation that we were doing uh was basically, so we just had like the test set. And so that's all good. Like if you're not gonna do like, I mean a lot of stuff, a lot of like hyper parameter tricking. So if you are changing like the number of epochs that we use, the number uh of like yeah, the batch size, the number of layers in the architecture or the number of neurons per layers. But if you are changing all of these things to get better results, you can just use like the test set because in a sense by doing that, you are um kind of like optimizing uh the the the results like of the model to uh the test data to this test set as well. So what you want to do with the test set is just like create like split like from the main data set and keep it there until you're done with training and hyper parameter tweaking and then just use it in the end so that uh the model you, we are sure that has never seen that data before, right? And so, and this is where the validation set comes in.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=179s",
        "start_time": "179.039"
    },
    {
        "id": "f6748f6d",
        "text": "you can just use like the test set because in a sense by doing that, you are um kind of like optimizing uh the the the results like of the model to uh the test data to this test set as well. So what you want to do with the test set is just like create like split like from the main data set and keep it there until you're done with training and hyper parameter tweaking and then just use it in the end so that uh the model you, we are sure that has never seen that data before, right? And so, and this is where the validation set comes in. So we can split like our data set into the training set, the validation and the test set. So we're gonna use the validation set for evaluating um like our model while we track all the hyper parameters and see how well like it does there. And so we optimize it also like on the validation set, but then we'll leave",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=207s",
        "start_time": "207.429"
    },
    {
        "id": "101ead3b",
        "text": "So what you want to do with the test set is just like create like split like from the main data set and keep it there until you're done with training and hyper parameter tweaking and then just use it in the end so that uh the model you, we are sure that has never seen that data before, right? And so, and this is where the validation set comes in. So we can split like our data set into the training set, the validation and the test set. So we're gonna use the validation set for evaluating um like our model while we track all the hyper parameters and see how well like it does there. And so we optimize it also like on the validation set, but then we'll leave test that in the end so that we've never seen, the model has never seen the data before, right? OK. So how do we get that? Yeah, the splits. Well, we are gonna build a custom function for D do. So what we expect here is uh first of all A X train and now with, with XS, I'm referring like to the inputs, right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=223s",
        "start_time": "223.85"
    },
    {
        "id": "d499f123",
        "text": "So we can split like our data set into the training set, the validation and the test set. So we're gonna use the validation set for evaluating um like our model while we track all the hyper parameters and see how well like it does there. And so we optimize it also like on the validation set, but then we'll leave test that in the end so that we've never seen, the model has never seen the data before, right? OK. So how do we get that? Yeah, the splits. Well, we are gonna build a custom function for D do. So what we expect here is uh first of all A X train and now with, with XS, I'm referring like to the inputs, right? So input train and then we're gonna do an X uh validation and X uh test. And then we also wanna get the Y uh train,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=249s",
        "start_time": "249.059"
    },
    {
        "id": "1a05cbc0",
        "text": "test that in the end so that we've never seen, the model has never seen the data before, right? OK. So how do we get that? Yeah, the splits. Well, we are gonna build a custom function for D do. So what we expect here is uh first of all A X train and now with, with XS, I'm referring like to the inputs, right? So input train and then we're gonna do an X uh validation and X uh test. And then we also wanna get the Y uh train, the Y validation and the Y test.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=271s",
        "start_time": "271.065"
    },
    {
        "id": "4c68f640",
        "text": "So input train and then we're gonna do an X uh validation and X uh test. And then we also wanna get the Y uh train, the Y validation and the Y test. Cool. So, and for why here I'm referring to the outputs or the targets. Cool. OK? So let's uh create like this uh this thing, right? So the, the, the, the function or let's just like use it. OK? So we are gonna use a function that's called prepare data sets.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=298s",
        "start_time": "298.45"
    },
    {
        "id": "8afd49f2",
        "text": "the Y validation and the Y test. Cool. So, and for why here I'm referring to the outputs or the targets. Cool. OK? So let's uh create like this uh this thing, right? So the, the, the, the function or let's just like use it. OK? So we are gonna use a function that's called prepare data sets. Now, this is a custom function. So we need to uh define it and here we're gonna pass in a couple, a couple of arguments. So one is uh is gonna be called the test size and uh we'll put this like to 0.25 and the other one is gonna be called the uh validation size",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=313s",
        "start_time": "313.529"
    },
    {
        "id": "f564a071",
        "text": "Cool. So, and for why here I'm referring to the outputs or the targets. Cool. OK? So let's uh create like this uh this thing, right? So the, the, the, the function or let's just like use it. OK? So we are gonna use a function that's called prepare data sets. Now, this is a custom function. So we need to uh define it and here we're gonna pass in a couple, a couple of arguments. So one is uh is gonna be called the test size and uh we'll put this like to 0.25 and the other one is gonna be called the uh validation size and we'll put this uh to 0.2. Uh So what the test size uh like it tells us is basically how much of the training set we wanna use for um the, the test set, right? And for the validation size over here. We, we're basically saying like with that value that",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=318s",
        "start_time": "318.799"
    },
    {
        "id": "a132c9e6",
        "text": "Now, this is a custom function. So we need to uh define it and here we're gonna pass in a couple, a couple of arguments. So one is uh is gonna be called the test size and uh we'll put this like to 0.25 and the other one is gonna be called the uh validation size and we'll put this uh to 0.2. Uh So what the test size uh like it tells us is basically how much of the training set we wanna use for um the, the test set, right? And for the validation size over here. We, we're basically saying like with that value that 20% of the training set that we've already separated from the test set is gonna be used for uh the validation set, right? OK. So now let's go build uh this function. So we'll define it over here.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=342s",
        "start_time": "342.44"
    },
    {
        "id": "19fb8498",
        "text": "and we'll put this uh to 0.2. Uh So what the test size uh like it tells us is basically how much of the training set we wanna use for um the, the test set, right? And for the validation size over here. We, we're basically saying like with that value that 20% of the training set that we've already separated from the test set is gonna be used for uh the validation set, right? OK. So now let's go build uh this function. So we'll define it over here. So we'll say, hey, give me uh a prepare uh data sets and we already say thats uh we want a test size and a validation size arguments.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=362s",
        "start_time": "362.91"
    },
    {
        "id": "84d3ce05",
        "text": "20% of the training set that we've already separated from the test set is gonna be used for uh the validation set, right? OK. So now let's go build uh this function. So we'll define it over here. So we'll say, hey, give me uh a prepare uh data sets and we already say thats uh we want a test size and a validation size arguments. Cool. OK. So now what should we do here? So what we want to do here is first of all load in the data. So we'll do a, we want to uh load",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=384s",
        "start_time": "384.25"
    },
    {
        "id": "462123af",
        "text": "So we'll say, hey, give me uh a prepare uh data sets and we already say thats uh we want a test size and a validation size arguments. Cool. OK. So now what should we do here? So what we want to do here is first of all load in the data. So we'll do a, we want to uh load uh data as the first step. Then as a second step, what we wanna do is basically create the sh trainin",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=399s",
        "start_time": "399.44"
    },
    {
        "id": "db1200d2",
        "text": "Cool. OK. So now what should we do here? So what we want to do here is first of all load in the data. So we'll do a, we want to uh load uh data as the first step. Then as a second step, what we wanna do is basically create the sh trainin test split. The first step here is create the train validation split and uh let's stop uh to this uh next steps for now, right. OK. So how do we load the data here? Uh This is straightforward and we already have a function here that we can use. And so I'm gonna use this one that, as we see here, returns X and Y for like all of our data",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=415s",
        "start_time": "415.35"
    },
    {
        "id": "539d8ec1",
        "text": "uh data as the first step. Then as a second step, what we wanna do is basically create the sh trainin test split. The first step here is create the train validation split and uh let's stop uh to this uh next steps for now, right. OK. So how do we load the data here? Uh This is straightforward and we already have a function here that we can use. And so I'm gonna use this one that, as we see here, returns X and Y for like all of our data uh fetching it from uh the JSON file that we used. And it's this guy over here uh where we extracted all the MF CCS, all the labels and we have a mapping with the indexes and the relative um labels here. And so we're gonna uh like read that file and extract the X and the Y.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=431s",
        "start_time": "431.5"
    },
    {
        "id": "fdcd7b21",
        "text": "test split. The first step here is create the train validation split and uh let's stop uh to this uh next steps for now, right. OK. So how do we load the data here? Uh This is straightforward and we already have a function here that we can use. And so I'm gonna use this one that, as we see here, returns X and Y for like all of our data uh fetching it from uh the JSON file that we used. And it's this guy over here uh where we extracted all the MF CCS, all the labels and we have a mapping with the indexes and the relative um labels here. And so we're gonna uh like read that file and extract the X and the Y. So the inputs and the outputs. So we'll do XY and we'll do a load data and I'm gonna pass this data path, that's a constant that I have over here. And this is my uh path. You may have something else to remember to uh just like input the right path there, right? OK. So now we have uh the data. So now what we wanna do is split these days into train and test uh sets.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=443s",
        "start_time": "443.049"
    },
    {
        "id": "35caf76a",
        "text": "uh fetching it from uh the JSON file that we used. And it's this guy over here uh where we extracted all the MF CCS, all the labels and we have a mapping with the indexes and the relative um labels here. And so we're gonna uh like read that file and extract the X and the Y. So the inputs and the outputs. So we'll do XY and we'll do a load data and I'm gonna pass this data path, that's a constant that I have over here. And this is my uh path. You may have something else to remember to uh just like input the right path there, right? OK. So now we have uh the data. So now what we wanna do is split these days into train and test uh sets. Uh OK. So in order to do that, we need a function that we've come to know uh quite a lot by now. And that comes from uh psychic learn. And so from psych learn model selection, uh we want to import",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=472s",
        "start_time": "472.769"
    },
    {
        "id": "3bc400c0",
        "text": "So the inputs and the outputs. So we'll do XY and we'll do a load data and I'm gonna pass this data path, that's a constant that I have over here. And this is my uh path. You may have something else to remember to uh just like input the right path there, right? OK. So now we have uh the data. So now what we wanna do is split these days into train and test uh sets. Uh OK. So in order to do that, we need a function that we've come to know uh quite a lot by now. And that comes from uh psychic learn. And so from psych learn model selection, uh we want to import train test split. This is a nice function that we can use to split a data into like a train and, and, and test split. OK. So here we expect X train,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=495s",
        "start_time": "495.32"
    },
    {
        "id": "ffa6d337",
        "text": "Uh OK. So in order to do that, we need a function that we've come to know uh quite a lot by now. And that comes from uh psychic learn. And so from psych learn model selection, uh we want to import train test split. This is a nice function that we can use to split a data into like a train and, and, and test split. OK. So here we expect X train, then we expect uh X test and then Y uh",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=525s",
        "start_time": "525.52"
    },
    {
        "id": "02644aa1",
        "text": "train test split. This is a nice function that we can use to split a data into like a train and, and, and test split. OK. So here we expect X train, then we expect uh X test and then Y uh Y train and a Y test and we'll do a train test split and we'll pass in X and, and Y. And so this is gonna shuffle around the X and Y and then it's gonna split them and we, we should specify the proportion that we want to split this into. And so we have a test size argument here. Well, not surprisingly, we're gonna pass in our test size",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=544s",
        "start_time": "544.299"
    },
    {
        "id": "6d12f4b2",
        "text": "then we expect uh X test and then Y uh Y train and a Y test and we'll do a train test split and we'll pass in X and, and Y. And so this is gonna shuffle around the X and Y and then it's gonna split them and we, we should specify the proportion that we want to split this into. And so we have a test size argument here. Well, not surprisingly, we're gonna pass in our test size uh argument that, that the prepared data sets uh function accepts, right? And so this way we should have our X train, X test way train way test. Nice. Now, we should build the train validation split. Ok. So what we'll do is again, we'll do X train here, we'll do uh uh X validation here.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=558s",
        "start_time": "558.27"
    },
    {
        "id": "ec5e02c1",
        "text": "Y train and a Y test and we'll do a train test split and we'll pass in X and, and Y. And so this is gonna shuffle around the X and Y and then it's gonna split them and we, we should specify the proportion that we want to split this into. And so we have a test size argument here. Well, not surprisingly, we're gonna pass in our test size uh argument that, that the prepared data sets uh function accepts, right? And so this way we should have our X train, X test way train way test. Nice. Now, we should build the train validation split. Ok. So what we'll do is again, we'll do X train here, we'll do uh uh X validation here. And then here we again expect why train? And this time we'll have Y uh validation.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=566s",
        "start_time": "566.07"
    },
    {
        "id": "dabb48e1",
        "text": "uh argument that, that the prepared data sets uh function accepts, right? And so this way we should have our X train, X test way train way test. Nice. Now, we should build the train validation split. Ok. So what we'll do is again, we'll do X train here, we'll do uh uh X validation here. And then here we again expect why train? And this time we'll have Y uh validation. Again, we want to reuse the train to splits function. But this time we're going to pass in uh X train",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=594s",
        "start_time": "594.32"
    },
    {
        "id": "f8a8e9e0",
        "text": "And then here we again expect why train? And this time we'll have Y uh validation. Again, we want to reuse the train to splits function. But this time we're going to pass in uh X train and Y train, right? So we want to split the train set and split it into train and validation right, into two subsets. And again, like the, the test size, this time is gonna be our validation size. And so this is gonna be the percentage that's gonna be used for uh validation, right? And in this case, given we've given a 0.2 it's gonna be a 20% right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=620s",
        "start_time": "620.369"
    },
    {
        "id": "7093df40",
        "text": "Again, we want to reuse the train to splits function. But this time we're going to pass in uh X train and Y train, right? So we want to split the train set and split it into train and validation right, into two subsets. And again, like the, the test size, this time is gonna be our validation size. And so this is gonna be the percentage that's gonna be used for uh validation, right? And in this case, given we've given a 0.2 it's gonna be a 20% right? OK. So now we are, we could say like that we are basically done, right? So because we have uh X train X uh test X validation, we train, we test, we validation so we could return all of these guys and we would be done. But unfortunately,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=628s",
        "start_time": "628.049"
    },
    {
        "id": "b1141fb1",
        "text": "and Y train, right? So we want to split the train set and split it into train and validation right, into two subsets. And again, like the, the test size, this time is gonna be our validation size. And so this is gonna be the percentage that's gonna be used for uh validation, right? And in this case, given we've given a 0.2 it's gonna be a 20% right? OK. So now we are, we could say like that we are basically done, right? So because we have uh X train X uh test X validation, we train, we test, we validation so we could return all of these guys and we would be done. But unfortunately, this is uh not the case. And the reason why this is not the case is but because uh tensorflow in this case is gonna uh for an X and N, sorry for a CNN, I don't know what an X and N is, but I mean, it could be like a new type of network, who knows? Right. OK. So for a CNN um tensorflow expects a 3d array uh for each",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=637s",
        "start_time": "637.13"
    },
    {
        "id": "355ee475",
        "text": "OK. So now we are, we could say like that we are basically done, right? So because we have uh X train X uh test X validation, we train, we test, we validation so we could return all of these guys and we would be done. But unfortunately, this is uh not the case. And the reason why this is not the case is but because uh tensorflow in this case is gonna uh for an X and N, sorry for a CNN, I don't know what an X and N is, but I mean, it could be like a new type of network, who knows? Right. OK. So for a CNN um tensorflow expects a 3d array uh for each right. And so far, uh this X strain here basically has samples where each sample is a two D array which should have this shape if I'm not wrong. So where 100 30 is the number of like time bins that we have and at each of these time bins, we're taking the 13 MFCC uh values right now, if you guys remember from my previous view,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=666s",
        "start_time": "666.08"
    },
    {
        "id": "88108e8f",
        "text": "this is uh not the case. And the reason why this is not the case is but because uh tensorflow in this case is gonna uh for an X and N, sorry for a CNN, I don't know what an X and N is, but I mean, it could be like a new type of network, who knows? Right. OK. So for a CNN um tensorflow expects a 3d array uh for each right. And so far, uh this X strain here basically has samples where each sample is a two D array which should have this shape if I'm not wrong. So where 100 30 is the number of like time bins that we have and at each of these time bins, we're taking the 13 MFCC uh values right now, if you guys remember from my previous view, uh CNN and like image data where that we usually use with CNN S expect three dimensional um arrays. And that's because we have a third dimension which is the channel. And in this case, uh the channel is gonna be just a dimension uh just the mono dimensional, right? It's as if",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=683s",
        "start_time": "683.4"
    },
    {
        "id": "a8c9038b",
        "text": "right. And so far, uh this X strain here basically has samples where each sample is a two D array which should have this shape if I'm not wrong. So where 100 30 is the number of like time bins that we have and at each of these time bins, we're taking the 13 MFCC uh values right now, if you guys remember from my previous view, uh CNN and like image data where that we usually use with CNN S expect three dimensional um arrays. And that's because we have a third dimension which is the channel. And in this case, uh the channel is gonna be just a dimension uh just the mono dimensional, right? It's as if like all your data was uh gray scale images. So where you have like one value uh for each pixel that's determined by X and Y, right? OK. If it was like an R GB, we would have like three here, right? But uh we, we need to like add these extra dimension over here. So how do we do that? Well, this is like quite simple to do",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=713s",
        "start_time": "713.539"
    },
    {
        "id": "80de9efe",
        "text": "uh CNN and like image data where that we usually use with CNN S expect three dimensional um arrays. And that's because we have a third dimension which is the channel. And in this case, uh the channel is gonna be just a dimension uh just the mono dimensional, right? It's as if like all your data was uh gray scale images. So where you have like one value uh for each pixel that's determined by X and Y, right? OK. If it was like an R GB, we would have like three here, right? But uh we, we need to like add these extra dimension over here. So how do we do that? Well, this is like quite simple to do and uh so we'll do it on X train first. And so all we need to do here is take X train and given this is a nun pi array. So we're going to say that we,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=742s",
        "start_time": "742.159"
    },
    {
        "id": "7160017c",
        "text": "like all your data was uh gray scale images. So where you have like one value uh for each pixel that's determined by X and Y, right? OK. If it was like an R GB, we would have like three here, right? But uh we, we need to like add these extra dimension over here. So how do we do that? Well, this is like quite simple to do and uh so we'll do it on X train first. And so all we need to do here is take X train and given this is a nun pi array. So we're going to say that we, we're gonna put like three dots there and we're gonna put in a NP dot near axis, right? So with this three dots, we're basically saying, hey, give me what I have so far in terms of like the, the, the X train array and then give me an extra axis after that, right? And now X train is gonna be a four D array. So why is that?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=766s",
        "start_time": "766.03"
    },
    {
        "id": "395f0223",
        "text": "and uh so we'll do it on X train first. And so all we need to do here is take X train and given this is a nun pi array. So we're going to say that we, we're gonna put like three dots there and we're gonna put in a NP dot near axis, right? So with this three dots, we're basically saying, hey, give me what I have so far in terms of like the, the, the X train array and then give me an extra axis after that, right? And now X train is gonna be a four D array. So why is that? Well, because we have the number, it's not the boon, but it's the number of samples. And then we're gonna have 100 3013 and one, right? So here like the, the, the, the first dimension is just like the number of samples. So if we have 5000 samples, this is gonna be equal to 5000, right? OK. So X trainin is not the only",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=794s",
        "start_time": "794.38"
    },
    {
        "id": "a0fa5a66",
        "text": "we're gonna put like three dots there and we're gonna put in a NP dot near axis, right? So with this three dots, we're basically saying, hey, give me what I have so far in terms of like the, the, the X train array and then give me an extra axis after that, right? And now X train is gonna be a four D array. So why is that? Well, because we have the number, it's not the boon, but it's the number of samples. And then we're gonna have 100 3013 and one, right? So here like the, the, the, the first dimension is just like the number of samples. So if we have 5000 samples, this is gonna be equal to 5000, right? OK. So X trainin is not the only um",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=808s",
        "start_time": "808.28"
    },
    {
        "id": "f4cf411e",
        "text": "Well, because we have the number, it's not the boon, but it's the number of samples. And then we're gonna have 100 3013 and one, right? So here like the, the, the, the first dimension is just like the number of samples. So if we have 5000 samples, this is gonna be equal to 5000, right? OK. So X trainin is not the only um yeah, a ray that we need to uh add a a dimension on. So we also want to uh change uh X validation. And so we'll basically do the same here",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=836s",
        "start_time": "836.64"
    },
    {
        "id": "3ba754d9",
        "text": "um yeah, a ray that we need to uh add a a dimension on. So we also want to uh change uh X validation. And so we'll basically do the same here and same thing for X test.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=865s",
        "start_time": "865.83"
    },
    {
        "id": "ecb89fdd",
        "text": "yeah, a ray that we need to uh add a a dimension on. So we also want to uh change uh X validation. And so we'll basically do the same here and same thing for X test. Cool.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=867s",
        "start_time": "867.609"
    },
    {
        "id": "7c23b8a5",
        "text": "and same thing for X test. Cool. OK. So here we have XTA",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=880s",
        "start_time": "880.969"
    },
    {
        "id": "6fd56f6b",
        "text": "Cool. OK. So here we have XTA good. So now we should have all we need for like our training process and validation and testing, right? OK. So now we can return all of this test, split, all of this data set splits. So we'll take these guys here. So we have already written them. So I'm not gonna rewrite them",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=884s",
        "start_time": "884.82"
    },
    {
        "id": "2e9e029a",
        "text": "OK. So here we have XTA good. So now we should have all we need for like our training process and validation and testing, right? OK. So now we can return all of this test, split, all of this data set splits. So we'll take these guys here. So we have already written them. So I'm not gonna rewrite them uh cool. So this should be done. So if this works correctly, this function works correctly. Now, we should be able to have all of our train validation and test sets down here, which is great. Now on to the next step building the CNN. So here we need to build the, the architecture itself, the network.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=886s",
        "start_time": "886.28"
    },
    {
        "id": "77c2bd33",
        "text": "good. So now we should have all we need for like our training process and validation and testing, right? OK. So now we can return all of this test, split, all of this data set splits. So we'll take these guys here. So we have already written them. So I'm not gonna rewrite them uh cool. So this should be done. So if this works correctly, this function works correctly. Now, we should be able to have all of our train validation and test sets down here, which is great. Now on to the next step building the CNN. So here we need to build the, the architecture itself, the network. So what we, we're gonna use again, another like custom function for doing that. I'm not gonna write like all the instructions here like in the uh yeah here like in the main. And the main reason for that is because like, it's a lot of like uh stuff that we're gonna write. And so I don't want to like have a lot of mass. So I prefer just like to modular everything. By the way, this is a good, a very good advice if you have like a lot of instructions that go well, like together, even if you're doing just like simple scripting,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=889s",
        "start_time": "889.88"
    },
    {
        "id": "258514ea",
        "text": "uh cool. So this should be done. So if this works correctly, this function works correctly. Now, we should be able to have all of our train validation and test sets down here, which is great. Now on to the next step building the CNN. So here we need to build the, the architecture itself, the network. So what we, we're gonna use again, another like custom function for doing that. I'm not gonna write like all the instructions here like in the uh yeah here like in the main. And the main reason for that is because like, it's a lot of like uh stuff that we're gonna write. And so I don't want to like have a lot of mass. So I prefer just like to modular everything. By the way, this is a good, a very good advice if you have like a lot of instructions that go well, like together, even if you're doing just like simple scripting, just try like to, to use like either functional programming or like object oriented programming. So that, I mean, you don't end up like with a lot of like instructions that are difficult to understand.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=913s",
        "start_time": "913.01"
    },
    {
        "id": "e224f3fa",
        "text": "So what we, we're gonna use again, another like custom function for doing that. I'm not gonna write like all the instructions here like in the uh yeah here like in the main. And the main reason for that is because like, it's a lot of like uh stuff that we're gonna write. And so I don't want to like have a lot of mass. So I prefer just like to modular everything. By the way, this is a good, a very good advice if you have like a lot of instructions that go well, like together, even if you're doing just like simple scripting, just try like to, to use like either functional programming or like object oriented programming. So that, I mean, you don't end up like with a lot of like instructions that are difficult to understand. OK. So we'll do so we expect like the model itself. And so we'll create a uh function here that we'll call it uh built model. And now this function is gonna accept an argument uh that's called uh the input shape, right? So now we, we'll see what this is like in a second. But before that let's start building this function.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=935s",
        "start_time": "935.309"
    },
    {
        "id": "27d5bc73",
        "text": "just try like to, to use like either functional programming or like object oriented programming. So that, I mean, you don't end up like with a lot of like instructions that are difficult to understand. OK. So we'll do so we expect like the model itself. And so we'll create a uh function here that we'll call it uh built model. And now this function is gonna accept an argument uh that's called uh the input shape, right? So now we, we'll see what this is like in a second. But before that let's start building this function. So yeah, let me slide this down. OK. So yeah, we don't want all of that for sure. But we want uh to define BUILD model, we have the argument that's uh input shape, right? OK. So now we need to do like a bunch of things. So we want to, first of all uh create a model then this model",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=965s",
        "start_time": "965.479"
    },
    {
        "id": "2de45e05",
        "text": "OK. So we'll do so we expect like the model itself. And so we'll create a uh function here that we'll call it uh built model. And now this function is gonna accept an argument uh that's called uh the input shape, right? So now we, we'll see what this is like in a second. But before that let's start building this function. So yeah, let me slide this down. OK. So yeah, we don't want all of that for sure. But we want uh to define BUILD model, we have the argument that's uh input shape, right? OK. So now we need to do like a bunch of things. So we want to, first of all uh create a model then this model uh is gonna be a CNN with three convolutional layers followed by max pooling uh layers. So we are gonna write first con uh layer, then we are gonna have a second con uh layer",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=978s",
        "start_time": "978.4"
    },
    {
        "id": "f85de03c",
        "text": "So yeah, let me slide this down. OK. So yeah, we don't want all of that for sure. But we want uh to define BUILD model, we have the argument that's uh input shape, right? OK. So now we need to do like a bunch of things. So we want to, first of all uh create a model then this model uh is gonna be a CNN with three convolutional layers followed by max pooling uh layers. So we are gonna write first con uh layer, then we are gonna have a second con uh layer and a third com layer.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1007s",
        "start_time": "1007.179"
    },
    {
        "id": "8e3238f1",
        "text": "uh is gonna be a CNN with three convolutional layers followed by max pooling uh layers. So we are gonna write first con uh layer, then we are gonna have a second con uh layer and a third com layer. Then we're gonna flatten",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1032s",
        "start_time": "1032.448"
    },
    {
        "id": "27e686cc",
        "text": "and a third com layer. Then we're gonna flatten uh the output of the convolutional layers",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1053s",
        "start_time": "1053.52"
    },
    {
        "id": "ba3c34c1",
        "text": "Then we're gonna flatten uh the output of the convolutional layers and fit",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1057s",
        "start_time": "1057.75"
    },
    {
        "id": "58a4fb65",
        "text": "uh the output of the convolutional layers and fit uh into dense layer. So we'll feed that into dense layer. And uh finally, we'll have an output layer that uses soft max cool. OK. So let's let's uh build all of this. But before we can do that, we need to import keras, right. OK. So we'll do a import",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1061s",
        "start_time": "1061.689"
    },
    {
        "id": "317f1e81",
        "text": "and fit uh into dense layer. So we'll feed that into dense layer. And uh finally, we'll have an output layer that uses soft max cool. OK. So let's let's uh build all of this. But before we can do that, we need to import keras, right. OK. So we'll do a import and we should say tensorflow dot uh carers and we'll import this as carers, right? So, yeah, I'm really busy. I don't want to write too many things. OK. So we need to build the model uh initially. So we'll do a model uh that's equal to uh we should say uh KIS and uh sequential. So this is a sequential model,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1066s",
        "start_time": "1066.38"
    },
    {
        "id": "cf3e7c24",
        "text": "uh into dense layer. So we'll feed that into dense layer. And uh finally, we'll have an output layer that uses soft max cool. OK. So let's let's uh build all of this. But before we can do that, we need to import keras, right. OK. So we'll do a import and we should say tensorflow dot uh carers and we'll import this as carers, right? So, yeah, I'm really busy. I don't want to write too many things. OK. So we need to build the model uh initially. So we'll do a model uh that's equal to uh we should say uh KIS and uh sequential. So this is a sequential model, right? And now we want to build uh the first uh convolutional layer. So how do we do that. Well, we're gonna take the model and there's a great method that we can use on the, on the model. And it's called a here. And so here we can add a layer to a model that's basically like the, the, the idea like the semantics of model dot A.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1068s",
        "start_time": "1068.569"
    },
    {
        "id": "6e85f648",
        "text": "and we should say tensorflow dot uh carers and we'll import this as carers, right? So, yeah, I'm really busy. I don't want to write too many things. OK. So we need to build the model uh initially. So we'll do a model uh that's equal to uh we should say uh KIS and uh sequential. So this is a sequential model, right? And now we want to build uh the first uh convolutional layer. So how do we do that. Well, we're gonna take the model and there's a great method that we can use on the, on the model. And it's called a here. And so here we can add a layer to a model that's basically like the, the, the idea like the semantics of model dot A. OK. So what do we wanna uh add here? So we want to add a layer and specifically uh we want to add a con to D layer. So it's convolutional uh layer. Now, we need to pass quite uh a lot of values like to, to this uh layer.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1094s",
        "start_time": "1094.54"
    },
    {
        "id": "29c0edc3",
        "text": "right? And now we want to build uh the first uh convolutional layer. So how do we do that. Well, we're gonna take the model and there's a great method that we can use on the, on the model. And it's called a here. And so here we can add a layer to a model that's basically like the, the, the idea like the semantics of model dot A. OK. So what do we wanna uh add here? So we want to add a layer and specifically uh we want to add a con to D layer. So it's convolutional uh layer. Now, we need to pass quite uh a lot of values like to, to this uh layer. So first of all, we should decide which uh not which, but how many kernels, how many filters we want in these convolutional layers. And so we'll say we want 32 filters, then we should decide uh the grid size of the kernel. And this is gonna be a quite customary three by three.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1121s",
        "start_time": "1121.55"
    },
    {
        "id": "872ed753",
        "text": "OK. So what do we wanna uh add here? So we want to add a layer and specifically uh we want to add a con to D layer. So it's convolutional uh layer. Now, we need to pass quite uh a lot of values like to, to this uh layer. So first of all, we should decide which uh not which, but how many kernels, how many filters we want in these convolutional layers. And so we'll say we want 32 filters, then we should decide uh the grid size of the kernel. And this is gonna be a quite customary three by three. And then what we wanna uh specify here is the uh type of activation that we wanna use. So the activation function and we're gonna be using R",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1145s",
        "start_time": "1145.189"
    },
    {
        "id": "fa0e90b0",
        "text": "So first of all, we should decide which uh not which, but how many kernels, how many filters we want in these convolutional layers. And so we'll say we want 32 filters, then we should decide uh the grid size of the kernel. And this is gonna be a quite customary three by three. And then what we wanna uh specify here is the uh type of activation that we wanna use. So the activation function and we're gonna be using R so rectified linear unit. Now, if you're not really familiar with all of these like weird terms that I'm using so uh like kernel uh great size convolution. So you can check out uh my previous video that introduced like the theory behind uh CNN si think it's gonna show up any time like over here. And so you can click that and check that out and then come back here because this is the fun stuff. OK.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1166s",
        "start_time": "1166.229"
    },
    {
        "id": "cc425454",
        "text": "And then what we wanna uh specify here is the uh type of activation that we wanna use. So the activation function and we're gonna be using R so rectified linear unit. Now, if you're not really familiar with all of these like weird terms that I'm using so uh like kernel uh great size convolution. So you can check out uh my previous video that introduced like the theory behind uh CNN si think it's gonna show up any time like over here. And so you can click that and check that out and then come back here because this is the fun stuff. OK. So now we've specified the number of kernels, the uh the size of the grid, uh the size of the kernel and the activation function that we want to use. And since we are like the first hidden layer here, we should specify the input shape, right? So what's the input shape? Well, the input shape, it's as simple as the argument input shape, right? And then we passed in the build model function,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1189s",
        "start_time": "1189.359"
    },
    {
        "id": "60add010",
        "text": "so rectified linear unit. Now, if you're not really familiar with all of these like weird terms that I'm using so uh like kernel uh great size convolution. So you can check out uh my previous video that introduced like the theory behind uh CNN si think it's gonna show up any time like over here. And so you can click that and check that out and then come back here because this is the fun stuff. OK. So now we've specified the number of kernels, the uh the size of the grid, uh the size of the kernel and the activation function that we want to use. And since we are like the first hidden layer here, we should specify the input shape, right? So what's the input shape? Well, the input shape, it's as simple as the argument input shape, right? And then we passed in the build model function, all right. But obviously, this doesn't tell us much about the input shape itself. So let's go back, let's go back here to, to the main.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1201s",
        "start_time": "1201.06"
    },
    {
        "id": "3c484d27",
        "text": "So now we've specified the number of kernels, the uh the size of the grid, uh the size of the kernel and the activation function that we want to use. And since we are like the first hidden layer here, we should specify the input shape, right? So what's the input shape? Well, the input shape, it's as simple as the argument input shape, right? And then we passed in the build model function, all right. But obviously, this doesn't tell us much about the input shape itself. So let's go back, let's go back here to, to the main. OK. So now let's extract the input shape. So we'll create the input shape, which is gonna be a uh topple here, right? OK. And so the input shape is gonna be given by",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1230s",
        "start_time": "1230.839"
    },
    {
        "id": "d8b2300f",
        "text": "all right. But obviously, this doesn't tell us much about the input shape itself. So let's go back, let's go back here to, to the main. OK. So now let's extract the input shape. So we'll create the input shape, which is gonna be a uh topple here, right? OK. And so the input shape is gonna be given by um X train",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1257s",
        "start_time": "1257.359"
    },
    {
        "id": "2d90c00e",
        "text": "OK. So now let's extract the input shape. So we'll create the input shape, which is gonna be a uh topple here, right? OK. And so the input shape is gonna be given by um X train and here this is gonna be equal to uh X trainin oops, sorry, not that. So X train dot shape. OK. And then uh one. So basically, like we are taking the uh this like shape here uh of the of the X strain. We could have like taken X validation or like X test for that matter.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1267s",
        "start_time": "1267.17"
    },
    {
        "id": "d2710ec5",
        "text": "um X train and here this is gonna be equal to uh X trainin oops, sorry, not that. So X train dot shape. OK. And then uh one. So basically, like we are taking the uh this like shape here uh of the of the X strain. We could have like taken X validation or like X test for that matter. Uh But then we are taking like the shape of at index one",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1287s",
        "start_time": "1287.04"
    },
    {
        "id": "ed0b9779",
        "text": "and here this is gonna be equal to uh X trainin oops, sorry, not that. So X train dot shape. OK. And then uh one. So basically, like we are taking the uh this like shape here uh of the of the X strain. We could have like taken X validation or like X test for that matter. Uh But then we are taking like the shape of at index one and then we are going to take",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1291s",
        "start_time": "1291.89"
    },
    {
        "id": "a5df6975",
        "text": "Uh But then we are taking like the shape of at index one and then we are going to take the extreme shape and index two.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1319s",
        "start_time": "1319.17"
    },
    {
        "id": "61d471ee",
        "text": "and then we are going to take the extreme shape and index two. And uh finally, we're gonna take the shape as index three, right? So if you guys remember I told you that X strain over here, right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1325s",
        "start_time": "1325.55"
    },
    {
        "id": "48136cca",
        "text": "the extreme shape and index two. And uh finally, we're gonna take the shape as index three, right? So if you guys remember I told you that X strain over here, right? X train. Yeah,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1330s",
        "start_time": "1330.069"
    },
    {
        "id": "56b473c7",
        "text": "And uh finally, we're gonna take the shape as index three, right? So if you guys remember I told you that X strain over here, right? X train. Yeah, over here it's a four D uh array where we have like the number of samples and then uh we have like 100 30 like time bins. And then we have 13 MF CCS and one which is like the channel like the depth, right? So we know that each sample has this shape 100 30 by 13 by one, right? And so this is the shape that we want to use like as an input for our CNN.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1334s",
        "start_time": "1334.699"
    },
    {
        "id": "2dfa8dd6",
        "text": "X train. Yeah, over here it's a four D uh array where we have like the number of samples and then uh we have like 100 30 like time bins. And then we have 13 MF CCS and one which is like the channel like the depth, right? So we know that each sample has this shape 100 30 by 13 by one, right? And so this is the shape that we want to use like as an input for our CNN. And so here we build like this input shape uh topple, we pass it in here in the BUILD model and then we pass it down here when we are uh creating like the first convolutional layer, right? And so this is the input shape, hope this is clear, right? OK. So now we have our first convolutional",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1351s",
        "start_time": "1351.459"
    },
    {
        "id": "9d3f8e73",
        "text": "over here it's a four D uh array where we have like the number of samples and then uh we have like 100 30 like time bins. And then we have 13 MF CCS and one which is like the channel like the depth, right? So we know that each sample has this shape 100 30 by 13 by one, right? And so this is the shape that we want to use like as an input for our CNN. And so here we build like this input shape uh topple, we pass it in here in the BUILD model and then we pass it down here when we are uh creating like the first convolutional layer, right? And so this is the input shape, hope this is clear, right? OK. So now we have our first convolutional uh layer. So what we want to do next is add another layer here, but this is not gonna be a convolutional layer but a max pooling uh layer which is gonna down sample our uh input.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1354s",
        "start_time": "1354.209"
    },
    {
        "id": "7aefe578",
        "text": "And so here we build like this input shape uh topple, we pass it in here in the BUILD model and then we pass it down here when we are uh creating like the first convolutional layer, right? And so this is the input shape, hope this is clear, right? OK. So now we have our first convolutional uh layer. So what we want to do next is add another layer here, but this is not gonna be a convolutional layer but a max pooling uh layer which is gonna down sample our uh input. OK. So this is gonna be a max pool two D, right. OK. And so we know from our previous video that max pooling has a bunch of settings that we should set there. So uh the first thing that we want to set is the uh so called like pool size or like the grid size. And here we're gonna use like grid a pooling uh a pool size of uh three by three.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1382s",
        "start_time": "1382.78"
    },
    {
        "id": "2bab306f",
        "text": "uh layer. So what we want to do next is add another layer here, but this is not gonna be a convolutional layer but a max pooling uh layer which is gonna down sample our uh input. OK. So this is gonna be a max pool two D, right. OK. And so we know from our previous video that max pooling has a bunch of settings that we should set there. So uh the first thing that we want to set is the uh so called like pool size or like the grid size. And here we're gonna use like grid a pooling uh a pool size of uh three by three. Then we want to specify the strides. So, and uh the strides vertical and the horizontal are gonna be uh two by two. And here we also want to add a padding and the type of padding, the zero padding uh that we'll use uh it's the same. So we're gonna just like use like padding uh throughout like all the edges like around all of the. Um",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1405s",
        "start_time": "1405.52"
    },
    {
        "id": "60a9fd6b",
        "text": "OK. So this is gonna be a max pool two D, right. OK. And so we know from our previous video that max pooling has a bunch of settings that we should set there. So uh the first thing that we want to set is the uh so called like pool size or like the grid size. And here we're gonna use like grid a pooling uh a pool size of uh three by three. Then we want to specify the strides. So, and uh the strides vertical and the horizontal are gonna be uh two by two. And here we also want to add a padding and the type of padding, the zero padding uh that we'll use uh it's the same. So we're gonna just like use like padding uh throughout like all the edges like around all of the. Um hm Yeah, all of like the, the the convolutional like output that we get out of like this first convolutional layer",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1423s",
        "start_time": "1423.869"
    },
    {
        "id": "4f303c32",
        "text": "Then we want to specify the strides. So, and uh the strides vertical and the horizontal are gonna be uh two by two. And here we also want to add a padding and the type of padding, the zero padding uh that we'll use uh it's the same. So we're gonna just like use like padding uh throughout like all the edges like around all of the. Um hm Yeah, all of like the, the the convolutional like output that we get out of like this first convolutional layer uh cool. So now we have max pooling as well as like a convolutional uh convolutional layer. Uh And what we want to add here is a final thing is a final layer here and this layer it's basically",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1451s",
        "start_time": "1451.93"
    },
    {
        "id": "a3ae3d90",
        "text": "hm Yeah, all of like the, the the convolutional like output that we get out of like this first convolutional layer uh cool. So now we have max pooling as well as like a convolutional uh convolutional layer. Uh And what we want to add here is a final thing is a final layer here and this layer it's basically batch normalization. So now batch normalization is a quite complicated like mathematical, I mean like the, the mathematical process beyond uh behind ba batch normalization, it's quite uh complicated. So",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1481s",
        "start_time": "1481.55"
    },
    {
        "id": "f7376b04",
        "text": "uh cool. So now we have max pooling as well as like a convolutional uh convolutional layer. Uh And what we want to add here is a final thing is a final layer here and this layer it's basically batch normalization. So now batch normalization is a quite complicated like mathematical, I mean like the, the mathematical process beyond uh behind ba batch normalization, it's quite uh complicated. So and as they usually like say in these cases, and it's well beyond the scope of this uh introductory course, but all you should more or less like the intuition that you can have about botch normalization is that it's a process",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1491s",
        "start_time": "1491.02"
    },
    {
        "id": "f2310c41",
        "text": "batch normalization. So now batch normalization is a quite complicated like mathematical, I mean like the, the mathematical process beyond uh behind ba batch normalization, it's quite uh complicated. So and as they usually like say in these cases, and it's well beyond the scope of this uh introductory course, but all you should more or less like the intuition that you can have about botch normalization is that it's a process that standardizes, that normalizes the activations in a current layer and the activations that get presented to like the subsequent layer by doing so, the great thing, the great advantage is that we uh kind of like speed up training by a lot really. So the the models are gonna convert way faster.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1511s",
        "start_time": "1511.229"
    },
    {
        "id": "676661cd",
        "text": "and as they usually like say in these cases, and it's well beyond the scope of this uh introductory course, but all you should more or less like the intuition that you can have about botch normalization is that it's a process that standardizes, that normalizes the activations in a current layer and the activations that get presented to like the subsequent layer by doing so, the great thing, the great advantage is that we uh kind of like speed up training by a lot really. So the the models are gonna convert way faster. And then the other great thing is also like that the models are going to be way more reliable.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1528s",
        "start_time": "1528.42"
    },
    {
        "id": "3d9cc621",
        "text": "that standardizes, that normalizes the activations in a current layer and the activations that get presented to like the subsequent layer by doing so, the great thing, the great advantage is that we uh kind of like speed up training by a lot really. So the the models are gonna convert way faster. And then the other great thing is also like that the models are going to be way more reliable. Cool.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1543s",
        "start_time": "1543.81"
    },
    {
        "id": "667d028f",
        "text": "And then the other great thing is also like that the models are going to be way more reliable. Cool. Yeah, by the way, let me know like if you want to know more about batch normalization, but as I said, like this is like quite like complicated like mathematical topic. So I don't want to cover it in this series. But if I see like that, you guys like leave a lot of comments to know what this is, I may just like create a video about just batch normalization. So let me know",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1570s",
        "start_time": "1570.209"
    },
    {
        "id": "5dcebd47",
        "text": "Cool. Yeah, by the way, let me know like if you want to know more about batch normalization, but as I said, like this is like quite like complicated like mathematical topic. So I don't want to cover it in this series. But if I see like that, you guys like leave a lot of comments to know what this is, I may just like create a video about just batch normalization. So let me know uh cool. So this is basically like the, the overall first convolutional layer I would say. So now we want to build another couple of this. So the second one",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1576s",
        "start_time": "1576.9"
    },
    {
        "id": "6528b4ec",
        "text": "Yeah, by the way, let me know like if you want to know more about batch normalization, but as I said, like this is like quite like complicated like mathematical topic. So I don't want to cover it in this series. But if I see like that, you guys like leave a lot of comments to know what this is, I may just like create a video about just batch normalization. So let me know uh cool. So this is basically like the, the overall first convolutional layer I would say. So now we want to build another couple of this. So the second one um is gonna be basically like the uh the, the same as this. Uh But uh in uh the third layer, we're gonna change a couple of like parameters here. So here we're gonna change the uh the size of the kernel and we're gonna move it to two by two and same thing here for uh the, the max pooling",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1579s",
        "start_time": "1579.229"
    },
    {
        "id": "3143acf0",
        "text": "uh cool. So this is basically like the, the overall first convolutional layer I would say. So now we want to build another couple of this. So the second one um is gonna be basically like the uh the, the same as this. Uh But uh in uh the third layer, we're gonna change a couple of like parameters here. So here we're gonna change the uh the size of the kernel and we're gonna move it to two by two and same thing here for uh the, the max pooling the pool size here, we're gonna move it like to uh two by two as well. Cool. So now we are done with the uh convolutional layers and now the next step is to flatten the output. And so, and we know that out of like this three convolutional layers, we're just uh expecting a two dimensional array. And so we want to flatten that into a one D array.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1599s",
        "start_time": "1599.329"
    },
    {
        "id": "8bac6590",
        "text": "um is gonna be basically like the uh the, the same as this. Uh But uh in uh the third layer, we're gonna change a couple of like parameters here. So here we're gonna change the uh the size of the kernel and we're gonna move it to two by two and same thing here for uh the, the max pooling the pool size here, we're gonna move it like to uh two by two as well. Cool. So now we are done with the uh convolutional layers and now the next step is to flatten the output. And so, and we know that out of like this three convolutional layers, we're just uh expecting a two dimensional array. And so we want to flatten that into a one D array. So how do we do that again? This is like very, very simple with keras and tensor flow because it's as simple as calling Kas dot uh Layers dot flatter. And we, I think like we, I don't know like if we've seen this already like in a previous video, but if we haven't, it's as simple as this, right.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1611s",
        "start_time": "1611.369"
    },
    {
        "id": "06e3ff16",
        "text": "the pool size here, we're gonna move it like to uh two by two as well. Cool. So now we are done with the uh convolutional layers and now the next step is to flatten the output. And so, and we know that out of like this three convolutional layers, we're just uh expecting a two dimensional array. And so we want to flatten that into a one D array. So how do we do that again? This is like very, very simple with keras and tensor flow because it's as simple as calling Kas dot uh Layers dot flatter. And we, I think like we, I don't know like if we've seen this already like in a previous video, but if we haven't, it's as simple as this, right. OK. So now we flattened uh uh the, the output uh of the convolutions. And so the next thing that we want to do is add a dense layer, a fully connected layer for classification.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1637s",
        "start_time": "1637.109"
    },
    {
        "id": "cbc2dd44",
        "text": "So how do we do that again? This is like very, very simple with keras and tensor flow because it's as simple as calling Kas dot uh Layers dot flatter. And we, I think like we, I don't know like if we've seen this already like in a previous video, but if we haven't, it's as simple as this, right. OK. So now we flattened uh uh the, the output uh of the convolutions. And so the next thing that we want to do is add a dense layer, a fully connected layer for classification. And so, and here we'll do again a model uh dot art but this time we're gonna use a dense layer. So we'll do Kous dot Layers dot uh dense. And I'm sure like we've seen a lot of like dense dance layers like in the, in the previous videos. So now",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1664s",
        "start_time": "1664.54"
    },
    {
        "id": "d45facce",
        "text": "OK. So now we flattened uh uh the, the output uh of the convolutions. And so the next thing that we want to do is add a dense layer, a fully connected layer for classification. And so, and here we'll do again a model uh dot art but this time we're gonna use a dense layer. So we'll do Kous dot Layers dot uh dense. And I'm sure like we've seen a lot of like dense dance layers like in the, in the previous videos. So now um here like in the dense layer, if you guys remember, we should specify how many neurons we want. And uh for this uh network, we're gonna use 64 neurons. And then we should specify the type of activation and the activation here again is gonna be R so rectified linear unit. Now, if you guys don't remember what A R is, what an activation function is, again, I have a video on that which hopefully should be",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1684s",
        "start_time": "1684.81"
    },
    {
        "id": "5408defb",
        "text": "And so, and here we'll do again a model uh dot art but this time we're gonna use a dense layer. So we'll do Kous dot Layers dot uh dense. And I'm sure like we've seen a lot of like dense dance layers like in the, in the previous videos. So now um here like in the dense layer, if you guys remember, we should specify how many neurons we want. And uh for this uh network, we're gonna use 64 neurons. And then we should specify the type of activation and the activation here again is gonna be R so rectified linear unit. Now, if you guys don't remember what A R is, what an activation function is, again, I have a video on that which hopefully should be over there. So just like click there and, and just like go learn that. So these are all like very important things that we need to learn to like master uh like deep learning. So if you don't know about that, go check that out. OK. So now let's move on. So we've a, we've flattened the convolution output.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1698s",
        "start_time": "1698.469"
    },
    {
        "id": "538d9409",
        "text": "um here like in the dense layer, if you guys remember, we should specify how many neurons we want. And uh for this uh network, we're gonna use 64 neurons. And then we should specify the type of activation and the activation here again is gonna be R so rectified linear unit. Now, if you guys don't remember what A R is, what an activation function is, again, I have a video on that which hopefully should be over there. So just like click there and, and just like go learn that. So these are all like very important things that we need to learn to like master uh like deep learning. So if you don't know about that, go check that out. OK. So now let's move on. So we've a, we've flattened the convolution output. Uh we've fed that into a dense layer. But now I in order to avoid, avoid over fitting, sorry, I'm going to add an extra thing here. And again, this is another thing that we've seen",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1721s",
        "start_time": "1721.199"
    },
    {
        "id": "c96c8b5e",
        "text": "over there. So just like click there and, and just like go learn that. So these are all like very important things that we need to learn to like master uh like deep learning. So if you don't know about that, go check that out. OK. So now let's move on. So we've a, we've flattened the convolution output. Uh we've fed that into a dense layer. But now I in order to avoid, avoid over fitting, sorry, I'm going to add an extra thing here. And again, this is another thing that we've seen and this is uh a drop out, right? And, and I'll set a drop out to 30 dropout probability to uh 30%. Again, if you don't remember what dropout is or how to uh combat how to solve um overfitting, I have a vat on that. Just go check that out should be over here, right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1751s",
        "start_time": "1751.04"
    },
    {
        "id": "2d23f9e4",
        "text": "Uh we've fed that into a dense layer. But now I in order to avoid, avoid over fitting, sorry, I'm going to add an extra thing here. And again, this is another thing that we've seen and this is uh a drop out, right? And, and I'll set a drop out to 30 dropout probability to uh 30%. Again, if you don't remember what dropout is or how to uh combat how to solve um overfitting, I have a vat on that. Just go check that out should be over here, right? OK. Cool. So now we are at the output layer, right? And the output layer here is again a um dance layer. So we'll do Kous dot Layers dot dance. But now",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1773s",
        "start_time": "1773.719"
    },
    {
        "id": "633c0e3b",
        "text": "and this is uh a drop out, right? And, and I'll set a drop out to 30 dropout probability to uh 30%. Again, if you don't remember what dropout is or how to uh combat how to solve um overfitting, I have a vat on that. Just go check that out should be over here, right? OK. Cool. So now we are at the output layer, right? And the output layer here is again a um dance layer. So we'll do Kous dot Layers dot dance. But now we want as many neurons as the number of genres that we want to predict. So here guys, let's go to the to this data thing. So here we have 10 different genres and so we want 10 different neurons and each neuron obviously is going to represent a different genre. And as the activation",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1791s",
        "start_time": "1791.319"
    },
    {
        "id": "c9a8ce77",
        "text": "OK. Cool. So now we are at the output layer, right? And the output layer here is again a um dance layer. So we'll do Kous dot Layers dot dance. But now we want as many neurons as the number of genres that we want to predict. So here guys, let's go to the to this data thing. So here we have 10 different genres and so we want 10 different neurons and each neuron obviously is going to represent a different genre. And as the activation uh this time, we want to use soft maths and if you guys remember soft, what soft maths does is it creates a probability distribution kind of like scores like for each of these like 10 neurons, 10 possible like categories. And if we add up like all of these like values for the 10 different genres, then we're gonna get one.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1817s",
        "start_time": "1817.25"
    },
    {
        "id": "d959a87c",
        "text": "we want as many neurons as the number of genres that we want to predict. So here guys, let's go to the to this data thing. So here we have 10 different genres and so we want 10 different neurons and each neuron obviously is going to represent a different genre. And as the activation uh this time, we want to use soft maths and if you guys remember soft, what soft maths does is it creates a probability distribution kind of like scores like for each of these like 10 neurons, 10 possible like categories. And if we add up like all of these like values for the 10 different genres, then we're gonna get one. So how do we do predictions there? Well, we just like take the the index with the highest value and we're gonna map that onto like a relative genre.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1836s",
        "start_time": "1836.849"
    },
    {
        "id": "c705044b",
        "text": "uh this time, we want to use soft maths and if you guys remember soft, what soft maths does is it creates a probability distribution kind of like scores like for each of these like 10 neurons, 10 possible like categories. And if we add up like all of these like values for the 10 different genres, then we're gonna get one. So how do we do predictions there? Well, we just like take the the index with the highest value and we're gonna map that onto like a relative genre. Cool. So this is the apple layer. So we are done. So as I said, as a promise, this was like a quite long process to build this network. But now we are done. So we have our nice CNN uh with three combinational layers. And after each com layer, we have max bulling and we've added also batch normalization.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1855s",
        "start_time": "1855.939"
    },
    {
        "id": "07ad3d87",
        "text": "So how do we do predictions there? Well, we just like take the the index with the highest value and we're gonna map that onto like a relative genre. Cool. So this is the apple layer. So we are done. So as I said, as a promise, this was like a quite long process to build this network. But now we are done. So we have our nice CNN uh with three combinational layers. And after each com layer, we have max bulling and we've added also batch normalization. And uh out of after that, we, we flatten the results and then we feed like the uh the one dimensional array into a dense layer. And finally, we feed all of that into soft mark classifier",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1880s",
        "start_time": "1880.06"
    },
    {
        "id": "037c586c",
        "text": "Cool. So this is the apple layer. So we are done. So as I said, as a promise, this was like a quite long process to build this network. But now we are done. So we have our nice CNN uh with three combinational layers. And after each com layer, we have max bulling and we've added also batch normalization. And uh out of after that, we, we flatten the results and then we feed like the uh the one dimensional array into a dense layer. And finally, we feed all of that into soft mark classifier good. Now we are done so we can return the model. That's like the thing that we want. Uh Yeah, right. We, we, we want the most",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1889s",
        "start_time": "1889.719"
    },
    {
        "id": "60e22110",
        "text": "And uh out of after that, we, we flatten the results and then we feed like the uh the one dimensional array into a dense layer. And finally, we feed all of that into soft mark classifier good. Now we are done so we can return the model. That's like the thing that we want. Uh Yeah, right. We, we, we want the most OK. So this is the model over here. Now we need to compile the network. So we've done this like multiple times already guys. So I'm gonna try to power through this. So we want to specify the optimizer here. And so we're gonna uh just like do Kas dot optimizer and I'm gonna choose atom",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1911s",
        "start_time": "1911.449"
    },
    {
        "id": "9a40231b",
        "text": "good. Now we are done so we can return the model. That's like the thing that we want. Uh Yeah, right. We, we, we want the most OK. So this is the model over here. Now we need to compile the network. So we've done this like multiple times already guys. So I'm gonna try to power through this. So we want to specify the optimizer here. And so we're gonna uh just like do Kas dot optimizer and I'm gonna choose atom and I'm gonna specify the learning rate, which is gonna be equal to 0.0001",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1928s",
        "start_time": "1928.939"
    },
    {
        "id": "d3991a9a",
        "text": "OK. So this is the model over here. Now we need to compile the network. So we've done this like multiple times already guys. So I'm gonna try to power through this. So we want to specify the optimizer here. And so we're gonna uh just like do Kas dot optimizer and I'm gonna choose atom and I'm gonna specify the learning rate, which is gonna be equal to 0.0001 right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1938s",
        "start_time": "1938.9"
    },
    {
        "id": "121976c2",
        "text": "and I'm gonna specify the learning rate, which is gonna be equal to 0.0001 right? OK. But this is not the opti what, what have I written here? The opti miser,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1962s",
        "start_time": "1962.959"
    },
    {
        "id": "44e5251c",
        "text": "right? OK. But this is not the opti what, what have I written here? The opti miser, right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1970s",
        "start_time": "1970.989"
    },
    {
        "id": "bb309fb5",
        "text": "OK. But this is not the opti what, what have I written here? The opti miser, right? OK. And then we'll do a model dot uh compile",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1972s",
        "start_time": "1972.92"
    },
    {
        "id": "e536853a",
        "text": "right? OK. And then we'll do a model dot uh compile and in the model dot compile, we, we, we need to pass a bunch of things. So the first thing that we need to pass again is the optimize it, right? And that's the one that we've just built. Then uh we want to pass the loss function, we need to specify that. And in this case, we'll use the pa category",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1981s",
        "start_time": "1981.89"
    },
    {
        "id": "b862c534",
        "text": "OK. And then we'll do a model dot uh compile and in the model dot compile, we, we, we need to pass a bunch of things. So the first thing that we need to pass again is the optimize it, right? And that's the one that we've just built. Then uh we want to pass the loss function, we need to specify that. And in this case, we'll use the pa category cross entropy function. So let me see if I've spelled this well, if I typed this, well, the spars categorical cross entropy. Yeah, it seems fine.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1983s",
        "start_time": "1983.209"
    },
    {
        "id": "7cbb280c",
        "text": "and in the model dot compile, we, we, we need to pass a bunch of things. So the first thing that we need to pass again is the optimize it, right? And that's the one that we've just built. Then uh we want to pass the loss function, we need to specify that. And in this case, we'll use the pa category cross entropy function. So let me see if I've spelled this well, if I typed this, well, the spars categorical cross entropy. Yeah, it seems fine. And then we need to pass the accurate. Uh um I think it's called metrics. Sorry, it's the metrics and the metrics that we wanna um track here is accuracy.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=1988s",
        "start_time": "1988.829"
    },
    {
        "id": "ae3edc84",
        "text": "cross entropy function. So let me see if I've spelled this well, if I typed this, well, the spars categorical cross entropy. Yeah, it seems fine. And then we need to pass the accurate. Uh um I think it's called metrics. Sorry, it's the metrics and the metrics that we wanna um track here is accuracy. Cool. OK. So now we need to train the model and",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2015s",
        "start_time": "2015.43"
    },
    {
        "id": "2e12db4f",
        "text": "And then we need to pass the accurate. Uh um I think it's called metrics. Sorry, it's the metrics and the metrics that we wanna um track here is accuracy. Cool. OK. So now we need to train the model and again, we've done this already multiple times. So I'm going to power through this. So we do a model uh dot fit and now we want to pass X train. So these are the inputs uh for like the train set, then we are going to pass the labels for the training set, then we want to pass the validation data. So this is the cross validation split that we've created. Uh And uh here",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2026s",
        "start_time": "2026.869"
    },
    {
        "id": "ac536bf7",
        "text": "Cool. OK. So now we need to train the model and again, we've done this already multiple times. So I'm going to power through this. So we do a model uh dot fit and now we want to pass X train. So these are the inputs uh for like the train set, then we are going to pass the labels for the training set, then we want to pass the validation data. So this is the cross validation split that we've created. Uh And uh here uh we're gonna pass in the X of",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2042s",
        "start_time": "2042.67"
    },
    {
        "id": "339cd70f",
        "text": "again, we've done this already multiple times. So I'm going to power through this. So we do a model uh dot fit and now we want to pass X train. So these are the inputs uh for like the train set, then we are going to pass the labels for the training set, then we want to pass the validation data. So this is the cross validation split that we've created. Uh And uh here uh we're gonna pass in the X of validation, right? And the Y validation,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2049s",
        "start_time": "2049.33"
    },
    {
        "id": "23fc0015",
        "text": "uh we're gonna pass in the X of validation, right? And the Y validation, right? So now we have another couple of hyper parameters that we should uh specify. So the batch size 32 and then we need to specify the number of epochs that we want to like run this uh training for and we'll speci we'll, yeah, just put in 30. Now,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2077s",
        "start_time": "2077.658"
    },
    {
        "id": "5d954a0a",
        "text": "validation, right? And the Y validation, right? So now we have another couple of hyper parameters that we should uh specify. So the batch size 32 and then we need to specify the number of epochs that we want to like run this uh training for and we'll speci we'll, yeah, just put in 30. Now, these are like other like high level hyper parameters that we can trick. We're not gonna do this here. Uh But uh remember guys like that, you can trick like the batch size, the number of E books as well as like other hyper parameters to, to find what works best for your uh problem,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2082s",
        "start_time": "2082.06"
    },
    {
        "id": "9b0dd2f2",
        "text": "right? So now we have another couple of hyper parameters that we should uh specify. So the batch size 32 and then we need to specify the number of epochs that we want to like run this uh training for and we'll speci we'll, yeah, just put in 30. Now, these are like other like high level hyper parameters that we can trick. We're not gonna do this here. Uh But uh remember guys like that, you can trick like the batch size, the number of E books as well as like other hyper parameters to, to find what works best for your uh problem, right? OK. So I think we should be done like with, with training here. So now like the, the last thing that remains to do is to evaluate the CNN on the test set. Now, let's do that. And uh in order to do that, we, we can use",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2087s",
        "start_time": "2087.928"
    },
    {
        "id": "0d359322",
        "text": "these are like other like high level hyper parameters that we can trick. We're not gonna do this here. Uh But uh remember guys like that, you can trick like the batch size, the number of E books as well as like other hyper parameters to, to find what works best for your uh problem, right? OK. So I think we should be done like with, with training here. So now like the, the last thing that remains to do is to evaluate the CNN on the test set. Now, let's do that. And uh in order to do that, we, we can use a uh really handy function from a KIS. So that's gonna return us the test uh error",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2110s",
        "start_time": "2110.709"
    },
    {
        "id": "23e34fa4",
        "text": "right? OK. So I think we should be done like with, with training here. So now like the, the last thing that remains to do is to evaluate the CNN on the test set. Now, let's do that. And uh in order to do that, we, we can use a uh really handy function from a KIS. So that's gonna return us the test uh error as well as the test accuracy.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2129s",
        "start_time": "2129.82"
    },
    {
        "id": "64ae47a0",
        "text": "a uh really handy function from a KIS. So that's gonna return us the test uh error as well as the test accuracy. And so we'll do a model dot uh evaluate over here and uh we're gonna pass in the X test. So the inputs for the, for the test split as well as the targets for the test split, which is not we train but Y test. Uh Right. And we're gonna save both uh equal one.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2151s",
        "start_time": "2151.82"
    },
    {
        "id": "68255a26",
        "text": "as well as the test accuracy. And so we'll do a model dot uh evaluate over here and uh we're gonna pass in the X test. So the inputs for the, for the test split as well as the targets for the test split, which is not we train but Y test. Uh Right. And we're gonna save both uh equal one. Uh Right. OK. So now let's print uh the results here. And so we'll say um accuracy",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2162s",
        "start_time": "2162.209"
    },
    {
        "id": "0c56f490",
        "text": "And so we'll do a model dot uh evaluate over here and uh we're gonna pass in the X test. So the inputs for the, for the test split as well as the targets for the test split, which is not we train but Y test. Uh Right. And we're gonna save both uh equal one. Uh Right. OK. So now let's print uh the results here. And so we'll say um accuracy on test set ease and will",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2166s",
        "start_time": "2166.439"
    },
    {
        "id": "5d8868d5",
        "text": "Uh Right. OK. So now let's print uh the results here. And so we'll say um accuracy on test set ease and will just pass in to format uh test uh accuracy over here.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2191s",
        "start_time": "2191.199"
    },
    {
        "id": "dfe9922e",
        "text": "on test set ease and will just pass in to format uh test uh accuracy over here. Good.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2202s",
        "start_time": "2202.669"
    },
    {
        "id": "37b19800",
        "text": "just pass in to format uh test uh accuracy over here. Good. I think uh we, we've, we've done like a lot of work so far. So before we, we make like any predictions, uh I feel like we should just like run the script and hopefully if I haven't made like any mistakes and probably I have made a few here, uh We should be able like to, to see like uh this and see like if, if our CNN works. So this is exciting. So let's try this. So",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2209s",
        "start_time": "2209.79"
    },
    {
        "id": "497473eb",
        "text": "Good. I think uh we, we've, we've done like a lot of work so far. So before we, we make like any predictions, uh I feel like we should just like run the script and hopefully if I haven't made like any mistakes and probably I have made a few here, uh We should be able like to, to see like uh this and see like if, if our CNN works. So this is exciting. So let's try this. So OK, so I'm running here. So it's gonna take some time if you guys remember also from previous video to load uh like the data. So hopefully this is gonna work. So",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2216s",
        "start_time": "2216.07"
    },
    {
        "id": "52aada8c",
        "text": "I think uh we, we've, we've done like a lot of work so far. So before we, we make like any predictions, uh I feel like we should just like run the script and hopefully if I haven't made like any mistakes and probably I have made a few here, uh We should be able like to, to see like uh this and see like if, if our CNN works. So this is exciting. So let's try this. So OK, so I'm running here. So it's gonna take some time if you guys remember also from previous video to load uh like the data. So hopefully this is gonna work. So that's great. Yes, it's working now. It's gonna take some time to, to do the whole training. So I'm gonna post the video and reprise like once like the whole thing is done",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2219s",
        "start_time": "2219.05"
    },
    {
        "id": "4e9224d5",
        "text": "OK, so I'm running here. So it's gonna take some time if you guys remember also from previous video to load uh like the data. So hopefully this is gonna work. So that's great. Yes, it's working now. It's gonna take some time to, to do the whole training. So I'm gonna post the video and reprise like once like the whole thing is done and here we go with the results. So it's quite exciting because uh the accuracy that we're, we're able to get on the test set is 70% which is like a good, good improvement on the previous accuracy that we were able to get on the uh with the multi-layered perception architecture, which is nice. And, and again, like it shows like how like CNN S are very effective liquid audio data",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2247s",
        "start_time": "2247.419"
    },
    {
        "id": "67de487d",
        "text": "that's great. Yes, it's working now. It's gonna take some time to, to do the whole training. So I'm gonna post the video and reprise like once like the whole thing is done and here we go with the results. So it's quite exciting because uh the accuracy that we're, we're able to get on the test set is 70% which is like a good, good improvement on the previous accuracy that we were able to get on the uh with the multi-layered perception architecture, which is nice. And, and again, like it shows like how like CNN S are very effective liquid audio data and we haven't done like very like crazy things like on anything like this is a very simple architecture but still like it's a nice result like 70% on a genre classification task with like 10 genres. I mean, it's starting to, to get like nice.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2259s",
        "start_time": "2259.62"
    },
    {
        "id": "739f9bd0",
        "text": "and here we go with the results. So it's quite exciting because uh the accuracy that we're, we're able to get on the test set is 70% which is like a good, good improvement on the previous accuracy that we were able to get on the uh with the multi-layered perception architecture, which is nice. And, and again, like it shows like how like CNN S are very effective liquid audio data and we haven't done like very like crazy things like on anything like this is a very simple architecture but still like it's a nice result like 70% on a genre classification task with like 10 genres. I mean, it's starting to, to get like nice. OK. So now let's take a look at the accuracy that we have on the train set over here like in the last ebook and it's 74 then we have the accuracy like here on the, on the validation which is like 71. Well, I mean,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2271s",
        "start_time": "2271.469"
    },
    {
        "id": "58bb4ea9",
        "text": "and we haven't done like very like crazy things like on anything like this is a very simple architecture but still like it's a nice result like 70% on a genre classification task with like 10 genres. I mean, it's starting to, to get like nice. OK. So now let's take a look at the accuracy that we have on the train set over here like in the last ebook and it's 74 then we have the accuracy like here on the, on the validation which is like 71. Well, I mean, it's good. It's like 70%. So we should be uh OK with this nice. OK. So now uh we need to do like the, the last bit, right? So just let me close this. OK. So we need to like make a prediction on a single sample. So, so we want to do like the so called inference, right? OK. So let's write a function for doing that. And so we'll,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2299s",
        "start_time": "2299.51"
    },
    {
        "id": "cf059683",
        "text": "OK. So now let's take a look at the accuracy that we have on the train set over here like in the last ebook and it's 74 then we have the accuracy like here on the, on the validation which is like 71. Well, I mean, it's good. It's like 70%. So we should be uh OK with this nice. OK. So now uh we need to do like the, the last bit, right? So just let me close this. OK. So we need to like make a prediction on a single sample. So, so we want to do like the so called inference, right? OK. So let's write a function for doing that. And so we'll, we'll just like call it predicts. Yeah, it's very uh straightforward and we'll pass in uh an X and A Y. So this is like the uh yeah, the input data for that uh sample and this is like the, the label. So because we're gonna compare the actual label to the actual genre against the uh predicted genre.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2314s",
        "start_time": "2314.989"
    },
    {
        "id": "c00b2f0e",
        "text": "it's good. It's like 70%. So we should be uh OK with this nice. OK. So now uh we need to do like the, the last bit, right? So just let me close this. OK. So we need to like make a prediction on a single sample. So, so we want to do like the so called inference, right? OK. So let's write a function for doing that. And so we'll, we'll just like call it predicts. Yeah, it's very uh straightforward and we'll pass in uh an X and A Y. So this is like the uh yeah, the input data for that uh sample and this is like the, the label. So because we're gonna compare the actual label to the actual genre against the uh predicted genre. OK. So we need to, to like get X and get uh Y so how do we do that? Well, we can just like take uh any sample from the test set. Really? So let's say like we, we're gonna take like the, the sample at index 100 from uh the, the test set. And so, and for y uh we're gonna seek Y test and so",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2330s",
        "start_time": "2330.83"
    },
    {
        "id": "f3444b05",
        "text": "we'll just like call it predicts. Yeah, it's very uh straightforward and we'll pass in uh an X and A Y. So this is like the uh yeah, the input data for that uh sample and this is like the, the label. So because we're gonna compare the actual label to the actual genre against the uh predicted genre. OK. So we need to, to like get X and get uh Y so how do we do that? Well, we can just like take uh any sample from the test set. Really? So let's say like we, we're gonna take like the, the sample at index 100 from uh the, the test set. And so, and for y uh we're gonna seek Y test and so this is 100 now.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2359s",
        "start_time": "2359.949"
    },
    {
        "id": "0696c106",
        "text": "OK. So we need to, to like get X and get uh Y so how do we do that? Well, we can just like take uh any sample from the test set. Really? So let's say like we, we're gonna take like the, the sample at index 100 from uh the, the test set. And so, and for y uh we're gonna seek Y test and so this is 100 now. So we are passing X and Y into uh predict here, but obviously, like we're getting an error here because predict isn't defined yet. So we need to define predict. OK. So let's do define predicts. And so we have X and Y over here.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2385s",
        "start_time": "2385.33"
    },
    {
        "id": "05dcdbd5",
        "text": "this is 100 now. So we are passing X and Y into uh predict here, but obviously, like we're getting an error here because predict isn't defined yet. So we need to define predict. OK. So let's do define predicts. And so we have X and Y over here. Nice. So um in order to do a prediction, so it's, it's very simple, right? So we, we just like uh take the model,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2414s",
        "start_time": "2414.85"
    },
    {
        "id": "a151a8ac",
        "text": "So we are passing X and Y into uh predict here, but obviously, like we're getting an error here because predict isn't defined yet. So we need to define predict. OK. So let's do define predicts. And so we have X and Y over here. Nice. So um in order to do a prediction, so it's, it's very simple, right? So we, we just like uh take the model, I'm not passing in the model. Yes. So we need to pass the model here, right? So the train model, so we've trained the model, we need to pass it because otherwise, how are we gonna perform like the prediction? So we need the model. So we get, so here we need model as an argument. And so we'll, we'll do a model dot uh pre six, right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2418s",
        "start_time": "2418.03"
    },
    {
        "id": "bfcf3871",
        "text": "Nice. So um in order to do a prediction, so it's, it's very simple, right? So we, we just like uh take the model, I'm not passing in the model. Yes. So we need to pass the model here, right? So the train model, so we've trained the model, we need to pass it because otherwise, how are we gonna perform like the prediction? So we need the model. So we get, so here we need model as an argument. And so we'll, we'll do a model dot uh pre six, right? And then we'll pass in X. So we'll, we'll pass in uh the input data uh for that sample and then we're gonna get a prediction",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2439s",
        "start_time": "2439.389"
    },
    {
        "id": "aa978a30",
        "text": "I'm not passing in the model. Yes. So we need to pass the model here, right? So the train model, so we've trained the model, we need to pass it because otherwise, how are we gonna perform like the prediction? So we need the model. So we get, so here we need model as an argument. And so we'll, we'll do a model dot uh pre six, right? And then we'll pass in X. So we'll, we'll pass in uh the input data uh for that sample and then we're gonna get a prediction predictions or prediction, right.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2452s",
        "start_time": "2452.229"
    },
    {
        "id": "16771e9c",
        "text": "And then we'll pass in X. So we'll, we'll pass in uh the input data uh for that sample and then we're gonna get a prediction predictions or prediction, right. OK. So",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2481s",
        "start_time": "2481.31"
    },
    {
        "id": "92c33cf0",
        "text": "predictions or prediction, right. OK. So really here I'm lying to you because like X in itself is not gonna, it's not gonna be enough So we need to change this, right. So because uh X so if we analyze X so X uh is gonna be a two dimensional parade two dimensional array.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2496s",
        "start_time": "2496.459"
    },
    {
        "id": "0021ddb4",
        "text": "OK. So really here I'm lying to you because like X in itself is not gonna, it's not gonna be enough So we need to change this, right. So because uh X so if we analyze X so X uh is gonna be a two dimensional parade two dimensional array. Well, sorry, in this case, uh is gonna be a three dimensional array, right? So 100 30 by 13 by one, right? Uh But what uh model that predicts expects is a four dimensional array, right? And the fourth dimension should be like here at the beginning,",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2500s",
        "start_time": "2500.81"
    },
    {
        "id": "0fa04f3a",
        "text": "really here I'm lying to you because like X in itself is not gonna, it's not gonna be enough So we need to change this, right. So because uh X so if we analyze X so X uh is gonna be a two dimensional parade two dimensional array. Well, sorry, in this case, uh is gonna be a three dimensional array, right? So 100 30 by 13 by one, right? Uh But what uh model that predicts expects is a four dimensional array, right? And the fourth dimension should be like here at the beginning, this guy here. And it basically like uh is used like to specify like the, the number of samples that we are we want to predict. And that's done because we, when we use model do predict, usually we pass in a batch of samples that we want to predict. And so, and these are gonna be like, I mean, we, we need to specify all of these different samples and this means that we need an extra dimension for doing that. In our case, we're just gonna be like a one",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2503s",
        "start_time": "2503.07"
    },
    {
        "id": "9bf59fae",
        "text": "Well, sorry, in this case, uh is gonna be a three dimensional array, right? So 100 30 by 13 by one, right? Uh But what uh model that predicts expects is a four dimensional array, right? And the fourth dimension should be like here at the beginning, this guy here. And it basically like uh is used like to specify like the, the number of samples that we are we want to predict. And that's done because we, when we use model do predict, usually we pass in a batch of samples that we want to predict. And so, and these are gonna be like, I mean, we, we need to specify all of these different samples and this means that we need an extra dimension for doing that. In our case, we're just gonna be like a one here, right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2528s",
        "start_time": "2528.429"
    },
    {
        "id": "b68a8e5d",
        "text": "this guy here. And it basically like uh is used like to specify like the, the number of samples that we are we want to predict. And that's done because we, when we use model do predict, usually we pass in a batch of samples that we want to predict. And so, and these are gonna be like, I mean, we, we need to specify all of these different samples and this means that we need an extra dimension for doing that. In our case, we're just gonna be like a one here, right? Uh OK. So how do we do that? Well, we've already seen how to like augment an array with an extra dimension. So we'll, we'll do that once again. So we'll take X and we'll see that we'll do a MP dot uh new axis and then uh we're just gonna pass in the dots. And so basically, we are inserting a new",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2556s",
        "start_time": "2556.489"
    },
    {
        "id": "a20ca807",
        "text": "here, right? Uh OK. So how do we do that? Well, we've already seen how to like augment an array with an extra dimension. So we'll, we'll do that once again. So we'll take X and we'll see that we'll do a MP dot uh new axis and then uh we're just gonna pass in the dots. And so basically, we are inserting a new access like at the beginning of the array and then we are copying like all the rest. OK. And so now this should work. OK. So uh we make the prediction but what we should understand is that the prediction that we get is a uh two dimensional array. So like this prediction here, prediction is a two dimensional array.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2586s",
        "start_time": "2586.949"
    },
    {
        "id": "dd878142",
        "text": "Uh OK. So how do we do that? Well, we've already seen how to like augment an array with an extra dimension. So we'll, we'll do that once again. So we'll take X and we'll see that we'll do a MP dot uh new axis and then uh we're just gonna pass in the dots. And so basically, we are inserting a new access like at the beginning of the array and then we are copying like all the rest. OK. And so now this should work. OK. So uh we make the prediction but what we should understand is that the prediction that we get is a uh two dimensional array. So like this prediction here, prediction is a two dimensional array. And we have values",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2589s",
        "start_time": "2589.05"
    },
    {
        "id": "efd11324",
        "text": "access like at the beginning of the array and then we are copying like all the rest. OK. And so now this should work. OK. So uh we make the prediction but what we should understand is that the prediction that we get is a uh two dimensional array. So like this prediction here, prediction is a two dimensional array. And we have values over here",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2615s",
        "start_time": "2615.199"
    },
    {
        "id": "769eb8d7",
        "text": "And we have values over here uh where like we have basically 10 values and the 10 values represent the different scores for the 10 different genres, right? And so this is like the the results like of the of the soft mats",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2644s",
        "start_time": "2644.659"
    },
    {
        "id": "7677138f",
        "text": "over here uh where like we have basically 10 values and the 10 values represent the different scores for the 10 different genres, right? And so this is like the the results like of the of the soft mats activation function. So we are not really like at the, at the point where we already have like a prediction. So we have like the predicted index. We need to extract that from these two dimensional array. So what we want to do here is to uh get the max the index where we have the max value. So extracts",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2647s",
        "start_time": "2647.949"
    },
    {
        "id": "53c67fed",
        "text": "uh where like we have basically 10 values and the 10 values represent the different scores for the 10 different genres, right? And so this is like the the results like of the of the soft mats activation function. So we are not really like at the, at the point where we already have like a prediction. So we have like the predicted index. We need to extract that from these two dimensional array. So what we want to do here is to uh get the max the index where we have the max value. So extracts um index with max volume. So",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2649s",
        "start_time": "2649.409"
    },
    {
        "id": "bab86fd2",
        "text": "activation function. So we are not really like at the, at the point where we already have like a prediction. So we have like the predicted index. We need to extract that from these two dimensional array. So what we want to do here is to uh get the max the index where we have the max value. So extracts um index with max volume. So oops. So we'll do a pre",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2667s",
        "start_time": "2667.35"
    },
    {
        "id": "e3bc9457",
        "text": "um index with max volume. So oops. So we'll do a pre see it",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2691s",
        "start_time": "2691.739"
    },
    {
        "id": "f126e4b1",
        "text": "oops. So we'll do a pre see it index",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2698s",
        "start_time": "2698.37"
    },
    {
        "id": "b0897884",
        "text": "see it index and uh we're gonna use a nice utility uh function from Nimai that's called marks. And uh we pass in uh prediction",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2703s",
        "start_time": "2703.489"
    },
    {
        "id": "13561000",
        "text": "index and uh we're gonna use a nice utility uh function from Nimai that's called marks. And uh we pass in uh prediction and we'll specify that we want to uh calculate the um the, the, the max on the axis number one, which is basically like on this guy here, right? And what we're gonna get out of this is a uh one dimensional array where we have a value like this like between zero and nine in this case. And that's gonna be like the index that's",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2705s",
        "start_time": "2705.209"
    },
    {
        "id": "62683e9e",
        "text": "and uh we're gonna use a nice utility uh function from Nimai that's called marks. And uh we pass in uh prediction and we'll specify that we want to uh calculate the um the, the, the max on the axis number one, which is basically like on this guy here, right? And what we're gonna get out of this is a uh one dimensional array where we have a value like this like between zero and nine in this case. And that's gonna be like the index that's been predicted. And now we could potentially uh take this index and map it onto like a genre label and we could use like this mapping here. So like, for example, here we know that disco like is zero reggae one, but I'm not gonna do that because I mean, I don't want to, right? You guys can do that. Well, actually it's a nice like exercise for you. OK? But now we have the uh predicted index. So now let's do a uh print",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2707s",
        "start_time": "2707.239"
    },
    {
        "id": "b09670c6",
        "text": "and we'll specify that we want to uh calculate the um the, the, the max on the axis number one, which is basically like on this guy here, right? And what we're gonna get out of this is a uh one dimensional array where we have a value like this like between zero and nine in this case. And that's gonna be like the index that's been predicted. And now we could potentially uh take this index and map it onto like a genre label and we could use like this mapping here. So like, for example, here we know that disco like is zero reggae one, but I'm not gonna do that because I mean, I don't want to, right? You guys can do that. Well, actually it's a nice like exercise for you. OK? But now we have the uh predicted index. So now let's do a uh print where uh we say uh what do we want to say here? So we wanna say the, so the expected",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2720s",
        "start_time": "2720.939"
    },
    {
        "id": "3750cf2f",
        "text": "been predicted. And now we could potentially uh take this index and map it onto like a genre label and we could use like this mapping here. So like, for example, here we know that disco like is zero reggae one, but I'm not gonna do that because I mean, I don't want to, right? You guys can do that. Well, actually it's a nice like exercise for you. OK? But now we have the uh predicted index. So now let's do a uh print where uh we say uh what do we want to say here? So we wanna say the, so the expected um output or expected index",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2749s",
        "start_time": "2749.844"
    },
    {
        "id": "0c3a670a",
        "text": "where uh we say uh what do we want to say here? So we wanna say the, so the expected um output or expected index is equal",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2778s",
        "start_time": "2778.09"
    },
    {
        "id": "3194f26c",
        "text": "um output or expected index is equal to are available and the predicted",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2789s",
        "start_time": "2789.75"
    },
    {
        "id": "5781b5c8",
        "text": "is equal to are available and the predicted uh index is equal to another variable. And so let's fill in the variables here and the expected index is this Y variable over there Y argument and the predicted in index is just predicted index here. Cool. OK. So this should be working now. So what I'm gonna do is I'm gonna",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2794s",
        "start_time": "2794.04"
    },
    {
        "id": "c7cf3fae",
        "text": "to are available and the predicted uh index is equal to another variable. And so let's fill in the variables here and the expected index is this Y variable over there Y argument and the predicted in index is just predicted index here. Cool. OK. So this should be working now. So what I'm gonna do is I'm gonna rerun the script and obviously it's gonna take some time because it's gonna like retrain everything. But then by the end of this, we're gonna try to predict the sample, the sample at index 100 like in the test set, right? And see if the, the model is predicting it correctly. So now let me",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2796s",
        "start_time": "2796.219"
    },
    {
        "id": "e01475c7",
        "text": "uh index is equal to another variable. And so let's fill in the variables here and the expected index is this Y variable over there Y argument and the predicted in index is just predicted index here. Cool. OK. So this should be working now. So what I'm gonna do is I'm gonna rerun the script and obviously it's gonna take some time because it's gonna like retrain everything. But then by the end of this, we're gonna try to predict the sample, the sample at index 100 like in the test set, right? And see if the, the model is predicting it correctly. So now let me run the scripts, I'll post the video and just go back, come back when uh we have a results and here we are back guys. So here we have our results. So the expected index for uh our sample was nine, which we know is uh yeah, let's take it here. It's metal, right? So this was a",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2802s",
        "start_time": "2802.389"
    },
    {
        "id": "e6f775cc",
        "text": "rerun the script and obviously it's gonna take some time because it's gonna like retrain everything. But then by the end of this, we're gonna try to predict the sample, the sample at index 100 like in the test set, right? And see if the, the model is predicting it correctly. So now let me run the scripts, I'll post the video and just go back, come back when uh we have a results and here we are back guys. So here we have our results. So the expected index for uh our sample was nine, which we know is uh yeah, let's take it here. It's metal, right? So this was a uh as a metal sample and the predicted index was nine good. OK. So the uh the model I performed correctly in this instance. Nice. So guys, we are done, this was like a quite intense video and I hope you like you really enjoyed that because now you know how to build like a CNN",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2829s",
        "start_time": "2829.61"
    },
    {
        "id": "93ecb5d9",
        "text": "run the scripts, I'll post the video and just go back, come back when uh we have a results and here we are back guys. So here we have our results. So the expected index for uh our sample was nine, which we know is uh yeah, let's take it here. It's metal, right? So this was a uh as a metal sample and the predicted index was nine good. OK. So the uh the model I performed correctly in this instance. Nice. So guys, we are done, this was like a quite intense video and I hope you like you really enjoyed that because now you know how to build like a CNN uh classifier. And this like music genre classifier is doing like pretty well overall. Uh So like for next video, we're gonna start looking into recurrent neural networks. So which are like another architecture, another type of architecture that's very important like with audio data, like music data more specifically because like we can interpret that as like time series, right?",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2850s",
        "start_time": "2850.594"
    },
    {
        "id": "c6ea5f54",
        "text": "uh as a metal sample and the predicted index was nine good. OK. So the uh the model I performed correctly in this instance. Nice. So guys, we are done, this was like a quite intense video and I hope you like you really enjoyed that because now you know how to build like a CNN uh classifier. And this like music genre classifier is doing like pretty well overall. Uh So like for next video, we're gonna start looking into recurrent neural networks. So which are like another architecture, another type of architecture that's very important like with audio data, like music data more specifically because like we can interpret that as like time series, right? And so like next time it's gonna be all about like the theory behind R and M si really hope, you know, like you enjoyed this video. If that's the case, please remember to subscribe if you have any questions and you may have some now because like this was quite intense, just like",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2875s",
        "start_time": "2875.81"
    },
    {
        "id": "2885b3a0",
        "text": "uh classifier. And this like music genre classifier is doing like pretty well overall. Uh So like for next video, we're gonna start looking into recurrent neural networks. So which are like another architecture, another type of architecture that's very important like with audio data, like music data more specifically because like we can interpret that as like time series, right? And so like next time it's gonna be all about like the theory behind R and M si really hope, you know, like you enjoyed this video. If that's the case, please remember to subscribe if you have any questions and you may have some now because like this was quite intense, just like write them like in the comments section below and I'll see you next time. Cheers.",
        "video": "16- How to Implement a CNN for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "dOG-HxpbMSw",
        "youtube_link": "https://www.youtube.com/watch?v=dOG-HxpbMSw&t=2899s",
        "start_time": "2899.949"
    },
    {
        "id": "93792d70",
        "text": "Hi, everybody and welcome to another video in the Deep Learning for Rodeo with Python series. This time around, we're gonna talk about artificial neurons and we're gonna both understand the theory behind it and also implement them in Python. So a little bit more of a focus on what we'll be learning. So we'll have a quick look at a biological neurons. Obviously, it's not gonna be a neuro class by any means, it's just gonna be like some very introductory uh things about biological neurons. And then we're gonna move on to artificial neurons and see a little bit how the math works behind it and understanding the uh the theory there and then we'll move on and we're gonna basically implement an artificial neuron from scratch in Python. So let's get started. Uh the picture you have here is that of a biological neuron. So as you see, this is a quite complex system with a bunch of like different things, but we are mainly interested in three aspects of it. The dendrites, which are all of these filaments like down here. And basically these are",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=7s",
        "start_time": "7.78"
    },
    {
        "id": "f05d8776",
        "text": "class by any means, it's just gonna be like some very introductory uh things about biological neurons. And then we're gonna move on to artificial neurons and see a little bit how the math works behind it and understanding the uh the theory there and then we'll move on and we're gonna basically implement an artificial neuron from scratch in Python. So let's get started. Uh the picture you have here is that of a biological neuron. So as you see, this is a quite complex system with a bunch of like different things, but we are mainly interested in three aspects of it. The dendrites, which are all of these filaments like down here. And basically these are input modules which are important for the neuron because they get uh signals from other neurons and they input it to the cell body, which is the operational center of the neuron. And, and basically the cell body does",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=33s",
        "start_time": "33.625"
    },
    {
        "id": "9ad952d9",
        "text": "Uh the picture you have here is that of a biological neuron. So as you see, this is a quite complex system with a bunch of like different things, but we are mainly interested in three aspects of it. The dendrites, which are all of these filaments like down here. And basically these are input modules which are important for the neuron because they get uh signals from other neurons and they input it to the cell body, which is the operational center of the neuron. And, and basically the cell body does some kind of computation on this signal which is electric signal, it modulates it and then it passes it on along the Axion to all of these different synaptic terminals down here which are connected to other neurons. So through synapses and through the Axion terminals, basically,",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=60s",
        "start_time": "60.47"
    },
    {
        "id": "7b866d51",
        "text": "input modules which are important for the neuron because they get uh signals from other neurons and they input it to the cell body, which is the operational center of the neuron. And, and basically the cell body does some kind of computation on this signal which is electric signal, it modulates it and then it passes it on along the Axion to all of these different synaptic terminals down here which are connected to other neurons. So through synapses and through the Axion terminals, basically, um what the neuron does is connect, being connected to all other neurons in this sense, the neuron can be seen as an individual that stays within a very complex system within a network where you have loads and loads of neurons connected together.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=82s",
        "start_time": "82.779"
    },
    {
        "id": "404e8db6",
        "text": "some kind of computation on this signal which is electric signal, it modulates it and then it passes it on along the Axion to all of these different synaptic terminals down here which are connected to other neurons. So through synapses and through the Axion terminals, basically, um what the neuron does is connect, being connected to all other neurons in this sense, the neuron can be seen as an individual that stays within a very complex system within a network where you have loads and loads of neurons connected together. Now, if you take a neuron by itself, that isn't, it's obviously remarkable, but it's not super powerful, it becomes super powerful when",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=105s",
        "start_time": "105.089"
    },
    {
        "id": "0dd3627c",
        "text": "um what the neuron does is connect, being connected to all other neurons in this sense, the neuron can be seen as an individual that stays within a very complex system within a network where you have loads and loads of neurons connected together. Now, if you take a neuron by itself, that isn't, it's obviously remarkable, but it's not super powerful, it becomes super powerful when you put together billions of neurons. And the result that you have is basically the brain there and the power of the brain doesn't reside in the structure of the neurons themselves. But rather in the incredible number of connections we have among all of our neurons",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=127s",
        "start_time": "127.639"
    },
    {
        "id": "8143f710",
        "text": "Now, if you take a neuron by itself, that isn't, it's obviously remarkable, but it's not super powerful, it becomes super powerful when you put together billions of neurons. And the result that you have is basically the brain there and the power of the brain doesn't reside in the structure of the neurons themselves. But rather in the incredible number of connections we have among all of our neurons in our brains. And thanks like to, to these neurons and these connections. And we are talking here about trillions of connections of billions and billions of neurons. We can walk, we can play the piano and we can solve Sudoku and play chess, for example, right.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=146s",
        "start_time": "146.869"
    },
    {
        "id": "b1a0d0cc",
        "text": "you put together billions of neurons. And the result that you have is basically the brain there and the power of the brain doesn't reside in the structure of the neurons themselves. But rather in the incredible number of connections we have among all of our neurons in our brains. And thanks like to, to these neurons and these connections. And we are talking here about trillions of connections of billions and billions of neurons. We can walk, we can play the piano and we can solve Sudoku and play chess, for example, right. So now let's move on to the artificial neuron. And basically the the the story goes that we looked at the uh neuron at the biological neuron and we use a ve a very simplified version of it",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=158s",
        "start_time": "158.699"
    },
    {
        "id": "9049c6fc",
        "text": "in our brains. And thanks like to, to these neurons and these connections. And we are talking here about trillions of connections of billions and billions of neurons. We can walk, we can play the piano and we can solve Sudoku and play chess, for example, right. So now let's move on to the artificial neuron. And basically the the the story goes that we looked at the uh neuron at the biological neuron and we use a ve a very simplified version of it to create artificial neurons. And in artificial neurons here, as you can see, you have a series of inputs. So it is X one, X two, X three with certain weights associated to this different inputs like W 1 W-2 W three. And",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=177s",
        "start_time": "177.755"
    },
    {
        "id": "7e0092ba",
        "text": "So now let's move on to the artificial neuron. And basically the the the story goes that we looked at the uh neuron at the biological neuron and we use a ve a very simplified version of it to create artificial neurons. And in artificial neurons here, as you can see, you have a series of inputs. So it is X one, X two, X three with certain weights associated to this different inputs like W 1 W-2 W three. And as you can imagine here, uh this is the equivalent of the dendrites that we have in the biological neuron. Then we have the neuron itself uh which does a couple of things. It does uh computation in the form of a sum and an activation.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=197s",
        "start_time": "197.74"
    },
    {
        "id": "8eeaf937",
        "text": "to create artificial neurons. And in artificial neurons here, as you can see, you have a series of inputs. So it is X one, X two, X three with certain weights associated to this different inputs like W 1 W-2 W three. And as you can imagine here, uh this is the equivalent of the dendrites that we have in the biological neuron. Then we have the neuron itself uh which does a couple of things. It does uh computation in the form of a sum and an activation. And this part of the neuron in a sense can be equated to the, to the cell body. And finally, you have like the output of the neuron,",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=215s",
        "start_time": "215.304"
    },
    {
        "id": "733f533e",
        "text": "as you can imagine here, uh this is the equivalent of the dendrites that we have in the biological neuron. Then we have the neuron itself uh which does a couple of things. It does uh computation in the form of a sum and an activation. And this part of the neuron in a sense can be equated to the, to the cell body. And finally, you have like the output of the neuron, right. So as I just said, so the neuron does a couple of computation, one is called the sum or the nets input and the other one, it's the activation. So let's take a look at the sum first. So here we have H which stands for the",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=233s",
        "start_time": "233.179"
    },
    {
        "id": "cc722d5f",
        "text": "And this part of the neuron in a sense can be equated to the, to the cell body. And finally, you have like the output of the neuron, right. So as I just said, so the neuron does a couple of computation, one is called the sum or the nets input and the other one, it's the activation. So let's take a look at the sum first. So here we have H which stands for the net input and H is nothing. It is just uh the sum over uh the, all the inputs multiplied by their respective weights. So in other words over here, uh H in this case is basically X one by W one plus X 2 W-2 plus X three, W three.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=254s",
        "start_time": "254.339"
    },
    {
        "id": "0c2793f7",
        "text": "right. So as I just said, so the neuron does a couple of computation, one is called the sum or the nets input and the other one, it's the activation. So let's take a look at the sum first. So here we have H which stands for the net input and H is nothing. It is just uh the sum over uh the, all the inputs multiplied by their respective weights. So in other words over here, uh H in this case is basically X one by W one plus X 2 W-2 plus X three, W three. So the first phase in the first phase, the uh artificial neuron does this sum and, and it arrives at a net input, then",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=265s",
        "start_time": "265.98"
    },
    {
        "id": "5b5903bb",
        "text": "net input and H is nothing. It is just uh the sum over uh the, all the inputs multiplied by their respective weights. So in other words over here, uh H in this case is basically X one by W one plus X 2 W-2 plus X three, W three. So the first phase in the first phase, the uh artificial neuron does this sum and, and it arrives at a net input, then we have the second phase of the neuron where we have the activation itself. And so basically the output Y is a function of the activation function F where we pass in H which is the net input. So now there are a gazillion different activation functions in neural networks. But we'll be looking at one in particular right now because it's quite simple and it's very common as well",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=284s",
        "start_time": "284.6"
    },
    {
        "id": "0fe12814",
        "text": "So the first phase in the first phase, the uh artificial neuron does this sum and, and it arrives at a net input, then we have the second phase of the neuron where we have the activation itself. And so basically the output Y is a function of the activation function F where we pass in H which is the net input. So now there are a gazillion different activation functions in neural networks. But we'll be looking at one in particular right now because it's quite simple and it's very common as well called the sigmoid function. And so here on the right, you have the function Excel itself and on the left you have the graph of the function. So why is this a good function for being like an activation function? Well, first of all, it's bounded between zero and one. Then as you see here, it's a very smooth function. It doesn't have any discontinuity and this is great because it can be",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=311s",
        "start_time": "311.899"
    },
    {
        "id": "82f31c16",
        "text": "we have the second phase of the neuron where we have the activation itself. And so basically the output Y is a function of the activation function F where we pass in H which is the net input. So now there are a gazillion different activation functions in neural networks. But we'll be looking at one in particular right now because it's quite simple and it's very common as well called the sigmoid function. And so here on the right, you have the function Excel itself and on the left you have the graph of the function. So why is this a good function for being like an activation function? Well, first of all, it's bounded between zero and one. Then as you see here, it's a very smooth function. It doesn't have any discontinuity and this is great because it can be uh differentiated. So you can calculate the derivatives of this function quite easily. And so what this function does is basically modulating all the uh inputs and the net input into an output that's uh limited between zero and one,",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=322s",
        "start_time": "322.57"
    },
    {
        "id": "cbf24b50",
        "text": "called the sigmoid function. And so here on the right, you have the function Excel itself and on the left you have the graph of the function. So why is this a good function for being like an activation function? Well, first of all, it's bounded between zero and one. Then as you see here, it's a very smooth function. It doesn't have any discontinuity and this is great because it can be uh differentiated. So you can calculate the derivatives of this function quite easily. And so what this function does is basically modulating all the uh inputs and the net input into an output that's uh limited between zero and one, right. So if we take that function and we plug it into uh here, so into like this function over here, you'll see that we have this equation and this equation is basically the equation of a neuron starting from the inputs uh moving all the way to the output itself cool.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=351s",
        "start_time": "351.415"
    },
    {
        "id": "88cd060a",
        "text": "uh differentiated. So you can calculate the derivatives of this function quite easily. And so what this function does is basically modulating all the uh inputs and the net input into an output that's uh limited between zero and one, right. So if we take that function and we plug it into uh here, so into like this function over here, you'll see that we have this equation and this equation is basically the equation of a neuron starting from the inputs uh moving all the way to the output itself cool. So this is somewhat like simple to understand, but it's always like uh nicer to have like examples to understand what's going on, like really in detail. So let's take an example here. And so here we have three inputs again with our very simple neurons.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=380s",
        "start_time": "380.489"
    },
    {
        "id": "76345d67",
        "text": "right. So if we take that function and we plug it into uh here, so into like this function over here, you'll see that we have this equation and this equation is basically the equation of a neuron starting from the inputs uh moving all the way to the output itself cool. So this is somewhat like simple to understand, but it's always like uh nicer to have like examples to understand what's going on, like really in detail. So let's take an example here. And so here we have three inputs again with our very simple neurons. So the first one is 0.5 then we have 0.3 and then 0.2 and then we have um the respect the respective weights over here. So 0.4 naught 0.7 and N 0.2.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=401s",
        "start_time": "401.44"
    },
    {
        "id": "bb306d8d",
        "text": "So this is somewhat like simple to understand, but it's always like uh nicer to have like examples to understand what's going on, like really in detail. So let's take an example here. And so here we have three inputs again with our very simple neurons. So the first one is 0.5 then we have 0.3 and then 0.2 and then we have um the respect the respective weights over here. So 0.4 naught 0.7 and N 0.2. So now let's calculate the output. Why? By going through the two phases of computations of a neuron. So the sum and the activation. So let's calculate the sum first. And if you guys remember the, activate the sum over here is calculated by multiplying X one by",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=428s",
        "start_time": "428.839"
    },
    {
        "id": "d4d95e9c",
        "text": "So the first one is 0.5 then we have 0.3 and then 0.2 and then we have um the respect the respective weights over here. So 0.4 naught 0.7 and N 0.2. So now let's calculate the output. Why? By going through the two phases of computations of a neuron. So the sum and the activation. So let's calculate the sum first. And if you guys remember the, activate the sum over here is calculated by multiplying X one by um W one plus X 2 W-2 plus uh X three W three, which in our case is, is basically 0.5 by 0.4. It's these two guys over here and then 0.3 by uh 0.7 plus 0.2",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=448s",
        "start_time": "448.67"
    },
    {
        "id": "6c18cc9e",
        "text": "So now let's calculate the output. Why? By going through the two phases of computations of a neuron. So the sum and the activation. So let's calculate the sum first. And if you guys remember the, activate the sum over here is calculated by multiplying X one by um W one plus X 2 W-2 plus uh X three W three, which in our case is, is basically 0.5 by 0.4. It's these two guys over here and then 0.3 by uh 0.7 plus 0.2 uh by 0.2. So if you run the map over there, you'll find that the input, the net input is equal to 0.45. Cool. So now we have our net input,",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=462s",
        "start_time": "462.109"
    },
    {
        "id": "b04dd82d",
        "text": "um W one plus X 2 W-2 plus uh X three W three, which in our case is, is basically 0.5 by 0.4. It's these two guys over here and then 0.3 by uh 0.7 plus 0.2 uh by 0.2. So if you run the map over there, you'll find that the input, the net input is equal to 0.45. Cool. So now we have our net input, let's uh arrive at the output by using the activation function. And as we said, we're going to use the sigmoid activation function. And so basically we are plugging in this 0.45 which is our input into the activation function.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=485s",
        "start_time": "485.91"
    },
    {
        "id": "fbcf0ee7",
        "text": "uh by 0.2. So if you run the map over there, you'll find that the input, the net input is equal to 0.45. Cool. So now we have our net input, let's uh arrive at the output by using the activation function. And as we said, we're going to use the sigmoid activation function. And so basically we are plugging in this 0.45 which is our input into the activation function. And then we have the result which is no 0.61 that's the output of uh the neuron in this particular case. Now, we have an idea of how an artificial neuron uh works. So it's time to implement one from scratch in Python.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=507s",
        "start_time": "507.119"
    },
    {
        "id": "ba989505",
        "text": "let's uh arrive at the output by using the activation function. And as we said, we're going to use the sigmoid activation function. And so basically we are plugging in this 0.45 which is our input into the activation function. And then we have the result which is no 0.61 that's the output of uh the neuron in this particular case. Now, we have an idea of how an artificial neuron uh works. So it's time to implement one from scratch in Python. Now we'll implement the artificial neuron in Python. And I'm using py charm as uh my idea of choice. Obviously, you can use whatever you want. If you want to use Jupiter notebooks, like, please feel free to do that. I'm just using Python because like I'm used it and I love it, right. So let's get started here. So the first thing that I'll do is I'll just",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=521s",
        "start_time": "521.968"
    },
    {
        "id": "dc9c27ea",
        "text": "And then we have the result which is no 0.61 that's the output of uh the neuron in this particular case. Now, we have an idea of how an artificial neuron uh works. So it's time to implement one from scratch in Python. Now we'll implement the artificial neuron in Python. And I'm using py charm as uh my idea of choice. Obviously, you can use whatever you want. If you want to use Jupiter notebooks, like, please feel free to do that. I'm just using Python because like I'm used it and I love it, right. So let's get started here. So the first thing that I'll do is I'll just ensure that we can run",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=539s",
        "start_time": "539.909"
    },
    {
        "id": "a48d59b3",
        "text": "Now we'll implement the artificial neuron in Python. And I'm using py charm as uh my idea of choice. Obviously, you can use whatever you want. If you want to use Jupiter notebooks, like, please feel free to do that. I'm just using Python because like I'm used it and I love it, right. So let's get started here. So the first thing that I'll do is I'll just ensure that we can run the scripts easily. So, and then what I want to do is basically replicate the um structure of the neuron that we had. So the inputs, the weights and then the calculations that we did. So the inputs will be",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=562s",
        "start_time": "562.59"
    },
    {
        "id": "5ccb2484",
        "text": "ensure that we can run the scripts easily. So, and then what I want to do is basically replicate the um structure of the neuron that we had. So the inputs, the weights and then the calculations that we did. So the inputs will be uh represented by a list, a simple list. And then we'll have the weights which again are going to be represented by lists. So if you guys remember the example, artificial neuron",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=591s",
        "start_time": "591.299"
    },
    {
        "id": "58bec3e4",
        "text": "the scripts easily. So, and then what I want to do is basically replicate the um structure of the neuron that we had. So the inputs, the weights and then the calculations that we did. So the inputs will be uh represented by a list, a simple list. And then we'll have the weights which again are going to be represented by lists. So if you guys remember the example, artificial neuron or the exa mle parameters for the simple artificial neuron I used uh before, you'll remember probably that we have 0.50 0.3 and 0.2 respectively for X one, X two and uh X three. And then for the weight, we would just associate uh like the numbers respecting the SA me indexes as the input. So for W one, we would have",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=595s",
        "start_time": "595.39"
    },
    {
        "id": "8e09c5b2",
        "text": "uh represented by a list, a simple list. And then we'll have the weights which again are going to be represented by lists. So if you guys remember the example, artificial neuron or the exa mle parameters for the simple artificial neuron I used uh before, you'll remember probably that we have 0.50 0.3 and 0.2 respectively for X one, X two and uh X three. And then for the weight, we would just associate uh like the numbers respecting the SA me indexes as the input. So for W one, we would have uh 0.4. And then for W-2, we would have naught 0.7. And finally, uh 0.2 for uh W uh three. And so this way, we've basically uh recreated uh in this very simple um data format, the inputs and the weights cool. Now, the next uh phase is to calculate the output",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=615s",
        "start_time": "615.14"
    },
    {
        "id": "ae42ef7a",
        "text": "or the exa mle parameters for the simple artificial neuron I used uh before, you'll remember probably that we have 0.50 0.3 and 0.2 respectively for X one, X two and uh X three. And then for the weight, we would just associate uh like the numbers respecting the SA me indexes as the input. So for W one, we would have uh 0.4. And then for W-2, we would have naught 0.7. And finally, uh 0.2 for uh W uh three. And so this way, we've basically uh recreated uh in this very simple um data format, the inputs and the weights cool. Now, the next uh phase is to calculate the output and the output is given by the activate function where we pass I both inputs and",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=630s",
        "start_time": "630.655"
    },
    {
        "id": "8d33fa80",
        "text": "uh 0.4. And then for W-2, we would have naught 0.7. And finally, uh 0.2 for uh W uh three. And so this way, we've basically uh recreated uh in this very simple um data format, the inputs and the weights cool. Now, the next uh phase is to calculate the output and the output is given by the activate function where we pass I both inputs and wait.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=660s",
        "start_time": "660.63"
    },
    {
        "id": "11241aaf",
        "text": "and the output is given by the activate function where we pass I both inputs and wait. And obviously, you'll see here that the activate function doesn't exist because we haven't defined it yet. So it's not going to work.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=690s",
        "start_time": "690.979"
    },
    {
        "id": "c6be5acc",
        "text": "wait. And obviously, you'll see here that the activate function doesn't exist because we haven't defined it yet. So it's not going to work. Um But this is a function that takes inputs and weights as um as parameters as arguments. And obviously this is the, the computational unit of the neuron itself. And it's that function that's going to be responsible for doing uh the net for performing the net input and then the activation function itself. And then as the last step over here, we're going to do a print",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=700s",
        "start_time": "700.69"
    },
    {
        "id": "a6b0f3f2",
        "text": "And obviously, you'll see here that the activate function doesn't exist because we haven't defined it yet. So it's not going to work. Um But this is a function that takes inputs and weights as um as parameters as arguments. And obviously this is the, the computational unit of the neuron itself. And it's that function that's going to be responsible for doing uh the net for performing the net input and then the activation function itself. And then as the last step over here, we're going to do a print and we're gonna just print the output so that we can sit. Now, obviously, if you, I'm gonna run this, as you can see here, I'm gonna get an error because activate obviously hasn't been defined. So we need to define activate, which is the, the core function for our",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=702s",
        "start_time": "702.809"
    },
    {
        "id": "80af6ae6",
        "text": "Um But this is a function that takes inputs and weights as um as parameters as arguments. And obviously this is the, the computational unit of the neuron itself. And it's that function that's going to be responsible for doing uh the net for performing the net input and then the activation function itself. And then as the last step over here, we're going to do a print and we're gonna just print the output so that we can sit. Now, obviously, if you, I'm gonna run this, as you can see here, I'm gonna get an error because activate obviously hasn't been defined. So we need to define activate, which is the, the core function for our um uh for our neuron. So as we said, we define activate and we pass uh two arguments. One is called inputs. The other one is called weights and so activate, does two things and it just replicates the two phases of a neuron that we've seen in the theoretical part of things. And so the first thing is just perform",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=710s",
        "start_time": "710.989"
    },
    {
        "id": "5c558cf2",
        "text": "and we're gonna just print the output so that we can sit. Now, obviously, if you, I'm gonna run this, as you can see here, I'm gonna get an error because activate obviously hasn't been defined. So we need to define activate, which is the, the core function for our um uh for our neuron. So as we said, we define activate and we pass uh two arguments. One is called inputs. The other one is called weights and so activate, does two things and it just replicates the two phases of a neuron that we've seen in the theoretical part of things. And so the first thing is just perform net input. And the second side of things is to uh perform the activation,",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=739s",
        "start_time": "739.789"
    },
    {
        "id": "1f24cbca",
        "text": "um uh for our neuron. So as we said, we define activate and we pass uh two arguments. One is called inputs. The other one is called weights and so activate, does two things and it just replicates the two phases of a neuron that we've seen in the theoretical part of things. And so the first thing is just perform net input. And the second side of things is to uh perform the activation, right? So how do we calculate the net input? So we have to loop obviously",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=759s",
        "start_time": "759.239"
    },
    {
        "id": "d10a618c",
        "text": "net input. And the second side of things is to uh perform the activation, right? So how do we calculate the net input? So we have to loop obviously uh through inputs and weights and we have to multiply them. And so we need to multiply basically the um inputs and weights on the SA me index. And so how do we do that? Well, it's quite simple. So we, we create a for loop and we have the X and WS and I",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=787s",
        "start_time": "787.409"
    },
    {
        "id": "dc319abf",
        "text": "right? So how do we calculate the net input? So we have to loop obviously uh through inputs and weights and we have to multiply them. And so we need to multiply basically the um inputs and weights on the SA me index. And so how do we do that? Well, it's quite simple. So we, we create a for loop and we have the X and WS and I uh we use the zip function over here and we pass in inputs and weights, right? Like this. So if you guys are not familiar with the zip function, like it's a very nice function because it enables you like to unpack two lists and to",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=798s",
        "start_time": "798.599"
    },
    {
        "id": "d4063975",
        "text": "uh through inputs and weights and we have to multiply them. And so we need to multiply basically the um inputs and weights on the SA me index. And so how do we do that? Well, it's quite simple. So we, we create a for loop and we have the X and WS and I uh we use the zip function over here and we pass in inputs and weights, right? Like this. So if you guys are not familiar with the zip function, like it's a very nice function because it enables you like to unpack two lists and to uh just like pass them to like these variables over here, index by index. And so like here, for example, when we go through the loop for the first time, this X is gonna be basically inputs at index zero. And this uh W over here is gonna be weights at the index zero, right? So once we have this, then we'll have the H",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=807s",
        "start_time": "807.859"
    },
    {
        "id": "c5205c93",
        "text": "uh we use the zip function over here and we pass in inputs and weights, right? Like this. So if you guys are not familiar with the zip function, like it's a very nice function because it enables you like to unpack two lists and to uh just like pass them to like these variables over here, index by index. And so like here, for example, when we go through the loop for the first time, this X is gonna be basically inputs at index zero. And this uh W over here is gonna be weights at the index zero, right? So once we have this, then we'll have the H that's going to be, we're going to just like sum, add up the multiplication of X by",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=832s",
        "start_time": "832.489"
    },
    {
        "id": "567cc7e1",
        "text": "uh just like pass them to like these variables over here, index by index. And so like here, for example, when we go through the loop for the first time, this X is gonna be basically inputs at index zero. And this uh W over here is gonna be weights at the index zero, right? So once we have this, then we'll have the H that's going to be, we're going to just like sum, add up the multiplication of X by uh W to H which is our net input. Obviously H is not defined here. So we need to predefined it, declare it over here.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=852s",
        "start_time": "852.549"
    },
    {
        "id": "22476cf7",
        "text": "that's going to be, we're going to just like sum, add up the multiplication of X by uh W to H which is our net input. Obviously H is not defined here. So we need to predefined it, declare it over here. And so H is gonna start from zero and then we're gonna loop through all the inputs and weights and we're gonna multiply them and add them up to the net input. And so basically, now we're done. So the next step, once we have our H which is our net input is to perform the activation itself. So how do we do that? Well, uh it's kind of like really simple because the activation is, in our case",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=880s",
        "start_time": "880.28"
    },
    {
        "id": "09d05880",
        "text": "uh W to H which is our net input. Obviously H is not defined here. So we need to predefined it, declare it over here. And so H is gonna start from zero and then we're gonna loop through all the inputs and weights and we're gonna multiply them and add them up to the net input. And so basically, now we're done. So the next step, once we have our H which is our net input is to perform the activation itself. So how do we do that? Well, uh it's kind of like really simple because the activation is, in our case said that we're gonna use a sigmoid function activation. So we call sigmoid and we pass INH and now we can just return this and this is the output of the neuron itself. And this is like what we're gonna get down here. And then as a final thing, we just like print this output. Now again, sigmoid hasn't been",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=890s",
        "start_time": "890.099"
    },
    {
        "id": "f81ef172",
        "text": "And so H is gonna start from zero and then we're gonna loop through all the inputs and weights and we're gonna multiply them and add them up to the net input. And so basically, now we're done. So the next step, once we have our H which is our net input is to perform the activation itself. So how do we do that? Well, uh it's kind of like really simple because the activation is, in our case said that we're gonna use a sigmoid function activation. So we call sigmoid and we pass INH and now we can just return this and this is the output of the neuron itself. And this is like what we're gonna get down here. And then as a final thing, we just like print this output. Now again, sigmoid hasn't been um declare it defined anywhere. So we need to define this function. And so let's define it over here. So we define sigmoid, it's a function of X.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=901s",
        "start_time": "901.479"
    },
    {
        "id": "de65e632",
        "text": "said that we're gonna use a sigmoid function activation. So we call sigmoid and we pass INH and now we can just return this and this is the output of the neuron itself. And this is like what we're gonna get down here. And then as a final thing, we just like print this output. Now again, sigmoid hasn't been um declare it defined anywhere. So we need to define this function. And so let's define it over here. So we define sigmoid, it's a function of X. And",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=929s",
        "start_time": "929.705"
    },
    {
        "id": "8de6c396",
        "text": "um declare it defined anywhere. So we need to define this function. And so let's define it over here. So we define sigmoid, it's a function of X. And if you",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=953s",
        "start_time": "953.159"
    },
    {
        "id": "4a37f21b",
        "text": "And if you uh guys remember, so this is, we can define this as 1.0 divided by one plus",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=969s",
        "start_time": "969.219"
    },
    {
        "id": "32820e3f",
        "text": "if you uh guys remember, so this is, we can define this as 1.0 divided by one plus the exponential,",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=971s",
        "start_time": "971.2"
    },
    {
        "id": "9ad6a13f",
        "text": "uh guys remember, so this is, we can define this as 1.0 divided by one plus the exponential, which is math X of minus X over here.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=972s",
        "start_time": "972.84"
    },
    {
        "id": "1f4b8f12",
        "text": "the exponential, which is math X of minus X over here. Now we d we haven't defined this. Uh we haven't included uh this math module. So we need to include it because otherwise we're gonna get an error and so we're gonna import math. Uh Now the error",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=988s",
        "start_time": "988.2"
    },
    {
        "id": "e623692c",
        "text": "which is math X of minus X over here. Now we d we haven't defined this. Uh we haven't included uh this math module. So we need to include it because otherwise we're gonna get an error and so we're gonna import math. Uh Now the error is gone and so here we can return. Why? Perfect. So now we have all of the different elements in place. And so now when we run the script, we should be able to get an output and then print the output if you guys remember by using these inputs. And these,",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=990s",
        "start_time": "990.4"
    },
    {
        "id": "ac388761",
        "text": "Now we d we haven't defined this. Uh we haven't included uh this math module. So we need to include it because otherwise we're gonna get an error and so we're gonna import math. Uh Now the error is gone and so here we can return. Why? Perfect. So now we have all of the different elements in place. And so now when we run the script, we should be able to get an output and then print the output if you guys remember by using these inputs. And these, we, we got, when we run the, the math there, we got an output for the neuron of 0.61 something like that. So let's see if we are lucky and we're going to get like the same value. And as you see here, we do have the very same value. So uh the results of this",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=1000s",
        "start_time": "1000.539"
    },
    {
        "id": "1b8a94db",
        "text": "is gone and so here we can return. Why? Perfect. So now we have all of the different elements in place. And so now when we run the script, we should be able to get an output and then print the output if you guys remember by using these inputs. And these, we, we got, when we run the, the math there, we got an output for the neuron of 0.61 something like that. So let's see if we are lucky and we're going to get like the same value. And as you see here, we do have the very same value. So uh the results of this uh computation is 0.61 and that's correct. So here you have it, it's your first neuron implemented from scratch in Python.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=1016s",
        "start_time": "1016.63"
    },
    {
        "id": "3a434b12",
        "text": "we, we got, when we run the, the math there, we got an output for the neuron of 0.61 something like that. So let's see if we are lucky and we're going to get like the same value. And as you see here, we do have the very same value. So uh the results of this uh computation is 0.61 and that's correct. So here you have it, it's your first neuron implemented from scratch in Python. Cool. So this was it for implementing an artificial neuron in Python. So before living, I want to give you some takeaway points. So as we've seen, artificial neurons are loosely inspired to biologically",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=1037s",
        "start_time": "1037.479"
    },
    {
        "id": "5229d76e",
        "text": "uh computation is 0.61 and that's correct. So here you have it, it's your first neuron implemented from scratch in Python. Cool. So this was it for implementing an artificial neuron in Python. So before living, I want to give you some takeaway points. So as we've seen, artificial neurons are loosely inspired to biologically neurons and neurons are computational unions and precisely what they do is they receive certain inputs and they modulate that input and using an activation function, they arrive at an output.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=1060s",
        "start_time": "1060.88"
    },
    {
        "id": "c2964db9",
        "text": "Cool. So this was it for implementing an artificial neuron in Python. So before living, I want to give you some takeaway points. So as we've seen, artificial neurons are loosely inspired to biologically neurons and neurons are computational unions and precisely what they do is they receive certain inputs and they modulate that input and using an activation function, they arrive at an output. Cool. So this was it for uh artificial neurons. So now we have like an understanding of how uh like this uh units of computation work. So what's next Well, before getting into neural networks themselves, we have to understand a little bit more of math and specifically we are going to be looking at matrix operations and vector operations. So",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=1073s",
        "start_time": "1073.479"
    },
    {
        "id": "553f5d41",
        "text": "neurons and neurons are computational unions and precisely what they do is they receive certain inputs and they modulate that input and using an activation function, they arrive at an output. Cool. So this was it for uh artificial neurons. So now we have like an understanding of how uh like this uh units of computation work. So what's next Well, before getting into neural networks themselves, we have to understand a little bit more of math and specifically we are going to be looking at matrix operations and vector operations. So if you've enjoyed the video, please subscribe to the channel. Just leave a like and I guess I'll see you the next time. Cheers.",
        "video": "3- Implementing an artificial neuron from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "qxIaW-WvLDU",
        "youtube_link": "https://www.youtube.com/watch?v=qxIaW-WvLDU&t=1090s",
        "start_time": "1090.079"
    },
    {
        "id": "c8cb131c",
        "text": "Hi, everybody and welcome to a new video in the Deep learning for Rodeo with Python Series. This time, we're gonna build a neural network using tensorflow um tensorflow. Is this amazing deep learning library that's like used very much in the industry and also in academia and it has its own way of building and processing information and neural networks. So obviously, we're gonna follow that. So what should we do? Well, first thing that we want to do is to build a model. So we need to build the architecture of our network. Then once we have the architecture, we want to compile the model, which basically means passing information like the error function that we want to use or like the optimizer that we want to use uh in our training uh process. Then we indeed want to train the model",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "d6f6ef2a",
        "text": "very much in the industry and also in academia and it has its own way of building and processing information and neural networks. So obviously, we're gonna follow that. So what should we do? Well, first thing that we want to do is to build a model. So we need to build the architecture of our network. Then once we have the architecture, we want to compile the model, which basically means passing information like the error function that we want to use or like the optimizer that we want to use uh in our training uh process. Then we indeed want to train the model and then we want to evaluate uh the model to see how well it's doing. And finally, uh we want to make some predictions, right? But before we're gonna start building the model, we need to have a data set, right?",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=14s",
        "start_time": "14.939"
    },
    {
        "id": "68d57354",
        "text": "a model. So we need to build the architecture of our network. Then once we have the architecture, we want to compile the model, which basically means passing information like the error function that we want to use or like the optimizer that we want to use uh in our training uh process. Then we indeed want to train the model and then we want to evaluate uh the model to see how well it's doing. And finally, uh we want to make some predictions, right? But before we're gonna start building the model, we need to have a data set, right? And so for the data set, I'm gonna use like something similar. Well, actually the same artificial data set that we built in the previous video when we built uh a neural network completely from scratch in Python. So we want to have a data set uh that will enable us to train our model to perform an incredibly difficult task, the arithmetic sum. And uh so",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=30s",
        "start_time": "30.395"
    },
    {
        "id": "2630d919",
        "text": "and then we want to evaluate uh the model to see how well it's doing. And finally, uh we want to make some predictions, right? But before we're gonna start building the model, we need to have a data set, right? And so for the data set, I'm gonna use like something similar. Well, actually the same artificial data set that we built in the previous video when we built uh a neural network completely from scratch in Python. So we want to have a data set uh that will enable us to train our model to perform an incredibly difficult task, the arithmetic sum. And uh so uh for that, we're gonna need like uh to build like the inputs and the outputs and we want the inputs to be like this. So we want them to be uh NPI arrays and uh each sample is gonna be given uh by a couple of",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=53s",
        "start_time": "53.369"
    },
    {
        "id": "9b9d952b",
        "text": "And so for the data set, I'm gonna use like something similar. Well, actually the same artificial data set that we built in the previous video when we built uh a neural network completely from scratch in Python. So we want to have a data set uh that will enable us to train our model to perform an incredibly difficult task, the arithmetic sum. And uh so uh for that, we're gonna need like uh to build like the inputs and the outputs and we want the inputs to be like this. So we want them to be uh NPI arrays and uh each sample is gonna be given uh by a couple of values like this. And so these are gonna be like our inputs for a sample. And we want to basically like adapt 0.1 to 0.2 and get the results as 0.3. So",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=69s",
        "start_time": "69.879"
    },
    {
        "id": "1720af79",
        "text": "uh for that, we're gonna need like uh to build like the inputs and the outputs and we want the inputs to be like this. So we want them to be uh NPI arrays and uh each sample is gonna be given uh by a couple of values like this. And so these are gonna be like our inputs for a sample. And we want to basically like adapt 0.1 to 0.2 and get the results as 0.3. So basically, like our array is going to look something like this",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=98s",
        "start_time": "98.69"
    },
    {
        "id": "6c090425",
        "text": "values like this. And so these are gonna be like our inputs for a sample. And we want to basically like adapt 0.1 to 0.2 and get the results as 0.3. So basically, like our array is going to look something like this and our output array uh as we said is gonna be basically like the sum. So uh at index zero, like here, for example, if we add up 0.1 and 0.2 we're gonna have no 0.3 and here we want something like no 0.4 right?",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=121s",
        "start_time": "121.55"
    },
    {
        "id": "d576a24d",
        "text": "basically, like our array is going to look something like this and our output array uh as we said is gonna be basically like the sum. So uh at index zero, like here, for example, if we add up 0.1 and 0.2 we're gonna have no 0.3 and here we want something like no 0.4 right? So how do we build such a data set? Well, uh I've already done that like in the previous video, so I'm not gonna uh like cover that again, but uh we're basically gonna use like this couple of lines of code here. Cool uh obviously like to have this working, we need to import a nun pi. So we'll import nun pi as NP.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=133s",
        "start_time": "133.759"
    },
    {
        "id": "c2c32472",
        "text": "and our output array uh as we said is gonna be basically like the sum. So uh at index zero, like here, for example, if we add up 0.1 and 0.2 we're gonna have no 0.3 and here we want something like no 0.4 right? So how do we build such a data set? Well, uh I've already done that like in the previous video, so I'm not gonna uh like cover that again, but uh we're basically gonna use like this couple of lines of code here. Cool uh obviously like to have this working, we need to import a nun pi. So we'll import nun pi as NP. And uh we also need this random function uh which comes from the random module. So we can do from random import uh random. And you guys by now should know that this random function just like samples of values between zero and one afloat. Cool.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=139s",
        "start_time": "139.27"
    },
    {
        "id": "58b8bfaa",
        "text": "So how do we build such a data set? Well, uh I've already done that like in the previous video, so I'm not gonna uh like cover that again, but uh we're basically gonna use like this couple of lines of code here. Cool uh obviously like to have this working, we need to import a nun pi. So we'll import nun pi as NP. And uh we also need this random function uh which comes from the random module. So we can do from random import uh random. And you guys by now should know that this random function just like samples of values between zero and one afloat. Cool. OK. But now we have all our inputs, our exits and our uh outputs or outcomes are why? Why? Uh right. But it turns out that when you are training um a model in machine learning and obviously also deep learning,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=157s",
        "start_time": "157.41"
    },
    {
        "id": "ba88af05",
        "text": "And uh we also need this random function uh which comes from the random module. So we can do from random import uh random. And you guys by now should know that this random function just like samples of values between zero and one afloat. Cool. OK. But now we have all our inputs, our exits and our uh outputs or outcomes are why? Why? Uh right. But it turns out that when you are training um a model in machine learning and obviously also deep learning, uh what you want to do is to split your data set into a training set and the test set. So you're gonna train your model on the training set. And then once you're done with that, you're gonna use the test set to evaluate how well the model is doing on some data that uh the neural network has never seen before. And why do we do that? Well, we do that because",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=181s",
        "start_time": "181.77"
    },
    {
        "id": "a12e781f",
        "text": "OK. But now we have all our inputs, our exits and our uh outputs or outcomes are why? Why? Uh right. But it turns out that when you are training um a model in machine learning and obviously also deep learning, uh what you want to do is to split your data set into a training set and the test set. So you're gonna train your model on the training set. And then once you're done with that, you're gonna use the test set to evaluate how well the model is doing on some data that uh the neural network has never seen before. And why do we do that? Well, we do that because uh we want to see whether the neural network has been able to generalize the patterns that it has learned by uh training on a data set on the training set. And so in order to do that, we need to evaluate the um model on some data that the network has never seen before. So all we need to do here is to split our",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=200s",
        "start_time": "200.649"
    },
    {
        "id": "e620b7cc",
        "text": "uh what you want to do is to split your data set into a training set and the test set. So you're gonna train your model on the training set. And then once you're done with that, you're gonna use the test set to evaluate how well the model is doing on some data that uh the neural network has never seen before. And why do we do that? Well, we do that because uh we want to see whether the neural network has been able to generalize the patterns that it has learned by uh training on a data set on the training set. And so in order to do that, we need to evaluate the um model on some data that the network has never seen before. So all we need to do here is to split our data set into some training set. And uh a test set so we could do this like from scratch. But there's a very handy function that does this like for us. So I don't want to re invent the wheel here. And that function comes from the Psych Learn library, which is a fundamental library, super important library for traditional machine learning.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=219s",
        "start_time": "219.089"
    },
    {
        "id": "539bf660",
        "text": "uh we want to see whether the neural network has been able to generalize the patterns that it has learned by uh training on a data set on the training set. And so in order to do that, we need to evaluate the um model on some data that the network has never seen before. So all we need to do here is to split our data set into some training set. And uh a test set so we could do this like from scratch. But there's a very handy function that does this like for us. So I don't want to re invent the wheel here. And that function comes from the Psych Learn library, which is a fundamental library, super important library for traditional machine learning. So let me import that function. So I I'll do from psychic learn dot model selection imports and that's called train test split. So if you don't have psychic learn installed on your machine, just pip install it. It's gonna take you probably 30 seconds or less.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=243s",
        "start_time": "243.19"
    },
    {
        "id": "9c1ec136",
        "text": "data set into some training set. And uh a test set so we could do this like from scratch. But there's a very handy function that does this like for us. So I don't want to re invent the wheel here. And that function comes from the Psych Learn library, which is a fundamental library, super important library for traditional machine learning. So let me import that function. So I I'll do from psychic learn dot model selection imports and that's called train test split. So if you don't have psychic learn installed on your machine, just pip install it. It's gonna take you probably 30 seconds or less. Good. So here what we want to do is to have an X train,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=267s",
        "start_time": "267.29"
    },
    {
        "id": "bdbccafc",
        "text": "So let me import that function. So I I'll do from psychic learn dot model selection imports and that's called train test split. So if you don't have psychic learn installed on your machine, just pip install it. It's gonna take you probably 30 seconds or less. Good. So here what we want to do is to have an X train, an X tests, a wide train",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=291s",
        "start_time": "291.589"
    },
    {
        "id": "196b206e",
        "text": "Good. So here what we want to do is to have an X train, an X tests, a wide train and A Y test. And here we're gonna call the train test split function and as arguments we're gonna pass in our inputs and outputs and we are gonna specify the test size",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=315s",
        "start_time": "315.049"
    },
    {
        "id": "62429f12",
        "text": "an X tests, a wide train and A Y test. And here we're gonna call the train test split function and as arguments we're gonna pass in our inputs and outputs and we are gonna specify the test size and say we, we, we pass in 0.3. So basically what we are seeing here is that uh our training set is gonna be uh 30% of our whole data set of all the samples that we have in our inputs and outputs. Right? Cool. And so with this function, basically, we are dividing up our data set into a train set and a uh test set and we're gonna have both um",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=324s",
        "start_time": "324.809"
    },
    {
        "id": "dad72c73",
        "text": "and A Y test. And here we're gonna call the train test split function and as arguments we're gonna pass in our inputs and outputs and we are gonna specify the test size and say we, we, we pass in 0.3. So basically what we are seeing here is that uh our training set is gonna be uh 30% of our whole data set of all the samples that we have in our inputs and outputs. Right? Cool. And so with this function, basically, we are dividing up our data set into a train set and a uh test set and we're gonna have both um XS and Ys, right? And we can easily access them, right? But I don't like this very much because I want it to be a little bit like more modular. And in order to do that, I can define a function which I'll call, generate um",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=330s",
        "start_time": "330.179"
    },
    {
        "id": "be5a460b",
        "text": "and say we, we, we pass in 0.3. So basically what we are seeing here is that uh our training set is gonna be uh 30% of our whole data set of all the samples that we have in our inputs and outputs. Right? Cool. And so with this function, basically, we are dividing up our data set into a train set and a uh test set and we're gonna have both um XS and Ys, right? And we can easily access them, right? But I don't like this very much because I want it to be a little bit like more modular. And in order to do that, I can define a function which I'll call, generate um data set",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=346s",
        "start_time": "346.109"
    },
    {
        "id": "7d2dce93",
        "text": "XS and Ys, right? And we can easily access them, right? But I don't like this very much because I want it to be a little bit like more modular. And in order to do that, I can define a function which I'll call, generate um data set and I'll pass in a couple of arguments. So one being number of samples and the other one being test size.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=375s",
        "start_time": "375.54"
    },
    {
        "id": "63d755c7",
        "text": "data set and I'll pass in a couple of arguments. So one being number of samples and the other one being test size. So",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=393s",
        "start_time": "393.26"
    },
    {
        "id": "c971c44b",
        "text": "and I'll pass in a couple of arguments. So one being number of samples and the other one being test size. So here we are gonna, this function is gonna return this nice for a ray. So Xtra X test and Y trainin white test good. And so the number of samples here, we specify how many samples we want in our whole data set. And so we should like pass it over here and the test size, we are gonna pass it over here. Good. So now let's see if this is working.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=395s",
        "start_time": "395.459"
    },
    {
        "id": "ca60dc0f",
        "text": "So here we are gonna, this function is gonna return this nice for a ray. So Xtra X test and Y trainin white test good. And so the number of samples here, we specify how many samples we want in our whole data set. And so we should like pass it over here and the test size, we are gonna pass it over here. Good. So now let's see if this is working. And so we'll just do a if name is equal to main",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=408s",
        "start_time": "408.279"
    },
    {
        "id": "6a846bc8",
        "text": "here we are gonna, this function is gonna return this nice for a ray. So Xtra X test and Y trainin white test good. And so the number of samples here, we specify how many samples we want in our whole data set. And so we should like pass it over here and the test size, we are gonna pass it over here. Good. So now let's see if this is working. And so we'll just do a if name is equal to main and then we'll move in here and we'll say generate data sets. And uh we'll say, yeah, let's just have 10 samples. And let's say that we only want uh two of these samples as part of the test set. Cool. And so here this function is giving us back all of these nice guys over here. And so now let's print",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=410s",
        "start_time": "410.7"
    },
    {
        "id": "59a2ea10",
        "text": "And so we'll just do a if name is equal to main and then we'll move in here and we'll say generate data sets. And uh we'll say, yeah, let's just have 10 samples. And let's say that we only want uh two of these samples as part of the test set. Cool. And so here this function is giving us back all of these nice guys over here. And so now let's print uh a few of this just like to see whether like this is working cool. So we can say the X test. So basically the uh inputs for the test set,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=439s",
        "start_time": "439.989"
    },
    {
        "id": "6175e879",
        "text": "and then we'll move in here and we'll say generate data sets. And uh we'll say, yeah, let's just have 10 samples. And let's say that we only want uh two of these samples as part of the test set. Cool. And so here this function is giving us back all of these nice guys over here. And so now let's print uh a few of this just like to see whether like this is working cool. So we can say the X test. So basically the uh inputs for the test set, this is equal to, I'll just do like a new line there and I'll pass in uh X test nice. So here I want to see Y test and so I'll pass in Y test and this should work. So let's run this and see if it works. Cool.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=449s",
        "start_time": "449.69"
    },
    {
        "id": "fea29695",
        "text": "uh a few of this just like to see whether like this is working cool. So we can say the X test. So basically the uh inputs for the test set, this is equal to, I'll just do like a new line there and I'll pass in uh X test nice. So here I want to see Y test and so I'll pass in Y test and this should work. So let's run this and see if it works. Cool. Uh Yep. And it's working as expected. So over here, so for DX test, we have a two by two matrix and each sample is given by two values which are the values that we want to adopt. And for the Y test, we have the outcomes and this is a two by one matrix as we expect. And uh this guy, for example, at index zero is uh given by the",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=476s",
        "start_time": "476.559"
    },
    {
        "id": "bfffdcc7",
        "text": "this is equal to, I'll just do like a new line there and I'll pass in uh X test nice. So here I want to see Y test and so I'll pass in Y test and this should work. So let's run this and see if it works. Cool. Uh Yep. And it's working as expected. So over here, so for DX test, we have a two by two matrix and each sample is given by two values which are the values that we want to adopt. And for the Y test, we have the outcomes and this is a two by one matrix as we expect. And uh this guy, for example, at index zero is uh given by the arithmetic sum of these two guys here. So yeah, this is working great.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=488s",
        "start_time": "488.89"
    },
    {
        "id": "b1ae9e25",
        "text": "Uh Yep. And it's working as expected. So over here, so for DX test, we have a two by two matrix and each sample is given by two values which are the values that we want to adopt. And for the Y test, we have the outcomes and this is a two by one matrix as we expect. And uh this guy, for example, at index zero is uh given by the arithmetic sum of these two guys here. So yeah, this is working great. OK. So yeah, and we want to comment out like this guys here. And now we are",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=512s",
        "start_time": "512.59"
    },
    {
        "id": "b8836d80",
        "text": "arithmetic sum of these two guys here. So yeah, this is working great. OK. So yeah, and we want to comment out like this guys here. And now we are at the point where we start working with tensorflow. So first of all, what we need to do is just import tensorflow. So we'll do import tensorflow. STF. Now",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=540s",
        "start_time": "540.76"
    },
    {
        "id": "305a6758",
        "text": "OK. So yeah, and we want to comment out like this guys here. And now we are at the point where we start working with tensorflow. So first of all, what we need to do is just import tensorflow. So we'll do import tensorflow. STF. Now uh tensorflow comes in two flavors. So one is for CPU and one is for GP U. In this video, I'm using the one for CPU because like the stuff that I'll do like it's quite, I mean, it's quite cheap computationally. So I don't need to like use the, the big guns. Uh But if you, if you don't have it installed, you want to install tensorflow by doing pip install tensorflow. That way you should uh be able to install uh Yeah, this library",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=547s",
        "start_time": "547.03"
    },
    {
        "id": "bdd0e413",
        "text": "at the point where we start working with tensorflow. So first of all, what we need to do is just import tensorflow. So we'll do import tensorflow. STF. Now uh tensorflow comes in two flavors. So one is for CPU and one is for GP U. In this video, I'm using the one for CPU because like the stuff that I'll do like it's quite, I mean, it's quite cheap computationally. So I don't need to like use the, the big guns. Uh But if you, if you don't have it installed, you want to install tensorflow by doing pip install tensorflow. That way you should uh be able to install uh Yeah, this library uh If you want to install like this CPU version that's like way easier. The GP U one is a little bit trickier, but there's a lot of resources online. Um So uh you can just like uh go like Google them and you'll find that out good.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=556s",
        "start_time": "556.59"
    },
    {
        "id": "4b911139",
        "text": "uh tensorflow comes in two flavors. So one is for CPU and one is for GP U. In this video, I'm using the one for CPU because like the stuff that I'll do like it's quite, I mean, it's quite cheap computationally. So I don't need to like use the, the big guns. Uh But if you, if you don't have it installed, you want to install tensorflow by doing pip install tensorflow. That way you should uh be able to install uh Yeah, this library uh If you want to install like this CPU version that's like way easier. The GP U one is a little bit trickier, but there's a lot of resources online. Um So uh you can just like uh go like Google them and you'll find that out good. OK. So let's build the model here. And so we want uh a model which is basically our neural network. And uh for this, we'll do a tensorflow dot Caras dot sequential,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=573s",
        "start_time": "573.34"
    },
    {
        "id": "6ae43e41",
        "text": "uh If you want to install like this CPU version that's like way easier. The GP U one is a little bit trickier, but there's a lot of resources online. Um So uh you can just like uh go like Google them and you'll find that out good. OK. So let's build the model here. And so we want uh a model which is basically our neural network. And uh for this, we'll do a tensorflow dot Caras dot sequential, right. So Kas is a high level library that sits on top of tensorflow. It uses tensorflow but makes tensorflow code like super easy like to build. And so as you'll see, we can build very complex neural networks with just like a few lines of code.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=603s",
        "start_time": "603.58"
    },
    {
        "id": "f0c3e64e",
        "text": "OK. So let's build the model here. And so we want uh a model which is basically our neural network. And uh for this, we'll do a tensorflow dot Caras dot sequential, right. So Kas is a high level library that sits on top of tensorflow. It uses tensorflow but makes tensorflow code like super easy like to build. And so as you'll see, we can build very complex neural networks with just like a few lines of code. And now we are using sequential because because we want a network that's a sequential network. So it has an architecture where basically the input signal is moving from left to right. And specifically the model that we want is the same that we used in our previous video where we built uh the same model, the same neural network from scratch. Now, if you want to know like how that's done, just go back and watch uh our pre uh my previous video,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=619s",
        "start_time": "619.359"
    },
    {
        "id": "f3cc61b0",
        "text": "right. So Kas is a high level library that sits on top of tensorflow. It uses tensorflow but makes tensorflow code like super easy like to build. And so as you'll see, we can build very complex neural networks with just like a few lines of code. And now we are using sequential because because we want a network that's a sequential network. So it has an architecture where basically the input signal is moving from left to right. And specifically the model that we want is the same that we used in our previous video where we built uh the same model, the same neural network from scratch. Now, if you want to know like how that's done, just go back and watch uh our pre uh my previous video, um you should have done this by now but you haven't just go and check that out. But again, what we want is a model where we have an input um layer where we have two neurons, then we want a hidden layer with five neurons and finally an output layer with one neuron. So let's build this nice uh network to do that. We do TF dot Kas dot layers.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=636s",
        "start_time": "636.419"
    },
    {
        "id": "b8efedea",
        "text": "And now we are using sequential because because we want a network that's a sequential network. So it has an architecture where basically the input signal is moving from left to right. And specifically the model that we want is the same that we used in our previous video where we built uh the same model, the same neural network from scratch. Now, if you want to know like how that's done, just go back and watch uh our pre uh my previous video, um you should have done this by now but you haven't just go and check that out. But again, what we want is a model where we have an input um layer where we have two neurons, then we want a hidden layer with five neurons and finally an output layer with one neuron. So let's build this nice uh network to do that. We do TF dot Kas dot layers. And the type of layer that we want is a dense layer and a dense layer is a fully connected layer which basically connects all the neurons from the previous layer with the current layer nice. And so the uh the dense layer",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=656s",
        "start_time": "656.679"
    },
    {
        "id": "e3f54dfc",
        "text": "um you should have done this by now but you haven't just go and check that out. But again, what we want is a model where we have an input um layer where we have two neurons, then we want a hidden layer with five neurons and finally an output layer with one neuron. So let's build this nice uh network to do that. We do TF dot Kas dot layers. And the type of layer that we want is a dense layer and a dense layer is a fully connected layer which basically connects all the neurons from the previous layer with the current layer nice. And so the uh the dense layer uh has like a few arguments it accepts. So one which is fundamental is the number of neurons that we want. So we want five neurons and then given this is the first hidden layer, uh we want to pass the input dimension here. And so here we are specifying how many uh neurons we want in the input layer. And this is equal to two in our case.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=685s",
        "start_time": "685.38"
    },
    {
        "id": "daf79251",
        "text": "And the type of layer that we want is a dense layer and a dense layer is a fully connected layer which basically connects all the neurons from the previous layer with the current layer nice. And so the uh the dense layer uh has like a few arguments it accepts. So one which is fundamental is the number of neurons that we want. So we want five neurons and then given this is the first hidden layer, uh we want to pass the input dimension here. And so here we are specifying how many uh neurons we want in the input layer. And this is equal to two in our case. And finally, we need to specify which activation function we want to use for this layer. And we're gonna use not surprisingly the famous sigmoid function that we've used so far good. So now let's build the output layer. So the output layer is again a dense layer. But this time it only has one,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=714s",
        "start_time": "714.44"
    },
    {
        "id": "ea36ba2b",
        "text": "uh has like a few arguments it accepts. So one which is fundamental is the number of neurons that we want. So we want five neurons and then given this is the first hidden layer, uh we want to pass the input dimension here. And so here we are specifying how many uh neurons we want in the input layer. And this is equal to two in our case. And finally, we need to specify which activation function we want to use for this layer. And we're gonna use not surprisingly the famous sigmoid function that we've used so far good. So now let's build the output layer. So the output layer is again a dense layer. But this time it only has one, a neuron. We don't need the input dimension here, but we definitely need the activation which we want to be a sigmoid function good. So with just these few lines of code, we've built our model",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=730s",
        "start_time": "730.45"
    },
    {
        "id": "84c7214e",
        "text": "And finally, we need to specify which activation function we want to use for this layer. And we're gonna use not surprisingly the famous sigmoid function that we've used so far good. So now let's build the output layer. So the output layer is again a dense layer. But this time it only has one, a neuron. We don't need the input dimension here, but we definitely need the activation which we want to be a sigmoid function good. So with just these few lines of code, we've built our model cool. The next thing that we want to do is to compile the model. So we'll do model dot compile and here, we need to pass in uh some important information. So first of all, we want to specify the optimizer, we're gonna use stochastic gradient descent uh that we've seen in the previous video. Uh But",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=754s",
        "start_time": "754.309"
    },
    {
        "id": "9d206003",
        "text": "a neuron. We don't need the input dimension here, but we definitely need the activation which we want to be a sigmoid function good. So with just these few lines of code, we've built our model cool. The next thing that we want to do is to compile the model. So we'll do model dot compile and here, we need to pass in uh some important information. So first of all, we want to specify the optimizer, we're gonna use stochastic gradient descent uh that we've seen in the previous video. Uh But for now, like let's leave this like uh blank, I'll just like go back in a second to this. And the second thing that we want to pass is the loss function that so far I've called the error function. So loss and error like are the same thing cool. And for the loss, we're gonna use MS C which is the min squared error. Again, if you want to know more about all of these things, I've covered them both like theoretical, theoretically and uh an implementation",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=776s",
        "start_time": "776.2"
    },
    {
        "id": "c6fff303",
        "text": "cool. The next thing that we want to do is to compile the model. So we'll do model dot compile and here, we need to pass in uh some important information. So first of all, we want to specify the optimizer, we're gonna use stochastic gradient descent uh that we've seen in the previous video. Uh But for now, like let's leave this like uh blank, I'll just like go back in a second to this. And the second thing that we want to pass is the loss function that so far I've called the error function. So loss and error like are the same thing cool. And for the loss, we're gonna use MS C which is the min squared error. Again, if you want to know more about all of these things, I've covered them both like theoretical, theoretically and uh an implementation uh at any implementation level in the previous videos uh cool. So we were saying uh the, we were talking about the optimizer",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=791s",
        "start_time": "791.919"
    },
    {
        "id": "c2751bad",
        "text": "for now, like let's leave this like uh blank, I'll just like go back in a second to this. And the second thing that we want to pass is the loss function that so far I've called the error function. So loss and error like are the same thing cool. And for the loss, we're gonna use MS C which is the min squared error. Again, if you want to know more about all of these things, I've covered them both like theoretical, theoretically and uh an implementation uh at any implementation level in the previous videos uh cool. So we were saying uh the, we were talking about the optimizer cool. So the optimizer is gonna be equal to TF dot K dot optimizer. And here we want SGD, which is our nice stochastic gradient descent uh optimizer. And here we can pass in the learning rate and let's say we'll give it a no 0.1 good.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=817s",
        "start_time": "817.349"
    },
    {
        "id": "350569e1",
        "text": "uh at any implementation level in the previous videos uh cool. So we were saying uh the, we were talking about the optimizer cool. So the optimizer is gonna be equal to TF dot K dot optimizer. And here we want SGD, which is our nice stochastic gradient descent uh optimizer. And here we can pass in the learning rate and let's say we'll give it a no 0.1 good. So now we've compiled our model. So the next step is to train the model. So, and how do we do that? Well, it is as easy as it can be really. So we'll do a model dot",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=846s",
        "start_time": "846.89"
    },
    {
        "id": "c937e932",
        "text": "cool. So the optimizer is gonna be equal to TF dot K dot optimizer. And here we want SGD, which is our nice stochastic gradient descent uh optimizer. And here we can pass in the learning rate and let's say we'll give it a no 0.1 good. So now we've compiled our model. So the next step is to train the model. So, and how do we do that? Well, it is as easy as it can be really. So we'll do a model dot fit",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=857s",
        "start_time": "857.7"
    },
    {
        "id": "97562807",
        "text": "So now we've compiled our model. So the next step is to train the model. So, and how do we do that? Well, it is as easy as it can be really. So we'll do a model dot fit and here obviously, we want to pass in the, the training set and so we'll pass in",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=882s",
        "start_time": "882.0"
    },
    {
        "id": "3e4ba6fe",
        "text": "fit and here obviously, we want to pass in the, the training set and so we'll pass in so X underscore train and we'll pass in Y underscore train. So we are passing in the inputs and the outputs of the uh train split.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=899s",
        "start_time": "899.5"
    },
    {
        "id": "46f660fc",
        "text": "and here obviously, we want to pass in the, the training set and so we'll pass in so X underscore train and we'll pass in Y underscore train. So we are passing in the inputs and the outputs of the uh train split. Uh and then we need to pass in the number of apex. So let's say that we want uh yeah, 100 for example, cool. And so just doing this, we train our model and now we should be able like to get uh information when we do this and let's run like all of this code. But before doing that, let's put like a viable number here. So",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=901s",
        "start_time": "901.2"
    },
    {
        "id": "d4c82117",
        "text": "so X underscore train and we'll pass in Y underscore train. So we are passing in the inputs and the outputs of the uh train split. Uh and then we need to pass in the number of apex. So let's say that we want uh yeah, 100 for example, cool. And so just doing this, we train our model and now we should be able like to get uh information when we do this and let's run like all of this code. But before doing that, let's put like a viable number here. So for the data set here, let's say we want a data set with 5000 samples and we'll do, yeah, let's say one",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=908s",
        "start_time": "908.32"
    },
    {
        "id": "4a5f58fa",
        "text": "Uh and then we need to pass in the number of apex. So let's say that we want uh yeah, 100 for example, cool. And so just doing this, we train our model and now we should be able like to get uh information when we do this and let's run like all of this code. But before doing that, let's put like a viable number here. So for the data set here, let's say we want a data set with 5000 samples and we'll do, yeah, let's say one like 30% of that is going to make up the the test set. Cool. So now let's run this and see how it goes.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=921s",
        "start_time": "921.75"
    },
    {
        "id": "d46b9212",
        "text": "for the data set here, let's say we want a data set with 5000 samples and we'll do, yeah, let's say one like 30% of that is going to make up the the test set. Cool. So now let's run this and see how it goes. Uh Yeah, we have an error here. Uh Oh Yeah. Right. Yes, I just forgot to come over there. That's good. So let's see if it works now.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=951s",
        "start_time": "951.369"
    },
    {
        "id": "7c5eb0b5",
        "text": "like 30% of that is going to make up the the test set. Cool. So now let's run this and see how it goes. Uh Yeah, we have an error here. Uh Oh Yeah. Right. Yes, I just forgot to come over there. That's good. So let's see if it works now. Uh Yeah, it's running, we have another error over here. Oh Yeah, I know what's happening here. Cool. Yeah,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=960s",
        "start_time": "960.21"
    },
    {
        "id": "0aed0c68",
        "text": "Uh Yeah, we have an error here. Uh Oh Yeah. Right. Yes, I just forgot to come over there. That's good. So let's see if it works now. Uh Yeah, it's running, we have another error over here. Oh Yeah, I know what's happening here. Cool. Yeah, cool. So I've never passed the optimizer here. So let's take this optimizer here and pass it to compile over there. Cool. So now if all is good, we should be able to run this smoothly. Yeah, finally it's working and as you can see it's training and it's giving us like uh a report at the end of each epoch cool. And here you'll see like the, the loss function.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=973s",
        "start_time": "973.369"
    },
    {
        "id": "6b2d7fd3",
        "text": "Uh Yeah, it's running, we have another error over here. Oh Yeah, I know what's happening here. Cool. Yeah, cool. So I've never passed the optimizer here. So let's take this optimizer here and pass it to compile over there. Cool. So now if all is good, we should be able to run this smoothly. Yeah, finally it's working and as you can see it's training and it's giving us like uh a report at the end of each epoch cool. And here you'll see like the, the loss function. Uh um well, the loss, the error. And as you can see starting from here, it's",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=984s",
        "start_time": "984.71"
    },
    {
        "id": "adb2c14b",
        "text": "cool. So I've never passed the optimizer here. So let's take this optimizer here and pass it to compile over there. Cool. So now if all is good, we should be able to run this smoothly. Yeah, finally it's working and as you can see it's training and it's giving us like uh a report at the end of each epoch cool. And here you'll see like the, the loss function. Uh um well, the loss, the error. And as you can see starting from here, it's going down epoch after epoch, which is good until like we get that these results for the, for the error after 100 epochs, which is nice. Uh But you should keep in mind that all of this has been uh done also like the error has been calculated on the training set. But this is not really like what we want to do like for evaluation.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=993s",
        "start_time": "993.809"
    },
    {
        "id": "01de7b1e",
        "text": "Uh um well, the loss, the error. And as you can see starting from here, it's going down epoch after epoch, which is good until like we get that these results for the, for the error after 100 epochs, which is nice. Uh But you should keep in mind that all of this has been uh done also like the error has been calculated on the training set. But this is not really like what we want to do like for evaluation. And this is where like the uh test set comes in when we want to evaluate uh the model. So we uh let's evaluate the model. So let's just like write this and say uh model evaluation uh con here and let's just like give it a new line. Cool. So how do we do? We evaluate this? Well,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1023s",
        "start_time": "1023.909"
    },
    {
        "id": "7a8524c5",
        "text": "going down epoch after epoch, which is good until like we get that these results for the, for the error after 100 epochs, which is nice. Uh But you should keep in mind that all of this has been uh done also like the error has been calculated on the training set. But this is not really like what we want to do like for evaluation. And this is where like the uh test set comes in when we want to evaluate uh the model. So we uh let's evaluate the model. So let's just like write this and say uh model evaluation uh con here and let's just like give it a new line. Cool. So how do we do? We evaluate this? Well, this is like quite straightforward. We just do model dot evaluate and then we pass in this time, the X test and the Y test. So here we are basically evaluating the model on the test set on the test set and we are passing both the inputs and the outputs. Now, if we want to see a report here, we should do a verbose equal to one.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1032s",
        "start_time": "1032.3"
    },
    {
        "id": "e6044a1b",
        "text": "And this is where like the uh test set comes in when we want to evaluate uh the model. So we uh let's evaluate the model. So let's just like write this and say uh model evaluation uh con here and let's just like give it a new line. Cool. So how do we do? We evaluate this? Well, this is like quite straightforward. We just do model dot evaluate and then we pass in this time, the X test and the Y test. So here we are basically evaluating the model on the test set on the test set and we are passing both the inputs and the outputs. Now, if we want to see a report here, we should do a verbose equal to one. Cool. So now let's rerun uh the scripts and see whether like this is working correctly. Cool. Yeah. Again, we are getting the uh training over here and once this is done, we should see a an evaluation. Oh cool over here.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1058s",
        "start_time": "1058.15"
    },
    {
        "id": "b24b6762",
        "text": "this is like quite straightforward. We just do model dot evaluate and then we pass in this time, the X test and the Y test. So here we are basically evaluating the model on the test set on the test set and we are passing both the inputs and the outputs. Now, if we want to see a report here, we should do a verbose equal to one. Cool. So now let's rerun uh the scripts and see whether like this is working correctly. Cool. Yeah. Again, we are getting the uh training over here and once this is done, we should see a an evaluation. Oh cool over here. Good,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1082s",
        "start_time": "1082.829"
    },
    {
        "id": "8bd4e0a0",
        "text": "Cool. So now let's rerun uh the scripts and see whether like this is working correctly. Cool. Yeah. Again, we are getting the uh training over here and once this is done, we should see a an evaluation. Oh cool over here. Good, cool. So we have the evaluation which has been performed on 1500 samples, which is the test set and the loss function is seven by 10 to the minus four. And as you can see here, the loss, the error is slightly higher than the error that we have on the training set. So the test set is doing a little worse than the training set. And this is OK.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1113s",
        "start_time": "1113.069"
    },
    {
        "id": "88d882f9",
        "text": "Good, cool. So we have the evaluation which has been performed on 1500 samples, which is the test set and the loss function is seven by 10 to the minus four. And as you can see here, the loss, the error is slightly higher than the error that we have on the training set. So the test set is doing a little worse than the training set. And this is OK. Uh Because uh obviously the uh model has optimized its its weight and uh its parameters on the training set. But at the same time, we don't want to have like a huge difference between the loss on the training and on the test set. Uh Because if that was the case, then it would mean that the",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1134s",
        "start_time": "1134.26"
    },
    {
        "id": "ecb392bb",
        "text": "cool. So we have the evaluation which has been performed on 1500 samples, which is the test set and the loss function is seven by 10 to the minus four. And as you can see here, the loss, the error is slightly higher than the error that we have on the training set. So the test set is doing a little worse than the training set. And this is OK. Uh Because uh obviously the uh model has optimized its its weight and uh its parameters on the training set. But at the same time, we don't want to have like a huge difference between the loss on the training and on the test set. Uh Because if that was the case, then it would mean that the uh model hasn't been able to generalize uh the uh the learning process. So it's not able like to predict things well outside of the data that it has already sent. Cool. So now we are basically done. So the last thing that we want to do is to make predictions, right? And so, and in order to make predictions, we should create a some data. So we can just like go back over here and use this for example,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1136s",
        "start_time": "1136.5"
    },
    {
        "id": "7d82f3cf",
        "text": "Uh Because uh obviously the uh model has optimized its its weight and uh its parameters on the training set. But at the same time, we don't want to have like a huge difference between the loss on the training and on the test set. Uh Because if that was the case, then it would mean that the uh model hasn't been able to generalize uh the uh the learning process. So it's not able like to predict things well outside of the data that it has already sent. Cool. So now we are basically done. So the last thing that we want to do is to make predictions, right? And so, and in order to make predictions, we should create a some data. So we can just like go back over here and use this for example, and we'll do an NP dot array and we'll pass in like this to",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1164s",
        "start_time": "1164.5"
    },
    {
        "id": "701d748e",
        "text": "uh model hasn't been able to generalize uh the uh the learning process. So it's not able like to predict things well outside of the data that it has already sent. Cool. So now we are basically done. So the last thing that we want to do is to make predictions, right? And so, and in order to make predictions, we should create a some data. So we can just like go back over here and use this for example, and we'll do an NP dot array and we'll pass in like this to uh like samples here. And we would expect obviously no 0.3 and no 0.4.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1187s",
        "start_time": "1187.079"
    },
    {
        "id": "523377dc",
        "text": "and we'll do an NP dot array and we'll pass in like this to uh like samples here. And we would expect obviously no 0.3 and no 0.4. And so, uh let's uh",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1217s",
        "start_time": "1217.069"
    },
    {
        "id": "22c738c0",
        "text": "uh like samples here. And we would expect obviously no 0.3 and no 0.4. And so, uh let's uh make these predictions. Cool. So we'll do a model dots. Can you guys guess what's gonna be the name of the method like for predicting?",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1224s",
        "start_time": "1224.92"
    },
    {
        "id": "247ca133",
        "text": "And so, uh let's uh make these predictions. Cool. So we'll do a model dots. Can you guys guess what's gonna be the name of the method like for predicting? Uh Yeah, I'm quite sure you've uh you had it right? And it's just model dot predicts,",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1232s",
        "start_time": "1232.089"
    },
    {
        "id": "0e2756cd",
        "text": "make these predictions. Cool. So we'll do a model dots. Can you guys guess what's gonna be the name of the method like for predicting? Uh Yeah, I'm quite sure you've uh you had it right? And it's just model dot predicts, which is super nice and super simple to remember and then we want to pass in data cool. So now uh let's print and say uh some predictions over here. And uh yeah, let's do like a new line. And uh",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1236s",
        "start_time": "1236.01"
    },
    {
        "id": "55a3459b",
        "text": "Uh Yeah, I'm quite sure you've uh you had it right? And it's just model dot predicts, which is super nice and super simple to remember and then we want to pass in data cool. So now uh let's print and say uh some predictions over here. And uh yeah, let's do like a new line. And uh we want to have a for loop here where we unpack the data and the predictions. And so we'll do for DP in ZIP",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1246s",
        "start_time": "1246.89"
    },
    {
        "id": "c4ef9b48",
        "text": "which is super nice and super simple to remember and then we want to pass in data cool. So now uh let's print and say uh some predictions over here. And uh yeah, let's do like a new line. And uh we want to have a for loop here where we unpack the data and the predictions. And so we'll do for DP in ZIP and we'll pass in uh data and we'll pass in predictions. And here",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1255s",
        "start_time": "1255.65"
    },
    {
        "id": "a01b8d9e",
        "text": "we want to have a for loop here where we unpack the data and the predictions. And so we'll do for DP in ZIP and we'll pass in uh data and we'll pass in predictions. And here um for each of this, what we wanna do is to print.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1278s",
        "start_time": "1278.29"
    },
    {
        "id": "5d9379f2",
        "text": "and we'll pass in uh data and we'll pass in predictions. And here um for each of this, what we wanna do is to print. So we want to print",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1291s",
        "start_time": "1291.089"
    },
    {
        "id": "d8a4002f",
        "text": "um for each of this, what we wanna do is to print. So we want to print that this plus this is equal for our network to like this element here. And so now we want to",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1298s",
        "start_time": "1298.099"
    },
    {
        "id": "3d9c7e1a",
        "text": "So we want to print that this plus this is equal for our network to like this element here. And so now we want to just pass this uh values in. So we'll do",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1306s",
        "start_time": "1306.089"
    },
    {
        "id": "fd19a172",
        "text": "that this plus this is equal for our network to like this element here. And so now we want to just pass this uh values in. So we'll do D zero, so D",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1308s",
        "start_time": "1308.92"
    },
    {
        "id": "b6ee0ada",
        "text": "just pass this uh values in. So we'll do D zero, so D zero, then we'll do D one and finally, we'll have a P zero. OK. So this should be right. So let's run this and see the results",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1318s",
        "start_time": "1318.28"
    },
    {
        "id": "dddfeb03",
        "text": "D zero, so D zero, then we'll do D one and finally, we'll have a P zero. OK. So this should be right. So let's run this and see the results cool.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1323s",
        "start_time": "1323.02"
    },
    {
        "id": "f797c11b",
        "text": "zero, then we'll do D one and finally, we'll have a P zero. OK. So this should be right. So let's run this and see the results cool. So obviously, like it's taking some time for doing like this training. And that's again because I'm running tensor flow and CPU if it, if it was like a GPA, put me like way, way faster uh cool.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1326s",
        "start_time": "1326.099"
    },
    {
        "id": "a878aae5",
        "text": "cool. So obviously, like it's taking some time for doing like this training. And that's again because I'm running tensor flow and CPU if it, if it was like a GPA, put me like way, way faster uh cool. So here we have the results, so some predictions. So 0.1 plus 0.2 we are getting this 0.33. Well, I mean, it's not really no 0.3 which we would expect, but it's kind of like close to that and then 0.2 plus naught 0.2.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1343s",
        "start_time": "1343.699"
    },
    {
        "id": "4b201fdc",
        "text": "So obviously, like it's taking some time for doing like this training. And that's again because I'm running tensor flow and CPU if it, if it was like a GPA, put me like way, way faster uh cool. So here we have the results, so some predictions. So 0.1 plus 0.2 we are getting this 0.33. Well, I mean, it's not really no 0.3 which we would expect, but it's kind of like close to that and then 0.2 plus naught 0.2. It gives us like no 0.40. Well, it's 42. Well, again, it's close but not like super close. Well, what this basically is telling us is that we probably don't have like in we haven't had like enough data uh like for like getting like better like precision there.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1345s",
        "start_time": "1345.65"
    },
    {
        "id": "ef952d22",
        "text": "So here we have the results, so some predictions. So 0.1 plus 0.2 we are getting this 0.33. Well, I mean, it's not really no 0.3 which we would expect, but it's kind of like close to that and then 0.2 plus naught 0.2. It gives us like no 0.40. Well, it's 42. Well, again, it's close but not like super close. Well, what this basically is telling us is that we probably don't have like in we haven't had like enough data uh like for like getting like better like precision there. But then again, also we, we would need to like tweak all the parameters that we have in the network, like the learning rate or the type of loss function that we use or the architecture itself. So how many layers we have or how many neurons in the layer we have to get",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1360s",
        "start_time": "1360.13"
    },
    {
        "id": "b2f7237b",
        "text": "It gives us like no 0.40. Well, it's 42. Well, again, it's close but not like super close. Well, what this basically is telling us is that we probably don't have like in we haven't had like enough data uh like for like getting like better like precision there. But then again, also we, we would need to like tweak all the parameters that we have in the network, like the learning rate or the type of loss function that we use or the architecture itself. So how many layers we have or how many neurons in the layer we have to get like better results. But this is not the point of this video here. We just wanted to build a neural network with tensorflow and that's what we've done. And now guys, you should be super happy because you know now how to build a neural network with tensorflow. Cool. So",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1377s",
        "start_time": "1377.694"
    },
    {
        "id": "19747994",
        "text": "But then again, also we, we would need to like tweak all the parameters that we have in the network, like the learning rate or the type of loss function that we use or the architecture itself. So how many layers we have or how many neurons in the layer we have to get like better results. But this is not the point of this video here. We just wanted to build a neural network with tensorflow and that's what we've done. And now guys, you should be super happy because you know now how to build a neural network with tensorflow. Cool. So uh what are we gonna do next in the next video? Well, uh in the next video, we're finally at a moment where we can start to look at all your data. And so we'll preprocess all your data. So that we will have it in such a way that we then can use it with our deep learning algorithms, which is super nice. Cool. So this was it for this video. I hope you enjoyed it. And if that's the case, please subscribe and hit the notification bell to get more videos.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1395s",
        "start_time": "1395.489"
    },
    {
        "id": "7862c884",
        "text": "like better results. But this is not the point of this video here. We just wanted to build a neural network with tensorflow and that's what we've done. And now guys, you should be super happy because you know now how to build a neural network with tensorflow. Cool. So uh what are we gonna do next in the next video? Well, uh in the next video, we're finally at a moment where we can start to look at all your data. And so we'll preprocess all your data. So that we will have it in such a way that we then can use it with our deep learning algorithms, which is super nice. Cool. So this was it for this video. I hope you enjoyed it. And if that's the case, please subscribe and hit the notification bell to get more videos. And uh you it would be fantastic if you could leave a like to this video and I guess if you have any questions, you can just uh leave them in the comments section below and I'll see you next time. Cheers.",
        "video": "9- How to implement a (simple) neural network with TensorFlow 2",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "JdXxaZcQer8",
        "youtube_link": "https://www.youtube.com/watch?v=JdXxaZcQer8&t=1411s",
        "start_time": "1411.989"
    },
    {
        "id": "7c5815ac",
        "text": "Hi, everybody and welcome to another video in the Deep Learning for audio with Python series. This time we're gonna build a music genre classifier using a multi-layered perception network. Cool. So music genre classification is a type of problem that's called a classification problem, right? So what's a classification problem? Well, classification problem is I have uh a bunch of data and I want to classify that. So I have for example, like a bunch of uh tracks and I want to classify them into like rock music, uh blues music or like classical music, specifically what we're doing today is called multi class classification. So we have a bunch of tracks and those vinyls would uh want to like represent those tracks and then we want to classify them into a bunch of different genres, right? So it could be classical, it could be death metal could be EDM and whatever, right? But it's more than two and multi class classification is opposed to binary classification where we just have two categories out there. So for example, it could be uh tell me",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=0s",
        "start_time": "0.319"
    },
    {
        "id": "60fc4c98",
        "text": "So what's a classification problem? Well, classification problem is I have uh a bunch of data and I want to classify that. So I have for example, like a bunch of uh tracks and I want to classify them into like rock music, uh blues music or like classical music, specifically what we're doing today is called multi class classification. So we have a bunch of tracks and those vinyls would uh want to like represent those tracks and then we want to classify them into a bunch of different genres, right? So it could be classical, it could be death metal could be EDM and whatever, right? But it's more than two and multi class classification is opposed to binary classification where we just have two categories out there. So for example, it could be uh tell me uh whether like this track is classical or is not classical, right? Cool. OK. So with this in mind. Uh Let's get started like building like the classifier.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=20s",
        "start_time": "20.059"
    },
    {
        "id": "49ba729e",
        "text": "So we have a bunch of tracks and those vinyls would uh want to like represent those tracks and then we want to classify them into a bunch of different genres, right? So it could be classical, it could be death metal could be EDM and whatever, right? But it's more than two and multi class classification is opposed to binary classification where we just have two categories out there. So for example, it could be uh tell me uh whether like this track is classical or is not classical, right? Cool. OK. So with this in mind. Uh Let's get started like building like the classifier. And uh obviously, we are going to build uh like on top of the work that we've done in the previous video where we actually created a data set out of like the marsh. Uh So data set which is divided into a bunch of different genres, 10 genres to be specific. And there, what we did was building a JSON file So we can see it here",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=42s",
        "start_time": "42.049"
    },
    {
        "id": "89baab72",
        "text": "uh whether like this track is classical or is not classical, right? Cool. OK. So with this in mind. Uh Let's get started like building like the classifier. And uh obviously, we are going to build uh like on top of the work that we've done in the previous video where we actually created a data set out of like the marsh. Uh So data set which is divided into a bunch of different genres, 10 genres to be specific. And there, what we did was building a JSON file So we can see it here uh where uh we just extracted uh like all the different uh genres and map them. Uh We extracted the labels and then we also extracted the MFCC. So we have like both the inputs and outputs. So the inputs and the labels are targets for our network. So what we want to do as a first step here in our um uh genre classifier is load",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=70s",
        "start_time": "70.029"
    },
    {
        "id": "f1f56439",
        "text": "And uh obviously, we are going to build uh like on top of the work that we've done in the previous video where we actually created a data set out of like the marsh. Uh So data set which is divided into a bunch of different genres, 10 genres to be specific. And there, what we did was building a JSON file So we can see it here uh where uh we just extracted uh like all the different uh genres and map them. Uh We extracted the labels and then we also extracted the MFCC. So we have like both the inputs and outputs. So the inputs and the labels are targets for our network. So what we want to do as a first step here in our um uh genre classifier is load the data set, load data. Then once we've done that, we want to uh split the data into uh training train and uh test sets,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=81s",
        "start_time": "81.72"
    },
    {
        "id": "488bc426",
        "text": "uh where uh we just extracted uh like all the different uh genres and map them. Uh We extracted the labels and then we also extracted the MFCC. So we have like both the inputs and outputs. So the inputs and the labels are targets for our network. So what we want to do as a first step here in our um uh genre classifier is load the data set, load data. Then once we've done that, we want to uh split the data into uh training train and uh test sets, then using tensorflow and carrots specifically, we're gonna build the network",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=105s",
        "start_time": "105.699"
    },
    {
        "id": "0d05a93d",
        "text": "the data set, load data. Then once we've done that, we want to uh split the data into uh training train and uh test sets, then using tensorflow and carrots specifically, we're gonna build the network architecture,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=134s",
        "start_time": "134.44"
    },
    {
        "id": "e3c1e648",
        "text": "then using tensorflow and carrots specifically, we're gonna build the network architecture, then we want to compile a network and finally, we want to train the network, right. OK. So let's get started from the first phase which is loading data. And so we're doing that, we'll",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=150s",
        "start_time": "150.559"
    },
    {
        "id": "dd9e7b9c",
        "text": "architecture, then we want to compile a network and finally, we want to train the network, right. OK. So let's get started from the first phase which is loading data. And so we're doing that, we'll uh create a function which we call not surprisingly load data. And this function uh accepts one argument and the argument is the uh data set uh path,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=158s",
        "start_time": "158.99"
    },
    {
        "id": "08fa30b6",
        "text": "then we want to compile a network and finally, we want to train the network, right. OK. So let's get started from the first phase which is loading data. And so we're doing that, we'll uh create a function which we call not surprisingly load data. And this function uh accepts one argument and the argument is the uh data set uh path, right. OK. So we have the data set path here. Uh And uh now what we want to do is like load uh like this data. And we know that uh like this data is stored in adjacent file. And so as the first thing we want to uh open",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=161s",
        "start_time": "161.96"
    },
    {
        "id": "7c9bad2f",
        "text": "uh create a function which we call not surprisingly load data. And this function uh accepts one argument and the argument is the uh data set uh path, right. OK. So we have the data set path here. Uh And uh now what we want to do is like load uh like this data. And we know that uh like this data is stored in adjacent file. And so as the first thing we want to uh open uh and read from this JSON file. And so we'll do a with open and uh we'll pass in the uh data set path and we'll open it uh as um oops here, I, I should say like the, the mode that we want to read this, uh We want to like open uh this file for and we'll put an R here which stands for read. So we're opening for reading and we'll do as a FP.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=177s",
        "start_time": "177.729"
    },
    {
        "id": "347bc8d7",
        "text": "right. OK. So we have the data set path here. Uh And uh now what we want to do is like load uh like this data. And we know that uh like this data is stored in adjacent file. And so as the first thing we want to uh open uh and read from this JSON file. And so we'll do a with open and uh we'll pass in the uh data set path and we'll open it uh as um oops here, I, I should say like the, the mode that we want to read this, uh We want to like open uh this file for and we'll put an R here which stands for read. So we're opening for reading and we'll do as a FP. And then down here, we'll do a data, it's equal to Jason dot A load and uh we'll pass in FP. Now, Jason uh obviously like is a uh Python uh module. So we have to import it. And so we'll do an import Jason.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=194s",
        "start_time": "194.58"
    },
    {
        "id": "83a02033",
        "text": "uh and read from this JSON file. And so we'll do a with open and uh we'll pass in the uh data set path and we'll open it uh as um oops here, I, I should say like the, the mode that we want to read this, uh We want to like open uh this file for and we'll put an R here which stands for read. So we're opening for reading and we'll do as a FP. And then down here, we'll do a data, it's equal to Jason dot A load and uh we'll pass in FP. Now, Jason uh obviously like is a uh Python uh module. So we have to import it. And so we'll do an import Jason. And here in this data, we're, we're basically loading all of this huge uh dictionary here with mapping labels and MFCC Cool. OK. So once we have that, we want to do uh another thing. So we want to convert a nun pi arrays into, oh sorry,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=213s",
        "start_time": "213.649"
    },
    {
        "id": "21427350",
        "text": "And then down here, we'll do a data, it's equal to Jason dot A load and uh we'll pass in FP. Now, Jason uh obviously like is a uh Python uh module. So we have to import it. And so we'll do an import Jason. And here in this data, we're, we're basically loading all of this huge uh dictionary here with mapping labels and MFCC Cool. OK. So once we have that, we want to do uh another thing. So we want to convert a nun pi arrays into, oh sorry, it's actually the opposite. So convert lists into",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=242s",
        "start_time": "242.809"
    },
    {
        "id": "acd552e2",
        "text": "And here in this data, we're, we're basically loading all of this huge uh dictionary here with mapping labels and MFCC Cool. OK. So once we have that, we want to do uh another thing. So we want to convert a nun pi arrays into, oh sorry, it's actually the opposite. So convert lists into uh NP arrays. So, and that's because both the uh the labels like, for example, here and the MF CCS uh these guys here are stored and will be retrieved as lists. And so we want to convert them into NPI arrays. And so, first of all, let's import uh NP as NP.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=263s",
        "start_time": "263.799"
    },
    {
        "id": "1518c0e6",
        "text": "it's actually the opposite. So convert lists into uh NP arrays. So, and that's because both the uh the labels like, for example, here and the MF CCS uh these guys here are stored and will be retrieved as lists. And so we want to convert them into NPI arrays. And so, first of all, let's import uh NP as NP. And then what we want to do here is say, OK, so here I want the inputs, right? And so, and the inputs are equal to NP dot uh array. And we want to pass in the, um here we want to pass in the, the MFCC. And so we'll do data",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=290s",
        "start_time": "290.049"
    },
    {
        "id": "ea8c6f04",
        "text": "uh NP arrays. So, and that's because both the uh the labels like, for example, here and the MF CCS uh these guys here are stored and will be retrieved as lists. And so we want to convert them into NPI arrays. And so, first of all, let's import uh NP as NP. And then what we want to do here is say, OK, so here I want the inputs, right? And so, and the inputs are equal to NP dot uh array. And we want to pass in the, um here we want to pass in the, the MFCC. And so we'll do data and here we have this MFCC",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=294s",
        "start_time": "294.779"
    },
    {
        "id": "7b78fd74",
        "text": "And then what we want to do here is say, OK, so here I want the inputs, right? And so, and the inputs are equal to NP dot uh array. And we want to pass in the, um here we want to pass in the, the MFCC. And so we'll do data and here we have this MFCC and uh then we can do a similar thing for the targets or like the expected outcomes. But instead of the MFCC here, we'll have the uh labels, right? OK. And so what we want to do in the end is just passing the inputs and the targets out. And so this is all we need to load data.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=318s",
        "start_time": "318.529"
    },
    {
        "id": "c123f247",
        "text": "and here we have this MFCC and uh then we can do a similar thing for the targets or like the expected outcomes. But instead of the MFCC here, we'll have the uh labels, right? OK. And so what we want to do in the end is just passing the inputs and the targets out. And so this is all we need to load data. Well, so let's try this. So let's uh create if a name is equal to main.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=340s",
        "start_time": "340.32"
    },
    {
        "id": "b358057d",
        "text": "and uh then we can do a similar thing for the targets or like the expected outcomes. But instead of the MFCC here, we'll have the uh labels, right? OK. And so what we want to do in the end is just passing the inputs and the targets out. And so this is all we need to load data. Well, so let's try this. So let's uh create if a name is equal to main. And then what we want to do here is just like get the inputs and the targets and we'll get them by loading data. Now, we actually need the data set path. And so I'm gonna uh oops, not that I'm gonna create a uh constant over here. And I'll call",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=344s",
        "start_time": "344.07"
    },
    {
        "id": "bfc7588d",
        "text": "Well, so let's try this. So let's uh create if a name is equal to main. And then what we want to do here is just like get the inputs and the targets and we'll get them by loading data. Now, we actually need the data set path. And so I'm gonna uh oops, not that I'm gonna create a uh constant over here. And I'll call uh the, this guy here, right? So this is the path to the data set, right? So now as you notice, uh you can notice here. So I saved this um data set, this Jason file as data underscore 10 dot Jason. So uh if you remember, so the Marci data set has 1000 32nd exerts of like songs divided into",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=367s",
        "start_time": "367.97"
    },
    {
        "id": "7b78fd74",
        "text": "And then what we want to do here is just like get the inputs and the targets and we'll get them by loading data. Now, we actually need the data set path. And so I'm gonna uh oops, not that I'm gonna create a uh constant over here. And I'll call uh the, this guy here, right? So this is the path to the data set, right? So now as you notice, uh you can notice here. So I saved this um data set, this Jason file as data underscore 10 dot Jason. So uh if you remember, so the Marci data set has 1000 32nd exerts of like songs divided into 10 genres. Now, uh We said that that last time we mentioned that that is not really like that much like for training a deep learning system. So what it did was like segmenting those into uh like 10 different segments. And so this is why I have this data 10. So all of a sudden now we have 10,000 like the data that has 10,000 like tracks and each track uh should be like three second long, right?",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=378s",
        "start_time": "378.089"
    },
    {
        "id": "482690d2",
        "text": "uh the, this guy here, right? So this is the path to the data set, right? So now as you notice, uh you can notice here. So I saved this um data set, this Jason file as data underscore 10 dot Jason. So uh if you remember, so the Marci data set has 1000 32nd exerts of like songs divided into 10 genres. Now, uh We said that that last time we mentioned that that is not really like that much like for training a deep learning system. So what it did was like segmenting those into uh like 10 different segments. And so this is why I have this data 10. So all of a sudden now we have 10,000 like the data that has 10,000 like tracks and each track uh should be like three second long, right? OK. So now we have our data set path and we need to pass it in here.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=401s",
        "start_time": "401.16"
    },
    {
        "id": "d3f9a755",
        "text": "10 genres. Now, uh We said that that last time we mentioned that that is not really like that much like for training a deep learning system. So what it did was like segmenting those into uh like 10 different segments. And so this is why I have this data 10. So all of a sudden now we have 10,000 like the data that has 10,000 like tracks and each track uh should be like three second long, right? OK. So now we have our data set path and we need to pass it in here. Cool. OK. So yeah, let me just do this. So that makes more sense. OK. So uh this way we should be able to uh get the inputs and the targets. And these are like NP uh arrays right. Now, the next step that we want to perform instead of like splitting our data into train set and uh test set. And that's because we don't want to",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=429s",
        "start_time": "429.625"
    },
    {
        "id": "fb3cdcc8",
        "text": "OK. So now we have our data set path and we need to pass it in here. Cool. OK. So yeah, let me just do this. So that makes more sense. OK. So uh this way we should be able to uh get the inputs and the targets. And these are like NP uh arrays right. Now, the next step that we want to perform instead of like splitting our data into train set and uh test set. And that's because we don't want to um evaluate our classifier on the training data because uh otherwise it would be basically like cheating. So we want to evaluate on some data that the classifier has never seen before. So for doing that, uh we should uh import um a function from psychic learn. And this function is in the model selection uh module.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=458s",
        "start_time": "458.85"
    },
    {
        "id": "e094f818",
        "text": "Cool. OK. So yeah, let me just do this. So that makes more sense. OK. So uh this way we should be able to uh get the inputs and the targets. And these are like NP uh arrays right. Now, the next step that we want to perform instead of like splitting our data into train set and uh test set. And that's because we don't want to um evaluate our classifier on the training data because uh otherwise it would be basically like cheating. So we want to evaluate on some data that the classifier has never seen before. So for doing that, uh we should uh import um a function from psychic learn. And this function is in the model selection uh module. And uh we should say",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=465s",
        "start_time": "465.01"
    },
    {
        "id": "48a5fca1",
        "text": "um evaluate our classifier on the training data because uh otherwise it would be basically like cheating. So we want to evaluate on some data that the classifier has never seen before. So for doing that, uh we should uh import um a function from psychic learn. And this function is in the model selection uh module. And uh we should say uh whoop. Yeah, this should be from uh psych learn dot model selection imports. And uh this is train task split. This is like a very nice uh function we can use for this purpose. So here what we should do is say, inputs and we'll do a inputs, train,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=491s",
        "start_time": "491.959"
    },
    {
        "id": "356f3ceb",
        "text": "And uh we should say uh whoop. Yeah, this should be from uh psych learn dot model selection imports. And uh this is train task split. This is like a very nice uh function we can use for this purpose. So here what we should do is say, inputs and we'll do a inputs, train, we'll do inputs uh test, then we'll do a targets, train and a targets uh test and then we'll use the train test split. And here we need to pass three arguments. So obviously, we need to pass in the uh inputs",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=519s",
        "start_time": "519.65"
    },
    {
        "id": "e83e45af",
        "text": "uh whoop. Yeah, this should be from uh psych learn dot model selection imports. And uh this is train task split. This is like a very nice uh function we can use for this purpose. So here what we should do is say, inputs and we'll do a inputs, train, we'll do inputs uh test, then we'll do a targets, train and a targets uh test and then we'll use the train test split. And here we need to pass three arguments. So obviously, we need to pass in the uh inputs that we've arrived from the JSON file the targets. And finally, we want to specify the uh test site. Uh Well, this is probably a little bit less difficult to read. So I'll do it like this. So here uh in the test size, uh we could put uh no 0.3. So basically what I'm saying here is that 30% of uh this data is gonna be used for uh test set, right?",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=523s",
        "start_time": "523.549"
    },
    {
        "id": "83e5965f",
        "text": "we'll do inputs uh test, then we'll do a targets, train and a targets uh test and then we'll use the train test split. And here we need to pass three arguments. So obviously, we need to pass in the uh inputs that we've arrived from the JSON file the targets. And finally, we want to specify the uh test site. Uh Well, this is probably a little bit less difficult to read. So I'll do it like this. So here uh in the test size, uh we could put uh no 0.3. So basically what I'm saying here is that 30% of uh this data is gonna be used for uh test set, right? And the remaining 70% for the train set cool. OK. So now we have our own um",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=546s",
        "start_time": "546.27"
    },
    {
        "id": "a3f4523e",
        "text": "that we've arrived from the JSON file the targets. And finally, we want to specify the uh test site. Uh Well, this is probably a little bit less difficult to read. So I'll do it like this. So here uh in the test size, uh we could put uh no 0.3. So basically what I'm saying here is that 30% of uh this data is gonna be used for uh test set, right? And the remaining 70% for the train set cool. OK. So now we have our own um uh our, our own train set and uh test set, right? And so the next step is that of building the network architecture for doing that. Obviously, we are gonna need a tensorflow and then specifically uh we want uh Kas. So we'll do an import tensorflow dot uh Kas is Kas.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=568s",
        "start_time": "568.429"
    },
    {
        "id": "9775d5f9",
        "text": "And the remaining 70% for the train set cool. OK. So now we have our own um uh our, our own train set and uh test set, right? And so the next step is that of building the network architecture for doing that. Obviously, we are gonna need a tensorflow and then specifically uh we want uh Kas. So we'll do an import tensorflow dot uh Kas is Kas. So by now guys, you should be like familiar with this. Now, uh we should build the model and the model is going to be a sequential model. So we'll do a carers dot uh sequential. And here uh we should specify all the different layers that we want in the network, right?",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=597s",
        "start_time": "597.229"
    },
    {
        "id": "79f3454e",
        "text": "uh our, our own train set and uh test set, right? And so the next step is that of building the network architecture for doing that. Obviously, we are gonna need a tensorflow and then specifically uh we want uh Kas. So we'll do an import tensorflow dot uh Kas is Kas. So by now guys, you should be like familiar with this. Now, uh we should build the model and the model is going to be a sequential model. So we'll do a carers dot uh sequential. And here uh we should specify all the different layers that we want in the network, right? And I'm thinking of using a uh let's say like an input layer three hidden layers and an output layer. And given we are working with a simple multi-layered perception, I'm gonna be using all uh fully connected or dense layers. Now, if you do",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=605s",
        "start_time": "605.95"
    },
    {
        "id": "b8b542d8",
        "text": "So by now guys, you should be like familiar with this. Now, uh we should build the model and the model is going to be a sequential model. So we'll do a carers dot uh sequential. And here uh we should specify all the different layers that we want in the network, right? And I'm thinking of using a uh let's say like an input layer three hidden layers and an output layer. And given we are working with a simple multi-layered perception, I'm gonna be using all uh fully connected or dense layers. Now, if you do remember what a multi layer perception is, don't worry, just go back here. You have like the description like of one of my videos, it should be like in uh the top side over here. So click there if you want to learn more about the theory about M LP S otherwise let's move on. So the first thing that we want to do is the input uh layer. So",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=633s",
        "start_time": "633.78"
    },
    {
        "id": "c7c1e9b1",
        "text": "And I'm thinking of using a uh let's say like an input layer three hidden layers and an output layer. And given we are working with a simple multi-layered perception, I'm gonna be using all uh fully connected or dense layers. Now, if you do remember what a multi layer perception is, don't worry, just go back here. You have like the description like of one of my videos, it should be like in uh the top side over here. So click there if you want to learn more about the theory about M LP S otherwise let's move on. So the first thing that we want to do is the input uh layer. So now for the input layer, we want to uh u uh use a layer that's called uh like flatten. So what flatten uh does is basically takes a a multidimensional array and it flattens it out, right? So in this case, so we expect for uh the input shape uh to be of type uh inputs",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=657s",
        "start_time": "657.75"
    },
    {
        "id": "0efc707f",
        "text": "remember what a multi layer perception is, don't worry, just go back here. You have like the description like of one of my videos, it should be like in uh the top side over here. So click there if you want to learn more about the theory about M LP S otherwise let's move on. So the first thing that we want to do is the input uh layer. So now for the input layer, we want to uh u uh use a layer that's called uh like flatten. So what flatten uh does is basically takes a a multidimensional array and it flattens it out, right? So in this case, so we expect for uh the input shape uh to be of type uh inputs dot uh shape. And here we'll pass in a one and then we'll do, yeah, same thing here, but then we'll pass in two,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=675s",
        "start_time": "675.63"
    },
    {
        "id": "6e1f9058",
        "text": "now for the input layer, we want to uh u uh use a layer that's called uh like flatten. So what flatten uh does is basically takes a a multidimensional array and it flattens it out, right? So in this case, so we expect for uh the input shape uh to be of type uh inputs dot uh shape. And here we'll pass in a one and then we'll do, yeah, same thing here, but then we'll pass in two, right? So basically what I, what I'm saying here is that I want to flatten this two dimensional array which is uh like the uh the input that we have here. And why is it two dimensional? Well, because if you remember we have NF CCS here for each uh segment, for each track and for each track, we have many",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=701s",
        "start_time": "701.059"
    },
    {
        "id": "d1542ece",
        "text": "dot uh shape. And here we'll pass in a one and then we'll do, yeah, same thing here, but then we'll pass in two, right? So basically what I, what I'm saying here is that I want to flatten this two dimensional array which is uh like the uh the input that we have here. And why is it two dimensional? Well, because if you remember we have NF CCS here for each uh segment, for each track and for each track, we have many MFCC vectors and each MFCC vector is taken at a specific interval. So and that is like the hop length again, if you don't remember like what MF CCS are or uh how we calculate them, I have a video about that. Go watch that out. Cool. But here like in this two dimensional um array. So we have like the, the, the,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=731s",
        "start_time": "731.679"
    },
    {
        "id": "c0df7dc8",
        "text": "right? So basically what I, what I'm saying here is that I want to flatten this two dimensional array which is uh like the uh the input that we have here. And why is it two dimensional? Well, because if you remember we have NF CCS here for each uh segment, for each track and for each track, we have many MFCC vectors and each MFCC vector is taken at a specific interval. So and that is like the hop length again, if you don't remember like what MF CCS are or uh how we calculate them, I have a video about that. Go watch that out. Cool. But here like in this two dimensional um array. So we have like the, the, the, the, the, the first uh like dimension which is basically uh given by the uh intervals, right? And the second dimension is the values of the MF CCS uh like for that interval. And in this case, we have 13 MF CCS. That's like the number that I've decided to extract, but I could have done more 4030 whatever really doesn't matter",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=742s",
        "start_time": "742.82"
    },
    {
        "id": "b4bf04af",
        "text": "MFCC vectors and each MFCC vector is taken at a specific interval. So and that is like the hop length again, if you don't remember like what MF CCS are or uh how we calculate them, I have a video about that. Go watch that out. Cool. But here like in this two dimensional um array. So we have like the, the, the, the, the, the first uh like dimension which is basically uh given by the uh intervals, right? And the second dimension is the values of the MF CCS uh like for that interval. And in this case, we have 13 MF CCS. That's like the number that I've decided to extract, but I could have done more 4030 whatever really doesn't matter right now. You may be wondering, but why are you uh passing in input dot shape one? Why aren't you starting from index zero? Well, because inputs, actually this guy here is a three dimensional array and index zero represents like the different segments. So this is the uh input layer. So now we should move on and uh work with the first work out the first hidden layer. And so",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=767s",
        "start_time": "767.51"
    },
    {
        "id": "372947e5",
        "text": "the, the, the first uh like dimension which is basically uh given by the uh intervals, right? And the second dimension is the values of the MF CCS uh like for that interval. And in this case, we have 13 MF CCS. That's like the number that I've decided to extract, but I could have done more 4030 whatever really doesn't matter right now. You may be wondering, but why are you uh passing in input dot shape one? Why aren't you starting from index zero? Well, because inputs, actually this guy here is a three dimensional array and index zero represents like the different segments. So this is the uh input layer. So now we should move on and uh work with the first work out the first hidden layer. And so uh this is gonna be a simple uh dense uh layer. And here uh what we're gonna do is say uh how many neurons we want and we'll start with 512 neurons. And then we should",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=794s",
        "start_time": "794.03"
    },
    {
        "id": "7c6ddf9e",
        "text": "right now. You may be wondering, but why are you uh passing in input dot shape one? Why aren't you starting from index zero? Well, because inputs, actually this guy here is a three dimensional array and index zero represents like the different segments. So this is the uh input layer. So now we should move on and uh work with the first work out the first hidden layer. And so uh this is gonna be a simple uh dense uh layer. And here uh what we're gonna do is say uh how many neurons we want and we'll start with 512 neurons. And then we should specify which type of activation we want. And now up until now, we've always used the Sigma function. But this time I want to introduce you a new type of activation function that's called relu. Now relu is very, very important and, and very, very effective in deep learning. So it warrants some theoretical background. So let's move on to the PDF over here to the slide presentation",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=818s",
        "start_time": "818.21"
    },
    {
        "id": "07d64ec3",
        "text": "uh this is gonna be a simple uh dense uh layer. And here uh what we're gonna do is say uh how many neurons we want and we'll start with 512 neurons. And then we should specify which type of activation we want. And now up until now, we've always used the Sigma function. But this time I want to introduce you a new type of activation function that's called relu. Now relu is very, very important and, and very, very effective in deep learning. So it warrants some theoretical background. So let's move on to the PDF over here to the slide presentation uh right. So we have the binary classification. So here we have the rectified linear unit or R. So this is like this function. So, and as you can see it here, so R is a function of H and if you recall from our theoretical uh videos on um computation in neural network, H is the net input, right?",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=847s",
        "start_time": "847.059"
    },
    {
        "id": "5a89a135",
        "text": "specify which type of activation we want. And now up until now, we've always used the Sigma function. But this time I want to introduce you a new type of activation function that's called relu. Now relu is very, very important and, and very, very effective in deep learning. So it warrants some theoretical background. So let's move on to the PDF over here to the slide presentation uh right. So we have the binary classification. So here we have the rectified linear unit or R. So this is like this function. So, and as you can see it here, so R is a function of H and if you recall from our theoretical uh videos on um computation in neural network, H is the net input, right? And so if H uh is M uh is less than zero, then relu outputs zero. If H is a greater or equal, equal or greater than zero, then H uh basically uh is used as an output for REU. So relu it's E equal to H and so this is like the uh the plots that we have for R.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=864s",
        "start_time": "864.94"
    },
    {
        "id": "232b8ef0",
        "text": "uh right. So we have the binary classification. So here we have the rectified linear unit or R. So this is like this function. So, and as you can see it here, so R is a function of H and if you recall from our theoretical uh videos on um computation in neural network, H is the net input, right? And so if H uh is M uh is less than zero, then relu outputs zero. If H is a greater or equal, equal or greater than zero, then H uh basically uh is used as an output for REU. So relu it's E equal to H and so this is like the uh the plots that we have for R. Now, you may be wondering, but why should we care about a rectified linear unit? Can't we just use the sigmoid function it's very nice like we, we are familiar with that. Why using relu, well, it turns out that relu is very, very effective for uh training.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=894s",
        "start_time": "894.08"
    },
    {
        "id": "6a580ddf",
        "text": "And so if H uh is M uh is less than zero, then relu outputs zero. If H is a greater or equal, equal or greater than zero, then H uh basically uh is used as an output for REU. So relu it's E equal to H and so this is like the uh the plots that we have for R. Now, you may be wondering, but why should we care about a rectified linear unit? Can't we just use the sigmoid function it's very nice like we, we are familiar with that. Why using relu, well, it turns out that relu is very, very effective for uh training. So it uh when compared with uh the Sigma function, it enables us to train a network way faster. So it enables to have like better convergence of the network. And one of the reasons why this is the case, it's because R",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=919s",
        "start_time": "919.26"
    },
    {
        "id": "f3f9a083",
        "text": "Now, you may be wondering, but why should we care about a rectified linear unit? Can't we just use the sigmoid function it's very nice like we, we are familiar with that. Why using relu, well, it turns out that relu is very, very effective for uh training. So it uh when compared with uh the Sigma function, it enables us to train a network way faster. So it enables to have like better convergence of the network. And one of the reasons why this is the case, it's because R uses the uh probability of having the so called vanishing gradient. Now, the vanishing gradient sounds like a scary thing. And indeed uh like it is for training purposes. But what is that? Well, so if you remember from our video on back propagation,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=944s",
        "start_time": "944.369"
    },
    {
        "id": "042dc5db",
        "text": "So it uh when compared with uh the Sigma function, it enables us to train a network way faster. So it enables to have like better convergence of the network. And one of the reasons why this is the case, it's because R uses the uh probability of having the so called vanishing gradient. Now, the vanishing gradient sounds like a scary thing. And indeed uh like it is for training purposes. But what is that? Well, so if you remember from our video on back propagation, so what happens like uh when we train a network is that we uh basically back propagates the error from the upper layer towards like the input layer, right. And so, and that happens at each hidden layer going back from output to input.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=962s",
        "start_time": "962.039"
    },
    {
        "id": "de5ba716",
        "text": "uses the uh probability of having the so called vanishing gradient. Now, the vanishing gradient sounds like a scary thing. And indeed uh like it is for training purposes. But what is that? Well, so if you remember from our video on back propagation, so what happens like uh when we train a network is that we uh basically back propagates the error from the upper layer towards like the input layer, right. And so, and that happens at each hidden layer going back from output to input. Now, every time we we have a new uh layer and we want to propagate the error to uh AAA layer towards like the left towards the beginning towards the inputs. Uh what happens is that we multiply uh uh like this value like by the",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=978s",
        "start_time": "978.914"
    },
    {
        "id": "b55b5e5a",
        "text": "so what happens like uh when we train a network is that we uh basically back propagates the error from the upper layer towards like the input layer, right. And so, and that happens at each hidden layer going back from output to input. Now, every time we we have a new uh layer and we want to propagate the error to uh AAA layer towards like the left towards the beginning towards the inputs. Uh what happens is that we multiply uh uh like this value like by the uh derivative of the activation function. And what happens with the Sigma function is that the derivative of the sigmoid function at most can be no 0.25. Which basically means if you keep multiplying there like the values that you are getting like the errors that you are propagating are getting like",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1000s",
        "start_time": "1000.289"
    },
    {
        "id": "cb842408",
        "text": "Now, every time we we have a new uh layer and we want to propagate the error to uh AAA layer towards like the left towards the beginning towards the inputs. Uh what happens is that we multiply uh uh like this value like by the uh derivative of the activation function. And what happens with the Sigma function is that the derivative of the sigmoid function at most can be no 0.25. Which basically means if you keep multiplying there like the values that you are getting like the errors that you are propagating are getting like smaller and smaller and smaller until they vanish. And so basically the gradient is vanishing. And if the gradient is like very, very small, then it's very difficult to train a network. Now with R we avoid all of these issues, which basically means we can have uh um architectures like network architectures that are super complex with many, many",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1017s",
        "start_time": "1017.51"
    },
    {
        "id": "c4d38c25",
        "text": "uh derivative of the activation function. And what happens with the Sigma function is that the derivative of the sigmoid function at most can be no 0.25. Which basically means if you keep multiplying there like the values that you are getting like the errors that you are propagating are getting like smaller and smaller and smaller until they vanish. And so basically the gradient is vanishing. And if the gradient is like very, very small, then it's very difficult to train a network. Now with R we avoid all of these issues, which basically means we can have uh um architectures like network architectures that are super complex with many, many uh layers. But in the end, we're not going to have an issue of vanishing gradient. Whereas if we used a sigmoid function, we would have that issue, right? And so this is the beauty of rectified linear unit or R. So let's go back to the code now. Cool. OK. So this was just like the first hidden layer. So we said that we want other two hidden layers. And so what we'll do",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1036s",
        "start_time": "1036.77"
    },
    {
        "id": "f2ca13ae",
        "text": "smaller and smaller and smaller until they vanish. And so basically the gradient is vanishing. And if the gradient is like very, very small, then it's very difficult to train a network. Now with R we avoid all of these issues, which basically means we can have uh um architectures like network architectures that are super complex with many, many uh layers. But in the end, we're not going to have an issue of vanishing gradient. Whereas if we used a sigmoid function, we would have that issue, right? And so this is the beauty of rectified linear unit or R. So let's go back to the code now. Cool. OK. So this was just like the first hidden layer. So we said that we want other two hidden layers. And so what we'll do uh is just like copy these guys a couple of times. But now you can see that I made a mistake that I make like all the times no matter how much time I spend with this stuff. So I sometimes forget like to add comments, right? OK. So here we have the second hidden layer and here we have the third hidden layer. Now",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1057s",
        "start_time": "1057.319"
    },
    {
        "id": "5778f195",
        "text": "uh layers. But in the end, we're not going to have an issue of vanishing gradient. Whereas if we used a sigmoid function, we would have that issue, right? And so this is the beauty of rectified linear unit or R. So let's go back to the code now. Cool. OK. So this was just like the first hidden layer. So we said that we want other two hidden layers. And so what we'll do uh is just like copy these guys a couple of times. But now you can see that I made a mistake that I make like all the times no matter how much time I spend with this stuff. So I sometimes forget like to add comments, right? OK. So here we have the second hidden layer and here we have the third hidden layer. Now uh here, let's say that we want 256 neurons. And here let's say we want 64 neurons, right? So now the last thing that remains to do to build this network is to create the output layer",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1084s",
        "start_time": "1084.119"
    },
    {
        "id": "e2eb8ca2",
        "text": "uh is just like copy these guys a couple of times. But now you can see that I made a mistake that I make like all the times no matter how much time I spend with this stuff. So I sometimes forget like to add comments, right? OK. So here we have the second hidden layer and here we have the third hidden layer. Now uh here, let's say that we want 256 neurons. And here let's say we want 64 neurons, right? So now the last thing that remains to do to build this network is to create the output layer and So again, uh this is uh another uh dense layer but here uh we are gonna use 10 neurons. And why are we using 10 neurons? Well, because we have 10 categories which are like the 10 genres that we want to uh split, like our uh predict our data set into. And it's these guys here. So if we go to the uh data uh JSON file,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1111s",
        "start_time": "1111.65"
    },
    {
        "id": "9471899a",
        "text": "uh here, let's say that we want 256 neurons. And here let's say we want 64 neurons, right? So now the last thing that remains to do to build this network is to create the output layer and So again, uh this is uh another uh dense layer but here uh we are gonna use 10 neurons. And why are we using 10 neurons? Well, because we have 10 categories which are like the 10 genres that we want to uh split, like our uh predict our data set into. And it's these guys here. So if we go to the uh data uh JSON file, so it's these guys here. So, disco, reggae, rock, pop, blues country and so on and so forth. Cool. So we have like this 10 neurons and then we use uh as the activation, we use a soft max",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1136s",
        "start_time": "1136.099"
    },
    {
        "id": "07517c75",
        "text": "and So again, uh this is uh another uh dense layer but here uh we are gonna use 10 neurons. And why are we using 10 neurons? Well, because we have 10 categories which are like the 10 genres that we want to uh split, like our uh predict our data set into. And it's these guys here. So if we go to the uh data uh JSON file, so it's these guys here. So, disco, reggae, rock, pop, blues country and so on and so forth. Cool. So we have like this 10 neurons and then we use uh as the activation, we use a soft max again, I forgot to put in the come over there, right? OK. So, so what's soft max? Well, soft max, it's a um an activation function that basically",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1152s",
        "start_time": "1152.219"
    },
    {
        "id": "470b7a2e",
        "text": "so it's these guys here. So, disco, reggae, rock, pop, blues country and so on and so forth. Cool. So we have like this 10 neurons and then we use uh as the activation, we use a soft max again, I forgot to put in the come over there, right? OK. So, so what's soft max? Well, soft max, it's a um an activation function that basically enables us to have. So if you sum the values associated to all the 10 neurons here, all the the output neurons you're gonna get one, it basically normalizes like the output for us. And then when we do predictions, so we predict. So we, we pick the neuron that has the highest value and that represents the category like we are predicting.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1180s",
        "start_time": "1180.699"
    },
    {
        "id": "14926332",
        "text": "again, I forgot to put in the come over there, right? OK. So, so what's soft max? Well, soft max, it's a um an activation function that basically enables us to have. So if you sum the values associated to all the 10 neurons here, all the the output neurons you're gonna get one, it basically normalizes like the output for us. And then when we do predictions, so we predict. So we, we pick the neuron that has the highest value and that represents the category like we are predicting. Cool. So uh with this, we built our network architecture. So now we need to move on to the next phase which is uh compiling uh the network. So if you guys remember the first thing that we want to do here is to uh decide which optimizer we want to use, right? And here uh we are gonna use Adam.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1197s",
        "start_time": "1197.709"
    },
    {
        "id": "0529d192",
        "text": "enables us to have. So if you sum the values associated to all the 10 neurons here, all the the output neurons you're gonna get one, it basically normalizes like the output for us. And then when we do predictions, so we predict. So we, we pick the neuron that has the highest value and that represents the category like we are predicting. Cool. So uh with this, we built our network architecture. So now we need to move on to the next phase which is uh compiling uh the network. So if you guys remember the first thing that we want to do here is to uh decide which optimizer we want to use, right? And here uh we are gonna use Adam. So, and let's specify the learning rate here and we could say 0.0001. OK. Cool. So uh Adam is a, an optimizer that it's basically like a an extension like a variation of like a stochastic gradient descent and it's very, very effective uh with deep learning. So we're gonna use this, then the next step that we want to do is a model dot uh compile.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1211s",
        "start_time": "1211.65"
    },
    {
        "id": "0e863efd",
        "text": "Cool. So uh with this, we built our network architecture. So now we need to move on to the next phase which is uh compiling uh the network. So if you guys remember the first thing that we want to do here is to uh decide which optimizer we want to use, right? And here uh we are gonna use Adam. So, and let's specify the learning rate here and we could say 0.0001. OK. Cool. So uh Adam is a, an optimizer that it's basically like a an extension like a variation of like a stochastic gradient descent and it's very, very effective uh with deep learning. So we're gonna use this, then the next step that we want to do is a model dot uh compile. And here uh we should pass in a few uh things, right? So yeah, let's start with the optimizer. So the optimizer we pass in our optimizer which is atom. So then uh we need to decide uh which uh loss function",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1237s",
        "start_time": "1237.53"
    },
    {
        "id": "b114c96f",
        "text": "So, and let's specify the learning rate here and we could say 0.0001. OK. Cool. So uh Adam is a, an optimizer that it's basically like a an extension like a variation of like a stochastic gradient descent and it's very, very effective uh with deep learning. So we're gonna use this, then the next step that we want to do is a model dot uh compile. And here uh we should pass in a few uh things, right? So yeah, let's start with the optimizer. So the optimizer we pass in our optimizer which is atom. So then uh we need to decide uh which uh loss function uh we want to use or error function we want to use. And uh for this problem which is a uh multi class classification problem, we are gonna use spots cate uh cross entropy,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1265s",
        "start_time": "1265.439"
    },
    {
        "id": "90a04712",
        "text": "And here uh we should pass in a few uh things, right? So yeah, let's start with the optimizer. So the optimizer we pass in our optimizer which is atom. So then uh we need to decide uh which uh loss function uh we want to use or error function we want to use. And uh for this problem which is a uh multi class classification problem, we are gonna use spots cate uh cross entropy, right. And uh finally, we can't specify like the, the metrics that we want to track. And here we could say uh accuracy, right? OK.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1295s",
        "start_time": "1295.729"
    },
    {
        "id": "5aff1fb9",
        "text": "uh we want to use or error function we want to use. And uh for this problem which is a uh multi class classification problem, we are gonna use spots cate uh cross entropy, right. And uh finally, we can't specify like the, the metrics that we want to track. And here we could say uh accuracy, right? OK. So this way we've basically compiled uh our network. So a nice thing we could do here is a model dot summary uh which basically will give us like a print of uh a kind of like a summary of the architecture of the network specify the number of parameters we have the layers. It's, it's a nice thing that you have when you, when you train uh like this stuff. Ok. So let me just like move this thing up.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1316s",
        "start_time": "1316.099"
    },
    {
        "id": "ece187c4",
        "text": "right. And uh finally, we can't specify like the, the metrics that we want to track. And here we could say uh accuracy, right? OK. So this way we've basically compiled uh our network. So a nice thing we could do here is a model dot summary uh which basically will give us like a print of uh a kind of like a summary of the architecture of the network specify the number of parameters we have the layers. It's, it's a nice thing that you have when you, when you train uh like this stuff. Ok. So let me just like move this thing up. Ok. So the final thing that remains to do here is uh training the network. So how do we do that? Well, we've done this like before and it can't be much easier than this. So, and it's basically doing a model uh dot Fit. And now we need to pass in the inputs train, the um",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1332s",
        "start_time": "1332.979"
    },
    {
        "id": "55d5719b",
        "text": "So this way we've basically compiled uh our network. So a nice thing we could do here is a model dot summary uh which basically will give us like a print of uh a kind of like a summary of the architecture of the network specify the number of parameters we have the layers. It's, it's a nice thing that you have when you, when you train uh like this stuff. Ok. So let me just like move this thing up. Ok. So the final thing that remains to do here is uh training the network. So how do we do that? Well, we've done this like before and it can't be much easier than this. So, and it's basically doing a model uh dot Fit. And now we need to pass in the inputs train, the um uh here we need the targets train. So we are basically passing the, the uh inputs and targets for the, the training split and then we'll do a validation data. So this is basically like our uh testing uh uh uh data set that we want to pass in.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1345s",
        "start_time": "1345.15"
    },
    {
        "id": "a1935749",
        "text": "Ok. So the final thing that remains to do here is uh training the network. So how do we do that? Well, we've done this like before and it can't be much easier than this. So, and it's basically doing a model uh dot Fit. And now we need to pass in the inputs train, the um uh here we need the targets train. So we are basically passing the, the uh inputs and targets for the, the training split and then we'll do a validation data. So this is basically like our uh testing uh uh uh data set that we want to pass in. And uh here we'll, we'll pass in the inputs test and",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1372s",
        "start_time": "1372.839"
    },
    {
        "id": "6052d32e",
        "text": "uh here we need the targets train. So we are basically passing the, the uh inputs and targets for the, the training split and then we'll do a validation data. So this is basically like our uh testing uh uh uh data set that we want to pass in. And uh here we'll, we'll pass in the inputs test and uh where is it?",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1397s",
        "start_time": "1397.849"
    },
    {
        "id": "0bad4dee",
        "text": "And uh here we'll, we'll pass in the inputs test and uh where is it? It's the targets test now. Yeah, this is becoming a little bit inconvenient to follow. So I'll just like do new lines here. Uh Right.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1417s",
        "start_time": "1417.489"
    },
    {
        "id": "fb796931",
        "text": "uh where is it? It's the targets test now. Yeah, this is becoming a little bit inconvenient to follow. So I'll just like do new lines here. Uh Right. Uh So the other stuff that we want to specify is the number of APO. And yeah, we could say, yeah, we'll have like 50 aex here. And finally, we'll specify the batch size and we'll put this like to 32. Now, you may be wondering, but what's the batch size? Well, this is like something very, very important.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1425s",
        "start_time": "1425.489"
    },
    {
        "id": "bd4ace9d",
        "text": "It's the targets test now. Yeah, this is becoming a little bit inconvenient to follow. So I'll just like do new lines here. Uh Right. Uh So the other stuff that we want to specify is the number of APO. And yeah, we could say, yeah, we'll have like 50 aex here. And finally, we'll specify the batch size and we'll put this like to 32. Now, you may be wondering, but what's the batch size? Well, this is like something very, very important. And for that reason, we're gonna take a look at this like in our slide presentation over here, right? There are a bunch of different types of batching which is basically like the way like we, we train like our network. So in a pro",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1428s",
        "start_time": "1428.4"
    },
    {
        "id": "51399b45",
        "text": "Uh So the other stuff that we want to specify is the number of APO. And yeah, we could say, yeah, we'll have like 50 aex here. And finally, we'll specify the batch size and we'll put this like to 32. Now, you may be wondering, but what's the batch size? Well, this is like something very, very important. And for that reason, we're gonna take a look at this like in our slide presentation over here, right? There are a bunch of different types of batching which is basically like the way like we, we train like our network. So in a pro this video uh on back propagation we and uh and stochastic gradient descent, we we we looked at a type of batching which is called like stochastic. So in this case, with stochastic, for example, stochastic gradient descent, what you do is you uh calculate the gradient after you've considered just like one sample. So just one segment of our uh like tracks, right?",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1439s",
        "start_time": "1439.65"
    },
    {
        "id": "dd3a2ef4",
        "text": "And for that reason, we're gonna take a look at this like in our slide presentation over here, right? There are a bunch of different types of batching which is basically like the way like we, we train like our network. So in a pro this video uh on back propagation we and uh and stochastic gradient descent, we we we looked at a type of batching which is called like stochastic. So in this case, with stochastic, for example, stochastic gradient descent, what you do is you uh calculate the gradient after you've considered just like one sample. So just one segment of our uh like tracks, right? So you, you do a fit forward and then you do a back propagation there. Uh you calculate the gradient and you update uh the weights directly. This is like very quick to perform, but it's kind of like very, very inaccurate because like there's a lot of noise and this would basically be equal to having like the batch size over here equal to one, right.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1465s",
        "start_time": "1465.77"
    },
    {
        "id": "2f7fac2d",
        "text": "this video uh on back propagation we and uh and stochastic gradient descent, we we we looked at a type of batching which is called like stochastic. So in this case, with stochastic, for example, stochastic gradient descent, what you do is you uh calculate the gradient after you've considered just like one sample. So just one segment of our uh like tracks, right? So you, you do a fit forward and then you do a back propagation there. Uh you calculate the gradient and you update uh the weights directly. This is like very quick to perform, but it's kind of like very, very inaccurate because like there's a lot of noise and this would basically be equal to having like the batch size over here equal to one, right. Uh Now uh we have like the the opposite, which is basically you consider the the full batch. So you compute the gradient. So you, you update the weights on the whole training set. So you pass in the whole training set and only at that point uh you, you, you calculate the gradient.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1482s",
        "start_time": "1482.655"
    },
    {
        "id": "4fab1834",
        "text": "So you, you do a fit forward and then you do a back propagation there. Uh you calculate the gradient and you update uh the weights directly. This is like very quick to perform, but it's kind of like very, very inaccurate because like there's a lot of noise and this would basically be equal to having like the batch size over here equal to one, right. Uh Now uh we have like the the opposite, which is basically you consider the the full batch. So you compute the gradient. So you, you update the weights on the whole training set. So you pass in the whole training set and only at that point uh you, you, you calculate the gradient. Um this is problematic for deep learning because like we have usually huge, huge data sets. So this results in something that's super slow, it's super memory intensive and for all purposes and needs like it's actually impractical. But the great thing about this is that it's actually very accurate, right? Because we are calculating the grade on many, many uh samples on the whole samples, right?",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1509s",
        "start_time": "1509.43"
    },
    {
        "id": "050f9402",
        "text": "Uh Now uh we have like the the opposite, which is basically you consider the the full batch. So you compute the gradient. So you, you update the weights on the whole training set. So you pass in the whole training set and only at that point uh you, you, you calculate the gradient. Um this is problematic for deep learning because like we have usually huge, huge data sets. So this results in something that's super slow, it's super memory intensive and for all purposes and needs like it's actually impractical. But the great thing about this is that it's actually very accurate, right? Because we are calculating the grade on many, many uh samples on the whole samples, right? And for food batch, you basically have uh one pass, which is uh just like one epoch because we're passing the whole uh training set through uh the network for training purposes. Now, there's a middle ground there and it's called a mini batch. And here the idea is to basically compute the gradient on a subset of the data set, right?",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1533s",
        "start_time": "1533.459"
    },
    {
        "id": "18b7f696",
        "text": "Um this is problematic for deep learning because like we have usually huge, huge data sets. So this results in something that's super slow, it's super memory intensive and for all purposes and needs like it's actually impractical. But the great thing about this is that it's actually very accurate, right? Because we are calculating the grade on many, many uh samples on the whole samples, right? And for food batch, you basically have uh one pass, which is uh just like one epoch because we're passing the whole uh training set through uh the network for training purposes. Now, there's a middle ground there and it's called a mini batch. And here the idea is to basically compute the gradient on a subset of the data set, right? And we can consider like 1632 64 like samples. And then once we've considered those, we can actually calculate the gradient uh at that point. And then yeah, that propagates the error and uh and updates the weights. And so we are doing training on, on like some uh like mini batches, right. OK. So",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1556s",
        "start_time": "1556.3"
    },
    {
        "id": "b58b531e",
        "text": "And for food batch, you basically have uh one pass, which is uh just like one epoch because we're passing the whole uh training set through uh the network for training purposes. Now, there's a middle ground there and it's called a mini batch. And here the idea is to basically compute the gradient on a subset of the data set, right? And we can consider like 1632 64 like samples. And then once we've considered those, we can actually calculate the gradient uh at that point. And then yeah, that propagates the error and uh and updates the weights. And so we are doing training on, on like some uh like mini batches, right. OK. So in this uh with mini batch, usually you would use like from 16 to 100 and 28 samples. But then this is by no means like a universal rule rather uh it just like depends on the type of problem that you are tackling,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1583s",
        "start_time": "1583.839"
    },
    {
        "id": "0f8f6cbd",
        "text": "And we can consider like 1632 64 like samples. And then once we've considered those, we can actually calculate the gradient uh at that point. And then yeah, that propagates the error and uh and updates the weights. And so we are doing training on, on like some uh like mini batches, right. OK. So in this uh with mini batch, usually you would use like from 16 to 100 and 28 samples. But then this is by no means like a universal rule rather uh it just like depends on the type of problem that you are tackling, right? And the great thing about mini batch is that it's really like the best of the two worlds like it's kind of like relatively uh yeah, it's quick, it's not that memory intensive and it's quite accurate. So this is like the solution that we use like in deep in deep learning like the most. So now let's go back to the code,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1611s",
        "start_time": "1611.449"
    },
    {
        "id": "6cb5261b",
        "text": "in this uh with mini batch, usually you would use like from 16 to 100 and 28 samples. But then this is by no means like a universal rule rather uh it just like depends on the type of problem that you are tackling, right? And the great thing about mini batch is that it's really like the best of the two worlds like it's kind of like relatively uh yeah, it's quick, it's not that memory intensive and it's quite accurate. So this is like the solution that we use like in deep in deep learning like the most. So now let's go back to the code, right. So this batch size, it's basically um specifying the number of like samples that we want in our batch for uh before like we we we calculate the gradient, right? And we refer to this is like a quite customary uh like value as I was mentioning. Cool. Well, I think like we we are basically done here, right? So we so just like to",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1636s",
        "start_time": "1636.53"
    },
    {
        "id": "caa670de",
        "text": "right? And the great thing about mini batch is that it's really like the best of the two worlds like it's kind of like relatively uh yeah, it's quick, it's not that memory intensive and it's quite accurate. So this is like the solution that we use like in deep in deep learning like the most. So now let's go back to the code, right. So this batch size, it's basically um specifying the number of like samples that we want in our batch for uh before like we we we calculate the gradient, right? And we refer to this is like a quite customary uh like value as I was mentioning. Cool. Well, I think like we we are basically done here, right? So we so just like to uh review all of this. So we load the data, we split the data into training and uh test sets, we build, we build like our network architecture. We've compiled the network, we we have a nice model summary here and now we are ready to train the network. So now if there are no mistakes in my code, which I hope it's the case we should be able to train the network. So let's run the script and see what happens.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1652s",
        "start_time": "1652.4"
    },
    {
        "id": "de991ee5",
        "text": "right. So this batch size, it's basically um specifying the number of like samples that we want in our batch for uh before like we we we calculate the gradient, right? And we refer to this is like a quite customary uh like value as I was mentioning. Cool. Well, I think like we we are basically done here, right? So we so just like to uh review all of this. So we load the data, we split the data into training and uh test sets, we build, we build like our network architecture. We've compiled the network, we we have a nice model summary here and now we are ready to train the network. So now if there are no mistakes in my code, which I hope it's the case we should be able to train the network. So let's run the script and see what happens. OK? So it's taking a little bit of time because obviously like it's uh loading uh the data there.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1672s",
        "start_time": "1672.969"
    },
    {
        "id": "71b33256",
        "text": "uh review all of this. So we load the data, we split the data into training and uh test sets, we build, we build like our network architecture. We've compiled the network, we we have a nice model summary here and now we are ready to train the network. So now if there are no mistakes in my code, which I hope it's the case we should be able to train the network. So let's run the script and see what happens. OK? So it's taking a little bit of time because obviously like it's uh loading uh the data there. So",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1696s",
        "start_time": "1696.91"
    },
    {
        "id": "8453a7a1",
        "text": "OK? So it's taking a little bit of time because obviously like it's uh loading uh the data there. So let's see.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1727s",
        "start_time": "1727.479"
    },
    {
        "id": "718a6432",
        "text": "So let's see. Yeah, here we go. OK. It's working. So here, as you see, we have the, the model summary and we have like this flatten over here, then we have the dense layer and here we have the associated number of parameters on here. Yeah, it's a nice thing like you have like overall. So now uh here, as you see, we are tracking",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1735s",
        "start_time": "1735.369"
    },
    {
        "id": "a64ba9e9",
        "text": "let's see. Yeah, here we go. OK. It's working. So here, as you see, we have the, the model summary and we have like this flatten over here, then we have the dense layer and here we have the associated number of parameters on here. Yeah, it's a nice thing like you have like overall. So now uh here, as you see, we are tracking uh the the different epochs over here and here like we get like an output which, which gives us for each epoch the accuracy on the training set and the accuracy on, on the test set. You, you, you might start uh like and also like the the loss like for the uh training set and for like this uh test set.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1737s",
        "start_time": "1737.329"
    },
    {
        "id": "934f27f7",
        "text": "Yeah, here we go. OK. It's working. So here, as you see, we have the, the model summary and we have like this flatten over here, then we have the dense layer and here we have the associated number of parameters on here. Yeah, it's a nice thing like you have like overall. So now uh here, as you see, we are tracking uh the the different epochs over here and here like we get like an output which, which gives us for each epoch the accuracy on the training set and the accuracy on, on the test set. You, you, you might start uh like and also like the the loss like for the uh training set and for like this uh test set. So you may start seeing uh an issue uh like arising here. So take a look at the accuracy. Yeah, I'm just like, yeah, so let's wait like for this like to uh to end to finish uh before like we comment on that because every time I can move this, yeah, there's a new book and it gets just like, ah yeah, move back like to to yeah, to the start point. Well, right. But we are done.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1739s",
        "start_time": "1739.469"
    },
    {
        "id": "4e15da84",
        "text": "uh the the different epochs over here and here like we get like an output which, which gives us for each epoch the accuracy on the training set and the accuracy on, on the test set. You, you, you might start uh like and also like the the loss like for the uh training set and for like this uh test set. So you may start seeing uh an issue uh like arising here. So take a look at the accuracy. Yeah, I'm just like, yeah, so let's wait like for this like to uh to end to finish uh before like we comment on that because every time I can move this, yeah, there's a new book and it gets just like, ah yeah, move back like to to yeah, to the start point. Well, right. But we are done. So let's take a look at the accuracy. So here it's after 50 epochs. So uh here we have the loss for uh like the uh calculated on the training set, which is quite low. And here we have the accuracy which is fantastic. Well, we have almost like 97% accuracy, which is incredibly good. But is it really that good? Let's take a look at the",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1761s",
        "start_time": "1761.969"
    },
    {
        "id": "73fd16e3",
        "text": "So you may start seeing uh an issue uh like arising here. So take a look at the accuracy. Yeah, I'm just like, yeah, so let's wait like for this like to uh to end to finish uh before like we comment on that because every time I can move this, yeah, there's a new book and it gets just like, ah yeah, move back like to to yeah, to the start point. Well, right. But we are done. So let's take a look at the accuracy. So here it's after 50 epochs. So uh here we have the loss for uh like the uh calculated on the training set, which is quite low. And here we have the accuracy which is fantastic. Well, we have almost like 97% accuracy, which is incredibly good. But is it really that good? Let's take a look at the at the accuracy and the loss calculated on the um test set. Well, it turns out that there's a huge difference between this accuracy on the test set and the accuracy on the training set. Well, it's basically almost like 30% different. Uh well, uh",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1790s",
        "start_time": "1790.119"
    },
    {
        "id": "931f56f4",
        "text": "So let's take a look at the accuracy. So here it's after 50 epochs. So uh here we have the loss for uh like the uh calculated on the training set, which is quite low. And here we have the accuracy which is fantastic. Well, we have almost like 97% accuracy, which is incredibly good. But is it really that good? Let's take a look at the at the accuracy and the loss calculated on the um test set. Well, it turns out that there's a huge difference between this accuracy on the test set and the accuracy on the training set. Well, it's basically almost like 30% different. Uh well, uh uh 89. Well, it's, it's more, it's more, it's almost like 40% like difference there, which is incredible, right? So what's happening here? So we have like the, the training and like the the model performing extremely well, like on the training data but performing not that great, like on the um on the test set.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1817s",
        "start_time": "1817.65"
    },
    {
        "id": "61328eef",
        "text": "at the accuracy and the loss calculated on the um test set. Well, it turns out that there's a huge difference between this accuracy on the test set and the accuracy on the training set. Well, it's basically almost like 30% different. Uh well, uh uh 89. Well, it's, it's more, it's more, it's almost like 40% like difference there, which is incredible, right? So what's happening here? So we have like the, the training and like the the model performing extremely well, like on the training data but performing not that great, like on the um on the test set. Uh why, why, why is that the case? Well, it turns out we are over fitting. So basically, what's happening is that we are so like the model is tailoring, uh its weight in order to predict uh uh in a very like compelling way",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1846s",
        "start_time": "1846.55"
    },
    {
        "id": "ed03d1a0",
        "text": "uh 89. Well, it's, it's more, it's more, it's almost like 40% like difference there, which is incredible, right? So what's happening here? So we have like the, the training and like the the model performing extremely well, like on the training data but performing not that great, like on the um on the test set. Uh why, why, why is that the case? Well, it turns out we are over fitting. So basically, what's happening is that we are so like the model is tailoring, uh its weight in order to predict uh uh in a very like compelling way uh the the test set, but it's not really that able to generalize to data it has never seen before. So this is a huge, huge problem. And every time you do any type of machine learning, not just deep learning, this is something you have to fight against overfitting now. So",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1870s",
        "start_time": "1870.56"
    },
    {
        "id": "7cd61f0e",
        "text": "Uh why, why, why is that the case? Well, it turns out we are over fitting. So basically, what's happening is that we are so like the model is tailoring, uh its weight in order to predict uh uh in a very like compelling way uh the the test set, but it's not really that able to generalize to data it has never seen before. So this is a huge, huge problem. And every time you do any type of machine learning, not just deep learning, this is something you have to fight against overfitting now. So in the next video, we are gonna look at how to uh identify uh like overfitting, like in our um models and how to fight against overfitting. So we're gonna see a bunch of different uh techniques that we can use like drop out or um regularization uh just like to, to avoid uh overfitting,",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1893s",
        "start_time": "1893.5"
    },
    {
        "id": "a08f4727",
        "text": "uh the the test set, but it's not really that able to generalize to data it has never seen before. So this is a huge, huge problem. And every time you do any type of machine learning, not just deep learning, this is something you have to fight against overfitting now. So in the next video, we are gonna look at how to uh identify uh like overfitting, like in our um models and how to fight against overfitting. So we're gonna see a bunch of different uh techniques that we can use like drop out or um regularization uh just like to, to avoid uh overfitting, right? But for now, for this video, we are basically done and you should be like super happy because now we have a music genre classifier and it's not like the best classifier ever. And we still have to so overfitting. But here we have really all the fundamentals of our music genre classifier. Great. So I hope you've enjoyed this video.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1914s",
        "start_time": "1914.189"
    },
    {
        "id": "3baab254",
        "text": "in the next video, we are gonna look at how to uh identify uh like overfitting, like in our um models and how to fight against overfitting. So we're gonna see a bunch of different uh techniques that we can use like drop out or um regularization uh just like to, to avoid uh overfitting, right? But for now, for this video, we are basically done and you should be like super happy because now we have a music genre classifier and it's not like the best classifier ever. And we still have to so overfitting. But here we have really all the fundamentals of our music genre classifier. Great. So I hope you've enjoyed this video. If that's it, remember to subscribe and uh hit the notification bell. You'll never miss a new video when I upload them. And if you have any questions, please like leave a comment uh below and as always, I hope to see you next time. Cheers.",
        "video": "13- Implementing a neural network for music genre classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "_xcFAiufwd0",
        "youtube_link": "https://www.youtube.com/watch?v=_xcFAiufwd0&t=1934s",
        "start_time": "1934.959"
    },
    {
        "id": "8cf7e8a3",
        "text": "Hi, everybody and welcome to another video in the Deep Learning for audio with Python series. This time you are going to preprocess audio data and get it ready for our deep learning applications. So, and specifically, we're gonna look into how to visualize and learn waveforms, how to perform fourier transforms for getting spectrums, how we get spectrograms and how we extract MF CCS. Now, if all of this sounds like gibberish, you should definitely like watch uh my previous video where I cover like the theoretical side of all of these things. But uh let's just like get started. So we're not gonna build like any of these algorithms for like performing fourier transforms or extracting MF CCS from scratch. But rather we're gonna rely on a great audio analysis uh library called Libros. And so first thing we wanna do its import uh Libros.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "e2527a55",
        "text": "and specifically, we're gonna look into how to visualize and learn waveforms, how to perform fourier transforms for getting spectrums, how we get spectrograms and how we extract MF CCS. Now, if all of this sounds like gibberish, you should definitely like watch uh my previous video where I cover like the theoretical side of all of these things. But uh let's just like get started. So we're not gonna build like any of these algorithms for like performing fourier transforms or extracting MF CCS from scratch. But rather we're gonna rely on a great audio analysis uh library called Libros. And so first thing we wanna do its import uh Libros. So, uh and as long as uh we have like not just like Libros itself but also Li Brusa dot display uh which is a nice API for visualizing uh data like spectrograms. Uh So Libres dot display is built on top of uh Maple Lib and so we want to uh import also, uh, maple lib dot PP and we'll import it as PLT. Cool.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=12s",
        "start_time": "12.22"
    },
    {
        "id": "be6bff7c",
        "text": "So we're not gonna build like any of these algorithms for like performing fourier transforms or extracting MF CCS from scratch. But rather we're gonna rely on a great audio analysis uh library called Libros. And so first thing we wanna do its import uh Libros. So, uh and as long as uh we have like not just like Libros itself but also Li Brusa dot display uh which is a nice API for visualizing uh data like spectrograms. Uh So Libres dot display is built on top of uh Maple Lib and so we want to uh import also, uh, maple lib dot PP and we'll import it as PLT. Cool. So, uh, the first thing that we want to do now is just like to, to get a file so to get an audio file. So, and I have a very nice one which is called blues 0.00000 dot wav. And yeah, let's take a look at that. So that you have an idea of what, like we'll be working on.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=38s",
        "start_time": "38.02"
    },
    {
        "id": "522b7fd8",
        "text": "So, uh and as long as uh we have like not just like Libros itself but also Li Brusa dot display uh which is a nice API for visualizing uh data like spectrograms. Uh So Libres dot display is built on top of uh Maple Lib and so we want to uh import also, uh, maple lib dot PP and we'll import it as PLT. Cool. So, uh, the first thing that we want to do now is just like to, to get a file so to get an audio file. So, and I have a very nice one which is called blues 0.00000 dot wav. And yeah, let's take a look at that. So that you have an idea of what, like we'll be working on. Yeah, you get a, a uh you get the idea here. It's a, a nice, like blues song. It's just like 30 seconds of that song. Cool. So the first thing that we wanna do is load uh like this audio file. So uh for doing that, we'll call libros dot",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=57s",
        "start_time": "57.119"
    },
    {
        "id": "6cda2340",
        "text": "So, uh, the first thing that we want to do now is just like to, to get a file so to get an audio file. So, and I have a very nice one which is called blues 0.00000 dot wav. And yeah, let's take a look at that. So that you have an idea of what, like we'll be working on. Yeah, you get a, a uh you get the idea here. It's a, a nice, like blues song. It's just like 30 seconds of that song. Cool. So the first thing that we wanna do is load uh like this audio file. So uh for doing that, we'll call libros dot load and we'll pass in the, the path. So the file and uh we also want to specify the sum we want to load uh like this audio file with and we'll specify 22,000 and uh 50. And this is uh like perfectly fine, like when we uh work and analyze like audio uh data. Cool.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=86s",
        "start_time": "86.4"
    },
    {
        "id": "b23c801e",
        "text": "Yeah, you get a, a uh you get the idea here. It's a, a nice, like blues song. It's just like 30 seconds of that song. Cool. So the first thing that we wanna do is load uh like this audio file. So uh for doing that, we'll call libros dot load and we'll pass in the, the path. So the file and uh we also want to specify the sum we want to load uh like this audio file with and we'll specify 22,000 and uh 50. And this is uh like perfectly fine, like when we uh work and analyze like audio uh data. Cool. And uh so here as a result, we are gonna get like a signal and a sample rate. Now, the signal is gonna be a nin pi array, one dimensional array and uh it's gonna contain uh a number of like values that's equal to the sample rate are multiplied by the duration T",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=118s",
        "start_time": "118.849"
    },
    {
        "id": "aeb029af",
        "text": "load and we'll pass in the, the path. So the file and uh we also want to specify the sum we want to load uh like this audio file with and we'll specify 22,000 and uh 50. And this is uh like perfectly fine, like when we uh work and analyze like audio uh data. Cool. And uh so here as a result, we are gonna get like a signal and a sample rate. Now, the signal is gonna be a nin pi array, one dimensional array and uh it's gonna contain uh a number of like values that's equal to the sample rate are multiplied by the duration T uh of the uh of the song. So, in this case, we are looking at 2 22,050 multiplied by 30 seconds. So basically, like the signal array is gonna have more than 600,000 values. And at each of these values, you're gonna have the amplitude of the, of the waveform uh good. OK. So now let's try to visualize this waveform.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=138s",
        "start_time": "138.08"
    },
    {
        "id": "92b3baa0",
        "text": "And uh so here as a result, we are gonna get like a signal and a sample rate. Now, the signal is gonna be a nin pi array, one dimensional array and uh it's gonna contain uh a number of like values that's equal to the sample rate are multiplied by the duration T uh of the uh of the song. So, in this case, we are looking at 2 22,050 multiplied by 30 seconds. So basically, like the signal array is gonna have more than 600,000 values. And at each of these values, you're gonna have the amplitude of the, of the waveform uh good. OK. So now let's try to visualize this waveform. And uh for doing that, uh we can easily use Lisa dot display dot wave plots. And here in the wave plots, we want to specify the signal that we want to use and the sample rate and the sample rate uh it's equal to this thing over here. So 22,050",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=165s",
        "start_time": "165.919"
    },
    {
        "id": "2aad3611",
        "text": "uh of the uh of the song. So, in this case, we are looking at 2 22,050 multiplied by 30 seconds. So basically, like the signal array is gonna have more than 600,000 values. And at each of these values, you're gonna have the amplitude of the, of the waveform uh good. OK. So now let's try to visualize this waveform. And uh for doing that, uh we can easily use Lisa dot display dot wave plots. And here in the wave plots, we want to specify the signal that we want to use and the sample rate and the sample rate uh it's equal to this thing over here. So 22,050 good. So next thing we wanna do is we want to uh specify the uh label for the X and Y axis. So for the X axis, we are",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=192s",
        "start_time": "192.08"
    },
    {
        "id": "861e43b7",
        "text": "And uh for doing that, uh we can easily use Lisa dot display dot wave plots. And here in the wave plots, we want to specify the signal that we want to use and the sample rate and the sample rate uh it's equal to this thing over here. So 22,050 good. So next thing we wanna do is we want to uh specify the uh label for the X and Y axis. So for the X axis, we are expecting obviously time and for the Y axis, we have amplitude",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=222s",
        "start_time": "222.0"
    },
    {
        "id": "709b0072",
        "text": "good. So next thing we wanna do is we want to uh specify the uh label for the X and Y axis. So for the X axis, we are expecting obviously time and for the Y axis, we have amplitude nice and the final thing",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=249s",
        "start_time": "249.86"
    },
    {
        "id": "2dd2765d",
        "text": "expecting obviously time and for the Y axis, we have amplitude nice and the final thing we want to",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=262s",
        "start_time": "262.91"
    },
    {
        "id": "dcf16434",
        "text": "nice and the final thing we want to uh show uh this plot. And so we're gonna do a plot dot show. So if everything is correct, so we should be see, we should, we should be able to see our",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=271s",
        "start_time": "271.869"
    },
    {
        "id": "9366fe3f",
        "text": "we want to uh show uh this plot. And so we're gonna do a plot dot show. So if everything is correct, so we should be see, we should, we should be able to see our a nice plot and here we have it",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=275s",
        "start_time": "275.94"
    },
    {
        "id": "96f9351f",
        "text": "uh show uh this plot. And so we're gonna do a plot dot show. So if everything is correct, so we should be see, we should, we should be able to see our a nice plot and here we have it nice. So we have our nice uh waveform over here. And as you can see, the waveform tends to remain quite stable throughout the 32nd of this musical passage",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=277s",
        "start_time": "277.7"
    },
    {
        "id": "72daf36d",
        "text": "a nice plot and here we have it nice. So we have our nice uh waveform over here. And as you can see, the waveform tends to remain quite stable throughout the 32nd of this musical passage cool. So now uh the next step is moving from the time domain. So from the the waveform uh towards the frequency domain. And to do that, we need to perform a fast four A transform. And now for performing that,",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=289s",
        "start_time": "289.109"
    },
    {
        "id": "fe835584",
        "text": "nice. So we have our nice uh waveform over here. And as you can see, the waveform tends to remain quite stable throughout the 32nd of this musical passage cool. So now uh the next step is moving from the time domain. So from the the waveform uh towards the frequency domain. And to do that, we need to perform a fast four A transform. And now for performing that, we're gonna use NP. So we'll do an import NPI as MP.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=292s",
        "start_time": "292.529"
    },
    {
        "id": "513b8e3a",
        "text": "cool. So now uh the next step is moving from the time domain. So from the the waveform uh towards the frequency domain. And to do that, we need to perform a fast four A transform. And now for performing that, we're gonna use NP. So we'll do an import NPI as MP. So, so we'll do a FFT, it's equal to NP dot FFT dot FFT. And uh we'll uh pass in the signal,",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=306s",
        "start_time": "306.48"
    },
    {
        "id": "33db46dd",
        "text": "we're gonna use NP. So we'll do an import NPI as MP. So, so we'll do a FFT, it's equal to NP dot FFT dot FFT. And uh we'll uh pass in the signal, right? And so what we expect here is a uh an umpire array, one dimensional array which has as many uh values as the total number of samples we have in the, in the waveform. So it's more or less like this value here. So uh 600,000 plus and at each of those values, uh we have a complex value. Now,",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=323s",
        "start_time": "323.829"
    },
    {
        "id": "41ead8ad",
        "text": "So, so we'll do a FFT, it's equal to NP dot FFT dot FFT. And uh we'll uh pass in the signal, right? And so what we expect here is a uh an umpire array, one dimensional array which has as many uh values as the total number of samples we have in the, in the waveform. So it's more or less like this value here. So uh 600,000 plus and at each of those values, uh we have a complex value. Now, uh I don't want to like get into the details of how we get there because like it's completely outside the scope like of and it's not needed like for deep learning. But what we want to do is we want to move like from that complex value and get the amplitude of those values and or or sorry, get the magnitude of this value and for getting the magnitude,",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=329s",
        "start_time": "329.589"
    },
    {
        "id": "3b6baa9c",
        "text": "right? And so what we expect here is a uh an umpire array, one dimensional array which has as many uh values as the total number of samples we have in the, in the waveform. So it's more or less like this value here. So uh 600,000 plus and at each of those values, uh we have a complex value. Now, uh I don't want to like get into the details of how we get there because like it's completely outside the scope like of and it's not needed like for deep learning. But what we want to do is we want to move like from that complex value and get the amplitude of those values and or or sorry, get the magnitude of this value and for getting the magnitude, uh what we do is we call nimai dots absolute value and we pass in FFT. So basically, we are performing the absolute value on the complex values and then we end up with these magnitudes and these magnitudes indicate the",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=343s",
        "start_time": "343.29"
    },
    {
        "id": "344e9ee0",
        "text": "uh I don't want to like get into the details of how we get there because like it's completely outside the scope like of and it's not needed like for deep learning. But what we want to do is we want to move like from that complex value and get the amplitude of those values and or or sorry, get the magnitude of this value and for getting the magnitude, uh what we do is we call nimai dots absolute value and we pass in FFT. So basically, we are performing the absolute value on the complex values and then we end up with these magnitudes and these magnitudes indicate the contribution of each frequency bin to the overall sound. And so, and we want to map them onto like the, the relative like frequency bins, right? And for doing that, we'll do a frequent frequency, it's equal to NP five dots uh lens space.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=370s",
        "start_time": "370.04"
    },
    {
        "id": "7603dcf1",
        "text": "uh what we do is we call nimai dots absolute value and we pass in FFT. So basically, we are performing the absolute value on the complex values and then we end up with these magnitudes and these magnitudes indicate the contribution of each frequency bin to the overall sound. And so, and we want to map them onto like the, the relative like frequency bins, right? And for doing that, we'll do a frequent frequency, it's equal to NP five dots uh lens space. And A L space is a nice function that uh gives us a number of evenly spaced numbers in an interval,",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=397s",
        "start_time": "397.07"
    },
    {
        "id": "cd36dfbf",
        "text": "contribution of each frequency bin to the overall sound. And so, and we want to map them onto like the, the relative like frequency bins, right? And for doing that, we'll do a frequent frequency, it's equal to NP five dots uh lens space. And A L space is a nice function that uh gives us a number of evenly spaced numbers in an interval, right. And so here the, the uh frequency interval that we want to consider is between zero Hertz and the sample rate itself. And uh the number of like evenly paced uh values that we want and it's equal to the length of magnitude.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=417s",
        "start_time": "417.869"
    },
    {
        "id": "0755835b",
        "text": "And A L space is a nice function that uh gives us a number of evenly spaced numbers in an interval, right. And so here the, the uh frequency interval that we want to consider is between zero Hertz and the sample rate itself. And uh the number of like evenly paced uh values that we want and it's equal to the length of magnitude. And so basically, we have like these two arrays and magnitude has like the values. So the the actual like magnitudes of each frequency bin. And so it's basically so like these two like rays together are telling us how much each frequency is contributing to the overall uh sound. OK. So now let's plot this and uh for plotting this, which by the way is the power spectrum,",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=442s",
        "start_time": "442.279"
    },
    {
        "id": "e40f361f",
        "text": "right. And so here the, the uh frequency interval that we want to consider is between zero Hertz and the sample rate itself. And uh the number of like evenly paced uh values that we want and it's equal to the length of magnitude. And so basically, we have like these two arrays and magnitude has like the values. So the the actual like magnitudes of each frequency bin. And so it's basically so like these two like rays together are telling us how much each frequency is contributing to the overall uh sound. OK. So now let's plot this and uh for plotting this, which by the way is the power spectrum, uh we don't have like a fancy uh Li Brosa like shortcut function rather, we should use vanilla uh mat plot lib. So we'll do plots dot uh plot and we'll pass in the frequency as well as the uh magnet. And then, yeah, I guess we want to pass the, the labels as well. So on the X label given, we are in the frequency domain, we are expecting",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=453s",
        "start_time": "453.299"
    },
    {
        "id": "483d6b11",
        "text": "And so basically, we have like these two arrays and magnitude has like the values. So the the actual like magnitudes of each frequency bin. And so it's basically so like these two like rays together are telling us how much each frequency is contributing to the overall uh sound. OK. So now let's plot this and uh for plotting this, which by the way is the power spectrum, uh we don't have like a fancy uh Li Brosa like shortcut function rather, we should use vanilla uh mat plot lib. So we'll do plots dot uh plot and we'll pass in the frequency as well as the uh magnet. And then, yeah, I guess we want to pass the, the labels as well. So on the X label given, we are in the frequency domain, we are expecting frequencies, it's frequency. And on the y uh uh label, we are expecting magnitudes magnitude plot to show um it's all good. But before uh running the scripts, let me just like comment this out so that we are gonna have just one plot, the one we are interested in. OK. So let's run this and hopefully we have our power spectrum.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=475s",
        "start_time": "475.029"
    },
    {
        "id": "855c40c3",
        "text": "uh we don't have like a fancy uh Li Brosa like shortcut function rather, we should use vanilla uh mat plot lib. So we'll do plots dot uh plot and we'll pass in the frequency as well as the uh magnet. And then, yeah, I guess we want to pass the, the labels as well. So on the X label given, we are in the frequency domain, we are expecting frequencies, it's frequency. And on the y uh uh label, we are expecting magnitudes magnitude plot to show um it's all good. But before uh running the scripts, let me just like comment this out so that we are gonna have just one plot, the one we are interested in. OK. So let's run this and hopefully we have our power spectrum. That's great. And as you can see, most of the energy is concentrated in the lower frequencies and the higher we go with the frequencies and the less energy, the less contribution they will uh give us. Now, let's take a look at this um uh plot and there's when we analyze it, there's a very curious thing which is",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=504s",
        "start_time": "504.88"
    },
    {
        "id": "5278b457",
        "text": "frequencies, it's frequency. And on the y uh uh label, we are expecting magnitudes magnitude plot to show um it's all good. But before uh running the scripts, let me just like comment this out so that we are gonna have just one plot, the one we are interested in. OK. So let's run this and hopefully we have our power spectrum. That's great. And as you can see, most of the energy is concentrated in the lower frequencies and the higher we go with the frequencies and the less energy, the less contribution they will uh give us. Now, let's take a look at this um uh plot and there's when we analyze it, there's a very curious thing which is the plot is symmetrical and it's the, the, the kind of like point of symmetry here is the half of the plot which represents like half of the sample rate.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=534s",
        "start_time": "534.179"
    },
    {
        "id": "cff31536",
        "text": "That's great. And as you can see, most of the energy is concentrated in the lower frequencies and the higher we go with the frequencies and the less energy, the less contribution they will uh give us. Now, let's take a look at this um uh plot and there's when we analyze it, there's a very curious thing which is the plot is symmetrical and it's the, the, the kind of like point of symmetry here is the half of the plot which represents like half of the sample rate. Now this is like a property of the fourier transform. And that can be explained with a concept from DS P which is the um Nyquist theorem. I'm not going to get into those details again because we don't need them. But what we need to understand is that we don't need the whole plot because basically the only part of the plot that's bringing us like uh novel uh information is the, the first half, right, the left uh most half.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=560s",
        "start_time": "560.71"
    },
    {
        "id": "6787aa9e",
        "text": "the plot is symmetrical and it's the, the, the kind of like point of symmetry here is the half of the plot which represents like half of the sample rate. Now this is like a property of the fourier transform. And that can be explained with a concept from DS P which is the um Nyquist theorem. I'm not going to get into those details again because we don't need them. But what we need to understand is that we don't need the whole plot because basically the only part of the plot that's bringing us like uh novel uh information is the, the first half, right, the left uh most half. And that's because like once we, we cross half the frequency here, we're just like repeating uh like the, the same like information. So we just want to focus on the first half. So let's, let's do that here. So uh we can just like go back here and say that we want the left frequency",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=586s",
        "start_time": "586.21"
    },
    {
        "id": "8b6380b1",
        "text": "Now this is like a property of the fourier transform. And that can be explained with a concept from DS P which is the um Nyquist theorem. I'm not going to get into those details again because we don't need them. But what we need to understand is that we don't need the whole plot because basically the only part of the plot that's bringing us like uh novel uh information is the, the first half, right, the left uh most half. And that's because like once we, we cross half the frequency here, we're just like repeating uh like the, the same like information. So we just want to focus on the first half. So let's, let's do that here. So uh we can just like go back here and say that we want the left frequency frequency and this is gonna be equal to frequency. And uh we'll just like consider like from like the zero index to like half. And that we can express by saying this is equal to end of the length of frequency itself. And this is like divided by two. So we are",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=599s",
        "start_time": "599.409"
    },
    {
        "id": "d2450eb1",
        "text": "And that's because like once we, we cross half the frequency here, we're just like repeating uh like the, the same like information. So we just want to focus on the first half. So let's, let's do that here. So uh we can just like go back here and say that we want the left frequency frequency and this is gonna be equal to frequency. And uh we'll just like consider like from like the zero index to like half. And that we can express by saying this is equal to end of the length of frequency itself. And this is like divided by two. So we are uh just like considering the first half year of the frequency array and we should do the same thing for the magnitude uh array.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=627s",
        "start_time": "627.869"
    },
    {
        "id": "f1fe0358",
        "text": "frequency and this is gonna be equal to frequency. And uh we'll just like consider like from like the zero index to like half. And that we can express by saying this is equal to end of the length of frequency itself. And this is like divided by two. So we are uh just like considering the first half year of the frequency array and we should do the same thing for the magnitude uh array. So let's do this and this is the same. And now we'll just",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=651s",
        "start_time": "651.0"
    },
    {
        "id": "d2640253",
        "text": "uh just like considering the first half year of the frequency array and we should do the same thing for the magnitude uh array. So let's do this and this is the same. And now we'll just change frequency for left frequency and magnitude for left magnitude. So now let's rerun the script.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=680s",
        "start_time": "680.0"
    },
    {
        "id": "6c54f358",
        "text": "So let's do this and this is the same. And now we'll just change frequency for left frequency and magnitude for left magnitude. So now let's rerun the script. And now here we have it, our power spectrum focusing only on half of the sample rate. So",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=690s",
        "start_time": "690.169"
    },
    {
        "id": "e559fdb4",
        "text": "change frequency for left frequency and magnitude for left magnitude. So now let's rerun the script. And now here we have it, our power spectrum focusing only on half of the sample rate. So uh until uh yeah, I'd say like 11,000 something",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=697s",
        "start_time": "697.4"
    },
    {
        "id": "420670ef",
        "text": "And now here we have it, our power spectrum focusing only on half of the sample rate. So uh until uh yeah, I'd say like 11,000 something uh right Hertz there. And uh again, yeah, we, we can easily see that like most of the of the energy is in the uh like lower frequencies nice. The only problem that we have with the power spectrum is that it is a static snapshot of the whole sound. And it's considered averaging like the",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=705s",
        "start_time": "705.859"
    },
    {
        "id": "0682db12",
        "text": "uh until uh yeah, I'd say like 11,000 something uh right Hertz there. And uh again, yeah, we, we can easily see that like most of the of the energy is in the uh like lower frequencies nice. The only problem that we have with the power spectrum is that it is a static snapshot of the whole sound. And it's considered averaging like the um the energy of the different frequency beams throughout the whole sound. And we, what we want to do is like understanding how this uh frequencies are contributing to the overall sound throughout time. So in order to do that we need to do a short time",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=715s",
        "start_time": "715.049"
    },
    {
        "id": "fc73ed84",
        "text": "uh right Hertz there. And uh again, yeah, we, we can easily see that like most of the of the energy is in the uh like lower frequencies nice. The only problem that we have with the power spectrum is that it is a static snapshot of the whole sound. And it's considered averaging like the um the energy of the different frequency beams throughout the whole sound. And we, what we want to do is like understanding how this uh frequencies are contributing to the overall sound throughout time. So in order to do that we need to do a short time uh for transform an SSTFT and get a spectrogram. So the spectrogram is gonna give us information about the amplitude as a function of both frequency and time. So how do we get a Stft? Well, we use libros for that. So we do libros dot core and then we call Stft nice.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=720s",
        "start_time": "720.45"
    },
    {
        "id": "03f80bf5",
        "text": "um the energy of the different frequency beams throughout the whole sound. And we, what we want to do is like understanding how this uh frequencies are contributing to the overall sound throughout time. So in order to do that we need to do a short time uh for transform an SSTFT and get a spectrogram. So the spectrogram is gonna give us information about the amplitude as a function of both frequency and time. So how do we get a Stft? Well, we use libros for that. So we do libros dot core and then we call Stft nice. And uh here uh we should pass a few uh different values. So first of all, obviously, we need to fasten the, the signal, but then there are another couple of values. So one, it's, we can call it an uh number of samples per FFT.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=742s",
        "start_time": "742.669"
    },
    {
        "id": "abe6a489",
        "text": "uh for transform an SSTFT and get a spectrogram. So the spectrogram is gonna give us information about the amplitude as a function of both frequency and time. So how do we get a Stft? Well, we use libros for that. So we do libros dot core and then we call Stft nice. And uh here uh we should pass a few uh different values. So first of all, obviously, we need to fasten the, the signal, but then there are another couple of values. So one, it's, we can call it an uh number of samples per FFT. And uh we are gonna set this to 2048 and this is expressed in a number of samples. And so this is basically like the window uh that we are considering when um",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=761s",
        "start_time": "761.669"
    },
    {
        "id": "bbbfd4f6",
        "text": "And uh here uh we should pass a few uh different values. So first of all, obviously, we need to fasten the, the signal, but then there are another couple of values. So one, it's, we can call it an uh number of samples per FFT. And uh we are gonna set this to 2048 and this is expressed in a number of samples. And so this is basically like the window uh that we are considering when um uh performing a single uh fourier transform, fast fourier transform, right. So we are considering this amount of samples and then there's another value",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=790s",
        "start_time": "790.53"
    },
    {
        "id": "6314bb5a",
        "text": "And uh we are gonna set this to 2048 and this is expressed in a number of samples. And so this is basically like the window uh that we are considering when um uh performing a single uh fourier transform, fast fourier transform, right. So we are considering this amount of samples and then there's another value and that's called the hop length.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=809s",
        "start_time": "809.94"
    },
    {
        "id": "85879b73",
        "text": "uh performing a single uh fourier transform, fast fourier transform, right. So we are considering this amount of samples and then there's another value and that's called the hop length. And so let's set this to 512. So again, this is in number of samples and this is the amount we are shifting",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=829s",
        "start_time": "829.19"
    },
    {
        "id": "761be9c3",
        "text": "and that's called the hop length. And so let's set this to 512. So again, this is in number of samples and this is the amount we are shifting uh each fourier transform like to the right because as you know, when we do a short term fourier transform, we slide uh like an interval and at each interval like we, we calculate a, a fast fourier transform and the hop length tells us how much we are shifting. We are sliding towards the right. OK. Cool.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=841s",
        "start_time": "841.32"
    },
    {
        "id": "c1c9ed90",
        "text": "And so let's set this to 512. So again, this is in number of samples and this is the amount we are shifting uh each fourier transform like to the right because as you know, when we do a short term fourier transform, we slide uh like an interval and at each interval like we, we calculate a, a fast fourier transform and the hop length tells us how much we are shifting. We are sliding towards the right. OK. Cool. So like these two values that I've given here, so 2048 and 512 are quite like cus I mean, ordinary values that we use. Like we when analyzing music and even speech really? OK. So let's pass those two things in. So the hop length is equal to the hop length and the NFFT is equal to NFFT good. And so now we have the short time uh fourier transform.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=844s",
        "start_time": "844.309"
    },
    {
        "id": "e32a004f",
        "text": "uh each fourier transform like to the right because as you know, when we do a short term fourier transform, we slide uh like an interval and at each interval like we, we calculate a, a fast fourier transform and the hop length tells us how much we are shifting. We are sliding towards the right. OK. Cool. So like these two values that I've given here, so 2048 and 512 are quite like cus I mean, ordinary values that we use. Like we when analyzing music and even speech really? OK. So let's pass those two things in. So the hop length is equal to the hop length and the NFFT is equal to NFFT good. And so now we have the short time uh fourier transform. And again, uh now we need to move like from like these values to like the magnitude to the spec the spectrogram like itself. So to do that. So first of all, let's call this variable spectra uhm. And then we want to do an MP dot Absolute value and we'll pass in uh the short time uh four transform that we've extracted",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=856s",
        "start_time": "856.26"
    },
    {
        "id": "0e0ad5f8",
        "text": "So like these two values that I've given here, so 2048 and 512 are quite like cus I mean, ordinary values that we use. Like we when analyzing music and even speech really? OK. So let's pass those two things in. So the hop length is equal to the hop length and the NFFT is equal to NFFT good. And so now we have the short time uh fourier transform. And again, uh now we need to move like from like these values to like the magnitude to the spec the spectrogram like itself. So to do that. So first of all, let's call this variable spectra uhm. And then we want to do an MP dot Absolute value and we'll pass in uh the short time uh four transform that we've extracted cool. And so here, basically, we are passing from those like complex numbers towards like the, the magnitude. And here we, we get the whole uh spectrogram right now let's block the results.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=878s",
        "start_time": "878.21"
    },
    {
        "id": "6565554c",
        "text": "And again, uh now we need to move like from like these values to like the magnitude to the spec the spectrogram like itself. So to do that. So first of all, let's call this variable spectra uhm. And then we want to do an MP dot Absolute value and we'll pass in uh the short time uh four transform that we've extracted cool. And so here, basically, we are passing from those like complex numbers towards like the, the magnitude. And here we, we get the whole uh spectrogram right now let's block the results. So to uh do this, we are gonna use a uh function from libres display and the function it's called spec show. And spec show is a nice uh function that enables us to visualize uh spectrogram like um data.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=907s",
        "start_time": "907.419"
    },
    {
        "id": "2e112f0d",
        "text": "cool. And so here, basically, we are passing from those like complex numbers towards like the, the magnitude. And here we, we get the whole uh spectrogram right now let's block the results. So to uh do this, we are gonna use a uh function from libres display and the function it's called spec show. And spec show is a nice uh function that enables us to visualize uh spectrogram like um data. So, and this type of data, as you'll see, it's kind of like a heat map. So you have X axis y axis plus like a color that represents a third variable.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=937s",
        "start_time": "937.0"
    },
    {
        "id": "f373aeb7",
        "text": "So to uh do this, we are gonna use a uh function from libres display and the function it's called spec show. And spec show is a nice uh function that enables us to visualize uh spectrogram like um data. So, and this type of data, as you'll see, it's kind of like a heat map. So you have X axis y axis plus like a color that represents a third variable. So uh what do we need here? So here uh we need uh obviously like the, the spectrogram, right, then uh we need to pass the sample rate",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=951s",
        "start_time": "951.059"
    },
    {
        "id": "83e3b472",
        "text": "So, and this type of data, as you'll see, it's kind of like a heat map. So you have X axis y axis plus like a color that represents a third variable. So uh what do we need here? So here uh we need uh obviously like the, the spectrogram, right, then uh we need to pass the sample rate and then we want to pass the hub length",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=973s",
        "start_time": "973.729"
    },
    {
        "id": "239cf946",
        "text": "So uh what do we need here? So here uh we need uh obviously like the, the spectrogram, right, then uh we need to pass the sample rate and then we want to pass the hub length cool. And as usual, we want to take uh and put to this plot the X label and the Y label. So for the X axis at this time, we have",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=983s",
        "start_time": "983.34"
    },
    {
        "id": "e246ea35",
        "text": "and then we want to pass the hub length cool. And as usual, we want to take uh and put to this plot the X label and the Y label. So for the X axis at this time, we have time for the Y axis we have frequency",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=996s",
        "start_time": "996.64"
    },
    {
        "id": "45ef05ae",
        "text": "cool. And as usual, we want to take uh and put to this plot the X label and the Y label. So for the X axis at this time, we have time for the Y axis we have frequency now. So as we said, uh the spectrogram is a function. So it's the uh amplitude as a function of like time and frequency and uh the amplitude itself is expressed through a color. And so we can plot a color bar to see how the amplitude",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1002s",
        "start_time": "1002.21"
    },
    {
        "id": "c73094e4",
        "text": "time for the Y axis we have frequency now. So as we said, uh the spectrogram is a function. So it's the uh amplitude as a function of like time and frequency and uh the amplitude itself is expressed through a color. And so we can plot a color bar to see how the amplitude there is like a throughout like the spectrogram. OK. So now, as usual, let's uh comment, comment this out so that we were gonna have just the um the plot for the spectrogram. And now let's move on and run the script",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1016s",
        "start_time": "1016.409"
    },
    {
        "id": "d7d12210",
        "text": "now. So as we said, uh the spectrogram is a function. So it's the uh amplitude as a function of like time and frequency and uh the amplitude itself is expressed through a color. And so we can plot a color bar to see how the amplitude there is like a throughout like the spectrogram. OK. So now, as usual, let's uh comment, comment this out so that we were gonna have just the um the plot for the spectrogram. And now let's move on and run the script and here we go, we have our spectrogram",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1022s",
        "start_time": "1022.03"
    },
    {
        "id": "ff59fc88",
        "text": "there is like a throughout like the spectrogram. OK. So now, as usual, let's uh comment, comment this out so that we were gonna have just the um the plot for the spectrogram. And now let's move on and run the script and here we go, we have our spectrogram cool. OK. So as you can see, most of the frequencies basically have very, very low amplitudes. So they contribute very, very little to the overall sound.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1043s",
        "start_time": "1043.39"
    },
    {
        "id": "7b40c7b7",
        "text": "and here we go, we have our spectrogram cool. OK. So as you can see, most of the frequencies basically have very, very low amplitudes. So they contribute very, very little to the overall sound. And here like down in the bottom, you can see that there are certain like bursts of energy at the lower like frequencies which is also like what we would expect from the uh power um spectrum spectrum like that we say like before, right?",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1063s",
        "start_time": "1063.18"
    },
    {
        "id": "69befc0e",
        "text": "cool. OK. So as you can see, most of the frequencies basically have very, very low amplitudes. So they contribute very, very little to the overall sound. And here like down in the bottom, you can see that there are certain like bursts of energy at the lower like frequencies which is also like what we would expect from the uh power um spectrum spectrum like that we say like before, right? But now there's a way of like us moving like a little bit like this amplitude and like to like visualize them like in a, in a nicer way and in a way that makes also like more sense, like for the way we perceive loudness, which is not linear, which is like the way we are like visualizing these amplitudes here, but rather it's a logarithmic. And so we're gonna use uh so we're gonna calculate the so-called log uh spectrogram.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1067s",
        "start_time": "1067.619"
    },
    {
        "id": "488f5389",
        "text": "And here like down in the bottom, you can see that there are certain like bursts of energy at the lower like frequencies which is also like what we would expect from the uh power um spectrum spectrum like that we say like before, right? But now there's a way of like us moving like a little bit like this amplitude and like to like visualize them like in a, in a nicer way and in a way that makes also like more sense, like for the way we perceive loudness, which is not linear, which is like the way we are like visualizing these amplitudes here, but rather it's a logarithmic. And so we're gonna use uh so we're gonna calculate the so-called log uh spectrogram. And uh yeah, we can do it here. So we'll do a log spectrogram. And uh for doing that, we can use a nice uh li browser uh F function uh that's called amplitude to decimal. So we are taking uh the amplitude from our original spectrum which we should pass in",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1083s",
        "start_time": "1083.75"
    },
    {
        "id": "68d73307",
        "text": "But now there's a way of like us moving like a little bit like this amplitude and like to like visualize them like in a, in a nicer way and in a way that makes also like more sense, like for the way we perceive loudness, which is not linear, which is like the way we are like visualizing these amplitudes here, but rather it's a logarithmic. And so we're gonna use uh so we're gonna calculate the so-called log uh spectrogram. And uh yeah, we can do it here. So we'll do a log spectrogram. And uh for doing that, we can use a nice uh li browser uh F function uh that's called amplitude to decimal. So we are taking uh the amplitude from our original spectrum which we should pass in and then we are converting them to decibel. Uh we, and I mean, when we do that, we use, we apply",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1101s",
        "start_time": "1101.15"
    },
    {
        "id": "b57b8d83",
        "text": "And uh yeah, we can do it here. So we'll do a log spectrogram. And uh for doing that, we can use a nice uh li browser uh F function uh that's called amplitude to decimal. So we are taking uh the amplitude from our original spectrum which we should pass in and then we are converting them to decibel. Uh we, and I mean, when we do that, we use, we apply a logarithm cool. So now we have the log spectrogram. So let's pass that in here and let's take a look at the results",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1130s",
        "start_time": "1130.53"
    },
    {
        "id": "d5dac39f",
        "text": "and then we are converting them to decibel. Uh we, and I mean, when we do that, we use, we apply a logarithm cool. So now we have the log spectrogram. So let's pass that in here and let's take a look at the results and here we go. Nice.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1153s",
        "start_time": "1153.469"
    },
    {
        "id": "e3581d06",
        "text": "a logarithm cool. So now we have the log spectrogram. So let's pass that in here and let's take a look at the results and here we go. Nice. OK. So as you can see here, like all of these things like become like a little bit like more uh uh like intelligible I would say.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1161s",
        "start_time": "1161.699"
    },
    {
        "id": "54ec8327",
        "text": "and here we go. Nice. OK. So as you can see here, like all of these things like become like a little bit like more uh uh like intelligible I would say. And uh like here, like with the blue, we have like very, very quiet, sounds like minus 30 like decibels. And uh while we go towards like these more reddish like colors, we, we just like increase like the, the, the perceived basically like insensitive right. And as expected, we have most of the energy uh that's kind of concentrated in this like lower frequencies.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1171s",
        "start_time": "1171.949"
    },
    {
        "id": "bff44d21",
        "text": "OK. So as you can see here, like all of these things like become like a little bit like more uh uh like intelligible I would say. And uh like here, like with the blue, we have like very, very quiet, sounds like minus 30 like decibels. And uh while we go towards like these more reddish like colors, we, we just like increase like the, the, the perceived basically like insensitive right. And as expected, we have most of the energy uh that's kind of concentrated in this like lower frequencies. And if you guys re recall the uh the waveform, like, it was like quite, I would say, like quite stable, like throughout and like, and that could have been a little bit like of a,",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1176s",
        "start_time": "1176.16"
    },
    {
        "id": "04a8af42",
        "text": "And uh like here, like with the blue, we have like very, very quiet, sounds like minus 30 like decibels. And uh while we go towards like these more reddish like colors, we, we just like increase like the, the, the perceived basically like insensitive right. And as expected, we have most of the energy uh that's kind of concentrated in this like lower frequencies. And if you guys re recall the uh the waveform, like, it was like quite, I would say, like quite stable, like throughout and like, and that could have been a little bit like of a, of a hint into also like the, the way uh like the the spectrogram like would behave to a certain extent. And obviously like what we see here is that like the spectrogram remains like quite stable throughout time.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1185s",
        "start_time": "1185.229"
    },
    {
        "id": "c5c62604",
        "text": "And if you guys re recall the uh the waveform, like, it was like quite, I would say, like quite stable, like throughout and like, and that could have been a little bit like of a, of a hint into also like the, the way uh like the the spectrogram like would behave to a certain extent. And obviously like what we see here is that like the spectrogram remains like quite stable throughout time. Cool. OK. So now we've seen uh like the spectrogram, the log spectrogram. Now we want to calculate the last thing. So we want to extract the MF CCS.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1212s",
        "start_time": "1212.64"
    },
    {
        "id": "c07bbd87",
        "text": "of a hint into also like the, the way uh like the the spectrogram like would behave to a certain extent. And obviously like what we see here is that like the spectrogram remains like quite stable throughout time. Cool. OK. So now we've seen uh like the spectrogram, the log spectrogram. Now we want to calculate the last thing. So we want to extract the MF CCS. So how do we do that? Well, that's as simple as calling Li Brosa dots feature dot MFCC",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1226s",
        "start_time": "1226.469"
    },
    {
        "id": "c5d0f453",
        "text": "Cool. OK. So now we've seen uh like the spectrogram, the log spectrogram. Now we want to calculate the last thing. So we want to extract the MF CCS. So how do we do that? Well, that's as simple as calling Li Brosa dots feature dot MFCC nice. And so here uh for uh calculating this, we need to pass the signal. So the original signal and then we want to pass uh a, a bunch of like different uh values uh like for example, the uh number",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1240s",
        "start_time": "1240.41"
    },
    {
        "id": "a22863f4",
        "text": "So how do we do that? Well, that's as simple as calling Li Brosa dots feature dot MFCC nice. And so here uh for uh calculating this, we need to pass the signal. So the original signal and then we want to pass uh a, a bunch of like different uh values uh like for example, the uh number uh like the, the, the number of samples per FFT and",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1252s",
        "start_time": "1252.64"
    },
    {
        "id": "62ae317b",
        "text": "nice. And so here uh for uh calculating this, we need to pass the signal. So the original signal and then we want to pass uh a, a bunch of like different uh values uh like for example, the uh number uh like the, the, the number of samples per FFT and this is equal to this, we want to pass in the hop length",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1264s",
        "start_time": "1264.089"
    },
    {
        "id": "5e4bb1b9",
        "text": "uh like the, the, the number of samples per FFT and this is equal to this, we want to pass in the hop length which is equal to the hop length. And so here I'm just missed them. Oops",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1283s",
        "start_time": "1283.8"
    },
    {
        "id": "2cbde7d4",
        "text": "this is equal to this, we want to pass in the hop length which is equal to the hop length. And so here I'm just missed them. Oops uh over here.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1289s",
        "start_time": "1289.959"
    },
    {
        "id": "f5a4e960",
        "text": "which is equal to the hop length. And so here I'm just missed them. Oops uh over here. And uh we also want to pass another value that's called number of MF CCS. So the number of coefficients that we want to extract and let's say we want to extract uh 13 which is like a fair like number that's commonly used, like also for analyzing music.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1294s",
        "start_time": "1294.01"
    },
    {
        "id": "f0f9c489",
        "text": "uh over here. And uh we also want to pass another value that's called number of MF CCS. So the number of coefficients that we want to extract and let's say we want to extract uh 13 which is like a fair like number that's commonly used, like also for analyzing music. Uh OK. So now we have the MF CCS and as you can see here, we are passing these values. So the NFNFFT and the hop length to the window and the hop length that we usually use when we extract uh like the SDFT and you can see it here, right? And why is that the case? Well, because if you recall from the previous video,",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1299s",
        "start_time": "1299.489"
    },
    {
        "id": "0cb41c97",
        "text": "And uh we also want to pass another value that's called number of MF CCS. So the number of coefficients that we want to extract and let's say we want to extract uh 13 which is like a fair like number that's commonly used, like also for analyzing music. Uh OK. So now we have the MF CCS and as you can see here, we are passing these values. So the NFNFFT and the hop length to the window and the hop length that we usually use when we extract uh like the SDFT and you can see it here, right? And why is that the case? Well, because if you recall from the previous video, um one of the things, the first thing that we do for extracting an MFCC is performing a short time fourier transform.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1301s",
        "start_time": "1301.319"
    },
    {
        "id": "5ff74224",
        "text": "Uh OK. So now we have the MF CCS and as you can see here, we are passing these values. So the NFNFFT and the hop length to the window and the hop length that we usually use when we extract uh like the SDFT and you can see it here, right? And why is that the case? Well, because if you recall from the previous video, um one of the things, the first thing that we do for extracting an MFCC is performing a short time fourier transform. Cool. OK. So now we have the MFCC uh and we want to plot that. So to do that, we are gonna use the spec show um function from Libera display once again. But this time instead of the log spectrogram, we're gonna pass in the MF CCS",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1322s",
        "start_time": "1322.119"
    },
    {
        "id": "9e4d9c4d",
        "text": "um one of the things, the first thing that we do for extracting an MFCC is performing a short time fourier transform. Cool. OK. So now we have the MFCC uh and we want to plot that. So to do that, we are gonna use the spec show um function from Libera display once again. But this time instead of the log spectrogram, we're gonna pass in the MF CCS uh right? And uh the X label is gonna be time, the Y label, obviously it's not gonna be frequency but the MFCC itself. So it's gonna be like the different coefficients we want to call it bar and we want to plot dot show this. So let's um",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1346s",
        "start_time": "1346.68"
    },
    {
        "id": "30eb0840",
        "text": "Cool. OK. So now we have the MFCC uh and we want to plot that. So to do that, we are gonna use the spec show um function from Libera display once again. But this time instead of the log spectrogram, we're gonna pass in the MF CCS uh right? And uh the X label is gonna be time, the Y label, obviously it's not gonna be frequency but the MFCC itself. So it's gonna be like the different coefficients we want to call it bar and we want to plot dot show this. So let's um comment uh out the, the plot for the lux spectrogram and let's rerun the scripts. And so if all goes well, yeah, we have our MF CCS over time and nice. So here like on the Y axis, you'll see like these intervals uh like here and each of these is basically like a A coefficient. And if you count it should be like 13",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1355s",
        "start_time": "1355.79"
    },
    {
        "id": "128506e9",
        "text": "uh right? And uh the X label is gonna be time, the Y label, obviously it's not gonna be frequency but the MFCC itself. So it's gonna be like the different coefficients we want to call it bar and we want to plot dot show this. So let's um comment uh out the, the plot for the lux spectrogram and let's rerun the scripts. And so if all goes well, yeah, we have our MF CCS over time and nice. So here like on the Y axis, you'll see like these intervals uh like here and each of these is basically like a A coefficient. And if you count it should be like 13 and on the X axis we have time. And so we basically see here how, how the different MF CCS are evolving over time. And once again, like the, the MFCC like plots is quite stable. Cool. OK. So we basically went through",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1375s",
        "start_time": "1375.9"
    },
    {
        "id": "7b96dc13",
        "text": "comment uh out the, the plot for the lux spectrogram and let's rerun the scripts. And so if all goes well, yeah, we have our MF CCS over time and nice. So here like on the Y axis, you'll see like these intervals uh like here and each of these is basically like a A coefficient. And if you count it should be like 13 and on the X axis we have time. And so we basically see here how, how the different MF CCS are evolving over time. And once again, like the, the MFCC like plots is quite stable. Cool. OK. So we basically went through all the stuff that we need for preprocessing audio data for deep learning. Now, you know how to look at it at the waveform, how to extract like signal from a wave file, how to perform a fourier transform, how to arrive at a power spectrum spectrograms, log spectrograms, and most importantly MF CCS, which we're gonna be using in the next video",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1395s",
        "start_time": "1395.209"
    },
    {
        "id": "7ae2eeb5",
        "text": "and on the X axis we have time. And so we basically see here how, how the different MF CCS are evolving over time. And once again, like the, the MFCC like plots is quite stable. Cool. OK. So we basically went through all the stuff that we need for preprocessing audio data for deep learning. Now, you know how to look at it at the waveform, how to extract like signal from a wave file, how to perform a fourier transform, how to arrive at a power spectrum spectrograms, log spectrograms, and most importantly MF CCS, which we're gonna be using in the next video where we're gonna do something super exciting. So we're gonna use an M LP, a multi-layered perception for uh classifying music genres. So we're gonna have a data set of uh short experts of uh music and we're gonna classify uh the type of like genres like they belong to. Cool. I hope you really like enjoyed this uh video. If that's the case, please leave a like",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1422s",
        "start_time": "1422.41"
    },
    {
        "id": "4e807a1a",
        "text": "all the stuff that we need for preprocessing audio data for deep learning. Now, you know how to look at it at the waveform, how to extract like signal from a wave file, how to perform a fourier transform, how to arrive at a power spectrum spectrograms, log spectrograms, and most importantly MF CCS, which we're gonna be using in the next video where we're gonna do something super exciting. So we're gonna use an M LP, a multi-layered perception for uh classifying music genres. So we're gonna have a data set of uh short experts of uh music and we're gonna classify uh the type of like genres like they belong to. Cool. I hope you really like enjoyed this uh video. If that's the case, please leave a like and if you have any questions as usual, like post them in the comments section below and I'll see you next time. Cheers.",
        "video": "11- Preprocessing audio data for Deep Learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Oa_d-zaUti8",
        "youtube_link": "https://www.youtube.com/watch?v=Oa_d-zaUti8&t=1442s",
        "start_time": "1442.449"
    },
    {
        "id": "6191a52c",
        "text": "Hi, everybody and welcome to a new video in the Deep learning for audio with Python series. This time we're gonna implement back propagation and gradient descent. And to do that, we're gonna expand on the work we've done uh a couple of videos ago when we implemented a multi layer perception class, this M LP objects here. And uh in that case, we built a uh constructor where basically we built like the, the structure of the network. And then we mainly uh focused on this forward propagate method uh which is basically forward propagation which computes the inputs uh which travel uh from left to right and gives us a prediction good. So what are we gonna do specifically like this time around? Well, it's a bunch of like different things and so it's better just like to write them down so that we'll have more or less like a direction that we know we're following because by the way, this is gonna be like a quite intense view and probably quite long as well good. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "336887b1",
        "text": "And uh in that case, we built a uh constructor where basically we built like the, the structure of the network. And then we mainly uh focused on this forward propagate method uh which is basically forward propagation which computes the inputs uh which travel uh from left to right and gives us a prediction good. So what are we gonna do specifically like this time around? Well, it's a bunch of like different things and so it's better just like to write them down so that we'll have more or less like a direction that we know we're following because by the way, this is gonna be like a quite intense view and probably quite long as well good. So uh the first thing that we want to do is to save the activations and the derivatives",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=21s",
        "start_time": "21.079"
    },
    {
        "id": "79c3a411",
        "text": "So what are we gonna do specifically like this time around? Well, it's a bunch of like different things and so it's better just like to write them down so that we'll have more or less like a direction that we know we're following because by the way, this is gonna be like a quite intense view and probably quite long as well good. So uh the first thing that we want to do is to save the activations and the derivatives and derivative, right. So this is like while we to compute uh back propagation, we need information about activations and obviously about like the uh derivatives as well. When we'll compute a gradient descent, then uh what we want to do is to uh implement back propagate back propagation.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=46s",
        "start_time": "46.389"
    },
    {
        "id": "2824b2cb",
        "text": "uh the first thing that we want to do is to save the activations and the derivatives and derivative, right. So this is like while we to compute uh back propagation, we need information about activations and obviously about like the uh derivatives as well. When we'll compute a gradient descent, then uh what we want to do is to uh implement back propagate back propagation. So once we have it back propagation implemented, we want to implement a gradient",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=66s",
        "start_time": "66.37"
    },
    {
        "id": "f3c41a21",
        "text": "and derivative, right. So this is like while we to compute uh back propagation, we need information about activations and obviously about like the uh derivatives as well. When we'll compute a gradient descent, then uh what we want to do is to uh implement back propagate back propagation. So once we have it back propagation implemented, we want to implement a gradient Grady and uh descent. And once we have that it's time to go higher like from a higher level and implement a train method which will use both back propagation and gradient descent. And uh then we want to train our nets work with some dummy data set",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=77s",
        "start_time": "77.129"
    },
    {
        "id": "b74114dc",
        "text": "So once we have it back propagation implemented, we want to implement a gradient Grady and uh descent. And once we have that it's time to go higher like from a higher level and implement a train method which will use both back propagation and gradient descent. And uh then we want to train our nets work with some dummy data set and finally make some predictions",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=105s",
        "start_time": "105.11"
    },
    {
        "id": "88b99746",
        "text": "Grady and uh descent. And once we have that it's time to go higher like from a higher level and implement a train method which will use both back propagation and gradient descent. And uh then we want to train our nets work with some dummy data set and finally make some predictions good. So this is the plan for today's video. So let's get started from the first one. So save activations and derivatives, right? So first of all, uh we need uh a representations for uh data representation for these activations and derivatives. And we need to create uh this representation here in the M LP uh construct,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=111s",
        "start_time": "111.65"
    },
    {
        "id": "495e1761",
        "text": "and finally make some predictions good. So this is the plan for today's video. So let's get started from the first one. So save activations and derivatives, right? So first of all, uh we need uh a representations for uh data representation for these activations and derivatives. And we need to create uh this representation here in the M LP uh construct, right? And so as we did for the weights here where we basically created some random weights, uh We should do like a similar thing uh for the activations and derivatives. So let's get started activations and we'll start with uh an empty list. I don't like that. That is uh like that nice. And now we'll do a four loop and we'll go through all the uh layers here,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=139s",
        "start_time": "139.57"
    },
    {
        "id": "b4bf42ea",
        "text": "good. So this is the plan for today's video. So let's get started from the first one. So save activations and derivatives, right? So first of all, uh we need uh a representations for uh data representation for these activations and derivatives. And we need to create uh this representation here in the M LP uh construct, right? And so as we did for the weights here where we basically created some random weights, uh We should do like a similar thing uh for the activations and derivatives. So let's get started activations and we'll start with uh an empty list. I don't like that. That is uh like that nice. And now we'll do a four loop and we'll go through all the uh layers here, right? We'll go through all the layers and then we want to create a dummy uh activation array for each of the, of the layers. So how do we do that? So, it's quite simple. So it's uh we'll create an array A which is given by NP dot uh And we'll do zeros here. So it's all, it's an array of zeros. And we'll specify that we want",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=143s",
        "start_time": "143.289"
    },
    {
        "id": "58550a11",
        "text": "right? And so as we did for the weights here where we basically created some random weights, uh We should do like a similar thing uh for the activations and derivatives. So let's get started activations and we'll start with uh an empty list. I don't like that. That is uh like that nice. And now we'll do a four loop and we'll go through all the uh layers here, right? We'll go through all the layers and then we want to create a dummy uh activation array for each of the, of the layers. So how do we do that? So, it's quite simple. So it's uh we'll create an array A which is given by NP dot uh And we'll do zeros here. So it's all, it's an array of zeros. And we'll specify that we want uh an amount uh of zeros here. That's equal to the number of neurons that we have in uh each layer, right, for each layer. So then we'll do a activations dot append and we'll append uh this mono dimensional one dimensional array here to activations.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=164s",
        "start_time": "164.66"
    },
    {
        "id": "6b588dad",
        "text": "right? We'll go through all the layers and then we want to create a dummy uh activation array for each of the, of the layers. So how do we do that? So, it's quite simple. So it's uh we'll create an array A which is given by NP dot uh And we'll do zeros here. So it's all, it's an array of zeros. And we'll specify that we want uh an amount uh of zeros here. That's equal to the number of neurons that we have in uh each layer, right, for each layer. So then we'll do a activations dot append and we'll append uh this mono dimensional one dimensional array here to activations. So in the end activations is gonna be a list of arrays where each array in the list uh represents the activations for a given layer, right. So now we want to store all of this uh information in a instance, variable called uh activations. And so self dodge uh activations is gonna be equal to activations. Nice.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=195s",
        "start_time": "195.449"
    },
    {
        "id": "f938cac4",
        "text": "uh an amount uh of zeros here. That's equal to the number of neurons that we have in uh each layer, right, for each layer. So then we'll do a activations dot append and we'll append uh this mono dimensional one dimensional array here to activations. So in the end activations is gonna be a list of arrays where each array in the list uh represents the activations for a given layer, right. So now we want to store all of this uh information in a instance, variable called uh activations. And so self dodge uh activations is gonna be equal to activations. Nice. So now we should do something similar with derivatives as well. So I'll just copy all of this and paste it here. So instead of um activations, we'll have like derivatives here. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=226s",
        "start_time": "226.669"
    },
    {
        "id": "a96dd70d",
        "text": "So in the end activations is gonna be a list of arrays where each array in the list uh represents the activations for a given layer, right. So now we want to store all of this uh information in a instance, variable called uh activations. And so self dodge uh activations is gonna be equal to activations. Nice. So now we should do something similar with derivatives as well. So I'll just copy all of this and paste it here. So instead of um activations, we'll have like derivatives here. So we have this like empty list derivatives. And now we want to travel through so loop through",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=253s",
        "start_time": "253.899"
    },
    {
        "id": "c685feb9",
        "text": "So now we should do something similar with derivatives as well. So I'll just copy all of this and paste it here. So instead of um activations, we'll have like derivatives here. So we have this like empty list derivatives. And now we want to travel through so loop through the layers but all the layers like minus one. Because if you guys remember when we have, for example, a network with uh three layers, we only have two weight mattresses because the weight mattresses are in between layers, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=279s",
        "start_time": "279.369"
    },
    {
        "id": "1591a8dd",
        "text": "we have this like empty list derivatives. And now we want to travel through so loop through the layers but all the layers like minus one. Because if you guys remember when we have, for example, a network with uh three layers, we only have two weight mattresses because the weight mattresses are in between layers, right? And so, uh in this case, we are gonna have a number of like derivatives or uh that are like equal to the weight mattresses, right? Because the derivatives are the derivatives of the error function with respect to the to the weight cool. So let's change this. So we'll call this D",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=295s",
        "start_time": "295.19"
    },
    {
        "id": "1347c2a9",
        "text": "the layers but all the layers like minus one. Because if you guys remember when we have, for example, a network with uh three layers, we only have two weight mattresses because the weight mattresses are in between layers, right? And so, uh in this case, we are gonna have a number of like derivatives or uh that are like equal to the weight mattresses, right? Because the derivatives are the derivatives of the error function with respect to the to the weight cool. So let's change this. So we'll call this D and now we're not expecting a mono dimensional array rather like a two dimensional array, which is basically a matrix. And uh this matrix uh is gonna have uh the, the dimensions like say for, for the rows, we expect the number of um neurons that we have in the current layer I and for the columns, we expect the number of neurons uh that we have in the subsequent layer, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=302s",
        "start_time": "302.329"
    },
    {
        "id": "51105faf",
        "text": "And so, uh in this case, we are gonna have a number of like derivatives or uh that are like equal to the weight mattresses, right? Because the derivatives are the derivatives of the error function with respect to the to the weight cool. So let's change this. So we'll call this D and now we're not expecting a mono dimensional array rather like a two dimensional array, which is basically a matrix. And uh this matrix uh is gonna have uh the, the dimensions like say for, for the rows, we expect the number of um neurons that we have in the current layer I and for the columns, we expect the number of neurons uh that we have in the subsequent layer, right? So that's that. And now we want to uh just change it, change this and we want to paint derivatives. And here these activations",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=319s",
        "start_time": "319.679"
    },
    {
        "id": "84af475a",
        "text": "and now we're not expecting a mono dimensional array rather like a two dimensional array, which is basically a matrix. And uh this matrix uh is gonna have uh the, the dimensions like say for, for the rows, we expect the number of um neurons that we have in the current layer I and for the columns, we expect the number of neurons uh that we have in the subsequent layer, right? So that's that. And now we want to uh just change it, change this and we want to paint derivatives. And here these activations again should be changed into uh derivatives. Nice. OK. So now we have a nice way of storing derivatives as well good. So now that we have all like the, the representation place we should go and uh tweak our forward propagate so that we can save the activations. Why we, we, we, we create like we, well, we compute these activations for each layer. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=343s",
        "start_time": "343.399"
    },
    {
        "id": "afcb860d",
        "text": "So that's that. And now we want to uh just change it, change this and we want to paint derivatives. And here these activations again should be changed into uh derivatives. Nice. OK. So now we have a nice way of storing derivatives as well good. So now that we have all like the, the representation place we should go and uh tweak our forward propagate so that we can save the activations. Why we, we, we, we create like we, well, we compute these activations for each layer. So uh what about the activation",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=374s",
        "start_time": "374.69"
    },
    {
        "id": "32d9248e",
        "text": "again should be changed into uh derivatives. Nice. OK. So now we have a nice way of storing derivatives as well good. So now that we have all like the, the representation place we should go and uh tweak our forward propagate so that we can save the activations. Why we, we, we, we create like we, well, we compute these activations for each layer. So uh what about the activation self dot uh activations for the first layer for the input layer? Well, we know that uh this is basically just like the inputs, right? So we can save uh the activations for the first layer as the inputs that we are re receiving here as an argument for forward propagate. Nice.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=388s",
        "start_time": "388.649"
    },
    {
        "id": "918528ef",
        "text": "uh what about the activation self dot uh activations for the first layer for the input layer? Well, we know that uh this is basically just like the inputs, right? So we can save uh the activations for the first layer as the inputs that we are re receiving here as an argument for forward propagate. Nice. So now there's uh the next step which is uh this forward propagate iterates through like all the network layers and calculates the activations. And now here at this point, we want to save the activations and now we're gonna save uh the activations at I plus one",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=415s",
        "start_time": "415.679"
    },
    {
        "id": "4d1e91b4",
        "text": "self dot uh activations for the first layer for the input layer? Well, we know that uh this is basically just like the inputs, right? So we can save uh the activations for the first layer as the inputs that we are re receiving here as an argument for forward propagate. Nice. So now there's uh the next step which is uh this forward propagate iterates through like all the network layers and calculates the activations. And now here at this point, we want to save the activations and now we're gonna save uh the activations at I plus one and this is gonna be the activations. So now you may be wondering but uh if you are currently in I right at uh why are we storing this like at I plus one? Well, uh let me show you why that's the case. So if you, you may remember that the activation, say for example, like the activation of the third layer",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=420s",
        "start_time": "420.0"
    },
    {
        "id": "1eeb07e9",
        "text": "So now there's uh the next step which is uh this forward propagate iterates through like all the network layers and calculates the activations. And now here at this point, we want to save the activations and now we're gonna save uh the activations at I plus one and this is gonna be the activations. So now you may be wondering but uh if you are currently in I right at uh why are we storing this like at I plus one? Well, uh let me show you why that's the case. So if you, you may remember that the activation, say for example, like the activation of the third layer is equal to the sigmoid function even like we are only using sigmoid functions as activations functions. Uh It's the sigmoid function of three but now three, if you guys remember",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=441s",
        "start_time": "441.73"
    },
    {
        "id": "bef9f55a",
        "text": "and this is gonna be the activations. So now you may be wondering but uh if you are currently in I right at uh why are we storing this like at I plus one? Well, uh let me show you why that's the case. So if you, you may remember that the activation, say for example, like the activation of the third layer is equal to the sigmoid function even like we are only using sigmoid functions as activations functions. Uh It's the sigmoid function of three but now three, if you guys remember um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=464s",
        "start_time": "464.63"
    },
    {
        "id": "8582ca2e",
        "text": "is equal to the sigmoid function even like we are only using sigmoid functions as activations functions. Uh It's the sigmoid function of three but now three, if you guys remember um is equal",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=487s",
        "start_time": "487.709"
    },
    {
        "id": "c7873195",
        "text": "um is equal c",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=504s",
        "start_time": "504.5"
    },
    {
        "id": "56d313b4",
        "text": "is equal c the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=506s",
        "start_time": "506.329"
    },
    {
        "id": "aa91e3ba",
        "text": "c the uh matrix multiplication between A two and um and",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=508s",
        "start_time": "508.67"
    },
    {
        "id": "4177cd8f",
        "text": "the uh matrix multiplication between A two and um and we have W",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=510s",
        "start_time": "510.14"
    },
    {
        "id": "2257cbe0",
        "text": "uh matrix multiplication between A two and um and we have W two, right? And so here, basically we're saying that we are at say for example, this I is equal to two. And so here we are considering like the, the second wave matrix and then the activations connected with the second weight matrix is indeed uh the activation for the third layer. And so here we need to like add one to the current I. So in our example two plus 13, right? And so this is why we're doing this good.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=512s",
        "start_time": "512.848"
    },
    {
        "id": "72687f09",
        "text": "we have W two, right? And so here, basically we're saying that we are at say for example, this I is equal to two. And so here we are considering like the, the second wave matrix and then the activations connected with the second weight matrix is indeed uh the activation for the third layer. And so here we need to like add one to the current I. So in our example two plus 13, right? And so this is why we're doing this good. So nice. So now we have done basically the the the first part of the, the first uh task which is like saving activations and derivatives now uh to well saving just activations because derivatives uh we haven't saved them. We've just like created the representation for saving them, but we'll do that when we implement back propagation, which is happening right now.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=520s",
        "start_time": "520.169"
    },
    {
        "id": "2059917c",
        "text": "two, right? And so here, basically we're saying that we are at say for example, this I is equal to two. And so here we are considering like the, the second wave matrix and then the activations connected with the second weight matrix is indeed uh the activation for the third layer. And so here we need to like add one to the current I. So in our example two plus 13, right? And so this is why we're doing this good. So nice. So now we have done basically the the the first part of the, the first uh task which is like saving activations and derivatives now uh to well saving just activations because derivatives uh we haven't saved them. We've just like created the representation for saving them, but we'll do that when we implement back propagation, which is happening right now. Cool. So now we need to implement a back a new method called back proper",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=522s",
        "start_time": "522.908"
    },
    {
        "id": "b69f8635",
        "text": "So nice. So now we have done basically the the the first part of the, the first uh task which is like saving activations and derivatives now uh to well saving just activations because derivatives uh we haven't saved them. We've just like created the representation for saving them, but we'll do that when we implement back propagation, which is happening right now. Cool. So now we need to implement a back a new method called back proper gauge, right? So now, first of all, we want to pass uh the an error here. So and we'll see like what this error is like in in a few seconds. But first of all, how does this work? So we, we said like in the previous video that with back propagation, the idea is to have the error and uh back propagate the error from the output layer sorry",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=553s",
        "start_time": "553.299"
    },
    {
        "id": "15ecaf23",
        "text": "Cool. So now we need to implement a back a new method called back proper gauge, right? So now, first of all, we want to pass uh the an error here. So and we'll see like what this error is like in in a few seconds. But first of all, how does this work? So we, we said like in the previous video that with back propagation, the idea is to have the error and uh back propagate the error from the output layer sorry towards the uh input layer towards the left, right. So basically what this how, so the way we can translate this in code is that we can basically loop through all the layers starting from like the, the last one towards like the the previous ones, right? And so how do we do that? Well, it's quite simple. So we will do a full loop in a range",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=581s",
        "start_time": "581.07"
    },
    {
        "id": "50a40aec",
        "text": "gauge, right? So now, first of all, we want to pass uh the an error here. So and we'll see like what this error is like in in a few seconds. But first of all, how does this work? So we, we said like in the previous video that with back propagation, the idea is to have the error and uh back propagate the error from the output layer sorry towards the uh input layer towards the left, right. So basically what this how, so the way we can translate this in code is that we can basically loop through all the layers starting from like the, the last one towards like the the previous ones, right? And so how do we do that? Well, it's quite simple. So we will do a full loop in a range uh range of the length of self",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=589s",
        "start_time": "589.9"
    },
    {
        "id": "07a99394",
        "text": "towards the uh input layer towards the left, right. So basically what this how, so the way we can translate this in code is that we can basically loop through all the layers starting from like the, the last one towards like the the previous ones, right? And so how do we do that? Well, it's quite simple. So we will do a full loop in a range uh range of the length of self dot um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=620s",
        "start_time": "620.44"
    },
    {
        "id": "c5e63c10",
        "text": "uh range of the length of self dot um uh derivatives here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=646s",
        "start_time": "646.489"
    },
    {
        "id": "1695fdf8",
        "text": "dot um uh derivatives here. So, but this is just like going from left to right, right. Because we are uh incrementing I from zero, like towards like the, the number of like uh elements we have like in self dot derivatives, but we want to go the other way around from right to left. So how do we do that? Well, we just do a reversed",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=652s",
        "start_time": "652.69"
    },
    {
        "id": "6553d3d9",
        "text": "uh derivatives here. So, but this is just like going from left to right, right. Because we are uh incrementing I from zero, like towards like the, the number of like uh elements we have like in self dot derivatives, but we want to go the other way around from right to left. So how do we do that? Well, we just do a reversed of this guy here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=655s",
        "start_time": "655.4"
    },
    {
        "id": "e705c0ef",
        "text": "So, but this is just like going from left to right, right. Because we are uh incrementing I from zero, like towards like the, the number of like uh elements we have like in self dot derivatives, but we want to go the other way around from right to left. So how do we do that? Well, we just do a reversed of this guy here. Cool. And uh now,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=658s",
        "start_time": "658.869"
    },
    {
        "id": "2fa9fb6b",
        "text": "of this guy here. Cool. And uh now, OK, let me just check.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=682s",
        "start_time": "682.539"
    },
    {
        "id": "71b03408",
        "text": "Cool. And uh now, OK, let me just check. So we may have an issue with the number of parentheses here. No. Yeah, no, it's fine. Good. Uh OK. So now we are just like going through uh we are living through like the, the neural network but starting from uh right, and then moving all the way back towards the inputs.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=686s",
        "start_time": "686.88"
    },
    {
        "id": "426fddcc",
        "text": "OK, let me just check. So we may have an issue with the number of parentheses here. No. Yeah, no, it's fine. Good. Uh OK. So now we are just like going through uh we are living through like the, the neural network but starting from uh right, and then moving all the way back towards the inputs. Now, what should we do? But in order to understand what we should do, we should remember uh like how back propagate uh how back propagation works. And so now I'm gonna pass in some things that I don't want to uh write from scratch because it will take too much time. But if you guys remember,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=690s",
        "start_time": "690.919"
    },
    {
        "id": "8c1ce34d",
        "text": "So we may have an issue with the number of parentheses here. No. Yeah, no, it's fine. Good. Uh OK. So now we are just like going through uh we are living through like the, the neural network but starting from uh right, and then moving all the way back towards the inputs. Now, what should we do? But in order to understand what we should do, we should remember uh like how back propagate uh how back propagation works. And so now I'm gonna pass in some things that I don't want to uh write from scratch because it will take too much time. But if you guys remember, uh let's assume we are like at the uh rightmost weight matrix, right? So say, for example, we are, we have like this network with three layers, then we are like a W-2 and then we want to calculate the error uh with respect the derivative of the error function with respect to uh W-2 or like in this, in this case, like if we want to keep it a general we could say Wy",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=694s",
        "start_time": "694.02"
    },
    {
        "id": "6efc6d13",
        "text": "Now, what should we do? But in order to understand what we should do, we should remember uh like how back propagate uh how back propagation works. And so now I'm gonna pass in some things that I don't want to uh write from scratch because it will take too much time. But if you guys remember, uh let's assume we are like at the uh rightmost weight matrix, right? So say, for example, we are, we have like this network with three layers, then we are like a W-2 and then we want to calculate the error uh with respect the derivative of the error function with respect to uh W-2 or like in this, in this case, like if we want to keep it a general we could say Wy and this is given by this formula down here, right. So if you guys remember, it's basically like this is the error which is the difference between uh why, which is the actual outcome that we are expecting. And the prediction.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=715s",
        "start_time": "715.57"
    },
    {
        "id": "c3f323aa",
        "text": "uh let's assume we are like at the uh rightmost weight matrix, right? So say, for example, we are, we have like this network with three layers, then we are like a W-2 and then we want to calculate the error uh with respect the derivative of the error function with respect to uh W-2 or like in this, in this case, like if we want to keep it a general we could say Wy and this is given by this formula down here, right. So if you guys remember, it's basically like this is the error which is the difference between uh why, which is the actual outcome that we are expecting. And the prediction. And we multiply that by uh the uh derivative of the sigmoid function calculated in uh the net input I plus one. And then we do like a dot pro max multiplication of all of this with uh the activations calculated in I nice",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=736s",
        "start_time": "736.869"
    },
    {
        "id": "761762e5",
        "text": "and this is given by this formula down here, right. So if you guys remember, it's basically like this is the error which is the difference between uh why, which is the actual outcome that we are expecting. And the prediction. And we multiply that by uh the uh derivative of the sigmoid function calculated in uh the net input I plus one. And then we do like a dot pro max multiplication of all of this with uh the activations calculated in I nice good. So the error that we are passing here, this argument is basically this guy here, this error here. So now the next thing that we want to calculate is this Sigma",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=765s",
        "start_time": "765.9"
    },
    {
        "id": "aff61718",
        "text": "And we multiply that by uh the uh derivative of the sigmoid function calculated in uh the net input I plus one. And then we do like a dot pro max multiplication of all of this with uh the activations calculated in I nice good. So the error that we are passing here, this argument is basically this guy here, this error here. So now the next thing that we want to calculate is this Sigma uh prime here, right? But uh what's this Sigma prime calculated in this uh net input here? But well, we can rewrite this like if you remember from the, the previous uh video,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=781s",
        "start_time": "781.539"
    },
    {
        "id": "6f7bd3f4",
        "text": "good. So the error that we are passing here, this argument is basically this guy here, this error here. So now the next thing that we want to calculate is this Sigma uh prime here, right? But uh what's this Sigma prime calculated in this uh net input here? But well, we can rewrite this like if you remember from the, the previous uh video, uh we can rewrite the Sigma prime here as the this derivative. The first derivative is the Sigma Sigma itself calculated in a point and multiplied by minus uh sorry one minus like Sigma calculated in that same point good. But",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=801s",
        "start_time": "801.03"
    },
    {
        "id": "0810bf18",
        "text": "uh prime here, right? But uh what's this Sigma prime calculated in this uh net input here? But well, we can rewrite this like if you remember from the, the previous uh video, uh we can rewrite the Sigma prime here as the this derivative. The first derivative is the Sigma Sigma itself calculated in a point and multiplied by minus uh sorry one minus like Sigma calculated in that same point good. But uh if you remember guys this, the sigma calculated in this point here in I plus one, it's basically the activation",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=815s",
        "start_time": "815.01"
    },
    {
        "id": "f4788f28",
        "text": "uh we can rewrite the Sigma prime here as the this derivative. The first derivative is the Sigma Sigma itself calculated in a point and multiplied by minus uh sorry one minus like Sigma calculated in that same point good. But uh if you remember guys this, the sigma calculated in this point here in I plus one, it's basically the activation calculated in I plus one. And so basically we can get this and just like pop it into uh like this function over here. And we'll get uh the Sigma prime uh information about Sigma prime. So all of this basically to say that we need to retrieve the activations here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=828s",
        "start_time": "828.799"
    },
    {
        "id": "5f4327c3",
        "text": "uh if you remember guys this, the sigma calculated in this point here in I plus one, it's basically the activation calculated in I plus one. And so basically we can get this and just like pop it into uh like this function over here. And we'll get uh the Sigma prime uh information about Sigma prime. So all of this basically to say that we need to retrieve the activations here. But these activations, we can access them because we've uh saved them in self dot activations but not in uh in uh not the activations in I but in I plus one. So in the subsequent layer, right? And so",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=850s",
        "start_time": "850.19"
    },
    {
        "id": "5d3636c9",
        "text": "calculated in I plus one. And so basically we can get this and just like pop it into uh like this function over here. And we'll get uh the Sigma prime uh information about Sigma prime. So all of this basically to say that we need to retrieve the activations here. But these activations, we can access them because we've uh saved them in self dot activations but not in uh in uh not the activations in I but in I plus one. So in the subsequent layer, right? And so we have this as activations nice. So now if we want to calculate uh this guy here, so the Sigma",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=862s",
        "start_time": "862.679"
    },
    {
        "id": "ba2037ad",
        "text": "But these activations, we can access them because we've uh saved them in self dot activations but not in uh in uh not the activations in I but in I plus one. So in the subsequent layer, right? And so we have this as activations nice. So now if we want to calculate uh this guy here, so the Sigma uh over here,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=886s",
        "start_time": "886.75"
    },
    {
        "id": "3a29411f",
        "text": "we have this as activations nice. So now if we want to calculate uh this guy here, so the Sigma uh over here, so what we need to do is basically do like a uh Sigma uh the first derivative of the Sigma function calculated in this,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=905s",
        "start_time": "905.799"
    },
    {
        "id": "2d30da43",
        "text": "uh over here, so what we need to do is basically do like a uh Sigma uh the first derivative of the Sigma function calculated in this, in these activations, right? But",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=916s",
        "start_time": "916.77"
    },
    {
        "id": "5f09e341",
        "text": "so what we need to do is basically do like a uh Sigma uh the first derivative of the Sigma function calculated in this, in these activations, right? But to do that, we'll do like something slightly uh well, we'll, we'll do like something like uh slightly different now, which is basically calculating delta and we'll define delta as these two elements together. So the error multiplied by the uh first derivative of the Sigma function. So let's do that. So delta is gonna be equal to uh the error",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=919s",
        "start_time": "919.15"
    },
    {
        "id": "75d8f29e",
        "text": "in these activations, right? But to do that, we'll do like something slightly uh well, we'll, we'll do like something like uh slightly different now, which is basically calculating delta and we'll define delta as these two elements together. So the error multiplied by the uh first derivative of the Sigma function. So let's do that. So delta is gonna be equal to uh the error and that's multiplied by self dot uh let's call it seek mo",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=929s",
        "start_time": "929.7"
    },
    {
        "id": "3ba34d19",
        "text": "to do that, we'll do like something slightly uh well, we'll, we'll do like something like uh slightly different now, which is basically calculating delta and we'll define delta as these two elements together. So the error multiplied by the uh first derivative of the Sigma function. So let's do that. So delta is gonna be equal to uh the error and that's multiplied by self dot uh let's call it seek mo uh derivative and we pass in the activations",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=932s",
        "start_time": "932.869"
    },
    {
        "id": "0c96c510",
        "text": "and that's multiplied by self dot uh let's call it seek mo uh derivative and we pass in the activations good. This is nice. But there's an issue here, which is that obviously, we don't have this uh uh function yet because we haven't like defined it. So uh let's build like this method now. So we'll build uh we'll define a new method called underscores mod",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=957s",
        "start_time": "957.969"
    },
    {
        "id": "83c37f5c",
        "text": "uh derivative and we pass in the activations good. This is nice. But there's an issue here, which is that obviously, we don't have this uh uh function yet because we haven't like defined it. So uh let's build like this method now. So we'll build uh we'll define a new method called underscores mod uh derivative",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=967s",
        "start_time": "967.27"
    },
    {
        "id": "be1d4931",
        "text": "good. This is nice. But there's an issue here, which is that obviously, we don't have this uh uh function yet because we haven't like defined it. So uh let's build like this method now. So we'll build uh we'll define a new method called underscores mod uh derivative and uh we'll pass in X and we'll return uh just like look here what this should look like. And so we'll uh return",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=972s",
        "start_time": "972.76"
    },
    {
        "id": "8b14c17c",
        "text": "uh derivative and uh we'll pass in X and we'll return uh just like look here what this should look like. And so we'll uh return X multiplied by",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=996s",
        "start_time": "996.799"
    },
    {
        "id": "1aff2600",
        "text": "and uh we'll pass in X and we'll return uh just like look here what this should look like. And so we'll uh return X multiplied by one",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=999s",
        "start_time": "999.26"
    },
    {
        "id": "136af2d4",
        "text": "X multiplied by one minus",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1012s",
        "start_time": "1012.659"
    },
    {
        "id": "9f2bb28d",
        "text": "one minus X, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1016s",
        "start_time": "1016.46"
    },
    {
        "id": "518f609c",
        "text": "minus X, right? And so we have our sigmoid derivative uh function here nice. So we have delta nice. So which basically means like we have all of this. Now, the uh next step that we need to do is instead of like uh calculating the this guy here, right? So we'll",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1019s",
        "start_time": "1019.52"
    },
    {
        "id": "d41c04c6",
        "text": "X, right? And so we have our sigmoid derivative uh function here nice. So we have delta nice. So which basically means like we have all of this. Now, the uh next step that we need to do is instead of like uh calculating the this guy here, right? So we'll need to, so in order to, to do this, uh we'll need to, to get uh this other activation uh which is the activation cal uh taken like a layer. Uh Like I,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1022s",
        "start_time": "1022.51"
    },
    {
        "id": "500ce94b",
        "text": "And so we have our sigmoid derivative uh function here nice. So we have delta nice. So which basically means like we have all of this. Now, the uh next step that we need to do is instead of like uh calculating the this guy here, right? So we'll need to, so in order to, to do this, uh we'll need to, to get uh this other activation uh which is the activation cal uh taken like a layer. Uh Like I, so we could call this uh current",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1025s",
        "start_time": "1025.02"
    },
    {
        "id": "5c2d7fed",
        "text": "need to, so in order to, to do this, uh we'll need to, to get uh this other activation uh which is the activation cal uh taken like a layer. Uh Like I, so we could call this uh current activation",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1052s",
        "start_time": "1052.92"
    },
    {
        "id": "68faee55",
        "text": "so we could call this uh current activation activations and that's equal to self dot activations. But",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1066s",
        "start_time": "1066.26"
    },
    {
        "id": "3c63c33f",
        "text": "activation activations and that's equal to self dot activations. But at uh I right.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1071s",
        "start_time": "1071.089"
    },
    {
        "id": "8f8ef7a6",
        "text": "activations and that's equal to self dot activations. But at uh I right. Nice. So now, at least in theory, we have like all the elements to",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1072s",
        "start_time": "1072.67"
    },
    {
        "id": "b91f72d6",
        "text": "at uh I right. Nice. So now, at least in theory, we have like all the elements to calculates the derivative in I, right. And uh the derivative in I uh is gonna be given by the dot product",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1080s",
        "start_time": "1080.13"
    },
    {
        "id": "6044b647",
        "text": "Nice. So now, at least in theory, we have like all the elements to calculates the derivative in I, right. And uh the derivative in I uh is gonna be given by the dot product of current activations with delta.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1083s",
        "start_time": "1083.54"
    },
    {
        "id": "e8ddabbe",
        "text": "calculates the derivative in I, right. And uh the derivative in I uh is gonna be given by the dot product of current activations with delta. Nice. And so now we have like the uh the derivative but really, we don't have it yet because uh I'm gonna explain why, but we'll need to do like some trickery which uh like nun pi to organize like this two arrays in a way where we can actually perform this matrix multiplication between the current activations and delta. So uh what we uh want to do uh as the first thing is uh",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1091s",
        "start_time": "1091.8"
    },
    {
        "id": "a5f14d69",
        "text": "of current activations with delta. Nice. And so now we have like the uh the derivative but really, we don't have it yet because uh I'm gonna explain why, but we'll need to do like some trickery which uh like nun pi to organize like this two arrays in a way where we can actually perform this matrix multiplication between the current activations and delta. So uh what we uh want to do uh as the first thing is uh basically rearranging this uh array uh and rearranging it in such a way that it will become a two dimensional array where we only have like a 11 column, right? So it's gonna be basically like a vector sorry, a vertical uh matrix. So what this basically means is that we want to OK. Yeah, let me just comment this. So we want to move from",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1106s",
        "start_time": "1106.949"
    },
    {
        "id": "00450e29",
        "text": "Nice. And so now we have like the uh the derivative but really, we don't have it yet because uh I'm gonna explain why, but we'll need to do like some trickery which uh like nun pi to organize like this two arrays in a way where we can actually perform this matrix multiplication between the current activations and delta. So uh what we uh want to do uh as the first thing is uh basically rearranging this uh array uh and rearranging it in such a way that it will become a two dimensional array where we only have like a 11 column, right? So it's gonna be basically like a vector sorry, a vertical uh matrix. So what this basically means is that we want to OK. Yeah, let me just comment this. So we want to move from uh an array. So we now have",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1113s",
        "start_time": "1113.38"
    },
    {
        "id": "ec8ffdf6",
        "text": "basically rearranging this uh array uh and rearranging it in such a way that it will become a two dimensional array where we only have like a 11 column, right? So it's gonna be basically like a vector sorry, a vertical uh matrix. So what this basically means is that we want to OK. Yeah, let me just comment this. So we want to move from uh an array. So we now have this current activations that's uh an array like this, say it could be like 0.1 and 0.2 and we want to rearrange it so that it has this structure.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1143s",
        "start_time": "1143.239"
    },
    {
        "id": "2282c01c",
        "text": "uh an array. So we now have this current activations that's uh an array like this, say it could be like 0.1 and 0.2 and we want to rearrange it so that it has this structure. So you're set here o sorry,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1172s",
        "start_time": "1172.04"
    },
    {
        "id": "9d9a45f6",
        "text": "this current activations that's uh an array like this, say it could be like 0.1 and 0.2 and we want to rearrange it so that it has this structure. So you're set here o sorry, like this.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1176s",
        "start_time": "1176.979"
    },
    {
        "id": "710bbaf1",
        "text": "So you're set here o sorry, like this. OK.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1192s",
        "start_time": "1192.88"
    },
    {
        "id": "f59ba3d9",
        "text": "like this. OK. So basically, this is gonna be like a two D array and uh it, it's gonna be like basically like a, a vertical uh vector, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1197s",
        "start_time": "1197.229"
    },
    {
        "id": "8fe7c3e1",
        "text": "OK. So basically, this is gonna be like a two D array and uh it, it's gonna be like basically like a, a vertical uh vector, right? Good. So how do we do that? Well, we need to do like some uh trickery uh with NP. So we'll do that, the current activations. Uh Let's call it uh reshaped,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1198s",
        "start_time": "1198.989"
    },
    {
        "id": "d173153c",
        "text": "So basically, this is gonna be like a two D array and uh it, it's gonna be like basically like a, a vertical uh vector, right? Good. So how do we do that? Well, we need to do like some uh trickery uh with NP. So we'll do that, the current activations. Uh Let's call it uh reshaped, it's equal to current activations. And then we need to call the uh reshape method which is a native method in NPI. And, and so here we need to do this thing.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1201s",
        "start_time": "1201.199"
    },
    {
        "id": "451bbc6d",
        "text": "Good. So how do we do that? Well, we need to do like some uh trickery uh with NP. So we'll do that, the current activations. Uh Let's call it uh reshaped, it's equal to current activations. And then we need to call the uh reshape method which is a native method in NPI. And, and so here we need to do this thing. So current activations dot shape uh and we take like the uh the shape like of the uh the first uh like index and then we do uh like a minus one. So doing this, we'll just move uh like we restructure our array from like this",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1213s",
        "start_time": "1213.469"
    },
    {
        "id": "82e0f525",
        "text": "it's equal to current activations. And then we need to call the uh reshape method which is a native method in NPI. And, and so here we need to do this thing. So current activations dot shape uh and we take like the uh the shape like of the uh the first uh like index and then we do uh like a minus one. So doing this, we'll just move uh like we restructure our array from like this to this, right? And so, and now we just need to change this over here and we've uh like input the current activations re shape. Now, we need to do something similar for uh delta as well. So we need a delta",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1228s",
        "start_time": "1228.069"
    },
    {
        "id": "a1b3289f",
        "text": "So current activations dot shape uh and we take like the uh the shape like of the uh the first uh like index and then we do uh like a minus one. So doing this, we'll just move uh like we restructure our array from like this to this, right? And so, and now we just need to change this over here and we've uh like input the current activations re shape. Now, we need to do something similar for uh delta as well. So we need a delta reshaped",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1245s",
        "start_time": "1245.209"
    },
    {
        "id": "667aa052",
        "text": "to this, right? And so, and now we just need to change this over here and we've uh like input the current activations re shape. Now, we need to do something similar for uh delta as well. So we need a delta reshaped good. But uh let's take a look at what we want to do first. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1265s",
        "start_time": "1265.579"
    },
    {
        "id": "b253b133",
        "text": "reshaped good. But uh let's take a look at what we want to do first. So for delta, so uh let me like rewrite this like this. So we are starting with a similar uh array. So which is like a one dimensional array. And then what we want to achieve now is a ND a two dimensional array where",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1285s",
        "start_time": "1285.209"
    },
    {
        "id": "a3af9de0",
        "text": "good. But uh let's take a look at what we want to do first. So for delta, so uh let me like rewrite this like this. So we are starting with a similar uh array. So which is like a one dimensional array. And then what we want to achieve now is a ND a two dimensional array where we,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1289s",
        "start_time": "1289.219"
    },
    {
        "id": "bd30b5cf",
        "text": "for delta, so uh let me like rewrite this like this. So we are starting with a similar uh array. So which is like a one dimensional array. And then what we want to achieve now is a ND a two dimensional array where we, well, just give me a sec here",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1294s",
        "start_time": "1294.91"
    },
    {
        "id": "e9a51916",
        "text": "we, well, just give me a sec here where we have this, right. So it's basically just like a two dimensional array with a single uh row. So how do we do that? Well, we can do like something very similar to this uh to what we've done with the current activations. But obviously, in, instead of uh current activations, we should use delta, we use delta. And then here we have a, obviously like after we perform this reshape",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1315s",
        "start_time": "1315.709"
    },
    {
        "id": "77e3364e",
        "text": "well, just give me a sec here where we have this, right. So it's basically just like a two dimensional array with a single uh row. So how do we do that? Well, we can do like something very similar to this uh to what we've done with the current activations. But obviously, in, instead of uh current activations, we should use delta, we use delta. And then here we have a, obviously like after we perform this reshape until now here we have a vertical uh matrix. And so if we do dot T capital T, this will transpose the um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1317s",
        "start_time": "1317.05"
    },
    {
        "id": "f28aa100",
        "text": "where we have this, right. So it's basically just like a two dimensional array with a single uh row. So how do we do that? Well, we can do like something very similar to this uh to what we've done with the current activations. But obviously, in, instead of uh current activations, we should use delta, we use delta. And then here we have a, obviously like after we perform this reshape until now here we have a vertical uh matrix. And so if we do dot T capital T, this will transpose the um uh the array. And so we will basically move from a structure",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1321s",
        "start_time": "1321.229"
    },
    {
        "id": "e6819289",
        "text": "until now here we have a vertical uh matrix. And so if we do dot T capital T, this will transpose the um uh the array. And so we will basically move from a structure like this one to a structure like this",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1349s",
        "start_time": "1349.89"
    },
    {
        "id": "65c85a39",
        "text": "uh the array. And so we will basically move from a structure like this one to a structure like this nice. So now we have everything in place to do uh uh our to calculate like our derivative uh here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1361s",
        "start_time": "1361.979"
    },
    {
        "id": "ab18bef9",
        "text": "like this one to a structure like this nice. So now we have everything in place to do uh uh our to calculate like our derivative uh here. And so yeah,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1366s",
        "start_time": "1366.77"
    },
    {
        "id": "44d0a119",
        "text": "nice. So now we have everything in place to do uh uh our to calculate like our derivative uh here. And so yeah, this is like basically all we need to do for like the, the first",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1372s",
        "start_time": "1372.239"
    },
    {
        "id": "13c0da24",
        "text": "And so yeah, this is like basically all we need to do for like the, the first parts over here like of our back propagation. So for the utmost uh uh like weight weight matrix. So when we do like the uh the derivative of the error function with respect to uh w uh calculated in I right.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1382s",
        "start_time": "1382.31"
    },
    {
        "id": "17072c83",
        "text": "this is like basically all we need to do for like the, the first parts over here like of our back propagation. So for the utmost uh uh like weight weight matrix. So when we do like the uh the derivative of the error function with respect to uh w uh calculated in I right. OK. So now we know though that uh we, we want to just like go back one step and uh do uh a another like derivative of the area of function. But this time in uh w of I minus one.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1384s",
        "start_time": "1384.439"
    },
    {
        "id": "006130e1",
        "text": "parts over here like of our back propagation. So for the utmost uh uh like weight weight matrix. So when we do like the uh the derivative of the error function with respect to uh w uh calculated in I right. OK. So now we know though that uh we, we want to just like go back one step and uh do uh a another like derivative of the area of function. But this time in uh w of I minus one. So, and how do we do that uh like with a formula and we've already sent this and I'm gonna just like copy, paste it here just like because like it's, it's easier,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1391s",
        "start_time": "1391.979"
    },
    {
        "id": "13265261",
        "text": "OK. So now we know though that uh we, we want to just like go back one step and uh do uh a another like derivative of the area of function. But this time in uh w of I minus one. So, and how do we do that uh like with a formula and we've already sent this and I'm gonna just like copy, paste it here just like because like it's, it's easier, right? And so as you see here, so when we are calculating the next",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1409s",
        "start_time": "1409.369"
    },
    {
        "id": "9112cb84",
        "text": "So, and how do we do that uh like with a formula and we've already sent this and I'm gonna just like copy, paste it here just like because like it's, it's easier, right? And so as you see here, so when we are calculating the next um derivative towards the left, what we, what we are going to bring back uh from the previous um derivative that we've calculated is this guy here, right? So the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1429s",
        "start_time": "1429.189"
    },
    {
        "id": "f513e1b1",
        "text": "right? And so as you see here, so when we are calculating the next um derivative towards the left, what we, what we are going to bring back uh from the previous um derivative that we've calculated is this guy here, right? So the delta itself. So delta is gonna be like moved uh to pushed back to towards like the, the first the, the previous uh layer. But along with that given like we are like at I right now, we could also calculate all of this. So basically doing um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1440s",
        "start_time": "1440.91"
    },
    {
        "id": "ffb9cced",
        "text": "um derivative towards the left, what we, what we are going to bring back uh from the previous um derivative that we've calculated is this guy here, right? So the delta itself. So delta is gonna be like moved uh to pushed back to towards like the, the first the, the previous uh layer. But along with that given like we are like at I right now, we could also calculate all of this. So basically doing um we're going to calculate a new error here, which is going to be given by the NP",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1446s",
        "start_time": "1446.91"
    },
    {
        "id": "00c41881",
        "text": "delta itself. So delta is gonna be like moved uh to pushed back to towards like the, the first the, the previous uh layer. But along with that given like we are like at I right now, we could also calculate all of this. So basically doing um we're going to calculate a new error here, which is going to be given by the NP dots. So we want to do a matrix multiplication here between delta itself.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1463s",
        "start_time": "1463.349"
    },
    {
        "id": "055aaafe",
        "text": "we're going to calculate a new error here, which is going to be given by the NP dots. So we want to do a matrix multiplication here between delta itself. And",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1486s",
        "start_time": "1486.0"
    },
    {
        "id": "7121f38f",
        "text": "dots. So we want to do a matrix multiplication here between delta itself. And we want to do that with the with the weights with the weight matrix.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1496s",
        "start_time": "1496.66"
    },
    {
        "id": "467aab3d",
        "text": "And we want to do that with the with the weights with the weight matrix. So it's this guy here",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1505s",
        "start_time": "1505.329"
    },
    {
        "id": "b921dcfc",
        "text": "we want to do that with the with the weights with the weight matrix. So it's this guy here uh for the I uh layer and here we need like to do just like the transpose uh matrix for that.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1507s",
        "start_time": "1507.13"
    },
    {
        "id": "e6dda6c9",
        "text": "So it's this guy here uh for the I uh layer and here we need like to do just like the transpose uh matrix for that. Uh cool. So now this error here is basically all of this guy here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1514s",
        "start_time": "1514.699"
    },
    {
        "id": "84d8a3ac",
        "text": "uh for the I uh layer and here we need like to do just like the transpose uh matrix for that. Uh cool. So now this error here is basically all of this guy here. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1517s",
        "start_time": "1517.579"
    },
    {
        "id": "c5ab84b2",
        "text": "Uh cool. So now this error here is basically all of this guy here. So now that we are here, you'll see that we are just like gonna start another loop and we are going down with I. So if the first time it was like, I equal two, now we're going for I equal one. So we are gonna do the derivatives for W one,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1527s",
        "start_time": "1527.229"
    },
    {
        "id": "ec6e87f5",
        "text": "So now that we are here, you'll see that we are just like gonna start another loop and we are going down with I. So if the first time it was like, I equal two, now we're going for I equal one. So we are gonna do the derivatives for W one, right? And so uh now we're just gonna do like the same thing once again. Uh But uh this time we have our error and our error is given by all of this guy here, which we calculated in the previous passage in the previous um iteration.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1534s",
        "start_time": "1534.709"
    },
    {
        "id": "0d30042b",
        "text": "now that we are here, you'll see that we are just like gonna start another loop and we are going down with I. So if the first time it was like, I equal two, now we're going for I equal one. So we are gonna do the derivatives for W one, right? And so uh now we're just gonna do like the same thing once again. Uh But uh this time we have our error and our error is given by all of this guy here, which we calculated in the previous passage in the previous um iteration. And now we are calculating delta uh which this time around is gonna be like this guy. And then we are gonna calculate uh this derivative here by doing the matrix multiplication between the current activations and which are like which is this and the delta reshaped like over here. Nice. And so now we can",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1536s",
        "start_time": "1536.42"
    },
    {
        "id": "1b6bd4d6",
        "text": "right? And so uh now we're just gonna do like the same thing once again. Uh But uh this time we have our error and our error is given by all of this guy here, which we calculated in the previous passage in the previous um iteration. And now we are calculating delta uh which this time around is gonna be like this guy. And then we are gonna calculate uh this derivative here by doing the matrix multiplication between the current activations and which are like which is this and the delta reshaped like over here. Nice. And so now we can go all the way back until we are like at the input level. So I know like this was like a little bit, I would say like more difficult to understand than like a simple case where we have like hard wired or like layers. But with this back propagation, we can have potentially infinite layers. And this back pro back propagate method is gonna work for all of them. Nice.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1553s",
        "start_time": "1553.599"
    },
    {
        "id": "b239b072",
        "text": "And now we are calculating delta uh which this time around is gonna be like this guy. And then we are gonna calculate uh this derivative here by doing the matrix multiplication between the current activations and which are like which is this and the delta reshaped like over here. Nice. And so now we can go all the way back until we are like at the input level. So I know like this was like a little bit, I would say like more difficult to understand than like a simple case where we have like hard wired or like layers. But with this back propagation, we can have potentially infinite layers. And this back pro back propagate method is gonna work for all of them. Nice. So now as the final thing, we could potentially return uh the error which in this case is going to be the error back propagated all the way back to the input layer",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1572s",
        "start_time": "1572.359"
    },
    {
        "id": "5c8b385e",
        "text": "go all the way back until we are like at the input level. So I know like this was like a little bit, I would say like more difficult to understand than like a simple case where we have like hard wired or like layers. But with this back propagation, we can have potentially infinite layers. And this back pro back propagate method is gonna work for all of them. Nice. So now as the final thing, we could potentially return uh the error which in this case is going to be the error back propagated all the way back to the input layer good.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1600s",
        "start_time": "1600.05"
    },
    {
        "id": "b56491cf",
        "text": "So now as the final thing, we could potentially return uh the error which in this case is going to be the error back propagated all the way back to the input layer good. OK. So this was uh like back propagation. So I suggest now like we uh do uh try to like run the code up until like what we've done uh like now to see like if everything is OK? Because we may have like some bugs because we don't know.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1625s",
        "start_time": "1625.78"
    },
    {
        "id": "10d66a0c",
        "text": "good. OK. So this was uh like back propagation. So I suggest now like we uh do uh try to like run the code up until like what we've done uh like now to see like if everything is OK? Because we may have like some bugs because we don't know. Right. So OK. So first of all,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1638s",
        "start_time": "1638.239"
    },
    {
        "id": "542876b6",
        "text": "OK. So this was uh like back propagation. So I suggest now like we uh do uh try to like run the code up until like what we've done uh like now to see like if everything is OK? Because we may have like some bugs because we don't know. Right. So OK. So first of all, yeah, let me",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1639s",
        "start_time": "1639.739"
    },
    {
        "id": "cb3bf6ef",
        "text": "Right. So OK. So first of all, yeah, let me to",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1658s",
        "start_time": "1658.16"
    },
    {
        "id": "73ecad53",
        "text": "yeah, let me to this if name is equal to main. And so here we are like ensuring that we run this like as the main script uh good. So what do we want to do here? Well, uh first of all, uh we want to um create uh an M LP,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1663s",
        "start_time": "1663.38"
    },
    {
        "id": "62decd32",
        "text": "to this if name is equal to main. And so here we are like ensuring that we run this like as the main script uh good. So what do we want to do here? Well, uh first of all, uh we want to um create uh an M LP, it's a multi layer uh perception and then we want to Yeah. Yeah, let's just like create a dummy,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1665s",
        "start_time": "1665.319"
    },
    {
        "id": "536f341b",
        "text": "this if name is equal to main. And so here we are like ensuring that we run this like as the main script uh good. So what do we want to do here? Well, uh first of all, uh we want to um create uh an M LP, it's a multi layer uh perception and then we want to Yeah. Yeah, let's just like create a dummy, data domain, inputs and targets and then we're gonna pass uh this data so to forward propagate. Uh So we, we'll do like a forward uh propagation and then we'll do a back propagation",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1667s",
        "start_time": "1667.349"
    },
    {
        "id": "f2ed57c1",
        "text": "it's a multi layer uh perception and then we want to Yeah. Yeah, let's just like create a dummy, data domain, inputs and targets and then we're gonna pass uh this data so to forward propagate. Uh So we, we'll do like a forward uh propagation and then we'll do a back propagation good.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1689s",
        "start_time": "1689.54"
    },
    {
        "id": "6321056e",
        "text": "data domain, inputs and targets and then we're gonna pass uh this data so to forward propagate. Uh So we, we'll do like a forward uh propagation and then we'll do a back propagation good. OK. So let's start by creating an M LP. So that's quite simple now because we already know",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1698s",
        "start_time": "1698.599"
    },
    {
        "id": "66f91065",
        "text": "good. OK. So let's start by creating an M LP. So that's quite simple now because we already know and we have this guy here. And so let's say here we should pass the number of like um inputs, the number of hidden layers and the number of outputs. So let's do something like this. So two neurons for the input layers. So now we want only a one hidden layer which say like five neurons and then we have just one neuron for the output layer good.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1718s",
        "start_time": "1718.349"
    },
    {
        "id": "6349ee73",
        "text": "OK. So let's start by creating an M LP. So that's quite simple now because we already know and we have this guy here. And so let's say here we should pass the number of like um inputs, the number of hidden layers and the number of outputs. So let's do something like this. So two neurons for the input layers. So now we want only a one hidden layer which say like five neurons and then we have just one neuron for the output layer good. OK. So now let's create some dummy uh data. So we'll create uh an input and this input is going to be an array where we pass in couple of numbers, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1719s",
        "start_time": "1719.489"
    },
    {
        "id": "0a492fac",
        "text": "and we have this guy here. And so let's say here we should pass the number of like um inputs, the number of hidden layers and the number of outputs. So let's do something like this. So two neurons for the input layers. So now we want only a one hidden layer which say like five neurons and then we have just one neuron for the output layer good. OK. So now let's create some dummy uh data. So we'll create uh an input and this input is going to be an array where we pass in couple of numbers, right? And why should we pass a couple of numbers here? Because we have like two neurons and basically each of these guys uh in the array is gonna uh go to one of the two different neurons, right? OK. So we have the input and now we want the, the target",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1727s",
        "start_time": "1727.41"
    },
    {
        "id": "3151eac3",
        "text": "OK. So now let's create some dummy uh data. So we'll create uh an input and this input is going to be an array where we pass in couple of numbers, right? And why should we pass a couple of numbers here? Because we have like two neurons and basically each of these guys uh in the array is gonna uh go to one of the two different neurons, right? OK. So we have the input and now we want the, the target and the target is gonna be",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1752s",
        "start_time": "1752.26"
    },
    {
        "id": "ae5cf032",
        "text": "And why should we pass a couple of numbers here? Because we have like two neurons and basically each of these guys uh in the array is gonna uh go to one of the two different neurons, right? OK. So we have the input and now we want the, the target and the target is gonna be uh itself a a an array and uh let's say 0.3. So you probably are seeing what I'm doing here. So I'm expecting a the sum like of these two numbers, right? So I hope like my network is gonna uh like learn how to do the sum operation. And yeah, that's what we are gonna use like as a uh as a task dummy like toy task like for uh this video",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1768s",
        "start_time": "1768.17"
    },
    {
        "id": "c5581586",
        "text": "and the target is gonna be uh itself a a an array and uh let's say 0.3. So you probably are seeing what I'm doing here. So I'm expecting a the sum like of these two numbers, right? So I hope like my network is gonna uh like learn how to do the sum operation. And yeah, that's what we are gonna use like as a uh as a task dummy like toy task like for uh this video good. So, but here just we have just a single input and a single target. That's all we need uh for now. So what should we do now? Well, Uh First of all, we should do an M LP dot forward propagate and we want to pass the input in. And so we'll pass this guy here and then we'll do a back uh propagation here. So M LP dot",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1785s",
        "start_time": "1785.13"
    },
    {
        "id": "6ee85ff4",
        "text": "uh itself a a an array and uh let's say 0.3. So you probably are seeing what I'm doing here. So I'm expecting a the sum like of these two numbers, right? So I hope like my network is gonna uh like learn how to do the sum operation. And yeah, that's what we are gonna use like as a uh as a task dummy like toy task like for uh this video good. So, but here just we have just a single input and a single target. That's all we need uh for now. So what should we do now? Well, Uh First of all, we should do an M LP dot forward propagate and we want to pass the input in. And so we'll pass this guy here and then we'll do a back uh propagation here. So M LP dot back propagate and obviously, we are expecting an output over here",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1790s",
        "start_time": "1790.01"
    },
    {
        "id": "a46fb667",
        "text": "good. So, but here just we have just a single input and a single target. That's all we need uh for now. So what should we do now? Well, Uh First of all, we should do an M LP dot forward propagate and we want to pass the input in. And so we'll pass this guy here and then we'll do a back uh propagation here. So M LP dot back propagate and obviously, we are expecting an output over here uh which is basically like the, the uh after like all the computation has been done uh like on the inputs we're getting like the prediction, which is this output. Well, uh I forgot to mention one step here uh before doing back propagation. And you should know by now what that step should be and it's calculates uh the error, right? So, and how do we calculate the error? So, well, we calculate the error doing a target",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1817s",
        "start_time": "1817.959"
    },
    {
        "id": "70a34385",
        "text": "back propagate and obviously, we are expecting an output over here uh which is basically like the, the uh after like all the computation has been done uh like on the inputs we're getting like the prediction, which is this output. Well, uh I forgot to mention one step here uh before doing back propagation. And you should know by now what that step should be and it's calculates uh the error, right? So, and how do we calculate the error? So, well, we calculate the error doing a target minus the oh",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1847s",
        "start_time": "1847.01"
    },
    {
        "id": "6d044248",
        "text": "uh which is basically like the, the uh after like all the computation has been done uh like on the inputs we're getting like the prediction, which is this output. Well, uh I forgot to mention one step here uh before doing back propagation. And you should know by now what that step should be and it's calculates uh the error, right? So, and how do we calculate the error? So, well, we calculate the error doing a target minus the oh the output.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1854s",
        "start_time": "1854.959"
    },
    {
        "id": "67004cef",
        "text": "minus the oh the output. And then when we do uh back propagation, we'll just pass in the error, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1884s",
        "start_time": "1884.38"
    },
    {
        "id": "ed55f587",
        "text": "the output. And then when we do uh back propagation, we'll just pass in the error, right? OK.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1887s",
        "start_time": "1887.959"
    },
    {
        "id": "46985cfb",
        "text": "And then when we do uh back propagation, we'll just pass in the error, right? OK. So uh let's try to see if uh we get what we expect uh from uh back propagation. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1890s",
        "start_time": "1890.219"
    },
    {
        "id": "100c8ac8",
        "text": "OK. So uh let's try to see if uh we get what we expect uh from uh back propagation. So I'd say",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1899s",
        "start_time": "1899.03"
    },
    {
        "id": "966630c1",
        "text": "So uh let's try to see if uh we get what we expect uh from uh back propagation. So I'd say what we could do here is we could",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1900s",
        "start_time": "1900.31"
    },
    {
        "id": "d1c5b351",
        "text": "I'd say what we could do here is we could uh",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1911s",
        "start_time": "1911.18"
    },
    {
        "id": "7f67db8c",
        "text": "what we could do here is we could uh have for the timing, a verbose",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1913s",
        "start_time": "1913.18"
    },
    {
        "id": "dd2a54c9",
        "text": "uh have for the timing, a verbose argument which will set as false initially. And then we say",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1918s",
        "start_time": "1918.04"
    },
    {
        "id": "f14cea2f",
        "text": "have for the timing, a verbose argument which will set as false initially. And then we say if the boys,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1919s",
        "start_time": "1919.319"
    },
    {
        "id": "5451d459",
        "text": "argument which will set as false initially. And then we say if the boys, so if we are in verbose merge, then we want to print",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1925s",
        "start_time": "1925.089"
    },
    {
        "id": "9991bc9d",
        "text": "if the boys, so if we are in verbose merge, then we want to print uh what do we want to print here? Well, we want to print the uh derivatives that we've calculated uh like uh for like a specific weight matrix. So how do we do that? Well, uh we do that by saying",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1932s",
        "start_time": "1932.02"
    },
    {
        "id": "67b11d67",
        "text": "so if we are in verbose merge, then we want to print uh what do we want to print here? Well, we want to print the uh derivatives that we've calculated uh like uh for like a specific weight matrix. So how do we do that? Well, uh we do that by saying uh derivatives",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1935s",
        "start_time": "1935.069"
    },
    {
        "id": "647d7b17",
        "text": "uh what do we want to print here? Well, we want to print the uh derivatives that we've calculated uh like uh for like a specific weight matrix. So how do we do that? Well, uh we do that by saying uh derivatives four",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1939s",
        "start_time": "1939.849"
    },
    {
        "id": "4c11cb75",
        "text": "uh derivatives four W",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1958s",
        "start_time": "1958.209"
    },
    {
        "id": "0fc2f173",
        "text": "four W and then here we'll say",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1962s",
        "start_time": "1962.459"
    },
    {
        "id": "d2193508",
        "text": "W and then here we'll say uh I,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1966s",
        "start_time": "1966.13"
    },
    {
        "id": "66d97396",
        "text": "and then here we'll say uh I, and yeah, and I think like we are like uh",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1967s",
        "start_time": "1967.739"
    },
    {
        "id": "f5cdfbf5",
        "text": "uh I, and yeah, and I think like we are like uh uh this is, this is equal",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1971s",
        "start_time": "1971.489"
    },
    {
        "id": "deb62c9d",
        "text": "and yeah, and I think like we are like uh uh this is, this is equal two",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1973s",
        "start_time": "1973.78"
    },
    {
        "id": "15621d13",
        "text": "uh this is, this is equal two uh",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1978s",
        "start_time": "1978.39"
    },
    {
        "id": "0329c038",
        "text": "two uh uh to this uh self derivatives calculated in I good.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1983s",
        "start_time": "1983.589"
    },
    {
        "id": "c51df0bf",
        "text": "uh uh to this uh self derivatives calculated in I good. OK. Cool. So fingers crossed, this should work until like we've made some mistakes in the process. So let's see if this works.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1985s",
        "start_time": "1985.93"
    },
    {
        "id": "fd152545",
        "text": "uh to this uh self derivatives calculated in I good. OK. Cool. So fingers crossed, this should work until like we've made some mistakes in the process. So let's see if this works. Yeah.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1986s",
        "start_time": "1986.969"
    },
    {
        "id": "4f2341e8",
        "text": "OK. Cool. So fingers crossed, this should work until like we've made some mistakes in the process. So let's see if this works. Yeah. Well, obviously I've run this but I should pass in the verbose",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=1993s",
        "start_time": "1993.729"
    },
    {
        "id": "5ac3d213",
        "text": "Yeah. Well, obviously I've run this but I should pass in the verbose uh equals true. Like if you, if you want to see like something coming up.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2004s",
        "start_time": "2004.91"
    },
    {
        "id": "17655db0",
        "text": "Well, obviously I've run this but I should pass in the verbose uh equals true. Like if you, if you want to see like something coming up. So let's say this. Oh Nice, nice.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2007s",
        "start_time": "2007.5"
    },
    {
        "id": "071d6959",
        "text": "uh equals true. Like if you, if you want to see like something coming up. So let's say this. Oh Nice, nice. So what do we have here? So we have the derivatives for W one. And indeed, we are expecting this structure like for like this matrix which is like uh basically five by one. So it's just like a column",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2014s",
        "start_time": "2014.55"
    },
    {
        "id": "4515b74a",
        "text": "So let's say this. Oh Nice, nice. So what do we have here? So we have the derivatives for W one. And indeed, we are expecting this structure like for like this matrix which is like uh basically five by one. So it's just like a column matrix. And why is that the case? Well, because we have five neurons like in the, in this hidden layer and just like one neuron here like as the uh output layer. And so we expect like a five by one matrix there for W",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2019s",
        "start_time": "2019.56"
    },
    {
        "id": "dac83034",
        "text": "So what do we have here? So we have the derivatives for W one. And indeed, we are expecting this structure like for like this matrix which is like uh basically five by one. So it's just like a column matrix. And why is that the case? Well, because we have five neurons like in the, in this hidden layer and just like one neuron here like as the uh output layer. And so we expect like a five by one matrix there for W uh one, right? So now for W zero over here, which is gonna be like between like the input layer and the, and the first hidden layer or the only hidden layer. So we are expecting a matrix which should be",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2023s",
        "start_time": "2023.219"
    },
    {
        "id": "031b7816",
        "text": "matrix. And why is that the case? Well, because we have five neurons like in the, in this hidden layer and just like one neuron here like as the uh output layer. And so we expect like a five by one matrix there for W uh one, right? So now for W zero over here, which is gonna be like between like the input layer and the, and the first hidden layer or the only hidden layer. So we are expecting a matrix which should be and we have it here uh two by five. And indeed we have it. So as you see it here, we have like five different values for each row. And that's like what we were expecting. So we are like on, on, on the right path here, but it ain't finished yet. So we, we should keep like moving on.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2040s",
        "start_time": "2040.31"
    },
    {
        "id": "4d4d7c3a",
        "text": "uh one, right? So now for W zero over here, which is gonna be like between like the input layer and the, and the first hidden layer or the only hidden layer. So we are expecting a matrix which should be and we have it here uh two by five. And indeed we have it. So as you see it here, we have like five different values for each row. And that's like what we were expecting. So we are like on, on, on the right path here, but it ain't finished yet. So we, we should keep like moving on. And so what we've done so far is we've activated. So we saved the activations and the derivatives, we've implemented back propagation. Now, we need to implement guardian descent. But I promise this is gonna be like way, way faster than back propagation because it's quite straightforward. So good. OK. So let's move on uh with uh a new method which will call gradient uh the scent.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2058s",
        "start_time": "2058.878"
    },
    {
        "id": "28907403",
        "text": "and we have it here uh two by five. And indeed we have it. So as you see it here, we have like five different values for each row. And that's like what we were expecting. So we are like on, on, on the right path here, but it ain't finished yet. So we, we should keep like moving on. And so what we've done so far is we've activated. So we saved the activations and the derivatives, we've implemented back propagation. Now, we need to implement guardian descent. But I promise this is gonna be like way, way faster than back propagation because it's quite straightforward. So good. OK. So let's move on uh with uh a new method which will call gradient uh the scent. And here,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2080s",
        "start_time": "2080.01"
    },
    {
        "id": "453a656d",
        "text": "And so what we've done so far is we've activated. So we saved the activations and the derivatives, we've implemented back propagation. Now, we need to implement guardian descent. But I promise this is gonna be like way, way faster than back propagation because it's quite straightforward. So good. OK. So let's move on uh with uh a new method which will call gradient uh the scent. And here, so we need to pass an argument. This is called the learning rate. And we've seen this in the last video and this is like the, the size of the step we want to take against the gradient uh just like to optimize uh our uh error function, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2102s",
        "start_time": "2102.03"
    },
    {
        "id": "ef681d7d",
        "text": "And here, so we need to pass an argument. This is called the learning rate. And we've seen this in the last video and this is like the, the size of the step we want to take against the gradient uh just like to optimize uh our uh error function, right? Uh good. So now, what should we do?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2132s",
        "start_time": "2132.26"
    },
    {
        "id": "6481f62c",
        "text": "so we need to pass an argument. This is called the learning rate. And we've seen this in the last video and this is like the, the size of the step we want to take against the gradient uh just like to optimize uh our uh error function, right? Uh good. So now, what should we do? Well,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2134s",
        "start_time": "2134.439"
    },
    {
        "id": "27da119c",
        "text": "Uh good. So now, what should we do? Well, uh so now we should go loop through all the weights.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2153s",
        "start_time": "2153.189"
    },
    {
        "id": "17fa1ef3",
        "text": "Well, uh so now we should go loop through all the weights. And so we're gonna do that by doing a four loop in, we could say on a range L",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2159s",
        "start_time": "2159.57"
    },
    {
        "id": "c56a1c06",
        "text": "uh so now we should go loop through all the weights. And so we're gonna do that by doing a four loop in, we could say on a range L of self dot uh weights. So we are going through like all the uh different, sorry, different weight mattresses. And then we can say that the weight uh weight mattress matrix for",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2160s",
        "start_time": "2160.989"
    },
    {
        "id": "602cc10a",
        "text": "And so we're gonna do that by doing a four loop in, we could say on a range L of self dot uh weights. So we are going through like all the uh different, sorry, different weight mattresses. And then we can say that the weight uh weight mattress matrix for bye",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2167s",
        "start_time": "2167.959"
    },
    {
        "id": "6a0bfd11",
        "text": "of self dot uh weights. So we are going through like all the uh different, sorry, different weight mattresses. And then we can say that the weight uh weight mattress matrix for bye the current layer over here. It's gonna be self weights calculated uh like N I and we can do like the the sa me thing uh with derivatives. So we'll just change this to.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2178s",
        "start_time": "2178.51"
    },
    {
        "id": "f98f0682",
        "text": "bye the current layer over here. It's gonna be self weights calculated uh like N I and we can do like the the sa me thing uh with derivatives. So we'll just change this to. And so basically we are getting uh we are retrieving the weights and the relative derivatives for a given layer, right? And so what we want to do here is to update the weights and to update the weights. If you guys remember if you're good students, you may, you should know this by now because we've covered this last in the last video. But what we should do here is",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2196s",
        "start_time": "2196.34"
    },
    {
        "id": "cdfe6a26",
        "text": "the current layer over here. It's gonna be self weights calculated uh like N I and we can do like the the sa me thing uh with derivatives. So we'll just change this to. And so basically we are getting uh we are retrieving the weights and the relative derivatives for a given layer, right? And so what we want to do here is to update the weights and to update the weights. If you guys remember if you're good students, you may, you should know this by now because we've covered this last in the last video. But what we should do here is uh taking the weights and um we should",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2197s",
        "start_time": "2197.36"
    },
    {
        "id": "15fb2b19",
        "text": "And so basically we are getting uh we are retrieving the weights and the relative derivatives for a given layer, right? And so what we want to do here is to update the weights and to update the weights. If you guys remember if you're good students, you may, you should know this by now because we've covered this last in the last video. But what we should do here is uh taking the weights and um we should add to the weight, the derivatives",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2214s",
        "start_time": "2214.679"
    },
    {
        "id": "b7259093",
        "text": "uh taking the weights and um we should add to the weight, the derivatives multiplied by the learning rate. Nice. So let's rewrite this in a more compact way. And so we can just like",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2240s",
        "start_time": "2240.209"
    },
    {
        "id": "286d1b08",
        "text": "add to the weight, the derivatives multiplied by the learning rate. Nice. So let's rewrite this in a more compact way. And so we can just like do this. It's the same thing. So we are uh oops sorry. Yeah. So we are uh adding the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2245s",
        "start_time": "2245.719"
    },
    {
        "id": "d2d17fa8",
        "text": "multiplied by the learning rate. Nice. So let's rewrite this in a more compact way. And so we can just like do this. It's the same thing. So we are uh oops sorry. Yeah. So we are uh adding the this color multiplication of like learning rate, um derivatives to weights. And so these are weights and derivatives are two matrices like with the uh with the same dimensions, right? And so we are just like adding them up once we've tweaked the derivatives by dec decide by applying the learning rate",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2251s",
        "start_time": "2251.08"
    },
    {
        "id": "af2ae733",
        "text": "do this. It's the same thing. So we are uh oops sorry. Yeah. So we are uh adding the this color multiplication of like learning rate, um derivatives to weights. And so these are weights and derivatives are two matrices like with the uh with the same dimensions, right? And so we are just like adding them up once we've tweaked the derivatives by dec decide by applying the learning rate good. And so here we have done",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2261s",
        "start_time": "2261.78"
    },
    {
        "id": "5d80b058",
        "text": "this color multiplication of like learning rate, um derivatives to weights. And so these are weights and derivatives are two matrices like with the uh with the same dimensions, right? And so we are just like adding them up once we've tweaked the derivatives by dec decide by applying the learning rate good. And so here we have done uh so we, we, we now have like also gradient descent which is great. So now let's go back to uh like our script here. So we've done back propagation. So the next thing we want to do is we want to apply gradient descent good. So how do we do that? Well, now that we have our uh gradient descent method, this is gonna be super simple to do. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2271s",
        "start_time": "2271.85"
    },
    {
        "id": "6806e600",
        "text": "good. And so here we have done uh so we, we, we now have like also gradient descent which is great. So now let's go back to uh like our script here. So we've done back propagation. So the next thing we want to do is we want to apply gradient descent good. So how do we do that? Well, now that we have our uh gradient descent method, this is gonna be super simple to do. So we pass in",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2291s",
        "start_time": "2291.739"
    },
    {
        "id": "fa5ba0d8",
        "text": "uh so we, we, we now have like also gradient descent which is great. So now let's go back to uh like our script here. So we've done back propagation. So the next thing we want to do is we want to apply gradient descent good. So how do we do that? Well, now that we have our uh gradient descent method, this is gonna be super simple to do. So we pass in uh a learning rate. And let's say we want to, to pass in 0.1 for example, as our learning rate good. OK. But uh let's see if gradient descent uh like is working properly. And so for doing that, uh I would come here and I would like to",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2295s",
        "start_time": "2295.659"
    },
    {
        "id": "f94067ff",
        "text": "we pass in uh a learning rate. And let's say we want to, to pass in 0.1 for example, as our learning rate good. OK. But uh let's see if gradient descent uh like is working properly. And so for doing that, uh I would come here and I would like to uh print,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2325s",
        "start_time": "2325.51"
    },
    {
        "id": "d5335b2b",
        "text": "uh a learning rate. And let's say we want to, to pass in 0.1 for example, as our learning rate good. OK. But uh let's see if gradient descent uh like is working properly. And so for doing that, uh I would come here and I would like to uh print, yeah, let's do it here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2327s",
        "start_time": "2327.35"
    },
    {
        "id": "cf989965",
        "text": "uh print, yeah, let's do it here. So let's do a print. We'll print the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2347s",
        "start_time": "2347.81"
    },
    {
        "id": "a39d5214",
        "text": "yeah, let's do it here. So let's do a print. We'll print the uh weights.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2350s",
        "start_time": "2350.05"
    },
    {
        "id": "d89787de",
        "text": "So let's do a print. We'll print the uh weights. So we'll, we'll do a w",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2352s",
        "start_time": "2352.07"
    },
    {
        "id": "998c303e",
        "text": "uh weights. So we'll, we'll do a w and I'll go to format and we'll pass in.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2358s",
        "start_time": "2358.01"
    },
    {
        "id": "7a0d8648",
        "text": "So we'll, we'll do a w and I'll go to format and we'll pass in. I, and then here we want actually",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2363s",
        "start_time": "2363.129"
    },
    {
        "id": "20133c76",
        "text": "and I'll go to format and we'll pass in. I, and then here we want actually to pass the oops, sorry. So we ah damn. So I, and here we have the, the weights and so we, we want to pass this in and this is uh yeah, let's call them original.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2369s",
        "start_time": "2369.1"
    },
    {
        "id": "7b686726",
        "text": "I, and then here we want actually to pass the oops, sorry. So we ah damn. So I, and here we have the, the weights and so we, we want to pass this in and this is uh yeah, let's call them original. Um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2376s",
        "start_time": "2376.159"
    },
    {
        "id": "d680eaa1",
        "text": "to pass the oops, sorry. So we ah damn. So I, and here we have the, the weights and so we, we want to pass this in and this is uh yeah, let's call them original. Um W so these are like the original weights and then we can take this and after. So let's, let's, let's put some new lines here because like this is becoming like a mess, right? So here we, we take the, we retrieve the weights, we print them then uh we retrieve the derivatives, we uh apply like a gradient descent to the weights. And now we have the updated",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2380s",
        "start_time": "2380.919"
    },
    {
        "id": "c4075f1a",
        "text": "Um W so these are like the original weights and then we can take this and after. So let's, let's, let's put some new lines here because like this is becoming like a mess, right? So here we, we take the, we retrieve the weights, we print them then uh we retrieve the derivatives, we uh apply like a gradient descent to the weights. And now we have the updated oops, here we go dated uh weights",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2397s",
        "start_time": "2397.08"
    },
    {
        "id": "2722ddf2",
        "text": "W so these are like the original weights and then we can take this and after. So let's, let's, let's put some new lines here because like this is becoming like a mess, right? So here we, we take the, we retrieve the weights, we print them then uh we retrieve the derivatives, we uh apply like a gradient descent to the weights. And now we have the updated oops, here we go dated uh weights and uh yeah. And so we have like these weights which now should be different. So let's say like if this is working,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2398s",
        "start_time": "2398.879"
    },
    {
        "id": "c51a30e9",
        "text": "oops, here we go dated uh weights and uh yeah. And so we have like these weights which now should be different. So let's say like if this is working, OK. So now I'd say we don't want um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2426s",
        "start_time": "2426.479"
    },
    {
        "id": "f8661416",
        "text": "and uh yeah. And so we have like these weights which now should be different. So let's say like if this is working, OK. So now I'd say we don't want um to print this because otherwise we're gonna have like a mess. So let's try this and see. OK. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2430s",
        "start_time": "2430.469"
    },
    {
        "id": "7fcdceb2",
        "text": "OK. So now I'd say we don't want um to print this because otherwise we're gonna have like a mess. So let's try this and see. OK. So uh we have,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2438s",
        "start_time": "2438.689"
    },
    {
        "id": "a66eeb34",
        "text": "to print this because otherwise we're gonna have like a mess. So let's try this and see. OK. So uh we have, yeah. Right. There's a difference but it's so take like for example this, right? That, that's like the uh the first. So element 00 of like W zero and element 00 of like W zero here, like after we've applied it and as you can see like they are slightly different but the difference like it's quite minimal because like we have like a small learning rate. So if we change this learning rate and we put it like as one, so",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2444s",
        "start_time": "2444.889"
    },
    {
        "id": "edd66587",
        "text": "uh we have, yeah. Right. There's a difference but it's so take like for example this, right? That, that's like the uh the first. So element 00 of like W zero and element 00 of like W zero here, like after we've applied it and as you can see like they are slightly different but the difference like it's quite minimal because like we have like a small learning rate. So if we change this learning rate and we put it like as one, so this should be like quite bigger.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2454s",
        "start_time": "2454.07"
    },
    {
        "id": "22491d12",
        "text": "yeah. Right. There's a difference but it's so take like for example this, right? That, that's like the uh the first. So element 00 of like W zero and element 00 of like W zero here, like after we've applied it and as you can see like they are slightly different but the difference like it's quite minimal because like we have like a small learning rate. So if we change this learning rate and we put it like as one, so this should be like quite bigger. Yeah. So yeah,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2459s",
        "start_time": "2459.08"
    },
    {
        "id": "65d60990",
        "text": "this should be like quite bigger. Yeah. So yeah, you, you can see it from here like w one here like the, the first element here uh like it's uh 0.8 and here",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2489s",
        "start_time": "2489.169"
    },
    {
        "id": "05249e6b",
        "text": "Yeah. So yeah, you, you can see it from here like w one here like the, the first element here uh like it's uh 0.8 and here uh in the updated W one like the first element is 0.76. So basically uh grading the center is working properly and this is great news nice. Uh But now we actually really don't need this like uh at all. Yeah. Yeah, because I mean like it was just like for showing whether like we this was working. So let's remove that nice.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2494s",
        "start_time": "2494.06"
    },
    {
        "id": "2a45f4d5",
        "text": "you, you can see it from here like w one here like the, the first element here uh like it's uh 0.8 and here uh in the updated W one like the first element is 0.76. So basically uh grading the center is working properly and this is great news nice. Uh But now we actually really don't need this like uh at all. Yeah. Yeah, because I mean like it was just like for showing whether like we this was working. So let's remove that nice. So now what's what remains to do here? So we've implemented gradient descent. Now we should implement the train method nice. So back propagate gradient descent and now let's do train.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2497s",
        "start_time": "2497.209"
    },
    {
        "id": "2c951c13",
        "text": "uh in the updated W one like the first element is 0.76. So basically uh grading the center is working properly and this is great news nice. Uh But now we actually really don't need this like uh at all. Yeah. Yeah, because I mean like it was just like for showing whether like we this was working. So let's remove that nice. So now what's what remains to do here? So we've implemented gradient descent. Now we should implement the train method nice. So back propagate gradient descent and now let's do train. So the train method is gonna have",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2507s",
        "start_time": "2507.34"
    },
    {
        "id": "4371942f",
        "text": "So now what's what remains to do here? So we've implemented gradient descent. Now we should implement the train method nice. So back propagate gradient descent and now let's do train. So the train method is gonna have uh oops, there's a mistake here,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2531s",
        "start_time": "2531.989"
    },
    {
        "id": "f0aba94a",
        "text": "So the train method is gonna have uh oops, there's a mistake here, are you?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2548s",
        "start_time": "2548.679"
    },
    {
        "id": "b0b810bc",
        "text": "uh oops, there's a mistake here, are you? Yeah, here we go.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2555s",
        "start_time": "2555.239"
    },
    {
        "id": "15021741",
        "text": "are you? Yeah, here we go. Uh The train method is gonna uh have a bunch of different arguments. So first of all, we want some inputs, then uh we want uh targets",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2558s",
        "start_time": "2558.05"
    },
    {
        "id": "b95c371b",
        "text": "Yeah, here we go. Uh The train method is gonna uh have a bunch of different arguments. So first of all, we want some inputs, then uh we want uh targets and uh these inputs and targets are our training set uh which uh like our X is like in Y basically. And then we want uh epics and I'll explain what an epic is in a second. And then we want the learning rate, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2559s",
        "start_time": "2559.27"
    },
    {
        "id": "0a37e51d",
        "text": "Uh The train method is gonna uh have a bunch of different arguments. So first of all, we want some inputs, then uh we want uh targets and uh these inputs and targets are our training set uh which uh like our X is like in Y basically. And then we want uh epics and I'll explain what an epic is in a second. And then we want the learning rate, right? Good. OK. So what should we do for training? Well, if you guys remember? So uh we, we pass all the inputs like one by one to the network, we fit it and then the network does some uh forward propagation and then back propagation. Uh But then once we've passed all of the um elements, all of the samples like in the, in the inputs",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2560s",
        "start_time": "2560.639"
    },
    {
        "id": "7fcee7b0",
        "text": "and uh these inputs and targets are our training set uh which uh like our X is like in Y basically. And then we want uh epics and I'll explain what an epic is in a second. And then we want the learning rate, right? Good. OK. So what should we do for training? Well, if you guys remember? So uh we, we pass all the inputs like one by one to the network, we fit it and then the network does some uh forward propagation and then back propagation. Uh But then once we've passed all of the um elements, all of the samples like in the, in the inputs uh in the training set, we've finished an epoch. So the number of epics tells us basically how many times we want to feed the whole data set to uh the uh to the neural network with the idea that the more times we do this and hopefully like the more like the network is going to be able to make better predictions. So,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2575s",
        "start_time": "2575.239"
    },
    {
        "id": "cd787404",
        "text": "Good. OK. So what should we do for training? Well, if you guys remember? So uh we, we pass all the inputs like one by one to the network, we fit it and then the network does some uh forward propagation and then back propagation. Uh But then once we've passed all of the um elements, all of the samples like in the, in the inputs uh in the training set, we've finished an epoch. So the number of epics tells us basically how many times we want to feed the whole data set to uh the uh to the neural network with the idea that the more times we do this and hopefully like the more like the network is going to be able to make better predictions. So, uh basically, what we need to do is like uh go through all the, the Epics and so look through the number of epics. And so, so let's say for i in a range",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2593s",
        "start_time": "2593.479"
    },
    {
        "id": "b122f36a",
        "text": "uh in the training set, we've finished an epoch. So the number of epics tells us basically how many times we want to feed the whole data set to uh the uh to the neural network with the idea that the more times we do this and hopefully like the more like the network is going to be able to make better predictions. So, uh basically, what we need to do is like uh go through all the, the Epics and so look through the number of epics. And so, so let's say for i in a range Epics",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2621s",
        "start_time": "2621.489"
    },
    {
        "id": "09d4a055",
        "text": "uh basically, what we need to do is like uh go through all the, the Epics and so look through the number of epics. And so, so let's say for i in a range Epics And so here",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2646s",
        "start_time": "2646.679"
    },
    {
        "id": "cd5351fb",
        "text": "Epics And so here we should do",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2660s",
        "start_time": "2660.6"
    },
    {
        "id": "985734ad",
        "text": "And so here we should do uh so we should do really like a bunch of different things. So first of all, we should take",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2662s",
        "start_time": "2662.989"
    },
    {
        "id": "2b6b9204",
        "text": "we should do uh so we should do really like a bunch of different things. So first of all, we should take the,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2666s",
        "start_time": "2666.09"
    },
    {
        "id": "7b89a832",
        "text": "uh so we should do really like a bunch of different things. So first of all, we should take the, so we should go through like all the inputs and uh the uh the uh the targets are like one by one. So how are we gonna do this? Well, uh there's like a nice trick we can use here and we could say,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2668s",
        "start_time": "2668.729"
    },
    {
        "id": "4a57d098",
        "text": "the, so we should go through like all the inputs and uh the uh the uh the targets are like one by one. So how are we gonna do this? Well, uh there's like a nice trick we can use here and we could say, uh yell at me just like write this and I'll explain what this is in a second. So for J input target in",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2677s",
        "start_time": "2677.5"
    },
    {
        "id": "a7b9338c",
        "text": "so we should go through like all the inputs and uh the uh the uh the targets are like one by one. So how are we gonna do this? Well, uh there's like a nice trick we can use here and we could say, uh yell at me just like write this and I'll explain what this is in a second. So for J input target in and now we do an a numerate and then we do a zip over here and we pass in the inputs and we pass in the targets. Cool.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2679s",
        "start_time": "2679.699"
    },
    {
        "id": "d681f531",
        "text": "uh yell at me just like write this and I'll explain what this is in a second. So for J input target in and now we do an a numerate and then we do a zip over here and we pass in the inputs and we pass in the targets. Cool. So this is like a very compact way of like getting uh like",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2695s",
        "start_time": "2695.3"
    },
    {
        "id": "8310d440",
        "text": "and now we do an a numerate and then we do a zip over here and we pass in the inputs and we pass in the targets. Cool. So this is like a very compact way of like getting uh like inputs targets one by one and, and, and it's like this input and target and also getting like the index that we are unpacking. So zip unpacks like this two different lists, inputs and targets and give us like elements one by one for input and for targets. And then if we apply a numeration on top of that, we both get these the values themselves and uh the index that we are unpacking good. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2703s",
        "start_time": "2703.149"
    },
    {
        "id": "c1f20b93",
        "text": "So this is like a very compact way of like getting uh like inputs targets one by one and, and, and it's like this input and target and also getting like the index that we are unpacking. So zip unpacks like this two different lists, inputs and targets and give us like elements one by one for input and for targets. And then if we apply a numeration on top of that, we both get these the values themselves and uh the index that we are unpacking good. So yeah, this is like a nice trick. Uh Cool. So now what should we do? Well, uh we've partially done a bunch of these things already. So uh let's take these guys here, right? So, and we'll uh move them",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2715s",
        "start_time": "2715.939"
    },
    {
        "id": "5eb38169",
        "text": "inputs targets one by one and, and, and it's like this input and target and also getting like the index that we are unpacking. So zip unpacks like this two different lists, inputs and targets and give us like elements one by one for input and for targets. And then if we apply a numeration on top of that, we both get these the values themselves and uh the index that we are unpacking good. So yeah, this is like a nice trick. Uh Cool. So now what should we do? Well, uh we've partially done a bunch of these things already. So uh let's take these guys here, right? So, and we'll uh move them here",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2722s",
        "start_time": "2722.389"
    },
    {
        "id": "9d622adf",
        "text": "yeah, this is like a nice trick. Uh Cool. So now what should we do? Well, uh we've partially done a bunch of these things already. So uh let's take these guys here, right? So, and we'll uh move them here say",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2750s",
        "start_time": "2750.75"
    },
    {
        "id": "be0f8f9f",
        "text": "here say what do we want to do? Well,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2768s",
        "start_time": "2768.02"
    },
    {
        "id": "144fa96b",
        "text": "say what do we want to do? Well, first thing we want to apply some forward propagation",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2769s",
        "start_time": "2769.51"
    },
    {
        "id": "e8b37196",
        "text": "what do we want to do? Well, first thing we want to apply some forward propagation but this is not gonna be M LP but it, this is gonna be a self. So we do forward propagate on the input, then we calculate the error and uh this is gonna be the target minus the output and then we apply some back propagation. But this again is gonna be like a self back propagate and we pass in uh the error and then we apply gradient, the sound.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2770s",
        "start_time": "2770.679"
    },
    {
        "id": "564410b8",
        "text": "first thing we want to apply some forward propagation but this is not gonna be M LP but it, this is gonna be a self. So we do forward propagate on the input, then we calculate the error and uh this is gonna be the target minus the output and then we apply some back propagation. But this again is gonna be like a self back propagate and we pass in uh the error and then we apply gradient, the sound. Uh here we go",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2773s",
        "start_time": "2773.81"
    },
    {
        "id": "906d6621",
        "text": "but this is not gonna be M LP but it, this is gonna be a self. So we do forward propagate on the input, then we calculate the error and uh this is gonna be the target minus the output and then we apply some back propagation. But this again is gonna be like a self back propagate and we pass in uh the error and then we apply gradient, the sound. Uh here we go change M LP for itself and the learning rate. Uh Yeah, we already have it here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2779s",
        "start_time": "2779.29"
    },
    {
        "id": "21551ec4",
        "text": "Uh here we go change M LP for itself and the learning rate. Uh Yeah, we already have it here. Oh By the way, I noticed there's a mistake over here. So it's not uh for I in range apex but is in the length.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2806s",
        "start_time": "2806.86"
    },
    {
        "id": "115ff743",
        "text": "change M LP for itself and the learning rate. Uh Yeah, we already have it here. Oh By the way, I noticed there's a mistake over here. So it's not uh for I in range apex but is in the length. Oh No, sorry, I was right. I thought like for a, for a second that apex was a, was a list. It's just an integer. So it's fine, good. OK. Uh cool. So we have uh grade in the sand and we've uh applied uh grade in the sand. So",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2809s",
        "start_time": "2809.679"
    },
    {
        "id": "876af485",
        "text": "Oh By the way, I noticed there's a mistake over here. So it's not uh for I in range apex but is in the length. Oh No, sorry, I was right. I thought like for a, for a second that apex was a, was a list. It's just an integer. So it's fine, good. OK. Uh cool. So we have uh grade in the sand and we've uh applied uh grade in the sand. So now one last thing that we want to do is to report",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2818s",
        "start_time": "2818.419"
    },
    {
        "id": "a29bbf90",
        "text": "Oh No, sorry, I was right. I thought like for a, for a second that apex was a, was a list. It's just an integer. So it's fine, good. OK. Uh cool. So we have uh grade in the sand and we've uh applied uh grade in the sand. So now one last thing that we want to do is to report the error for each epoch so that we can see whether we are uh improving. So how do we do that? Well, first of all, we want to um initialize some error uh variable over here and we'll initialize that obviously, that's a zero. And then at the end of each um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2828s",
        "start_time": "2828.75"
    },
    {
        "id": "3c2904d3",
        "text": "now one last thing that we want to do is to report the error for each epoch so that we can see whether we are uh improving. So how do we do that? Well, first of all, we want to um initialize some error uh variable over here and we'll initialize that obviously, that's a zero. And then at the end of each um like a training session like for, for uh for each input, we are uh gonna calculate this some error and we'll add to some error. What are we gonna add? So we're gonna add",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2848s",
        "start_time": "2848.36"
    },
    {
        "id": "b543f42d",
        "text": "the error for each epoch so that we can see whether we are uh improving. So how do we do that? Well, first of all, we want to um initialize some error uh variable over here and we'll initialize that obviously, that's a zero. And then at the end of each um like a training session like for, for uh for each input, we are uh gonna calculate this some error and we'll add to some error. What are we gonna add? So we're gonna add self dot",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2855s",
        "start_time": "2855.61"
    },
    {
        "id": "6493313d",
        "text": "like a training session like for, for uh for each input, we are uh gonna calculate this some error and we'll add to some error. What are we gonna add? So we're gonna add self dot MS E",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2877s",
        "start_time": "2877.77"
    },
    {
        "id": "31b15f47",
        "text": "self dot MS E and uh we'll pass the target",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2894s",
        "start_time": "2894.419"
    },
    {
        "id": "126cdec4",
        "text": "MS E and uh we'll pass the target and the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2898s",
        "start_time": "2898.199"
    },
    {
        "id": "ea585352",
        "text": "and uh we'll pass the target and the output",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2900s",
        "start_time": "2900.239"
    },
    {
        "id": "8c9376b7",
        "text": "and the output good. So what's this, MS E here? So this is the M squared uh error, right? And so we don't have this uh function, this method already. And so we need to build it. So let's implement it down here. So we'll do the MS E self and we'll pass in target",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2905s",
        "start_time": "2905.33"
    },
    {
        "id": "606e99fe",
        "text": "output good. So what's this, MS E here? So this is the M squared uh error, right? And so we don't have this uh function, this method already. And so we need to build it. So let's implement it down here. So we'll do the MS E self and we'll pass in target and the output. And so, so it's not MS R, it's MS E so M squared error. So, and what is this? So how do we calculate this? Well, this is basically the average of the, the squared error, right? And so we can use a native um uh method from NP it's called average super handy. And we'll",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2907s",
        "start_time": "2907.37"
    },
    {
        "id": "cd7577dc",
        "text": "good. So what's this, MS E here? So this is the M squared uh error, right? And so we don't have this uh function, this method already. And so we need to build it. So let's implement it down here. So we'll do the MS E self and we'll pass in target and the output. And so, so it's not MS R, it's MS E so M squared error. So, and what is this? So how do we calculate this? Well, this is basically the average of the, the squared error, right? And so we can use a native um uh method from NP it's called average super handy. And we'll basically rewrite this uh like this. So we'll do the average of the target minus the output and we want to square this. So we'll do this, right? And so this is the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2908s",
        "start_time": "2908.86"
    },
    {
        "id": "5f811e68",
        "text": "and the output. And so, so it's not MS R, it's MS E so M squared error. So, and what is this? So how do we calculate this? Well, this is basically the average of the, the squared error, right? And so we can use a native um uh method from NP it's called average super handy. And we'll basically rewrite this uh like this. So we'll do the average of the target minus the output and we want to square this. So we'll do this, right? And so this is the or",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2931s",
        "start_time": "2931.629"
    },
    {
        "id": "69ee0e2c",
        "text": "basically rewrite this uh like this. So we'll do the average of the target minus the output and we want to square this. So we'll do this, right? And so this is the or uh means squared error",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2958s",
        "start_time": "2958.179"
    },
    {
        "id": "d8a4e944",
        "text": "or uh means squared error nice. So now we have a, an error that's accumulating like at every step that we take like in our training and, and we want to report it like at the end of an epoch,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2975s",
        "start_time": "2975.51"
    },
    {
        "id": "09bce26b",
        "text": "uh means squared error nice. So now we have a, an error that's accumulating like at every step that we take like in our training and, and we want to report it like at the end of an epoch, right? And so how do we uh do that? Right. So we'll",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2977s",
        "start_time": "2977.06"
    },
    {
        "id": "940172cf",
        "text": "nice. So now we have a, an error that's accumulating like at every step that we take like in our training and, and we want to report it like at the end of an epoch, right? And so how do we uh do that? Right. So we'll um we'll do that",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2979s",
        "start_time": "2979.31"
    },
    {
        "id": "e633fa88",
        "text": "right? And so how do we uh do that? Right. So we'll um we'll do that by",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=2992s",
        "start_time": "2992.6"
    },
    {
        "id": "3de692f8",
        "text": "um we'll do that by uh yes. So we'll just like do a print over here",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3001s",
        "start_time": "3001.649"
    },
    {
        "id": "d5849575",
        "text": "by uh yes. So we'll just like do a print over here and uh will write uh error",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3006s",
        "start_time": "3006.08"
    },
    {
        "id": "d6b37e0e",
        "text": "uh yes. So we'll just like do a print over here and uh will write uh error and we'll say error is equal to something at epoch",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3008s",
        "start_time": "3008.199"
    },
    {
        "id": "be3e491c",
        "text": "and uh will write uh error and we'll say error is equal to something at epoch and we'll have the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3014s",
        "start_time": "3014.459"
    },
    {
        "id": "dde4f81f",
        "text": "and we'll say error is equal to something at epoch and we'll have the epoch over here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3023s",
        "start_time": "3023.0"
    },
    {
        "id": "226ad69b",
        "text": "and we'll have the epoch over here. Cool. And so we'll do a format and uh we'll pass in for the error, some",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3029s",
        "start_time": "3029.949"
    },
    {
        "id": "27a8aec6",
        "text": "epoch over here. Cool. And so we'll do a format and uh we'll pass in for the error, some error",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3033s",
        "start_time": "3033.79"
    },
    {
        "id": "2c8f222e",
        "text": "Cool. And so we'll do a format and uh we'll pass in for the error, some error uh divided uh by the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3036s",
        "start_time": "3036.449"
    },
    {
        "id": "0fe9f48d",
        "text": "error uh divided uh by the length of the",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3046s",
        "start_time": "3046.75"
    },
    {
        "id": "1428878f",
        "text": "uh divided uh by the length of the inputs,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3048s",
        "start_time": "3048.79"
    },
    {
        "id": "a1631755",
        "text": "length of the inputs, right? Because this way, we are basically like uh normalizing that. And um what we also want to do is we want to pass",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3053s",
        "start_time": "3053.419"
    },
    {
        "id": "d0ed5b75",
        "text": "inputs, right? Because this way, we are basically like uh normalizing that. And um what we also want to do is we want to pass um the number of epochs. So in this case is I, right? So let me just double check if it's all good over here. So we have this format. Yeah, this is not working.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3057s",
        "start_time": "3057.409"
    },
    {
        "id": "74c31d23",
        "text": "right? Because this way, we are basically like uh normalizing that. And um what we also want to do is we want to pass um the number of epochs. So in this case is I, right? So let me just double check if it's all good over here. So we have this format. Yeah, this is not working. OK. So yeah, so we have the first argument. That's this one here and then we have",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3059s",
        "start_time": "3059.84"
    },
    {
        "id": "d26c5088",
        "text": "um the number of epochs. So in this case is I, right? So let me just double check if it's all good over here. So we have this format. Yeah, this is not working. OK. So yeah, so we have the first argument. That's this one here and then we have I",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3074s",
        "start_time": "3074.36"
    },
    {
        "id": "b0fa9759",
        "text": "OK. So yeah, so we have the first argument. That's this one here and then we have I and this yeah, closes the format which yeah, so this should be fine now.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3093s",
        "start_time": "3093.669"
    },
    {
        "id": "acc9f80f",
        "text": "I and this yeah, closes the format which yeah, so this should be fine now. Uh Good.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3100s",
        "start_time": "3100.03"
    },
    {
        "id": "92a40391",
        "text": "and this yeah, closes the format which yeah, so this should be fine now. Uh Good. OK. So now I think we have all the elements uh if I'm not uh",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3101s",
        "start_time": "3101.909"
    },
    {
        "id": "de2ef093",
        "text": "Uh Good. OK. So now I think we have all the elements uh if I'm not uh if I'm mistaken, we have all the elements in place for doing a run",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3109s",
        "start_time": "3109.06"
    },
    {
        "id": "dcdf3ec6",
        "text": "OK. So now I think we have all the elements uh if I'm not uh if I'm mistaken, we have all the elements in place for doing a run of our um uh neural network good. So, but before doing that, I just want to, to, to double check of things because like, I did this like, I did like this fancy thing here, but I'm not using like j uh like anywhere. So I don't really think like we're gonna need uh like this J uh Yeah. Yeah, I didn't see a point here. So, yeah,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3110s",
        "start_time": "3110.679"
    },
    {
        "id": "856ff9cc",
        "text": "if I'm mistaken, we have all the elements in place for doing a run of our um uh neural network good. So, but before doing that, I just want to, to, to double check of things because like, I did this like, I did like this fancy thing here, but I'm not using like j uh like anywhere. So I don't really think like we're gonna need uh like this J uh Yeah. Yeah, I didn't see a point here. So, yeah, well, at least like you've learned a trick, but it's not really needed. So we'll just like simplify this and we'll just drop this and numerate um like that, right? So now, now this uh should work properly good. OK. So, uh now",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3117s",
        "start_time": "3117.689"
    },
    {
        "id": "827b4c7d",
        "text": "of our um uh neural network good. So, but before doing that, I just want to, to, to double check of things because like, I did this like, I did like this fancy thing here, but I'm not using like j uh like anywhere. So I don't really think like we're gonna need uh like this J uh Yeah. Yeah, I didn't see a point here. So, yeah, well, at least like you've learned a trick, but it's not really needed. So we'll just like simplify this and we'll just drop this and numerate um like that, right? So now, now this uh should work properly good. OK. So, uh now uh let's go back to our tasks. So we, we basically did all of these things here. So now we want to train our nets with some dummy uh data set and then make some predictions. OK? So let's go back here. So now we are back like in the, in the scripts. OK. So we've already created uh an M LP. And uh now we want to",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3123s",
        "start_time": "3123.37"
    },
    {
        "id": "03b82d20",
        "text": "well, at least like you've learned a trick, but it's not really needed. So we'll just like simplify this and we'll just drop this and numerate um like that, right? So now, now this uh should work properly good. OK. So, uh now uh let's go back to our tasks. So we, we basically did all of these things here. So now we want to train our nets with some dummy uh data set and then make some predictions. OK? So let's go back here. So now we are back like in the, in the scripts. OK. So we've already created uh an M LP. And uh now we want to uh train",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3151s",
        "start_time": "3151.209"
    },
    {
        "id": "eff9377b",
        "text": "uh let's go back to our tasks. So we, we basically did all of these things here. So now we want to train our nets with some dummy uh data set and then make some predictions. OK? So let's go back here. So now we are back like in the, in the scripts. OK. So we've already created uh an M LP. And uh now we want to uh train uh our M LP.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3170s",
        "start_time": "3170.469"
    },
    {
        "id": "2ff836aa",
        "text": "uh train uh our M LP. And how do we do that? Well, we do M LP dot train.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3196s",
        "start_time": "3196.11"
    },
    {
        "id": "f9308188",
        "text": "uh our M LP. And how do we do that? Well, we do M LP dot train. Uh we need to pass in the inputs, the targets, then uh we need to pass the number of epics, let's say 50 for example, and the learning rate, let's say 0.1. And uh yeah, and we need some dummy data. So for the inputs and uh for uh the targets. Now,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3199s",
        "start_time": "3199.979"
    },
    {
        "id": "2965b26d",
        "text": "And how do we do that? Well, we do M LP dot train. Uh we need to pass in the inputs, the targets, then uh we need to pass the number of epics, let's say 50 for example, and the learning rate, let's say 0.1. And uh yeah, and we need some dummy data. So for the inputs and uh for uh the targets. Now, I want to, as I mentioned earlier, I want to",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3202s",
        "start_time": "3202.629"
    },
    {
        "id": "9e0af809",
        "text": "Uh we need to pass in the inputs, the targets, then uh we need to pass the number of epics, let's say 50 for example, and the learning rate, let's say 0.1. And uh yeah, and we need some dummy data. So for the inputs and uh for uh the targets. Now, I want to, as I mentioned earlier, I want to um train our network so that it will be able to uh to compute uh the sum operation. So I'm gonna just like copy paste a um radiation like a data set like for this. And so I, I'm not gonna like explain everything here because like this is like not the point. Uh But uh so, first of all, like we need this random function",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3208s",
        "start_time": "3208.51"
    },
    {
        "id": "c4712014",
        "text": "I want to, as I mentioned earlier, I want to um train our network so that it will be able to uh to compute uh the sum operation. So I'm gonna just like copy paste a um radiation like a data set like for this. And so I, I'm not gonna like explain everything here because like this is like not the point. Uh But uh so, first of all, like we need this random function and so we need to import a random here otherwise it's not gonna uh work. So we'd say from uh random import uh random. And so we have that now down here.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3235s",
        "start_time": "3235.28"
    },
    {
        "id": "1115e09e",
        "text": "um train our network so that it will be able to uh to compute uh the sum operation. So I'm gonna just like copy paste a um radiation like a data set like for this. And so I, I'm not gonna like explain everything here because like this is like not the point. Uh But uh so, first of all, like we need this random function and so we need to import a random here otherwise it's not gonna uh work. So we'd say from uh random import uh random. And so we have that now down here. Uh Yeah, which is good.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3240s",
        "start_time": "3240.199"
    },
    {
        "id": "b65243eb",
        "text": "and so we need to import a random here otherwise it's not gonna uh work. So we'd say from uh random import uh random. And so we have that now down here. Uh Yeah, which is good. OK. So now we should have, but uh let's call this inputs and let's call this like uh targets, right? OK. So what's the point of this? Um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3266s",
        "start_time": "3266.55"
    },
    {
        "id": "b6615c2a",
        "text": "Uh Yeah, which is good. OK. So now we should have, but uh let's call this inputs and let's call this like uh targets, right? OK. So what's the point of this? Um Yeah, this fancy like list comprehensions like we had a raise and things like that. Well, basically this is going to be an array",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3281s",
        "start_time": "3281.379"
    },
    {
        "id": "9b8305a9",
        "text": "OK. So now we should have, but uh let's call this inputs and let's call this like uh targets, right? OK. So what's the point of this? Um Yeah, this fancy like list comprehensions like we had a raise and things like that. Well, basically this is going to be an array uh where we have uh so something like this. So this is gonna be like a nr A uh where we have",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3283s",
        "start_time": "3283.379"
    },
    {
        "id": "6d9ad658",
        "text": "Yeah, this fancy like list comprehensions like we had a raise and things like that. Well, basically this is going to be an array uh where we have uh so something like this. So this is gonna be like a nr A uh where we have this type of structure here. So say like 0.10 0.2 and then we have another",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3295s",
        "start_time": "3295.8"
    },
    {
        "id": "c80e3ae4",
        "text": "uh where we have uh so something like this. So this is gonna be like a nr A uh where we have this type of structure here. So say like 0.10 0.2 and then we have another are we here? And so these are like each of these guys in here is gonna be a sample that we pass to forward propagation, right? And uh then we have the targets and the targets are,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3305s",
        "start_time": "3305.219"
    },
    {
        "id": "8ce7ed03",
        "text": "this type of structure here. So say like 0.10 0.2 and then we have another are we here? And so these are like each of these guys in here is gonna be a sample that we pass to forward propagation, right? And uh then we have the targets and the targets are, it's similar to this, but",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3319s",
        "start_time": "3319.07"
    },
    {
        "id": "e7b8b6ba",
        "text": "are we here? And so these are like each of these guys in here is gonna be a sample that we pass to forward propagation, right? And uh then we have the targets and the targets are, it's similar to this, but it's just like the,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3329s",
        "start_time": "3329.129"
    },
    {
        "id": "ecafb719",
        "text": "it's similar to this, but it's just like the, the sum over here. So instead of having like two values, you're just gonna have one which is zero point point three in this case. And this is gonna be uh 0.7",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3345s",
        "start_time": "3345.82"
    },
    {
        "id": "6412c7ae",
        "text": "it's just like the, the sum over here. So instead of having like two values, you're just gonna have one which is zero point point three in this case. And this is gonna be uh 0.7 uh right, this is items is not correct. So we'll just need to move it from inputs. And right, so this is fine now good. So now if everything is correct now we should be able to train our multi-layered perception from scratch. Let's see if it works.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3348s",
        "start_time": "3348.82"
    },
    {
        "id": "de4315d2",
        "text": "the sum over here. So instead of having like two values, you're just gonna have one which is zero point point three in this case. And this is gonna be uh 0.7 uh right, this is items is not correct. So we'll just need to move it from inputs. And right, so this is fine now good. So now if everything is correct now we should be able to train our multi-layered perception from scratch. Let's see if it works. Whoa This is working. And that's fantastic because we have also a very nice record over here. So we started with this error at",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3350s",
        "start_time": "3350.919"
    },
    {
        "id": "41bff906",
        "text": "uh right, this is items is not correct. So we'll just need to move it from inputs. And right, so this is fine now good. So now if everything is correct now we should be able to train our multi-layered perception from scratch. Let's see if it works. Whoa This is working. And that's fantastic because we have also a very nice record over here. So we started with this error at epoch uh zero.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3361s",
        "start_time": "3361.909"
    },
    {
        "id": "1fc70b76",
        "text": "Whoa This is working. And that's fantastic because we have also a very nice record over here. So we started with this error at epoch uh zero. And then all the way through, we went down, down, down at each epoch until we reached this",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3383s",
        "start_time": "3383.33"
    },
    {
        "id": "e44b8610",
        "text": "epoch uh zero. And then all the way through, we went down, down, down at each epoch until we reached this uh error over here. So as you see, the training is working because the error is going down. And so the the network is slowly learning to, to do something very simple, which is like the some operation someone could say, well, this is really like overkill to do like some, yeah, some but I'm in. Yeah, that's what it is, right. OK. So now we've uh we've implemented like",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3397s",
        "start_time": "3397.11"
    },
    {
        "id": "bf15c88b",
        "text": "And then all the way through, we went down, down, down at each epoch until we reached this uh error over here. So as you see, the training is working because the error is going down. And so the the network is slowly learning to, to do something very simple, which is like the some operation someone could say, well, this is really like overkill to do like some, yeah, some but I'm in. Yeah, that's what it is, right. OK. So now we've uh we've implemented like all back propagation grid and descent. We have a, an amazing mop. So now what remains to do like the last thing that we said we would do is make some predictions, right? OK. So how do we do that? So, yeah, this is uh again, uh quite simple. So yeah, let's create uh create uh dummy data.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3399s",
        "start_time": "3399.419"
    },
    {
        "id": "127dfe87",
        "text": "uh error over here. So as you see, the training is working because the error is going down. And so the the network is slowly learning to, to do something very simple, which is like the some operation someone could say, well, this is really like overkill to do like some, yeah, some but I'm in. Yeah, that's what it is, right. OK. So now we've uh we've implemented like all back propagation grid and descent. We have a, an amazing mop. So now what remains to do like the last thing that we said we would do is make some predictions, right? OK. So how do we do that? So, yeah, this is uh again, uh quite simple. So yeah, let's create uh create uh dummy data. And this time, let's call this input",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3408s",
        "start_time": "3408.34"
    },
    {
        "id": "6430b8a1",
        "text": "all back propagation grid and descent. We have a, an amazing mop. So now what remains to do like the last thing that we said we would do is make some predictions, right? OK. So how do we do that? So, yeah, this is uh again, uh quite simple. So yeah, let's create uh create uh dummy data. And this time, let's call this input and we do NP dot uh array. And here we want to pass in",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3435s",
        "start_time": "3435.57"
    },
    {
        "id": "e6e97f78",
        "text": "And this time, let's call this input and we do NP dot uh array. And here we want to pass in uh let's say 0.3 and 0.1",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3462s",
        "start_time": "3462.53"
    },
    {
        "id": "fc1774b0",
        "text": "and we do NP dot uh array. And here we want to pass in uh let's say 0.3 and 0.1 good. And now we want our targets and we know that our targets should be the sum of those two numbers. So 0.4 right? And so now what we need to do is M LP,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3467s",
        "start_time": "3467.0"
    },
    {
        "id": "e90ed1f5",
        "text": "uh let's say 0.3 and 0.1 good. And now we want our targets and we know that our targets should be the sum of those two numbers. So 0.4 right? And so now what we need to do is M LP, tell it",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3475s",
        "start_time": "3475.09"
    },
    {
        "id": "d40d715d",
        "text": "good. And now we want our targets and we know that our targets should be the sum of those two numbers. So 0.4 right? And so now what we need to do is M LP, tell it mop dot forward propagate and then we want to pass in the input and then we expect some output, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3480s",
        "start_time": "3480.399"
    },
    {
        "id": "16aa8f10",
        "text": "tell it mop dot forward propagate and then we want to pass in the input and then we expect some output, right? So the idea like the, the, the, the whole like train like a for here is we have like our uh training data set, we built like our mop, we train our M LP and now we create some Demi data and we pass that in uh forward propagate so that we'll get uh like a prediction out there. So let's see how this prediction is doing.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3498s",
        "start_time": "3498.419"
    },
    {
        "id": "b9cba8e1",
        "text": "mop dot forward propagate and then we want to pass in the input and then we expect some output, right? So the idea like the, the, the, the whole like train like a for here is we have like our uh training data set, we built like our mop, we train our M LP and now we create some Demi data and we pass that in uh forward propagate so that we'll get uh like a prediction out there. So let's see how this prediction is doing. And so meantime, we'll do like just a print to give us a couple of prints to, to give us like a couple of new lines and then we'll do print and we'll say",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3499s",
        "start_time": "3499.969"
    },
    {
        "id": "c75b8f30",
        "text": "So the idea like the, the, the, the whole like train like a for here is we have like our uh training data set, we built like our mop, we train our M LP and now we create some Demi data and we pass that in uh forward propagate so that we'll get uh like a prediction out there. So let's see how this prediction is doing. And so meantime, we'll do like just a print to give us a couple of prints to, to give us like a couple of new lines and then we'll do print and we'll say um",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3512s",
        "start_time": "3512.179"
    },
    {
        "id": "0a8eb076",
        "text": "And so meantime, we'll do like just a print to give us a couple of prints to, to give us like a couple of new lines and then we'll do print and we'll say um so let's say",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3537s",
        "start_time": "3537.51"
    },
    {
        "id": "8fb81167",
        "text": "um so let's say this. So let's say our network",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3550s",
        "start_time": "3550.129"
    },
    {
        "id": "4f49a14c",
        "text": "so let's say this. So let's say our network the leaves.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3552s",
        "start_time": "3552.36"
    },
    {
        "id": "85ba1f62",
        "text": "this. So let's say our network the leaves. That's",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3555s",
        "start_time": "3555.28"
    },
    {
        "id": "23bad519",
        "text": "the leaves. That's this plus this is",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3561s",
        "start_time": "3561.449"
    },
    {
        "id": "b549092c",
        "text": "That's this plus this is equal",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3563s",
        "start_time": "3563.709"
    },
    {
        "id": "91232cf1",
        "text": "this plus this is equal equal to,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3566s",
        "start_time": "3566.33"
    },
    {
        "id": "48b53d9c",
        "text": "equal equal to, it's",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3570s",
        "start_time": "3570.169"
    },
    {
        "id": "57c74d6d",
        "text": "equal to, it's uh this right now, we need to pass all of this information in. So uh our beliefs that input,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3573s",
        "start_time": "3573.479"
    },
    {
        "id": "9cab4576",
        "text": "it's uh this right now, we need to pass all of this information in. So uh our beliefs that input, so this is input uh zero, this is input uh one. And finally, this is uh our target, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3577s",
        "start_time": "3577.409"
    },
    {
        "id": "389ac80b",
        "text": "uh this right now, we need to pass all of this information in. So uh our beliefs that input, so this is input uh zero, this is input uh one. And finally, this is uh our target, right? Uh Well, no, it's not our target. Sorry, it's our output.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3578s",
        "start_time": "3578.79"
    },
    {
        "id": "73c95892",
        "text": "so this is input uh zero, this is input uh one. And finally, this is uh our target, right? Uh Well, no, it's not our target. Sorry, it's our output. Cool, I'll put",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3590s",
        "start_time": "3590.1"
    },
    {
        "id": "4e8bd78a",
        "text": "Uh Well, no, it's not our target. Sorry, it's our output. Cool, I'll put calculation and zero, right? OK. So let's run this and see if our network is gonna be able uh to uh compute this, right?",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3604s",
        "start_time": "3604.199"
    },
    {
        "id": "da6584eb",
        "text": "Cool, I'll put calculation and zero, right? OK. So let's run this and see if our network is gonna be able uh to uh compute this, right? And um right. So let's do that. And",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3609s",
        "start_time": "3609.639"
    },
    {
        "id": "dccdb13f",
        "text": "calculation and zero, right? OK. So let's run this and see if our network is gonna be able uh to uh compute this, right? And um right. So let's do that. And let's see.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3612s",
        "start_time": "3612.459"
    },
    {
        "id": "4acf67ee",
        "text": "And um right. So let's do that. And let's see. Nice. Super good. So we, we have it here. Nice. So our network believes that 0.3 plus 0.1 is equal to 0.3996. Nice. So this is really close to our target, which is",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3625s",
        "start_time": "3625.61"
    },
    {
        "id": "11654c4e",
        "text": "let's see. Nice. Super good. So we, we have it here. Nice. So our network believes that 0.3 plus 0.1 is equal to 0.3996. Nice. So this is really close to our target, which is uh no 0.4 right? So this is like quite good actually. So we, we made it, we trained our network, we have this like network which is an absolute overkill for calculating like the same operation. But yeah, we made it. But apart from like the the toy application, I think like what we",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3633s",
        "start_time": "3633.35"
    },
    {
        "id": "5ddbafbd",
        "text": "Nice. Super good. So we, we have it here. Nice. So our network believes that 0.3 plus 0.1 is equal to 0.3996. Nice. So this is really close to our target, which is uh no 0.4 right? So this is like quite good actually. So we, we made it, we trained our network, we have this like network which is an absolute overkill for calculating like the same operation. But yeah, we made it. But apart from like the the toy application, I think like what we made it here like it's quite something because we made to basically build a whole neural network, fit forward neural network, multi layer perception from scratch. And now we have all of the different elements to it. We have forward propagation,",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3635s",
        "start_time": "3635.379"
    },
    {
        "id": "996fef53",
        "text": "uh no 0.4 right? So this is like quite good actually. So we, we made it, we trained our network, we have this like network which is an absolute overkill for calculating like the same operation. But yeah, we made it. But apart from like the the toy application, I think like what we made it here like it's quite something because we made to basically build a whole neural network, fit forward neural network, multi layer perception from scratch. And now we have all of the different elements to it. We have forward propagation, we have back propagation, we have gradient descent, we know how we can train it uh everything like it's super modular because you can put like as many uh layers uh in the network as you want and you can specify as many like neurons as you want like, so that's like also like really nice. And so yeah, I think like we should just like congratulate ourselves because I guess like we know way more than most people",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3654s",
        "start_time": "3654.399"
    },
    {
        "id": "dccdf228",
        "text": "made it here like it's quite something because we made to basically build a whole neural network, fit forward neural network, multi layer perception from scratch. And now we have all of the different elements to it. We have forward propagation, we have back propagation, we have gradient descent, we know how we can train it uh everything like it's super modular because you can put like as many uh layers uh in the network as you want and you can specify as many like neurons as you want like, so that's like also like really nice. And so yeah, I think like we should just like congratulate ourselves because I guess like we know way more than most people out there uh who are also like machine learning practitioners because like we know how to build a neural network, a simple neural network from scratch",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3673s",
        "start_time": "3673.159"
    },
    {
        "id": "b4037eae",
        "text": "we have back propagation, we have gradient descent, we know how we can train it uh everything like it's super modular because you can put like as many uh layers uh in the network as you want and you can specify as many like neurons as you want like, so that's like also like really nice. And so yeah, I think like we should just like congratulate ourselves because I guess like we know way more than most people out there uh who are also like machine learning practitioners because like we know how to build a neural network, a simple neural network from scratch good. So this was it for this very very long video. I hope you enjoyed it. And uh if you did please uh subscribe and hit the notification bell. So you'll just like get new videos when they are uploaded and then um for the next time, what we'll do is we're gonna basically build something very similar to this with tensorflow. And you'll see",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3690s",
        "start_time": "3690.8"
    },
    {
        "id": "375fca4c",
        "text": "out there uh who are also like machine learning practitioners because like we know how to build a neural network, a simple neural network from scratch good. So this was it for this very very long video. I hope you enjoyed it. And uh if you did please uh subscribe and hit the notification bell. So you'll just like get new videos when they are uploaded and then um for the next time, what we'll do is we're gonna basically build something very similar to this with tensorflow. And you'll see that all the time that we spent like doing this. It's gonna take, I don't know, like probably 1/10 like of the, of the time and the number of like uh uh lines of code for doing that. And so yeah, we'll get into tensor flow and carrots and we'll build a simple M LP from scratch. So stay tuned and like this video and I'll see you next time. Cheers.",
        "video": "8- TRAINING A NEURAL NETWORK: Implementing backpropagation and gradient descent from scratch",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Z97XGNUUx9o",
        "youtube_link": "https://www.youtube.com/watch?v=Z97XGNUUx9o&t=3716s",
        "start_time": "3716.199"
    },
    {
        "id": "01006f15",
        "text": "Hi, everybody and welcome to a new video in the Deep Learning for audio with Python series. This time we're gonna implement a neural network from scratch specifically, we're gonna implement a multi layer perception. So let's get started. So the first thing that we want to do is importing numpy the nun P if you're not familiar with it is a um scientific library for Python that's used in most uh libraries if you don't have nun pie installed. So you, you just go and go to your terminal and pip install it. It's quite straightforward, right? So uh the thing that we want to build here is a class that's called M LP or multi layer perception. So here we're gonna have all the attributes and methods that we can use to actually represent a multi layer of perception.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "900d6924",
        "text": "the nun P if you're not familiar with it is a um scientific library for Python that's used in most uh libraries if you don't have nun pie installed. So you, you just go and go to your terminal and pip install it. It's quite straightforward, right? So uh the thing that we want to build here is a class that's called M LP or multi layer perception. So here we're gonna have all the attributes and methods that we can use to actually represent a multi layer of perception. So what's the first thing that we need to do here? Well, it's the constructor. So we want the constructor for the M LP class. And so we are gonna",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=20s",
        "start_time": "20.709"
    },
    {
        "id": "2a1b7d7d",
        "text": "So uh the thing that we want to build here is a class that's called M LP or multi layer perception. So here we're gonna have all the attributes and methods that we can use to actually represent a multi layer of perception. So what's the first thing that we need to do here? Well, it's the constructor. So we want the constructor for the M LP class. And so we are gonna have this in it over here and we want to pass it three different attributes. So one it's the number of",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=39s",
        "start_time": "39.799"
    },
    {
        "id": "eaee4b79",
        "text": "So what's the first thing that we need to do here? Well, it's the constructor. So we want the constructor for the M LP class. And so we are gonna have this in it over here and we want to pass it three different attributes. So one it's the number of uh inputs, then we want to pass it the number of hidden layers.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=58s",
        "start_time": "58.709"
    },
    {
        "id": "e970209e",
        "text": "have this in it over here and we want to pass it three different attributes. So one it's the number of uh inputs, then we want to pass it the number of hidden layers. And finally, we want to pass it the number of outputs over here. And so we're gonna store these arguments internally in uh attributes and this not surprisingly are gonna be called uh self dots numb inputs. So, and this is gonna be the numb inputs over here. Then we're gonna have the",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=70s",
        "start_time": "70.62"
    },
    {
        "id": "a079442e",
        "text": "uh inputs, then we want to pass it the number of hidden layers. And finally, we want to pass it the number of outputs over here. And so we're gonna store these arguments internally in uh attributes and this not surprisingly are gonna be called uh self dots numb inputs. So, and this is gonna be the numb inputs over here. Then we're gonna have the number of",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=82s",
        "start_time": "82.709"
    },
    {
        "id": "582637ac",
        "text": "And finally, we want to pass it the number of outputs over here. And so we're gonna store these arguments internally in uh attributes and this not surprisingly are gonna be called uh self dots numb inputs. So, and this is gonna be the numb inputs over here. Then we're gonna have the number of hidden layers over here and this is gonna be number of hidden and then we're gonna have this num outputs",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=90s",
        "start_time": "90.349"
    },
    {
        "id": "1f981c54",
        "text": "number of hidden layers over here and this is gonna be number of hidden and then we're gonna have this num outputs outputs and we have the number of outputs over here. Well, so this way we just store internally to the instance all of this uh information about the, the number of neurons in the input layer in the uh hidden layers and the in the output layers. So we can pass in some default values over here.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=119s",
        "start_time": "119.23"
    },
    {
        "id": "f1da41d2",
        "text": "hidden layers over here and this is gonna be number of hidden and then we're gonna have this num outputs outputs and we have the number of outputs over here. Well, so this way we just store internally to the instance all of this uh information about the, the number of neurons in the input layer in the uh hidden layers and the in the output layers. So we can pass in some default values over here. Uh because uh that's gonna be like easier like when we implement an M LP. So we don't need to pass all of this information every time. So we could say that our default number of inputs. So number of neurons in the input layer is gonna be three for example. And for the hidden layers, it is gonna be slightly different because we want to pass in a list and this is gonna be a list of the integers and it's",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=121s",
        "start_time": "121.47"
    },
    {
        "id": "80e662a7",
        "text": "outputs and we have the number of outputs over here. Well, so this way we just store internally to the instance all of this uh information about the, the number of neurons in the input layer in the uh hidden layers and the in the output layers. So we can pass in some default values over here. Uh because uh that's gonna be like easier like when we implement an M LP. So we don't need to pass all of this information every time. So we could say that our default number of inputs. So number of neurons in the input layer is gonna be three for example. And for the hidden layers, it is gonna be slightly different because we want to pass in a list and this is gonna be a list of the integers and it's integer is gonna represent the number of neurons in a hidden layer. So uh basically, in this case, we have a neuron with two hidden layers. So the first hidden layer has three neurons and the second layer has a hidden layer has five neurons. And then we finally, we have the number of outputs. And so these are the output neurons, let's say we have two, for example, right. So",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=132s",
        "start_time": "132.71"
    },
    {
        "id": "1be6ca99",
        "text": "Uh because uh that's gonna be like easier like when we implement an M LP. So we don't need to pass all of this information every time. So we could say that our default number of inputs. So number of neurons in the input layer is gonna be three for example. And for the hidden layers, it is gonna be slightly different because we want to pass in a list and this is gonna be a list of the integers and it's integer is gonna represent the number of neurons in a hidden layer. So uh basically, in this case, we have a neuron with two hidden layers. So the first hidden layer has three neurons and the second layer has a hidden layer has five neurons. And then we finally, we have the number of outputs. And so these are the output neurons, let's say we have two, for example, right. So now we want to create a an internal uh representation of the layers. So how do we do that? Well, we want this as a as a list. And in order to do that, we are gonna",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=157s",
        "start_time": "157.589"
    },
    {
        "id": "0bc8223f",
        "text": "integer is gonna represent the number of neurons in a hidden layer. So uh basically, in this case, we have a neuron with two hidden layers. So the first hidden layer has three neurons and the second layer has a hidden layer has five neurons. And then we finally, we have the number of outputs. And so these are the output neurons, let's say we have two, for example, right. So now we want to create a an internal uh representation of the layers. So how do we do that? Well, we want this as a as a list. And in order to do that, we are gonna uh cast to at least type this self",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=187s",
        "start_time": "187.57"
    },
    {
        "id": "25c9c21e",
        "text": "now we want to create a an internal uh representation of the layers. So how do we do that? Well, we want this as a as a list. And in order to do that, we are gonna uh cast to at least type this self dot number of inputs over here, then we are gonna concatenate that with the number of hidden layers. And then we're gonna concatenate this with the number of output layers. So",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=214s",
        "start_time": "214.38"
    },
    {
        "id": "8cdb6d40",
        "text": "uh cast to at least type this self dot number of inputs over here, then we are gonna concatenate that with the number of hidden layers. And then we're gonna concatenate this with the number of output layers. So what this does is basically getting a list where each item in the list represents the number of neurons in a layer. And the layers obviously move from uh like zero to, to the, to the zero index to the uh number of layers that we have. OK. So next, what do we want to do? Well, we need to initiate a random weights.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=232s",
        "start_time": "232.559"
    },
    {
        "id": "d5504b62",
        "text": "dot number of inputs over here, then we are gonna concatenate that with the number of hidden layers. And then we're gonna concatenate this with the number of output layers. So what this does is basically getting a list where each item in the list represents the number of neurons in a layer. And the layers obviously move from uh like zero to, to the, to the zero index to the uh number of layers that we have. OK. So next, what do we want to do? Well, we need to initiate a random weights. So a a neuron network is isn't a neural network if you don't have weights uh for all the connections between the neurons in a subsequent layers. So how do we do that? Well, first of all, let's initiate",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=238s",
        "start_time": "238.199"
    },
    {
        "id": "47c3598a",
        "text": "what this does is basically getting a list where each item in the list represents the number of neurons in a layer. And the layers obviously move from uh like zero to, to the, to the zero index to the uh number of layers that we have. OK. So next, what do we want to do? Well, we need to initiate a random weights. So a a neuron network is isn't a neural network if you don't have weights uh for all the connections between the neurons in a subsequent layers. So how do we do that? Well, first of all, let's initiate a,",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=256s",
        "start_time": "256.049"
    },
    {
        "id": "4a9b7543",
        "text": "So a a neuron network is isn't a neural network if you don't have weights uh for all the connections between the neurons in a subsequent layers. So how do we do that? Well, first of all, let's initiate a, an internal like an attribute called weight. And this is gonna be an empty list for the time being. And then we want to iterate through all the layers and then create a matrix uh weight for each uh pair of layers. So how do we do that? Well, we do a full look in a range of the length",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=285s",
        "start_time": "285.309"
    },
    {
        "id": "f3f8a3ed",
        "text": "a, an internal like an attribute called weight. And this is gonna be an empty list for the time being. And then we want to iterate through all the layers and then create a matrix uh weight for each uh pair of layers. So how do we do that? Well, we do a full look in a range of the length uh layers minus uh one.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=302s",
        "start_time": "302.119"
    },
    {
        "id": "86ca9a86",
        "text": "an internal like an attribute called weight. And this is gonna be an empty list for the time being. And then we want to iterate through all the layers and then create a matrix uh weight for each uh pair of layers. So how do we do that? Well, we do a full look in a range of the length uh layers minus uh one. So, and then at each uh step, we're gonna build this weight matrix W and W is gonna be equal to the uh nun",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=303s",
        "start_time": "303.519"
    },
    {
        "id": "12dd602f",
        "text": "uh layers minus uh one. So, and then at each uh step, we're gonna build this weight matrix W and W is gonna be equal to the uh nun dot random dot run.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=333s",
        "start_time": "333.01"
    },
    {
        "id": "b0e67d09",
        "text": "So, and then at each uh step, we're gonna build this weight matrix W and W is gonna be equal to the uh nun dot random dot run. And here will pass in layers I layers I and here we have layers",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=339s",
        "start_time": "339.269"
    },
    {
        "id": "8623635c",
        "text": "dot random dot run. And here will pass in layers I layers I and here we have layers I plus one. OK. So what have I done here? So uh I'm basically creating a matrix",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=355s",
        "start_time": "355.82"
    },
    {
        "id": "9a3039f0",
        "text": "And here will pass in layers I layers I and here we have layers I plus one. OK. So what have I done here? So uh I'm basically creating a matrix uh which is this W here using this method uh from the numpy library which is random dot brand. And what I uh what this like um method does, it, it creates uh a random uh arrays and the arrays can have different uh dimensions. So in this case, we have like two dimensions. So and the two dimensions are given here by these values.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=360s",
        "start_time": "360.23"
    },
    {
        "id": "518b7c9d",
        "text": "I plus one. OK. So what have I done here? So uh I'm basically creating a matrix uh which is this W here using this method uh from the numpy library which is random dot brand. And what I uh what this like um method does, it, it creates uh a random uh arrays and the arrays can have different uh dimensions. So in this case, we have like two dimensions. So and the two dimensions are given here by these values. And so here, what I'm saying is that I want a two D array which is basically a matrix. And the, the, the number of rows that I want is the current uh layer I'm in. And the number of columns that I want is the uh the the number of neurons that I have in the subsequent layer.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=373s",
        "start_time": "373.239"
    },
    {
        "id": "f11c8849",
        "text": "uh which is this W here using this method uh from the numpy library which is random dot brand. And what I uh what this like um method does, it, it creates uh a random uh arrays and the arrays can have different uh dimensions. So in this case, we have like two dimensions. So and the two dimensions are given here by these values. And so here, what I'm saying is that I want a two D array which is basically a matrix. And the, the, the number of rows that I want is the current uh layer I'm in. And the number of columns that I want is the uh the the number of neurons that I have in the subsequent layer. And if you remember the weight matrix, this works because on the rows, we have all the connections of a neuron from a previous layer with the subsequent layer. So we want the for the",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=384s",
        "start_time": "384.98"
    },
    {
        "id": "91a27c41",
        "text": "And so here, what I'm saying is that I want a two D array which is basically a matrix. And the, the, the number of rows that I want is the current uh layer I'm in. And the number of columns that I want is the uh the the number of neurons that I have in the subsequent layer. And if you remember the weight matrix, this works because on the rows, we have all the connections of a neuron from a previous layer with the subsequent layer. So we want the for the number of rows to be equal to the number of neurons that we have like in a, in a layer. And for the number of columns we want to have uh the number of neurons that we have in the subsequent layers. And we can obtain that by using this expression here. Right.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=412s",
        "start_time": "412.72"
    },
    {
        "id": "f3a920f3",
        "text": "And if you remember the weight matrix, this works because on the rows, we have all the connections of a neuron from a previous layer with the subsequent layer. So we want the for the number of rows to be equal to the number of neurons that we have like in a, in a layer. And for the number of columns we want to have uh the number of neurons that we have in the subsequent layers. And we can obtain that by using this expression here. Right. And so, and what this random random does is obviously it creates this uh to the uh array. So this matrix and it initiates all the elements of the, the matrix randomly between zero and one,",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=435s",
        "start_time": "435.309"
    },
    {
        "id": "5ad28a56",
        "text": "number of rows to be equal to the number of neurons that we have like in a, in a layer. And for the number of columns we want to have uh the number of neurons that we have in the subsequent layers. And we can obtain that by using this expression here. Right. And so, and what this random random does is obviously it creates this uh to the uh array. So this matrix and it initiates all the elements of the, the matrix randomly between zero and one, right. So what else remains to do? Well, we we need to store this information and say how do we do that? Well, super simple. So we get the weights and we do append W",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=452s",
        "start_time": "452.48"
    },
    {
        "id": "4d34b888",
        "text": "And so, and what this random random does is obviously it creates this uh to the uh array. So this matrix and it initiates all the elements of the, the matrix randomly between zero and one, right. So what else remains to do? Well, we we need to store this information and say how do we do that? Well, super simple. So we get the weights and we do append W and so here uh weight is gonna be a list uh with as many um items as the number of weight mattresses in the layers, which is the number of layers minus one. Because if you remember if we have a network with three layers, we're gonna have two weight mattresses",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=470s",
        "start_time": "470.91"
    },
    {
        "id": "e50fcc3e",
        "text": "right. So what else remains to do? Well, we we need to store this information and say how do we do that? Well, super simple. So we get the weights and we do append W and so here uh weight is gonna be a list uh with as many um items as the number of weight mattresses in the layers, which is the number of layers minus one. Because if you remember if we have a network with three layers, we're gonna have two weight mattresses because the weight mattresses are always between two subsequent layers. So like the first um weight matrix is gonna be between uh layer one and layer two. And the second uh um weight matrix is gonna be between layer two and layer three,",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=485s",
        "start_time": "485.97"
    },
    {
        "id": "146e9c14",
        "text": "and so here uh weight is gonna be a list uh with as many um items as the number of weight mattresses in the layers, which is the number of layers minus one. Because if you remember if we have a network with three layers, we're gonna have two weight mattresses because the weight mattresses are always between two subsequent layers. So like the first um weight matrix is gonna be between uh layer one and layer two. And the second uh um weight matrix is gonna be between layer two and layer three, right. So we have the constructor and with this construct, we are able to build um a representation of a simple multi-layered perception. So what do we do need to do next. Well, we need to do the actual computation. So we need to do a forward propagation. So we'll create a new method called forward proper gate.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=499s",
        "start_time": "499.119"
    },
    {
        "id": "f4f88932",
        "text": "because the weight mattresses are always between two subsequent layers. So like the first um weight matrix is gonna be between uh layer one and layer two. And the second uh um weight matrix is gonna be between layer two and layer three, right. So we have the constructor and with this construct, we are able to build um a representation of a simple multi-layered perception. So what do we do need to do next. Well, we need to do the actual computation. So we need to do a forward propagation. So we'll create a new method called forward proper gate. And this method uh accepts as uh as an argument the inputs.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=523s",
        "start_time": "523.53"
    },
    {
        "id": "63a99c11",
        "text": "right. So we have the constructor and with this construct, we are able to build um a representation of a simple multi-layered perception. So what do we do need to do next. Well, we need to do the actual computation. So we need to do a forward propagation. So we'll create a new method called forward proper gate. And this method uh accepts as uh as an argument the inputs. And",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=541s",
        "start_time": "541.869"
    },
    {
        "id": "2c9ef189",
        "text": "And this method uh accepts as uh as an argument the inputs. And what do we do with this forward propagate? So if you remember um what forward propagation does it at each layer, how the the signal like travels through? Uh it does like two things. So the neurons first do like a a net input and then they do uh uh an activation. So the first part of this is going to be",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=569s",
        "start_time": "569.96"
    },
    {
        "id": "64c3de5e",
        "text": "And what do we do with this forward propagate? So if you remember um what forward propagation does it at each layer, how the the signal like travels through? Uh it does like two things. So the neurons first do like a a net input and then they do uh uh an activation. So the first part of this is going to be activations is equal to inputs. So the first time, so for the first layer, the activations are basically the inputs, then we want to do a full loop",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=579s",
        "start_time": "579.19"
    },
    {
        "id": "42b01654",
        "text": "what do we do with this forward propagate? So if you remember um what forward propagation does it at each layer, how the the signal like travels through? Uh it does like two things. So the neurons first do like a a net input and then they do uh uh an activation. So the first part of this is going to be activations is equal to inputs. So the first time, so for the first layer, the activations are basically the inputs, then we want to do a full loop and we want to loop through all the weight mattresses. And this basically means looping through all the the layers in the network. So if we do four WN self",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=580s",
        "start_time": "580.909"
    },
    {
        "id": "1c218889",
        "text": "activations is equal to inputs. So the first time, so for the first layer, the activations are basically the inputs, then we want to do a full loop and we want to loop through all the weight mattresses. And this basically means looping through all the the layers in the network. So if we do four WN self dot waits and then here we should perform a couple of things. So first thing is to",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=609s",
        "start_time": "609.19"
    },
    {
        "id": "02dd146c",
        "text": "and we want to loop through all the weight mattresses. And this basically means looping through all the the layers in the network. So if we do four WN self dot waits and then here we should perform a couple of things. So first thing is to um do the cal calculation. So calculate the net inputs for a given layer. And then the second thing that we want to do is to uh calculate",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=624s",
        "start_time": "624.2"
    },
    {
        "id": "7aba0de1",
        "text": "dot waits and then here we should perform a couple of things. So first thing is to um do the cal calculation. So calculate the net inputs for a given layer. And then the second thing that we want to do is to uh calculate the activations, right? So how do we do that? Yeah, with NP is quite simple because for the nets inputs, if you remember guys, what we should do is a matrix multiplication between the activations of the previous layer with the weight matrices",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=641s",
        "start_time": "641.01"
    },
    {
        "id": "ac1859d3",
        "text": "um do the cal calculation. So calculate the net inputs for a given layer. And then the second thing that we want to do is to uh calculate the activations, right? So how do we do that? Yeah, with NP is quite simple because for the nets inputs, if you remember guys, what we should do is a matrix multiplication between the activations of the previous layer with the weight matrices with the weight matrix. So how do we do that? It's quite simple. So we do an N uh NP dot uh dot And this method is basically performs a uh dot product between two batches. But if we have two matrices, it performs a matrix multiplication.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=651s",
        "start_time": "651.599"
    },
    {
        "id": "a7b3eb78",
        "text": "the activations, right? So how do we do that? Yeah, with NP is quite simple because for the nets inputs, if you remember guys, what we should do is a matrix multiplication between the activations of the previous layer with the weight matrices with the weight matrix. So how do we do that? It's quite simple. So we do an N uh NP dot uh dot And this method is basically performs a uh dot product between two batches. But if we have two matrices, it performs a matrix multiplication. So in this case, we pass in activations which is a vector and then we pass in the",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=668s",
        "start_time": "668.38"
    },
    {
        "id": "9b54f5d5",
        "text": "with the weight matrix. So how do we do that? It's quite simple. So we do an N uh NP dot uh dot And this method is basically performs a uh dot product between two batches. But if we have two matrices, it performs a matrix multiplication. So in this case, we pass in activations which is a vector and then we pass in the uh the weights W right. And this is a matrix. And so uh basically here and what MP dot dot does it uh performs uh this matrix multiplication between activations and weights and so we can get the net inputs. OK. So now we have the net inputs. What's the next phase? Well, we should calculate the activation. So how do we do that?",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=694s",
        "start_time": "694.099"
    },
    {
        "id": "e8fc6268",
        "text": "So in this case, we pass in activations which is a vector and then we pass in the uh the weights W right. And this is a matrix. And so uh basically here and what MP dot dot does it uh performs uh this matrix multiplication between activations and weights and so we can get the net inputs. OK. So now we have the net inputs. What's the next phase? Well, we should calculate the activation. So how do we do that? Yeah, it's quite straightforward because uh in this case, we want a, an M LP with a sigmoid activation function. So we just need to pass the uh net inputs to the sigmoid function. So how do we do that? Super simple? So we pass the net inputs to the Sigma function.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=712s",
        "start_time": "712.419"
    },
    {
        "id": "f89c67ca",
        "text": "uh the weights W right. And this is a matrix. And so uh basically here and what MP dot dot does it uh performs uh this matrix multiplication between activations and weights and so we can get the net inputs. OK. So now we have the net inputs. What's the next phase? Well, we should calculate the activation. So how do we do that? Yeah, it's quite straightforward because uh in this case, we want a, an M LP with a sigmoid activation function. So we just need to pass the uh net inputs to the sigmoid function. So how do we do that? Super simple? So we pass the net inputs to the Sigma function. Now you may be wondering but yeah, but we don't have a uh underscore Sigma uh method. Well, you, you're right, we, we need to implement that and if you remember, we already implemented this in a, in a previous video and it's quite straightforward and it will take us like two seconds to do that. OK. So now what we do is after we've gone through all of the loop, uh then we want to return the activations",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=720s",
        "start_time": "720.84"
    },
    {
        "id": "92b386ae",
        "text": "Yeah, it's quite straightforward because uh in this case, we want a, an M LP with a sigmoid activation function. So we just need to pass the uh net inputs to the sigmoid function. So how do we do that? Super simple? So we pass the net inputs to the Sigma function. Now you may be wondering but yeah, but we don't have a uh underscore Sigma uh method. Well, you, you're right, we, we need to implement that and if you remember, we already implemented this in a, in a previous video and it's quite straightforward and it will take us like two seconds to do that. OK. So now what we do is after we've gone through all of the loop, uh then we want to return the activations right.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=748s",
        "start_time": "748.7"
    },
    {
        "id": "01f44251",
        "text": "Now you may be wondering but yeah, but we don't have a uh underscore Sigma uh method. Well, you, you're right, we, we need to implement that and if you remember, we already implemented this in a, in a previous video and it's quite straightforward and it will take us like two seconds to do that. OK. So now what we do is after we've gone through all of the loop, uh then we want to return the activations right. So let's review this before moving forward. So with forward propagate, we start with the inputs and this is a vector. And for the first layer, the activations are basically the inputs. And then what we do, we do a forward propagation. So we just like move from one layer to the next one, from left to right. And so we, we do for doing that, we do a four, a four loop uh through all the work",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=772s",
        "start_time": "772.27"
    },
    {
        "id": "ab5ec0ea",
        "text": "right. So let's review this before moving forward. So with forward propagate, we start with the inputs and this is a vector. And for the first layer, the activations are basically the inputs. And then what we do, we do a forward propagation. So we just like move from one layer to the next one, from left to right. And so we, we do for doing that, we do a four, a four loop uh through all the work we iterate through all the weights. And then at each layer, we do two things, we calculate the net inputs first. And we do that with a matrix multiplication between the activation and the weights. And then we actually calculate the activations. And we do that by passing the net inputs to the sigmoid activation function. And then finally, after we are at the end of this journey, we just return the activations great. So",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=802s",
        "start_time": "802.45"
    },
    {
        "id": "40241d6e",
        "text": "So let's review this before moving forward. So with forward propagate, we start with the inputs and this is a vector. And for the first layer, the activations are basically the inputs. And then what we do, we do a forward propagation. So we just like move from one layer to the next one, from left to right. And so we, we do for doing that, we do a four, a four loop uh through all the work we iterate through all the weights. And then at each layer, we do two things, we calculate the net inputs first. And we do that with a matrix multiplication between the activation and the weights. And then we actually calculate the activations. And we do that by passing the net inputs to the sigmoid activation function. And then finally, after we are at the end of this journey, we just return the activations great. So what remains to do here? Well, we should implement this underscore sigmoid function.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=804s",
        "start_time": "804.039"
    },
    {
        "id": "c06f3fb2",
        "text": "we iterate through all the weights. And then at each layer, we do two things, we calculate the net inputs first. And we do that with a matrix multiplication between the activation and the weights. And then we actually calculate the activations. And we do that by passing the net inputs to the sigmoid activation function. And then finally, after we are at the end of this journey, we just return the activations great. So what remains to do here? Well, we should implement this underscore sigmoid function. So here we just want to pass X and this is super straightforward. We did it already. And the act the sigmoid function is basically 1/1 plus NP dot The exponential to the uh minus X right. Here we go super simple, right? So now we have all the elements",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=833s",
        "start_time": "833.085"
    },
    {
        "id": "4cb7c617",
        "text": "what remains to do here? Well, we should implement this underscore sigmoid function. So here we just want to pass X and this is super straightforward. We did it already. And the act the sigmoid function is basically 1/1 plus NP dot The exponential to the uh minus X right. Here we go super simple, right? So now we have all the elements it plays for our M LP class. And so now we can do a full uh forward propagation. And once we pass in some inputs, we can calculate like the outputs after like the mop has done its magic. So let's do that. And so in order to do that, let's uh ensure that uh we are running the script as like the main.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=860s",
        "start_time": "860.78"
    },
    {
        "id": "da24398f",
        "text": "So here we just want to pass X and this is super straightforward. We did it already. And the act the sigmoid function is basically 1/1 plus NP dot The exponential to the uh minus X right. Here we go super simple, right? So now we have all the elements it plays for our M LP class. And so now we can do a full uh forward propagation. And once we pass in some inputs, we can calculate like the outputs after like the mop has done its magic. So let's do that. And so in order to do that, let's uh ensure that uh we are running the script as like the main. So and we do that if name is equal to name and then we'll just move on down here. Cool. So what should we do here? So obviously, the first thing we want to do",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=868s",
        "start_time": "868.78"
    },
    {
        "id": "f2afdffc",
        "text": "it plays for our M LP class. And so now we can do a full uh forward propagation. And once we pass in some inputs, we can calculate like the outputs after like the mop has done its magic. So let's do that. And so in order to do that, let's uh ensure that uh we are running the script as like the main. So and we do that if name is equal to name and then we'll just move on down here. Cool. So what should we do here? So obviously, the first thing we want to do uh is to uh create an M LP. Obviously, we need a neural network. First of all, then we want to create uh some inputs,",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=896s",
        "start_time": "896.909"
    },
    {
        "id": "ee2451a6",
        "text": "So and we do that if name is equal to name and then we'll just move on down here. Cool. So what should we do here? So obviously, the first thing we want to do uh is to uh create an M LP. Obviously, we need a neural network. First of all, then we want to create uh some inputs, then we want to perform the forward prop propagation. And finally, we'll just like uh print the results, print oh what's that mistake there? Print the results? Cool. Okay. So let's do this. So how do we create an M LP? Super simple? So M LP is equal to",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=926s",
        "start_time": "926.679"
    },
    {
        "id": "399c7700",
        "text": "uh is to uh create an M LP. Obviously, we need a neural network. First of all, then we want to create uh some inputs, then we want to perform the forward prop propagation. And finally, we'll just like uh print the results, print oh what's that mistake there? Print the results? Cool. Okay. So let's do this. So how do we create an M LP? Super simple? So M LP is equal to the M LP class, the we call just the M LP constructor. So if we want to change the uh number of layers that we have and the number of neurons in each layer, we can do that. Uh But we'll just use this default values here. So I'm not gonna bother. OK. So let's create some inputs here. So",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=943s",
        "start_time": "943.52"
    },
    {
        "id": "af417112",
        "text": "then we want to perform the forward prop propagation. And finally, we'll just like uh print the results, print oh what's that mistake there? Print the results? Cool. Okay. So let's do this. So how do we create an M LP? Super simple? So M LP is equal to the M LP class, the we call just the M LP constructor. So if we want to change the uh number of layers that we have and the number of neurons in each layer, we can do that. Uh But we'll just use this default values here. So I'm not gonna bother. OK. So let's create some inputs here. So uh inputs. So inputs is should be a vector and the vector should be, should have the same number of items as the number of neurons in the input layer.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=957s",
        "start_time": "957.64"
    },
    {
        "id": "7725c7dc",
        "text": "the M LP class, the we call just the M LP constructor. So if we want to change the uh number of layers that we have and the number of neurons in each layer, we can do that. Uh But we'll just use this default values here. So I'm not gonna bother. OK. So let's create some inputs here. So uh inputs. So inputs is should be a vector and the vector should be, should have the same number of items as the number of neurons in the input layer. So we're gonna create some uh random inputs here. And so how do we do that again? We know our old friend random grand from the NPI LIB library here. This is a one dimensional array. So it's a vector basically. And so the di the dimension of uh the vector should be M LP dots number of inputs. So the number of neurons in the inputs, nice. We have the inputs.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=985s",
        "start_time": "985.929"
    },
    {
        "id": "ad2fdc43",
        "text": "uh inputs. So inputs is should be a vector and the vector should be, should have the same number of items as the number of neurons in the input layer. So we're gonna create some uh random inputs here. And so how do we do that again? We know our old friend random grand from the NPI LIB library here. This is a one dimensional array. So it's a vector basically. And so the di the dimension of uh the vector should be M LP dots number of inputs. So the number of neurons in the inputs, nice. We have the inputs. So we want to get the outputs out. And so to do that, it's super simple because we will just do a forward propagate and we'll pass in all the inputs. And so by now, we should have a uh a vector, an output vector.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1008s",
        "start_time": "1008.0"
    },
    {
        "id": "40f11860",
        "text": "So we're gonna create some uh random inputs here. And so how do we do that again? We know our old friend random grand from the NPI LIB library here. This is a one dimensional array. So it's a vector basically. And so the di the dimension of uh the vector should be M LP dots number of inputs. So the number of neurons in the inputs, nice. We have the inputs. So we want to get the outputs out. And so to do that, it's super simple because we will just do a forward propagate and we'll pass in all the inputs. And so by now, we should have a uh a vector, an output vector. And now we want to print like what we've done. OK. So let's do print uh network.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1021s",
        "start_time": "1021.969"
    },
    {
        "id": "3325b4a1",
        "text": "So we want to get the outputs out. And so to do that, it's super simple because we will just do a forward propagate and we'll pass in all the inputs. And so by now, we should have a uh a vector, an output vector. And now we want to print like what we've done. OK. So let's do print uh network. The net work output is and will pass uh the outputs here. And",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1049s",
        "start_time": "1049.569"
    },
    {
        "id": "0b8ece4e",
        "text": "And now we want to print like what we've done. OK. So let's do print uh network. The net work output is and will pass uh the outputs here. And OK. Yeah, let's also pass in the, let's also print the inputs. So the network input is, and here obviously, we need to change this to inputs. And down here we have the outputs. So cool. So now everything should be in place. So we've built our uh M LP objects, we created some random inputs, we passed that to the network which",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1072s",
        "start_time": "1072.17"
    },
    {
        "id": "040ab711",
        "text": "The net work output is and will pass uh the outputs here. And OK. Yeah, let's also pass in the, let's also print the inputs. So the network input is, and here obviously, we need to change this to inputs. And down here we have the outputs. So cool. So now everything should be in place. So we've built our uh M LP objects, we created some random inputs, we passed that to the network which uh performed a forward propagation. So we have the outputs and now we've uh should be able to both uh print the inputs and the outputs. So what remains to, to do here is we should just run this and hopefully, if I've done all correctly, uh we should be able to see some results",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1081s",
        "start_time": "1081.29"
    },
    {
        "id": "37d53e86",
        "text": "OK. Yeah, let's also pass in the, let's also print the inputs. So the network input is, and here obviously, we need to change this to inputs. And down here we have the outputs. So cool. So now everything should be in place. So we've built our uh M LP objects, we created some random inputs, we passed that to the network which uh performed a forward propagation. So we have the outputs and now we've uh should be able to both uh print the inputs and the outputs. So what remains to, to do here is we should just run this and hopefully, if I've done all correctly, uh we should be able to see some results go oh nice. So the network input is this three values over here. And uh it's three and it's correct because we are expecting three inputs free. Uh We, because we have three neurons in the input layers",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1095s",
        "start_time": "1095.119"
    },
    {
        "id": "ea700328",
        "text": "uh performed a forward propagation. So we have the outputs and now we've uh should be able to both uh print the inputs and the outputs. So what remains to, to do here is we should just run this and hopefully, if I've done all correctly, uh we should be able to see some results go oh nice. So the network input is this three values over here. And uh it's three and it's correct because we are expecting three inputs free. Uh We, because we have three neurons in the input layers and then the output is made up of two values here. So this two values here. And again, it's correct because we are expecting two values for the outputs because we have two neurons in the upper layer. Nice. So basically, you should just like uh congratulate yourself now because you've",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1120s",
        "start_time": "1120.14"
    },
    {
        "id": "04953f31",
        "text": "go oh nice. So the network input is this three values over here. And uh it's three and it's correct because we are expecting three inputs free. Uh We, because we have three neurons in the input layers and then the output is made up of two values here. So this two values here. And again, it's correct because we are expecting two values for the outputs because we have two neurons in the upper layer. Nice. So basically, you should just like uh congratulate yourself now because you've just finished implementing a neural network from scratch. So you now know more than most people like out there about how to actually create a neural network like from scratch. And now you may be wondering, OK, but there's an elephant in the room here and that's that we haven't uh trained like our network at all. So how do we do that?",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1140s",
        "start_time": "1140.729"
    },
    {
        "id": "873f6a19",
        "text": "and then the output is made up of two values here. So this two values here. And again, it's correct because we are expecting two values for the outputs because we have two neurons in the upper layer. Nice. So basically, you should just like uh congratulate yourself now because you've just finished implementing a neural network from scratch. So you now know more than most people like out there about how to actually create a neural network like from scratch. And now you may be wondering, OK, but there's an elephant in the room here and that's that we haven't uh trained like our network at all. So how do we do that? Well, this is a huge topic in itself and it's gonna be the focus of my next video where I'm gonna basically explain the theory behind back propagation and gradient descent uh which are like the two approaches, the two techniques which we used to uh train network with uh some uh training data set.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1161s",
        "start_time": "1161.119"
    },
    {
        "id": "83c50604",
        "text": "just finished implementing a neural network from scratch. So you now know more than most people like out there about how to actually create a neural network like from scratch. And now you may be wondering, OK, but there's an elephant in the room here and that's that we haven't uh trained like our network at all. So how do we do that? Well, this is a huge topic in itself and it's gonna be the focus of my next video where I'm gonna basically explain the theory behind back propagation and gradient descent uh which are like the two approaches, the two techniques which we used to uh train network with uh some uh training data set. So yeah, next time we're gonna do that. So I hope you enjoyed this video if you did. Uh And if you want to get like more videos, remember to subscribe and hit the notification bell. Uh because that way you're gonna get like all the videos once I upload them and if you like again the video just like also leave a like and I guess I'll see you next time. Cheers.",
        "video": "6- Implementing a neural network from scratch in Python",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "0oWnheK-gGk",
        "youtube_link": "https://www.youtube.com/watch?v=0oWnheK-gGk&t=1187s",
        "start_time": "1187.339"
    },
    {
        "id": "f7156820",
        "text": "Hi, everybody and welcome to another video in the Deep Learning for audio with Python series. This time we're gonna talk about recurrent neural networks and understand the theory behind it. So let's get started and understand what R and NS are, what they do, what type of data they process. Now there's a lot of data in which order is extremely important. That's true. For example, like for text and words. So let's analyze for example, the the sentence Anna loves John. Well, here order is very important because Anna loves John is very different from John loves Anna, right? And the point being that like order is so important that many like bad things can happen. Like if you don't understand whether like Anna loves you or John loves Anna, right? OK. So in that sense, like order is very important for data, certain type of data like time series as well where we have like data uh which are like measures like of of different values that are taken like a certain intervals and ordered like over time, right?",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "7c986615",
        "text": "there's a lot of data in which order is extremely important. That's true. For example, like for text and words. So let's analyze for example, the the sentence Anna loves John. Well, here order is very important because Anna loves John is very different from John loves Anna, right? And the point being that like order is so important that many like bad things can happen. Like if you don't understand whether like Anna loves you or John loves Anna, right? OK. So in that sense, like order is very important for data, certain type of data like time series as well where we have like data uh which are like measures like of of different values that are taken like a certain intervals and ordered like over time, right? So this is a type of data that's very important and we want like a network that can understand and process like that data and maintain and understand the order there, right? So another point is that uh usually say, for example, we have like a time series or, or even just like a sentence or a piece of music with a melody. So it turns out that like melodies or like sentences can have like a variable number of words or, or notes, right?",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=20s",
        "start_time": "20.59"
    },
    {
        "id": "2f43a90b",
        "text": "bad things can happen. Like if you don't understand whether like Anna loves you or John loves Anna, right? OK. So in that sense, like order is very important for data, certain type of data like time series as well where we have like data uh which are like measures like of of different values that are taken like a certain intervals and ordered like over time, right? So this is a type of data that's very important and we want like a network that can understand and process like that data and maintain and understand the order there, right? So another point is that uh usually say, for example, we have like a time series or, or even just like a sentence or a piece of music with a melody. So it turns out that like melodies or like sentences can have like a variable number of words or, or notes, right? And so we want a network that's able to process data, sequential data regardless of the number of like words or notes or data points that we have in there.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=45s",
        "start_time": "45.72"
    },
    {
        "id": "f0943a33",
        "text": "So this is a type of data that's very important and we want like a network that can understand and process like that data and maintain and understand the order there, right? So another point is that uh usually say, for example, we have like a time series or, or even just like a sentence or a piece of music with a melody. So it turns out that like melodies or like sentences can have like a variable number of words or, or notes, right? And so we want a network that's able to process data, sequential data regardless of the number of like words or notes or data points that we have in there. So RNNS in a sense, try to address all of these issues. They're used for sequential data. And the great thing about them is that they uh process data in such a way that uh each data point is pro processed in context. So we look back at what happened before to understand what to do next, like with our data and how to predict.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=71s",
        "start_time": "71.04"
    },
    {
        "id": "793b84c5",
        "text": "And so we want a network that's able to process data, sequential data regardless of the number of like words or notes or data points that we have in there. So RNNS in a sense, try to address all of these issues. They're used for sequential data. And the great thing about them is that they uh process data in such a way that uh each data point is pro processed in context. So we look back at what happened before to understand what to do next, like with our data and how to predict. So obviously, like audio and specifically music are",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=100s",
        "start_time": "100.36"
    },
    {
        "id": "86187c1f",
        "text": "So RNNS in a sense, try to address all of these issues. They're used for sequential data. And the great thing about them is that they uh process data in such a way that uh each data point is pro processed in context. So we look back at what happened before to understand what to do next, like with our data and how to predict. So obviously, like audio and specifically music are kind of like, so the the type of data that we have like for a music like resonance work very well with R and M because we can think of audio as a time series itself. So a series of points taken at certain intervals in time.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=113s",
        "start_time": "113.55"
    },
    {
        "id": "e33b27fd",
        "text": "So obviously, like audio and specifically music are kind of like, so the the type of data that we have like for a music like resonance work very well with R and M because we can think of audio as a time series itself. So a series of points taken at certain intervals in time. So if we think of a waveform, for example, we can think of it as a univariate time series. So what does univariate mean? Well, it means that we have only one value one measure that's taken at each interval. So now let's take a look at the, at the data shape that we have uh with a waveform uh expressed as a time series. So here we have it.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=139s",
        "start_time": "139.96"
    },
    {
        "id": "b34201e3",
        "text": "kind of like, so the the type of data that we have like for a music like resonance work very well with R and M because we can think of audio as a time series itself. So a series of points taken at certain intervals in time. So if we think of a waveform, for example, we can think of it as a univariate time series. So what does univariate mean? Well, it means that we have only one value one measure that's taken at each interval. So now let's take a look at the, at the data shape that we have uh with a waveform uh expressed as a time series. So here we have it. So the first dimension is basically given by the number of samples or number of intervals where we are taking measures. And this is given by the sample rates which in this case, here is 22,050 by not,",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=146s",
        "start_time": "146.339"
    },
    {
        "id": "db6e1d95",
        "text": "So if we think of a waveform, for example, we can think of it as a univariate time series. So what does univariate mean? Well, it means that we have only one value one measure that's taken at each interval. So now let's take a look at the, at the data shape that we have uh with a waveform uh expressed as a time series. So here we have it. So the first dimension is basically given by the number of samples or number of intervals where we are taking measures. And this is given by the sample rates which in this case, here is 22,050 by not, which is the number of seconds, right? So here we have more or less 200,000 data points in the time series. And then here we have as the second dimension one. And that's because we have only one dimension which is the amplitude that we are taking at each data point. Right.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=163s",
        "start_time": "163.899"
    },
    {
        "id": "90a17cfc",
        "text": "So the first dimension is basically given by the number of samples or number of intervals where we are taking measures. And this is given by the sample rates which in this case, here is 22,050 by not, which is the number of seconds, right? So here we have more or less 200,000 data points in the time series. And then here we have as the second dimension one. And that's because we have only one dimension which is the amplitude that we are taking at each data point. Right. Now, let's take a look at another type of data that we've used quite a lot like in over like the, the, the, the, the last few videos. And that's the MF CCS and we can think of MF CCS as a multi",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=190s",
        "start_time": "190.55"
    },
    {
        "id": "047e45cf",
        "text": "which is the number of seconds, right? So here we have more or less 200,000 data points in the time series. And then here we have as the second dimension one. And that's because we have only one dimension which is the amplitude that we are taking at each data point. Right. Now, let's take a look at another type of data that we've used quite a lot like in over like the, the, the, the, the last few videos. And that's the MF CCS and we can think of MF CCS as a multi create time series. Now, first of all, if you don't remember what MF CCS are just like, take a look at the video below here and you can understand what uh like MF CCS are and why they are super important for like all your analysis uh cool. So now let's go back to like the interpreting MF CCS as a Multivariate time series. Here, we have values at different intervals",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=208s",
        "start_time": "208.225"
    },
    {
        "id": "9118796a",
        "text": "Now, let's take a look at another type of data that we've used quite a lot like in over like the, the, the, the, the last few videos. And that's the MF CCS and we can think of MF CCS as a multi create time series. Now, first of all, if you don't remember what MF CCS are just like, take a look at the video below here and you can understand what uh like MF CCS are and why they are super important for like all your analysis uh cool. So now let's go back to like the interpreting MF CCS as a Multivariate time series. Here, we have values at different intervals and then for each um value for each interval, we have 13. In this case values and each of the values represents a coefficient. It's an MFCC right. And in this case, we are talking about a Multivariate time series because we are taking more uh measurements for each interval. And the,",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=226s",
        "start_time": "226.11"
    },
    {
        "id": "d0dc791f",
        "text": "create time series. Now, first of all, if you don't remember what MF CCS are just like, take a look at the video below here and you can understand what uh like MF CCS are and why they are super important for like all your analysis uh cool. So now let's go back to like the interpreting MF CCS as a Multivariate time series. Here, we have values at different intervals and then for each um value for each interval, we have 13. In this case values and each of the values represents a coefficient. It's an MFCC right. And in this case, we are talking about a Multivariate time series because we are taking more uh measurements for each interval. And the, the relative dimension here is given by the overall number of uh intervals. And in this case, we have um the sample rate divided by the hot length, which in this case is 512 which is like quite customary. Again, if you don't remember, just go back and watch like my previous video on this. And then we multiply that by nine, which is the number of seconds. So overall, we have",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=241s",
        "start_time": "241.684"
    },
    {
        "id": "4a048587",
        "text": "and then for each um value for each interval, we have 13. In this case values and each of the values represents a coefficient. It's an MFCC right. And in this case, we are talking about a Multivariate time series because we are taking more uh measurements for each interval. And the, the relative dimension here is given by the overall number of uh intervals. And in this case, we have um the sample rate divided by the hot length, which in this case is 512 which is like quite customary. Again, if you don't remember, just go back and watch like my previous video on this. And then we multiply that by nine, which is the number of seconds. So overall, we have around like 403 187 intervals. And then the second dimension is given by the number of MF CCS which is 13. So in other words, here we are, we have 300",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=269s",
        "start_time": "269.22"
    },
    {
        "id": "e2bdc0c4",
        "text": "the relative dimension here is given by the overall number of uh intervals. And in this case, we have um the sample rate divided by the hot length, which in this case is 512 which is like quite customary. Again, if you don't remember, just go back and watch like my previous video on this. And then we multiply that by nine, which is the number of seconds. So overall, we have around like 403 187 intervals. And then the second dimension is given by the number of MF CCS which is 13. So in other words, here we are, we have 300 87 intervals at each interval, we have 13 values, right? So this is the type of data that we can use with R and N. So this kind of like time series where you have a number of like values for uh like yeah, at different like intervals, right? OK. So let's take a look at how R and NS work like from a very, very high levels perspective. So",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=293s",
        "start_time": "293.095"
    },
    {
        "id": "b1fcd876",
        "text": "around like 403 187 intervals. And then the second dimension is given by the number of MF CCS which is 13. So in other words, here we are, we have 300 87 intervals at each interval, we have 13 values, right? So this is the type of data that we can use with R and N. So this kind of like time series where you have a number of like values for uh like yeah, at different like intervals, right? OK. So let's take a look at how R and NS work like from a very, very high levels perspective. So first of all, the idea here is that we have like this time series type of data",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=319s",
        "start_time": "319.059"
    },
    {
        "id": "bbc5aed1",
        "text": "87 intervals at each interval, we have 13 values, right? So this is the type of data that we can use with R and N. So this kind of like time series where you have a number of like values for uh like yeah, at different like intervals, right? OK. So let's take a look at how R and NS work like from a very, very high levels perspective. So first of all, the idea here is that we have like this time series type of data and we feed it into the network a point at a time. So we give it like a data point uh at a step like every time. And the idea is that the network is going to be able to predict the next step given the step we are currently giving it and the history there, right. So the prediction depends also on the context on what happened before,",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=334s",
        "start_time": "334.179"
    },
    {
        "id": "49d6ac4b",
        "text": "first of all, the idea here is that we have like this time series type of data and we feed it into the network a point at a time. So we give it like a data point uh at a step like every time. And the idea is that the network is going to be able to predict the next step given the step we are currently giving it and the history there, right. So the prediction depends also on the context on what happened before, right. So this is like a very, very high level intuition what an RNN is. So now let's take a look at let's like dive down down into uh an RNN architecture. So here we have our inputs which is like given by X, then we have a recurrent layer which is obviously like the the focus of a uh an RNN",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=361s",
        "start_time": "361.859"
    },
    {
        "id": "42f1739f",
        "text": "and we feed it into the network a point at a time. So we give it like a data point uh at a step like every time. And the idea is that the network is going to be able to predict the next step given the step we are currently giving it and the history there, right. So the prediction depends also on the context on what happened before, right. So this is like a very, very high level intuition what an RNN is. So now let's take a look at let's like dive down down into uh an RNN architecture. So here we have our inputs which is like given by X, then we have a recurrent layer which is obviously like the the focus of a uh an RNN and a recurrent layer is a layer that's able to process sequential data in a sequential way. And we'll see what this means in a second. Then the output of the recurrent layer usually this is fed into a dense layer which does some classification using soft macs for example. And then we have uh the output which is why?",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=366s",
        "start_time": "366.79"
    },
    {
        "id": "d18a570e",
        "text": "right. So this is like a very, very high level intuition what an RNN is. So now let's take a look at let's like dive down down into uh an RNN architecture. So here we have our inputs which is like given by X, then we have a recurrent layer which is obviously like the the focus of a uh an RNN and a recurrent layer is a layer that's able to process sequential data in a sequential way. And we'll see what this means in a second. Then the output of the recurrent layer usually this is fed into a dense layer which does some classification using soft macs for example. And then we have uh the output which is why? Right? OK. So now a thing that I want to uh try like to, to, to to look into here is the dimensionality of the input. So the shape of X. So in this case, we have a three dimensional input. So the first dimension is the batch size. So it's the number of samples of time windows that we are passing into the network at once.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=395s",
        "start_time": "395.98"
    },
    {
        "id": "52f395ce",
        "text": "and a recurrent layer is a layer that's able to process sequential data in a sequential way. And we'll see what this means in a second. Then the output of the recurrent layer usually this is fed into a dense layer which does some classification using soft macs for example. And then we have uh the output which is why? Right? OK. So now a thing that I want to uh try like to, to, to to look into here is the dimensionality of the input. So the shape of X. So in this case, we have a three dimensional input. So the first dimension is the batch size. So it's the number of samples of time windows that we are passing into the network at once. Then uh the second dimension is given by the number of steps that we have in the sequence. And the third dimension is the, the number of dimensions, the number of different measures that we have in uh the sequence itself. So for example, in the case of a waveform, which is a univariate time series, the, the this third dimension here would be equal to one. In the case of the MF CCS with 13 coefficients, we would have 13 here.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=421s",
        "start_time": "421.26"
    },
    {
        "id": "5cefd1d8",
        "text": "Right? OK. So now a thing that I want to uh try like to, to, to to look into here is the dimensionality of the input. So the shape of X. So in this case, we have a three dimensional input. So the first dimension is the batch size. So it's the number of samples of time windows that we are passing into the network at once. Then uh the second dimension is given by the number of steps that we have in the sequence. And the third dimension is the, the number of dimensions, the number of different measures that we have in uh the sequence itself. So for example, in the case of a waveform, which is a univariate time series, the, the this third dimension here would be equal to one. In the case of the MF CCS with 13 coefficients, we would have 13 here. And that's a Multivariate time series. Remember that. OK. So this is more or less the architecture from a very high level point of view. Now let's get into the recurrent layer and understand how it works cool. OK. So for the recurrent recurrent layer, the most important uh component is the cell, uh the cell, it's the one that's uh kind of like processes all the information. And here we can have",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=445s",
        "start_time": "445.29"
    },
    {
        "id": "d74e26ca",
        "text": "Then uh the second dimension is given by the number of steps that we have in the sequence. And the third dimension is the, the number of dimensions, the number of different measures that we have in uh the sequence itself. So for example, in the case of a waveform, which is a univariate time series, the, the this third dimension here would be equal to one. In the case of the MF CCS with 13 coefficients, we would have 13 here. And that's a Multivariate time series. Remember that. OK. So this is more or less the architecture from a very high level point of view. Now let's get into the recurrent layer and understand how it works cool. OK. So for the recurrent recurrent layer, the most important uh component is the cell, uh the cell, it's the one that's uh kind of like processes all the information. And here we can have a lot of like different types of cells. So for example here, like with a simple RNN, that's what we are analyzing. Like in this video, we're gonna look at a very simple like dense layer wi with a hyperbolic tangent as the activation function. But then we can have also other types",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=473s",
        "start_time": "473.614"
    },
    {
        "id": "ee6afd78",
        "text": "And that's a Multivariate time series. Remember that. OK. So this is more or less the architecture from a very high level point of view. Now let's get into the recurrent layer and understand how it works cool. OK. So for the recurrent recurrent layer, the most important uh component is the cell, uh the cell, it's the one that's uh kind of like processes all the information. And here we can have a lot of like different types of cells. So for example here, like with a simple RNN, that's what we are analyzing. Like in this video, we're gonna look at a very simple like dense layer wi with a hyperbolic tangent as the activation function. But then we can have also other types of cells like a LSDM long, short term memory cell or a gated recurrent unit. We're gonna look into this like in the next video and understand why they are important. But then again, the cell is the one that's responsible for processing this sequential data. Now, at each step, uh we uh kind of given uh a input data that's represented by this XT.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=502s",
        "start_time": "502.429"
    },
    {
        "id": "f3547470",
        "text": "a lot of like different types of cells. So for example here, like with a simple RNN, that's what we are analyzing. Like in this video, we're gonna look at a very simple like dense layer wi with a hyperbolic tangent as the activation function. But then we can have also other types of cells like a LSDM long, short term memory cell or a gated recurrent unit. We're gonna look into this like in the next video and understand why they are important. But then again, the cell is the one that's responsible for processing this sequential data. Now, at each step, uh we uh kind of given uh a input data that's represented by this XT. Then the cell does some processing and it outputs a couple of things. So it outputs H of T. Uh And HT is basically we can call it a state vector or a hidden state, right? And this represents and keeps memory like of the, of the cell uh well, of the, of the network like at, at a certain point in time.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=532s",
        "start_time": "532.059"
    },
    {
        "id": "5ab27442",
        "text": "of cells like a LSDM long, short term memory cell or a gated recurrent unit. We're gonna look into this like in the next video and understand why they are important. But then again, the cell is the one that's responsible for processing this sequential data. Now, at each step, uh we uh kind of given uh a input data that's represented by this XT. Then the cell does some processing and it outputs a couple of things. So it outputs H of T. Uh And HT is basically we can call it a state vector or a hidden state, right? And this represents and keeps memory like of the, of the cell uh well, of the, of the network like at, at a certain point in time. And then we also output YT which is the actual output. And here, the whole point of the, the, the recurrence here is given by the fact that uh the HT or the state vector is gonna be reused at a, at a later point at the next step. And so that we, we can have like information about the context.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=549s",
        "start_time": "549.655"
    },
    {
        "id": "ac869786",
        "text": "Then the cell does some processing and it outputs a couple of things. So it outputs H of T. Uh And HT is basically we can call it a state vector or a hidden state, right? And this represents and keeps memory like of the, of the cell uh well, of the, of the network like at, at a certain point in time. And then we also output YT which is the actual output. And here, the whole point of the, the, the recurrence here is given by the fact that uh the HT or the state vector is gonna be reused at a, at a later point at the next step. And so that we, we can have like information about the context. So basically the whole idea here is that we are giving information sequentially step by step to the cell here. And then we output both and output and a hidden state and the hidden state or state vector is going to be reused for the next step.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=578s",
        "start_time": "578.09"
    },
    {
        "id": "32541a38",
        "text": "And then we also output YT which is the actual output. And here, the whole point of the, the, the recurrence here is given by the fact that uh the HT or the state vector is gonna be reused at a, at a later point at the next step. And so that we, we can have like information about the context. So basically the whole idea here is that we are giving information sequentially step by step to the cell here. And then we output both and output and a hidden state and the hidden state or state vector is going to be reused for the next step. Now, the whole point is that we have like uh recursive here because we are just like feeding back some information uh into like the uh the cell from a previous uh like um computation. So let's try to unroll uh this process into like a linear way.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=605s",
        "start_time": "605.559"
    },
    {
        "id": "ecc2a1ab",
        "text": "So basically the whole idea here is that we are giving information sequentially step by step to the cell here. And then we output both and output and a hidden state and the hidden state or state vector is going to be reused for the next step. Now, the whole point is that we have like uh recursive here because we are just like feeding back some information uh into like the uh the cell from a previous uh like um computation. So let's try to unroll uh this process into like a linear way. OK. So Now, let's imagine we have a sequence with and different steps. So we'll start from the previous uh from the first step here. So we'll pass in X zero, X zero gets analyzed and process from the cell.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=630s",
        "start_time": "630.989"
    },
    {
        "id": "5bf8ba8d",
        "text": "Now, the whole point is that we have like uh recursive here because we are just like feeding back some information uh into like the uh the cell from a previous uh like um computation. So let's try to unroll uh this process into like a linear way. OK. So Now, let's imagine we have a sequence with and different steps. So we'll start from the previous uh from the first step here. So we'll pass in X zero, X zero gets analyzed and process from the cell. And we get an output which is H zero, which is the sli vector at this uh time step at time step zero. And then we have uh the uh output which is Y zero. Now, the next step is",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=648s",
        "start_time": "648.359"
    },
    {
        "id": "d615913b",
        "text": "OK. So Now, let's imagine we have a sequence with and different steps. So we'll start from the previous uh from the first step here. So we'll pass in X zero, X zero gets analyzed and process from the cell. And we get an output which is H zero, which is the sli vector at this uh time step at time step zero. And then we have uh the uh output which is Y zero. Now, the next step is sending in X one as a data point and X one is going to be used in conjunction with H zero to output H one which is the new uh state vector. And at the same time, we, the, the cell will also output Y one, right.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=670s",
        "start_time": "670.729"
    },
    {
        "id": "543fe97f",
        "text": "And we get an output which is H zero, which is the sli vector at this uh time step at time step zero. And then we have uh the uh output which is Y zero. Now, the next step is sending in X one as a data point and X one is going to be used in conjunction with H zero to output H one which is the new uh state vector. And at the same time, we, the, the cell will also output Y one, right. OK. So we continue and we uh we pass in X two, we consider H one and we output H two and Y two, we can continue this like for as many steps as we have until we get to the end.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=686s",
        "start_time": "686.835"
    },
    {
        "id": "a1c8c051",
        "text": "sending in X one as a data point and X one is going to be used in conjunction with H zero to output H one which is the new uh state vector. And at the same time, we, the, the cell will also output Y one, right. OK. So we continue and we uh we pass in X two, we consider H one and we output H two and Y two, we can continue this like for as many steps as we have until we get to the end. So in this case, it's uh uh the N step. And so here we obviously, it's the same thing that we've done so far. So we input XN, we consider the state of the cell at the previous step. So uh at N minus one",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=703s",
        "start_time": "703.2"
    },
    {
        "id": "1e8e4dd5",
        "text": "OK. So we continue and we uh we pass in X two, we consider H one and we output H two and Y two, we can continue this like for as many steps as we have until we get to the end. So in this case, it's uh uh the N step. And so here we obviously, it's the same thing that we've done so far. So we input XN, we consider the state of the cell at the previous step. So uh at N minus one and we combine this to information and we get a new state vector here HN and we get an output called YN. Now, the thing that's really important to understand here is that each uh like of these uh like timestamps. So at each of these time steps, we always use the very same set.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=722s",
        "start_time": "722.03"
    },
    {
        "id": "c74b35a7",
        "text": "So in this case, it's uh uh the N step. And so here we obviously, it's the same thing that we've done so far. So we input XN, we consider the state of the cell at the previous step. So uh at N minus one and we combine this to information and we get a new state vector here HN and we get an output called YN. Now, the thing that's really important to understand here is that each uh like of these uh like timestamps. So at each of these time steps, we always use the very same set. So we just have one cell but we use it recursively. And so from this like the whole point of like calling this network like a recurrent, right? Because we are recursively using the very same cell time and again, time and again to, to output like new, to process new information in a sequential way.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=738s",
        "start_time": "738.08"
    },
    {
        "id": "c45082ea",
        "text": "and we combine this to information and we get a new state vector here HN and we get an output called YN. Now, the thing that's really important to understand here is that each uh like of these uh like timestamps. So at each of these time steps, we always use the very same set. So we just have one cell but we use it recursively. And so from this like the whole point of like calling this network like a recurrent, right? Because we are recursively using the very same cell time and again, time and again to, to output like new, to process new information in a sequential way. Cool. So now what I want to do is take a look at the dimensionality of the data that flows through uh an R and M. So let's take it as an example, an input where we have this dimensionality here. So we have",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=754s",
        "start_time": "754.469"
    },
    {
        "id": "180e4312",
        "text": "So we just have one cell but we use it recursively. And so from this like the whole point of like calling this network like a recurrent, right? Because we are recursively using the very same cell time and again, time and again to, to output like new, to process new information in a sequential way. Cool. So now what I want to do is take a look at the dimensionality of the data that flows through uh an R and M. So let's take it as an example, an input where we have this dimensionality here. So we have uh a batch size is equal to 29 steps in the sequence. So a sequence with nine items and then we have a univariate uh time series here. So we only have like one value, one measure per step. OK. So here we have our",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=775s",
        "start_time": "775.034"
    },
    {
        "id": "ec86d6ea",
        "text": "Cool. So now what I want to do is take a look at the dimensionality of the data that flows through uh an R and M. So let's take it as an example, an input where we have this dimensionality here. So we have uh a batch size is equal to 29 steps in the sequence. So a sequence with nine items and then we have a univariate uh time series here. So we only have like one value, one measure per step. OK. So here we have our um RNN basically our recurrent layer um unrolled. And so, and as you can notice here, we have nine steps, obviously it's nine because it's the number of like steps that we have uh in the sequence. OK.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=796s",
        "start_time": "796.44"
    },
    {
        "id": "ed41ad23",
        "text": "uh a batch size is equal to 29 steps in the sequence. So a sequence with nine items and then we have a univariate uh time series here. So we only have like one value, one measure per step. OK. So here we have our um RNN basically our recurrent layer um unrolled. And so, and as you can notice here, we have nine steps, obviously it's nine because it's the number of like steps that we have uh in the sequence. OK. So for the input at each step, we have a dimensionality that's equal to the batch size and the number of dimensions. So in this case, uh each um each data point is equal ha well, it's not equal but it has a dimensionality of like 21 right.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=813s",
        "start_time": "813.859"
    },
    {
        "id": "d66a8e7a",
        "text": "um RNN basically our recurrent layer um unrolled. And so, and as you can notice here, we have nine steps, obviously it's nine because it's the number of like steps that we have uh in the sequence. OK. So for the input at each step, we have a dimensionality that's equal to the batch size and the number of dimensions. So in this case, uh each um each data point is equal ha well, it's not equal but it has a dimensionality of like 21 right. OK. So let's take a look at the output. Here, at each step, we have a dimensionality that's equal to the batch size and the number of units. So we, we are basically output uh many, well as many um",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=836s",
        "start_time": "836.159"
    },
    {
        "id": "8ff54698",
        "text": "So for the input at each step, we have a dimensionality that's equal to the batch size and the number of dimensions. So in this case, uh each um each data point is equal ha well, it's not equal but it has a dimensionality of like 21 right. OK. So let's take a look at the output. Here, at each step, we have a dimensionality that's equal to the batch size and the number of units. So we, we are basically output uh many, well as many um outputs as the number as the number of like a bat, well as the batch size, right. OK. So in this case, we have 23, right? And here we are saying basically that we have three units for the recurrent layer. OK.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=855s",
        "start_time": "855.07"
    },
    {
        "id": "f4e0c6ee",
        "text": "OK. So let's take a look at the output. Here, at each step, we have a dimensionality that's equal to the batch size and the number of units. So we, we are basically output uh many, well as many um outputs as the number as the number of like a bat, well as the batch size, right. OK. So in this case, we have 23, right? And here we are saying basically that we have three units for the recurrent layer. OK. So if we consider the whole output, so Y zero up until Y uh nine, all together the output shape is three dimensional, right? And so here we have the back size as the first dimension, the, the number of steps. And this is the second dimension and the number of units as the third dimension. And in this case, we have 293 cool.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=877s",
        "start_time": "877.429"
    },
    {
        "id": "e5254384",
        "text": "outputs as the number as the number of like a bat, well as the batch size, right. OK. So in this case, we have 23, right? And here we are saying basically that we have three units for the recurrent layer. OK. So if we consider the whole output, so Y zero up until Y uh nine, all together the output shape is three dimensional, right? And so here we have the back size as the first dimension, the, the number of steps. And this is the second dimension and the number of units as the third dimension. And in this case, we have 293 cool. So now what remains to do is look at the uh hidden state or the state vector. And it turns out that in simple R and NS the state vector is equal to the outputs, right. So for example, here this H zero is equal to Y zero. And obviously the dimensionality is the same. So we have like 23,",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=894s",
        "start_time": "894.94"
    },
    {
        "id": "80349ef6",
        "text": "So if we consider the whole output, so Y zero up until Y uh nine, all together the output shape is three dimensional, right? And so here we have the back size as the first dimension, the, the number of steps. And this is the second dimension and the number of units as the third dimension. And in this case, we have 293 cool. So now what remains to do is look at the uh hidden state or the state vector. And it turns out that in simple R and NS the state vector is equal to the outputs, right. So for example, here this H zero is equal to Y zero. And obviously the dimensionality is the same. So we have like 23, that's good.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=911s",
        "start_time": "911.59"
    },
    {
        "id": "96da2dd1",
        "text": "So now what remains to do is look at the uh hidden state or the state vector. And it turns out that in simple R and NS the state vector is equal to the outputs, right. So for example, here this H zero is equal to Y zero. And obviously the dimensionality is the same. So we have like 23, that's good. OK. So now uh let's take a look at a couple of very, I would say like uh flavors of R and NS. So we have one that's called sequence to vector R and N and here. The idea is that uh we input a, a sequence but the output that we get is only like a single vector here.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=936s",
        "start_time": "936.619"
    },
    {
        "id": "1313d326",
        "text": "that's good. OK. So now uh let's take a look at a couple of very, I would say like uh flavors of R and NS. So we have one that's called sequence to vector R and N and here. The idea is that uh we input a, a sequence but the output that we get is only like a single vector here. So basically here, uh we are just waiting for this uh Y nine value in a nine step sequence and we drop all of the other predictions before. So Y zero, Y one, Y two, we, we drop all of them and we are only focused in the, the, the, the last bit that's that gets predicted.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=962s",
        "start_time": "962.049"
    },
    {
        "id": "3e653f72",
        "text": "OK. So now uh let's take a look at a couple of very, I would say like uh flavors of R and NS. So we have one that's called sequence to vector R and N and here. The idea is that uh we input a, a sequence but the output that we get is only like a single vector here. So basically here, uh we are just waiting for this uh Y nine value in a nine step sequence and we drop all of the other predictions before. So Y zero, Y one, Y two, we, we drop all of them and we are only focused in the, the, the, the last bit that's that gets predicted. And this is like quite usual. So for example, if you use an R and M for generating a melody, so what you can do basically is you provide uh an input and this is a sequence um",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=963s",
        "start_time": "963.489"
    },
    {
        "id": "c54e15ed",
        "text": "So basically here, uh we are just waiting for this uh Y nine value in a nine step sequence and we drop all of the other predictions before. So Y zero, Y one, Y two, we, we drop all of them and we are only focused in the, the, the, the last bit that's that gets predicted. And this is like quite usual. So for example, if you use an R and M for generating a melody, so what you can do basically is you provide uh an input and this is a sequence um a melody, right? So we have like a sequence of notes and there are the, you are interested in the next notes, but you're not necessarily interested into like the previous notes, right. OK. So this is a sequence to vector RNN and it's opposed to a sequence to sequence RNN.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=990s",
        "start_time": "990.559"
    },
    {
        "id": "b78b1a46",
        "text": "And this is like quite usual. So for example, if you use an R and M for generating a melody, so what you can do basically is you provide uh an input and this is a sequence um a melody, right? So we have like a sequence of notes and there are the, you are interested in the next notes, but you're not necessarily interested into like the previous notes, right. OK. So this is a sequence to vector RNN and it's opposed to a sequence to sequence RNN. Here we consider like uh as an output all the outputs at each time step. And so basically here, the idea is that we uh input a batch of sequences and then we get as an output a bunch of sequences again, right? So this is a little bit of less common",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1013s",
        "start_time": "1013.489"
    },
    {
        "id": "fd31e9f5",
        "text": "a melody, right? So we have like a sequence of notes and there are the, you are interested in the next notes, but you're not necessarily interested into like the previous notes, right. OK. So this is a sequence to vector RNN and it's opposed to a sequence to sequence RNN. Here we consider like uh as an output all the outputs at each time step. And so basically here, the idea is that we uh input a batch of sequences and then we get as an output a bunch of sequences again, right? So this is a little bit of less common uh I would say then as sequences to vector and this is reflected also like in how uh Kous works and indeed the default um behavior for simple RNN uh layers instead of having a sequence to vector um network instead of sequence to sequence.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1026s",
        "start_time": "1026.739"
    },
    {
        "id": "c04c78d9",
        "text": "Here we consider like uh as an output all the outputs at each time step. And so basically here, the idea is that we uh input a batch of sequences and then we get as an output a bunch of sequences again, right? So this is a little bit of less common uh I would say then as sequences to vector and this is reflected also like in how uh Kous works and indeed the default um behavior for simple RNN uh layers instead of having a sequence to vector um network instead of sequence to sequence. OK.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1046s",
        "start_time": "1046.78"
    },
    {
        "id": "0b4d8548",
        "text": "uh I would say then as sequences to vector and this is reflected also like in how uh Kous works and indeed the default um behavior for simple RNN uh layers instead of having a sequence to vector um network instead of sequence to sequence. OK. So now we've spoken so far about the architecture of a network of a recurrent neural network, but we haven't really dug into a memory cell that much. So for simple R and MS uh the memory cell is a very simple neural network. The idea here is that we use a dense",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1069s",
        "start_time": "1069.54"
    },
    {
        "id": "228e861c",
        "text": "OK. So now we've spoken so far about the architecture of a network of a recurrent neural network, but we haven't really dug into a memory cell that much. So for simple R and MS uh the memory cell is a very simple neural network. The idea here is that we use a dense layer and the input is given by the combination of the state vector and the input data. So the state vector at the previous time time step and the input data obviously at the current time step and the activation function that we use is the hyperbolic tangent or tan H. Now you may be wondering but why are we using tan H haven't we always use like",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1090s",
        "start_time": "1090.069"
    },
    {
        "id": "c24d93bb",
        "text": "So now we've spoken so far about the architecture of a network of a recurrent neural network, but we haven't really dug into a memory cell that much. So for simple R and MS uh the memory cell is a very simple neural network. The idea here is that we use a dense layer and the input is given by the combination of the state vector and the input data. So the state vector at the previous time time step and the input data obviously at the current time step and the activation function that we use is the hyperbolic tangent or tan H. Now you may be wondering but why are we using tan H haven't we always use like like other types of activation function, right? Uh But it turns out that a training and RNN is quite difficult because of a number of reasons, but mainly we have like vanishing gradients. So the gradients tend to disappear and we'll understand why that's the case in a while.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1091s",
        "start_time": "1091.31"
    },
    {
        "id": "c9cbb0ee",
        "text": "layer and the input is given by the combination of the state vector and the input data. So the state vector at the previous time time step and the input data obviously at the current time step and the activation function that we use is the hyperbolic tangent or tan H. Now you may be wondering but why are we using tan H haven't we always use like like other types of activation function, right? Uh But it turns out that a training and RNN is quite difficult because of a number of reasons, but mainly we have like vanishing gradients. So the gradients tend to disappear and we'll understand why that's the case in a while. And at the same time, we could have the issue of exploding gradients. So gradients that are growing bigger and bigger. And the problem is that if we use R, which we know like works really, really well with CNN SR is not bound",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1114s",
        "start_time": "1114.864"
    },
    {
        "id": "5c0afc15",
        "text": "like other types of activation function, right? Uh But it turns out that a training and RNN is quite difficult because of a number of reasons, but mainly we have like vanishing gradients. So the gradients tend to disappear and we'll understand why that's the case in a while. And at the same time, we could have the issue of exploding gradients. So gradients that are growing bigger and bigger. And the problem is that if we use R, which we know like works really, really well with CNN SR is not bound it. And so R can explode whereas 10 H constrains the values between minus one and one. So by using 10 H, we're avoiding the issue of exploding gradients.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1143s",
        "start_time": "1143.43"
    },
    {
        "id": "5c3362a9",
        "text": "And at the same time, we could have the issue of exploding gradients. So gradients that are growing bigger and bigger. And the problem is that if we use R, which we know like works really, really well with CNN SR is not bound it. And so R can explode whereas 10 H constrains the values between minus one and one. So by using 10 H, we're avoiding the issue of exploding gradients. But now let's understand how we train a network. And so to train a recurrent neural network, we use a variant of back propagation which is called back propagation through time. So basically here, the idea is that the error is back propagated through time. So what we usually do here is that we take the RNN, we draw it",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1161s",
        "start_time": "1161.099"
    },
    {
        "id": "7c6d9f1d",
        "text": "it. And so R can explode whereas 10 H constrains the values between minus one and one. So by using 10 H, we're avoiding the issue of exploding gradients. But now let's understand how we train a network. And so to train a recurrent neural network, we use a variant of back propagation which is called back propagation through time. So basically here, the idea is that the error is back propagated through time. So what we usually do here is that we take the RNN, we draw it uh and we consider each time step as a layer in a feed forward network. Now, I guess like you, you start seeing the problem here, which is basically, so say, for example, we have an input where we have a sequence with 100 time steps, right?",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1176s",
        "start_time": "1176.484"
    },
    {
        "id": "1d7ce77e",
        "text": "But now let's understand how we train a network. And so to train a recurrent neural network, we use a variant of back propagation which is called back propagation through time. So basically here, the idea is that the error is back propagated through time. So what we usually do here is that we take the RNN, we draw it uh and we consider each time step as a layer in a feed forward network. Now, I guess like you, you start seeing the problem here, which is basically, so say, for example, we have an input where we have a sequence with 100 time steps, right? And there, that basically means that we, when we draw the R and N, we have 100 layers. Now this is a very deep network. And we know that when we have a very deep network, we have the issue of varnishing gradients. And so basically what we do at that point is we calculate, sorry, we calculate the error,",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1192s",
        "start_time": "1192.109"
    },
    {
        "id": "3637e859",
        "text": "uh and we consider each time step as a layer in a feed forward network. Now, I guess like you, you start seeing the problem here, which is basically, so say, for example, we have an input where we have a sequence with 100 time steps, right? And there, that basically means that we, when we draw the R and N, we have 100 layers. Now this is a very deep network. And we know that when we have a very deep network, we have the issue of varnishing gradients. And so basically what we do at that point is we calculate, sorry, we calculate the error, then we propagate back the error through all these",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1219s",
        "start_time": "1219.02"
    },
    {
        "id": "3602cd88",
        "text": "And there, that basically means that we, when we draw the R and N, we have 100 layers. Now this is a very deep network. And we know that when we have a very deep network, we have the issue of varnishing gradients. And so basically what we do at that point is we calculate, sorry, we calculate the error, then we propagate back the error through all these virtual layers which are like all the different uh time stamps here. And so while we go all the way back to um like the, the the early layers,",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1239s",
        "start_time": "1239.359"
    },
    {
        "id": "3c2f8cbc",
        "text": "then we propagate back the error through all these virtual layers which are like all the different uh time stamps here. And so while we go all the way back to um like the, the the early layers, we have an issue with vanishing gradients, right? So these gradients can disappear, they can explode. OK. So there's a way a nice way to like avoid some of these issues from happening. And basically what we do here",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1264s",
        "start_time": "1264.3"
    },
    {
        "id": "6853637b",
        "text": "virtual layers which are like all the different uh time stamps here. And so while we go all the way back to um like the, the the early layers, we have an issue with vanishing gradients, right? So these gradients can disappear, they can explode. OK. So there's a way a nice way to like avoid some of these issues from happening. And basically what we do here is we use a sequence to sequence approach. So here we, while we train at the network, we output uh predictions at each time step and then we uh compare them against the targets. And obviously we expect for the targets uh to be uh the, the inputs but shifted towards the right by one step.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1272s",
        "start_time": "1272.109"
    },
    {
        "id": "c6ac22eb",
        "text": "we have an issue with vanishing gradients, right? So these gradients can disappear, they can explode. OK. So there's a way a nice way to like avoid some of these issues from happening. And basically what we do here is we use a sequence to sequence approach. So here we, while we train at the network, we output uh predictions at each time step and then we uh compare them against the targets. And obviously we expect for the targets uh to be uh the, the inputs but shifted towards the right by one step. And so with that in mind, so we can say, for example, that uh the target here like at times zero is X one and at time one, when we input X one, we expect X two, right. So at each time step, we calculate the error and we back propagate uh the error. And by doing so we kind of like stabilize the",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1286s",
        "start_time": "1286.25"
    },
    {
        "id": "584ca351",
        "text": "is we use a sequence to sequence approach. So here we, while we train at the network, we output uh predictions at each time step and then we uh compare them against the targets. And obviously we expect for the targets uh to be uh the, the inputs but shifted towards the right by one step. And so with that in mind, so we can say, for example, that uh the target here like at times zero is X one and at time one, when we input X one, we expect X two, right. So at each time step, we calculate the error and we back propagate uh the error. And by doing so we kind of like stabilize the uh the training process because now we are calculating uh like gradients like also like locally. And so if we have like a, a big error at a specific time step, we can just like propagate the error like there and tweak like the weights like on like the layer which has like the, the, the most problems there. And by doing so, basically, we speed up the, the training process and we make it more stable.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1302s",
        "start_time": "1302.489"
    },
    {
        "id": "92a7ffbc",
        "text": "And so with that in mind, so we can say, for example, that uh the target here like at times zero is X one and at time one, when we input X one, we expect X two, right. So at each time step, we calculate the error and we back propagate uh the error. And by doing so we kind of like stabilize the uh the training process because now we are calculating uh like gradients like also like locally. And so if we have like a, a big error at a specific time step, we can just like propagate the error like there and tweak like the weights like on like the layer which has like the, the, the most problems there. And by doing so, basically, we speed up the, the training process and we make it more stable. But now this is only like a trick because in the end, once we've trained the network, what we can do is just magically drop",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1330s",
        "start_time": "1330.209"
    },
    {
        "id": "81bd3293",
        "text": "uh the training process because now we are calculating uh like gradients like also like locally. And so if we have like a, a big error at a specific time step, we can just like propagate the error like there and tweak like the weights like on like the layer which has like the, the, the most problems there. And by doing so, basically, we speed up the, the training process and we make it more stable. But now this is only like a trick because in the end, once we've trained the network, what we can do is just magically drop all of the um the different points here. So you drop all of the sequence to sequence uh predictions and just focus on the last prediction, which is the one that we usually want when we use a sequence to vector RNM",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1355s",
        "start_time": "1355.64"
    },
    {
        "id": "bc9f9179",
        "text": "But now this is only like a trick because in the end, once we've trained the network, what we can do is just magically drop all of the um the different points here. So you drop all of the sequence to sequence uh predictions and just focus on the last prediction, which is the one that we usually want when we use a sequence to vector RNM cool. So this is back propagation through time. Now, let's take a look at a little bit like at the math behind um an R and N and how it works. So for understanding that we should just like focus on one time step.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1382s",
        "start_time": "1382.969"
    },
    {
        "id": "b443e7db",
        "text": "all of the um the different points here. So you drop all of the sequence to sequence uh predictions and just focus on the last prediction, which is the one that we usually want when we use a sequence to vector RNM cool. So this is back propagation through time. Now, let's take a look at a little bit like at the math behind um an R and N and how it works. So for understanding that we should just like focus on one time step. So here we'll focus on XT, right? And so here we have the input at time T we have uh H uh with T minus one, which is basically the, the state vector at T minus one. Then we have HT which is the state vector at time T and YT which is the output. Now, here I also have in red UW and V and these are",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1392s",
        "start_time": "1392.219"
    },
    {
        "id": "396933eb",
        "text": "cool. So this is back propagation through time. Now, let's take a look at a little bit like at the math behind um an R and N and how it works. So for understanding that we should just like focus on one time step. So here we'll focus on XT, right? And so here we have the input at time T we have uh H uh with T minus one, which is basically the, the state vector at T minus one. Then we have HT which is the state vector at time T and YT which is the output. Now, here I also have in red UW and V and these are um weight matrices and we're gonna use them for calculating the state vector and the output. So U is connected with the inputs. W is connected with the um with the state vector and uh V is kind of connected with the dense layer and it enables us to get to the output. OK. So let's calculate first of all HT which is the state vector at the current time stamp right at T.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1410s",
        "start_time": "1410.979"
    },
    {
        "id": "57974d79",
        "text": "So here we'll focus on XT, right? And so here we have the input at time T we have uh H uh with T minus one, which is basically the, the state vector at T minus one. Then we have HT which is the state vector at time T and YT which is the output. Now, here I also have in red UW and V and these are um weight matrices and we're gonna use them for calculating the state vector and the output. So U is connected with the inputs. W is connected with the um with the state vector and uh V is kind of connected with the dense layer and it enables us to get to the output. OK. So let's calculate first of all HT which is the state vector at the current time stamp right at T. OK. So for doing this, we uh uh have like a function, the activation function which uh in a simple R and N is just like tan H. And uh so we, we have uh this function that depends on a couple of things, it depends on XT uh which is the current input and it depends on the previous state, right. So this is very intuitive because the current state depends on the input data for that state",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1429s",
        "start_time": "1429.199"
    },
    {
        "id": "02600cc5",
        "text": "um weight matrices and we're gonna use them for calculating the state vector and the output. So U is connected with the inputs. W is connected with the um with the state vector and uh V is kind of connected with the dense layer and it enables us to get to the output. OK. So let's calculate first of all HT which is the state vector at the current time stamp right at T. OK. So for doing this, we uh uh have like a function, the activation function which uh in a simple R and N is just like tan H. And uh so we, we have uh this function that depends on a couple of things, it depends on XT uh which is the current input and it depends on the previous state, right. So this is very intuitive because the current state depends on the input data for that state uh for that time step and the uh state vector for the previous time step. So that is like the, the context, right. So now that we have a HT, we can get the output and the output is just a soft max activation function applied to HT,",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1455s",
        "start_time": "1455.709"
    },
    {
        "id": "c978dbec",
        "text": "OK. So for doing this, we uh uh have like a function, the activation function which uh in a simple R and N is just like tan H. And uh so we, we have uh this function that depends on a couple of things, it depends on XT uh which is the current input and it depends on the previous state, right. So this is very intuitive because the current state depends on the input data for that state uh for that time step and the uh state vector for the previous time step. So that is like the, the context, right. So now that we have a HT, we can get the output and the output is just a soft max activation function applied to HT, right. And so, and here we have obviously like a multiplication between uh V the weight matrix and HT uh and that gets into basically the, the, the the dense layer, right?",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1485s",
        "start_time": "1485.589"
    },
    {
        "id": "b0201fd1",
        "text": "uh for that time step and the uh state vector for the previous time step. So that is like the, the context, right. So now that we have a HT, we can get the output and the output is just a soft max activation function applied to HT, right. And so, and here we have obviously like a multiplication between uh V the weight matrix and HT uh and that gets into basically the, the, the the dense layer, right? So now the question is, but what do we learn when we are training a recurrent neural network? Well, we actually learn these two functions and more specifically what we learn are this weight mattresses. So UW and V",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1515s",
        "start_time": "1515.449"
    },
    {
        "id": "d09ae70c",
        "text": "right. And so, and here we have obviously like a multiplication between uh V the weight matrix and HT uh and that gets into basically the, the, the the dense layer, right? So now the question is, but what do we learn when we are training a recurrent neural network? Well, we actually learn these two functions and more specifically what we learn are this weight mattresses. So UW and V nice. OK.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1538s",
        "start_time": "1538.099"
    },
    {
        "id": "9ae6389a",
        "text": "So now the question is, but what do we learn when we are training a recurrent neural network? Well, we actually learn these two functions and more specifically what we learn are this weight mattresses. So UW and V nice. OK. So recurrent neural networks are really, really good",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1553s",
        "start_time": "1553.369"
    },
    {
        "id": "74af9740",
        "text": "nice. OK. So recurrent neural networks are really, really good up to a certain point though. And the problem is that they really don't have that much of a long term memory, which basically means that they uh can't be used like to, to see that much like into the past which uh so",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1572s",
        "start_time": "1572.63"
    },
    {
        "id": "4fed1236",
        "text": "So recurrent neural networks are really, really good up to a certain point though. And the problem is that they really don't have that much of a long term memory, which basically means that they uh can't be used like to, to see that much like into the past which uh so the the problem with this is that they can't learn patterns with long dependencies. And we know that a lot of like this time series type of data, audio data, music data has like long term dependencies. So think for example, of a song. So there you have certain musical structures like a chorus and the verse and the verse that's um like, I don't know, like a verse that's used, like at",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1575s",
        "start_time": "1575.63"
    },
    {
        "id": "d4775f3d",
        "text": "up to a certain point though. And the problem is that they really don't have that much of a long term memory, which basically means that they uh can't be used like to, to see that much like into the past which uh so the the problem with this is that they can't learn patterns with long dependencies. And we know that a lot of like this time series type of data, audio data, music data has like long term dependencies. So think for example, of a song. So there you have certain musical structures like a chorus and the verse and the verse that's um like, I don't know, like a verse that's used, like at time, I don't know, like after two minutes in a song,",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1581s",
        "start_time": "1581.42"
    },
    {
        "id": "2c9049a6",
        "text": "the the problem with this is that they can't learn patterns with long dependencies. And we know that a lot of like this time series type of data, audio data, music data has like long term dependencies. So think for example, of a song. So there you have certain musical structures like a chorus and the verse and the verse that's um like, I don't know, like a verse that's used, like at time, I don't know, like after two minutes in a song, well, it kind of depends to the, the very, some verse that was used like at the beginning of the song. But with an R and N, we can't really remember those long term uh patterns, right? And so, and this is",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1599s",
        "start_time": "1599.119"
    },
    {
        "id": "f6ff9ce0",
        "text": "time, I don't know, like after two minutes in a song, well, it kind of depends to the, the very, some verse that was used like at the beginning of the song. But with an R and N, we can't really remember those long term uh patterns, right? And so, and this is a bit of a problem because that basically means that the R and N isn't capable of understanding like these patterns and under and pre making prediction that keep track of the context.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1626s",
        "start_time": "1626.8"
    },
    {
        "id": "9d5497bc",
        "text": "well, it kind of depends to the, the very, some verse that was used like at the beginning of the song. But with an R and N, we can't really remember those long term uh patterns, right? And so, and this is a bit of a problem because that basically means that the R and N isn't capable of understanding like these patterns and under and pre making prediction that keep track of the context. So in order to avoid that and tackle this issue, a number of different",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1630s",
        "start_time": "1630.78"
    },
    {
        "id": "7475defa",
        "text": "a bit of a problem because that basically means that the R and N isn't capable of understanding like these patterns and under and pre making prediction that keep track of the context. So in order to avoid that and tackle this issue, a number of different types of R and NS have been devised and a specific one that's very success, successful. It's called long short term memory network or LSLSTM. And this is going to be the focus of my next video. So I hope you enjoyed this video if that's the case, leave alike.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1647s",
        "start_time": "1647.9"
    },
    {
        "id": "a25348b7",
        "text": "So in order to avoid that and tackle this issue, a number of different types of R and NS have been devised and a specific one that's very success, successful. It's called long short term memory network or LSLSTM. And this is going to be the focus of my next video. So I hope you enjoyed this video if that's the case, leave alike. And if you have any questions, cos I know like this was a little bit like of a tough topic. Like to understand, please ask those questions in the comments section below. And",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1661s",
        "start_time": "1661.68"
    },
    {
        "id": "212478e0",
        "text": "types of R and NS have been devised and a specific one that's very success, successful. It's called long short term memory network or LSLSTM. And this is going to be the focus of my next video. So I hope you enjoyed this video if that's the case, leave alike. And if you have any questions, cos I know like this was a little bit like of a tough topic. Like to understand, please ask those questions in the comments section below. And uh if you, if you want to like have more videos like this, remember to subscribe and activate the notification bell and I'll see you next time. Cheers.",
        "video": "17- Recurrent Neural Networks Explained Easily",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "DY82Goknf0s",
        "youtube_link": "https://www.youtube.com/watch?v=DY82Goknf0s&t=1668s",
        "start_time": "1668.469"
    },
    {
        "id": "5250eeb0",
        "text": "Hi, everybody and welcome to a new video in the Deep Learning for audio with Python series. This time we're gonna try to tackle overfitting. So specifically, we're gonna look into techniques that we can use to identify overfitting and then to solve it. Cool. So if you guys remember last time we built a multi layer of perception that's able to do music genre classification, but we had a issue and the issue was overfitting, which basically means that the uh model was doing very well on the training set, but it was having issues with data, it had never seen before, right? So first of all, what we want to do is find a way of identifying all the fitting. And for doing that, we can use a couple of plots that are very informative. So it's basically taking a look at the accuracy and the error of both the train set and uh the test set over time over all the epochs and training cycles. So for doing that, obviously, we need to retain information about the training process. And uh fortunately for us, tensorflow has a super",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=0s",
        "start_time": "0.36"
    },
    {
        "id": "8b88993f",
        "text": "issue and the issue was overfitting, which basically means that the uh model was doing very well on the training set, but it was having issues with data, it had never seen before, right? So first of all, what we want to do is find a way of identifying all the fitting. And for doing that, we can use a couple of plots that are very informative. So it's basically taking a look at the accuracy and the error of both the train set and uh the test set over time over all the epochs and training cycles. So for doing that, obviously, we need to retain information about the training process. And uh fortunately for us, tensorflow has a super do like way of doing that. And so if we look at here uh When we train the model, we just like do a model dot Fit, you should know this by now guys. And if you don't just get back and watch my previous videos on this,",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=25s",
        "start_time": "25.163"
    },
    {
        "id": "e2021508",
        "text": "taking a look at the accuracy and the error of both the train set and uh the test set over time over all the epochs and training cycles. So for doing that, obviously, we need to retain information about the training process. And uh fortunately for us, tensorflow has a super do like way of doing that. And so if we look at here uh When we train the model, we just like do a model dot Fit, you should know this by now guys. And if you don't just get back and watch my previous videos on this, but the the return of this Fit method is a history object. Basically that uh retains information about the accuracy and the error of both the uh train set and the test set over time.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=49s",
        "start_time": "49.965"
    },
    {
        "id": "92a9d8f1",
        "text": "do like way of doing that. And so if we look at here uh When we train the model, we just like do a model dot Fit, you should know this by now guys. And if you don't just get back and watch my previous videos on this, but the the return of this Fit method is a history object. Basically that uh retains information about the accuracy and the error of both the uh train set and the test set over time. Cool. So we can just store that information during a history equal to this thing. Now, the next step that we want to do is basically to plot the accuracy and error over the AEX.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=74s",
        "start_time": "74.959"
    },
    {
        "id": "f8771390",
        "text": "but the the return of this Fit method is a history object. Basically that uh retains information about the accuracy and the error of both the uh train set and the test set over time. Cool. So we can just store that information during a history equal to this thing. Now, the next step that we want to do is basically to plot the accuracy and error over the AEX. Now, we don't have this function yet, but we'll fake it for the time being. And so we would say plots history and obviously we'll pass in this history argument. Cool. So now we should build this plot history function. So we'll define it over here.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=90s",
        "start_time": "90.97"
    },
    {
        "id": "3082658d",
        "text": "Cool. So we can just store that information during a history equal to this thing. Now, the next step that we want to do is basically to plot the accuracy and error over the AEX. Now, we don't have this function yet, but we'll fake it for the time being. And so we would say plots history and obviously we'll pass in this history argument. Cool. So now we should build this plot history function. So we'll define it over here. So we'll do a define plot history. And as we know, plot history accepts an argument, we'll call it history. And uh so here uh we need to build like this plot. Well, we want to build like a plot with a couple of subplots, right? So one is for the error and the other one is for the accuracy. But for building plots, as we know, we need to use Maloy I uh which is like this super interesting and super cool",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=107s",
        "start_time": "107.269"
    },
    {
        "id": "ed953289",
        "text": "Now, we don't have this function yet, but we'll fake it for the time being. And so we would say plots history and obviously we'll pass in this history argument. Cool. So now we should build this plot history function. So we'll define it over here. So we'll do a define plot history. And as we know, plot history accepts an argument, we'll call it history. And uh so here uh we need to build like this plot. Well, we want to build like a plot with a couple of subplots, right? So one is for the error and the other one is for the accuracy. But for building plots, as we know, we need to use Maloy I uh which is like this super interesting and super cool um a library for a plotting and we need to import that right. So we'll do an import mat uh plot lab dot plots and we'll import that as PLT,",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=124s",
        "start_time": "124.76"
    },
    {
        "id": "852a4b56",
        "text": "So we'll do a define plot history. And as we know, plot history accepts an argument, we'll call it history. And uh so here uh we need to build like this plot. Well, we want to build like a plot with a couple of subplots, right? So one is for the error and the other one is for the accuracy. But for building plots, as we know, we need to use Maloy I uh which is like this super interesting and super cool um a library for a plotting and we need to import that right. So we'll do an import mat uh plot lab dot plots and we'll import that as PLT, right? So here, what we wanna do is we'll get a figure and an access and we'll do a plot dot subplots and we'll pass in two. So basically what this does, it returns like a figure object and, and there's axis over here and we'll say that we want two subplots, right?",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=145s",
        "start_time": "145.149"
    },
    {
        "id": "40058024",
        "text": "um a library for a plotting and we need to import that right. So we'll do an import mat uh plot lab dot plots and we'll import that as PLT, right? So here, what we wanna do is we'll get a figure and an access and we'll do a plot dot subplots and we'll pass in two. So basically what this does, it returns like a figure object and, and there's axis over here and we'll say that we want two subplots, right? OK. So as the first step let's build, so let's create the accuracy. So plots,",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=174s",
        "start_time": "174.339"
    },
    {
        "id": "2626eddf",
        "text": "right? So here, what we wanna do is we'll get a figure and an access and we'll do a plot dot subplots and we'll pass in two. So basically what this does, it returns like a figure object and, and there's axis over here and we'll say that we want two subplots, right? OK. So as the first step let's build, so let's create the accuracy. So plots, OK. So",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=189s",
        "start_time": "189.32"
    },
    {
        "id": "d0544d8f",
        "text": "OK. So as the first step let's build, so let's create the accuracy. So plots, OK. So let's say so the the accuracy plot is gonna be axis in zero. And here we need to plot the stuff that we want to plot, right? And so here we want to plot uh first of all the um accuracy of the train set over time, right? And then the accuracy of the test set. So here we know that the accuracy of the train set",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=216s",
        "start_time": "216.289"
    },
    {
        "id": "344f4731",
        "text": "OK. So let's say so the the accuracy plot is gonna be axis in zero. And here we need to plot the stuff that we want to plot, right? And so here we want to plot uh first of all the um accuracy of the train set over time, right? And then the accuracy of the test set. So here we know that the accuracy of the train set is stored in a dictionary called not surprisingly history. And the the key is",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=225s",
        "start_time": "225.369"
    },
    {
        "id": "051c6b32",
        "text": "let's say so the the accuracy plot is gonna be axis in zero. And here we need to plot the stuff that we want to plot, right? And so here we want to plot uh first of all the um accuracy of the train set over time, right? And then the accuracy of the test set. So here we know that the accuracy of the train set is stored in a dictionary called not surprisingly history. And the the key is accuracy, right? So it's not really that surprising, but it's quite straightforward, right? And we want to associate a label uh to this and we'll just call it train accuracy, right? And now we are just going to duplicate that line. And instead of like the accuracy, we want the train accuracy, which is stored as va accuracy, right?",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=228s",
        "start_time": "228.46"
    },
    {
        "id": "ce525e8a",
        "text": "is stored in a dictionary called not surprisingly history. And the the key is accuracy, right? So it's not really that surprising, but it's quite straightforward, right? And we want to associate a label uh to this and we'll just call it train accuracy, right? And now we are just going to duplicate that line. And instead of like the accuracy, we want the train accuracy, which is stored as va accuracy, right? And uh so the label in this case is gonna be called uh test accuracy cool. So now we want to uh set the um the name of the, the Y axis, right? And so that is gonna be a set Y label and this is accuracy good. So now we have the uh y label. Next thing",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=256s",
        "start_time": "256.239"
    },
    {
        "id": "8aca93cd",
        "text": "accuracy, right? So it's not really that surprising, but it's quite straightforward, right? And we want to associate a label uh to this and we'll just call it train accuracy, right? And now we are just going to duplicate that line. And instead of like the accuracy, we want the train accuracy, which is stored as va accuracy, right? And uh so the label in this case is gonna be called uh test accuracy cool. So now we want to uh set the um the name of the, the Y axis, right? And so that is gonna be a set Y label and this is accuracy good. So now we have the uh y label. Next thing uh we want to s uh just like place the legend so that we can read the legend. And so we'll do a uh axis zero dot uh legend. And here we have an argument that's called lock location And we'll say that we want this in the lower right corner. And finally, we want to uh set a",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=264s",
        "start_time": "264.549"
    },
    {
        "id": "3da8cd24",
        "text": "And uh so the label in this case is gonna be called uh test accuracy cool. So now we want to uh set the um the name of the, the Y axis, right? And so that is gonna be a set Y label and this is accuracy good. So now we have the uh y label. Next thing uh we want to s uh just like place the legend so that we can read the legend. And so we'll do a uh axis zero dot uh legend. And here we have an argument that's called lock location And we'll say that we want this in the lower right corner. And finally, we want to uh set a uh title. So we'll do a set title which is gonna give a title to our subplots and the title not surprisingly is gonna be accuracy evil, right? And so this way we have all of our subplot for uh the accuracy. Now we can just like take this copy and, and just uh change a few things around to create the error subplot,",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=289s",
        "start_time": "289.42"
    },
    {
        "id": "98fdc890",
        "text": "uh we want to s uh just like place the legend so that we can read the legend. And so we'll do a uh axis zero dot uh legend. And here we have an argument that's called lock location And we'll say that we want this in the lower right corner. And finally, we want to uh set a uh title. So we'll do a set title which is gonna give a title to our subplots and the title not surprisingly is gonna be accuracy evil, right? And so this way we have all of our subplot for uh the accuracy. Now we can just like take this copy and, and just uh change a few things around to create the error subplot, right? So here this is not zero anymore. This is gonna be one. And then here we don't want to retrieve the accuracy. We want to retrieve the error which is indicated here, it's stored as loss. Now, if you've been watching my videos, you know, guys that I prefer error to loss. But unfortunately, the guys at tensor flow think it differently. Cool. So the label here is gonna be train error.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=316s",
        "start_time": "316.48"
    },
    {
        "id": "979ff2dd",
        "text": "uh title. So we'll do a set title which is gonna give a title to our subplots and the title not surprisingly is gonna be accuracy evil, right? And so this way we have all of our subplot for uh the accuracy. Now we can just like take this copy and, and just uh change a few things around to create the error subplot, right? So here this is not zero anymore. This is gonna be one. And then here we don't want to retrieve the accuracy. We want to retrieve the error which is indicated here, it's stored as loss. Now, if you've been watching my videos, you know, guys that I prefer error to loss. But unfortunately, the guys at tensor flow think it differently. Cool. So the label here is gonna be train error. Then we want to retrieve the uh error for uh the test set. So we'll do a valley a vowel underscore uh loss. And then the label is gonna be test uh error over here. And then we'll set the way uh a label and this time to error. And uh we want to locate the legend in the upper right corner and you'll see why that's the case in a second.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=342s",
        "start_time": "342.14"
    },
    {
        "id": "4944139f",
        "text": "right? So here this is not zero anymore. This is gonna be one. And then here we don't want to retrieve the accuracy. We want to retrieve the error which is indicated here, it's stored as loss. Now, if you've been watching my videos, you know, guys that I prefer error to loss. But unfortunately, the guys at tensor flow think it differently. Cool. So the label here is gonna be train error. Then we want to retrieve the uh error for uh the test set. So we'll do a valley a vowel underscore uh loss. And then the label is gonna be test uh error over here. And then we'll set the way uh a label and this time to error. And uh we want to locate the legend in the upper right corner and you'll see why that's the case in a second. And the title is gonna be uh Error, Evil.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=371s",
        "start_time": "371.649"
    },
    {
        "id": "fc219d78",
        "text": "Then we want to retrieve the uh error for uh the test set. So we'll do a valley a vowel underscore uh loss. And then the label is gonna be test uh error over here. And then we'll set the way uh a label and this time to error. And uh we want to locate the legend in the upper right corner and you'll see why that's the case in a second. And the title is gonna be uh Error, Evil. And then uh yeah, let's just put a, an X label here. We'll set the X label and obviously the X label is",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=400s",
        "start_time": "400.35"
    },
    {
        "id": "26af01df",
        "text": "And the title is gonna be uh Error, Evil. And then uh yeah, let's just put a, an X label here. We'll set the X label and obviously the X label is epoch cool. So now we have the two subplots, but we still need to, to show that. So we'll do a plot dot show.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=429s",
        "start_time": "429.41"
    },
    {
        "id": "66738350",
        "text": "And then uh yeah, let's just put a, an X label here. We'll set the X label and obviously the X label is epoch cool. So now we have the two subplots, but we still need to, to show that. So we'll do a plot dot show. That's great. OK. So now we should have everything in place. So I'm gonna run this script that's gonna use the um the music genre classifier that we built uh last time. And so we'll see how to identify all the fitting, looking at these two very important uh plots. Cool. So now I'm running, I'm gonna run the script. It's gonna take some time. So I'll just post the video and take it back",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=434s",
        "start_time": "434.38"
    },
    {
        "id": "943c6d56",
        "text": "epoch cool. So now we have the two subplots, but we still need to, to show that. So we'll do a plot dot show. That's great. OK. So now we should have everything in place. So I'm gonna run this script that's gonna use the um the music genre classifier that we built uh last time. And so we'll see how to identify all the fitting, looking at these two very important uh plots. Cool. So now I'm running, I'm gonna run the script. It's gonna take some time. So I'll just post the video and take it back and here we are back with the results of the uh training process. So as you can see here, guys, we have this nice plot of the accuracy over time and the uh overall like accuracy for the train uh set is quite high. And as you can see over time, it basically went all the way up almost like to 100%. And I remember guys like this down here are all like epochs and we have like 100 epochs, right?",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=443s",
        "start_time": "443.1"
    },
    {
        "id": "db8a782f",
        "text": "That's great. OK. So now we should have everything in place. So I'm gonna run this script that's gonna use the um the music genre classifier that we built uh last time. And so we'll see how to identify all the fitting, looking at these two very important uh plots. Cool. So now I'm running, I'm gonna run the script. It's gonna take some time. So I'll just post the video and take it back and here we are back with the results of the uh training process. So as you can see here, guys, we have this nice plot of the accuracy over time and the uh overall like accuracy for the train uh set is quite high. And as you can see over time, it basically went all the way up almost like to 100%. And I remember guys like this down here are all like epochs and we have like 100 epochs, right? Uh But for the test accuracy, we just like go up, up and then we just like scale at around like 60%. So as you can see here, there's a huge huge difference over time in the test accuracy compared uh against the the train accuracy. And so that is in itself a huge uh indication of overfitting.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=454s",
        "start_time": "454.41"
    },
    {
        "id": "812104e1",
        "text": "and here we are back with the results of the uh training process. So as you can see here, guys, we have this nice plot of the accuracy over time and the uh overall like accuracy for the train uh set is quite high. And as you can see over time, it basically went all the way up almost like to 100%. And I remember guys like this down here are all like epochs and we have like 100 epochs, right? Uh But for the test accuracy, we just like go up, up and then we just like scale at around like 60%. So as you can see here, there's a huge huge difference over time in the test accuracy compared uh against the the train accuracy. And so that is in itself a huge uh indication of overfitting. Now, let's take a look at the error evaluation subplot as well. And here we have like a similar thing, right? So we have the train error that obviously like goes down down down over time, it becomes like very, very like little, whereas like the",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=482s",
        "start_time": "482.209"
    },
    {
        "id": "501c2f36",
        "text": "Uh But for the test accuracy, we just like go up, up and then we just like scale at around like 60%. So as you can see here, there's a huge huge difference over time in the test accuracy compared uh against the the train accuracy. And so that is in itself a huge uh indication of overfitting. Now, let's take a look at the error evaluation subplot as well. And here we have like a similar thing, right? So we have the train error that obviously like goes down down down over time, it becomes like very, very like little, whereas like the error goes down for quite a while and then it just like remains unchanged and then it actually starts to increase again, right? So this again is another indication that our model is hugely overfitting. So",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=510s",
        "start_time": "510.809"
    },
    {
        "id": "9db1c5e5",
        "text": "Now, let's take a look at the error evaluation subplot as well. And here we have like a similar thing, right? So we have the train error that obviously like goes down down down over time, it becomes like very, very like little, whereas like the error goes down for quite a while and then it just like remains unchanged and then it actually starts to increase again, right? So this again is another indication that our model is hugely overfitting. So now the question is how do we solve this issue? Because obviously we want for our model to be able to generalize to data it has never seen before. Well, it turns out that there are a bunch of different techniques that we can use. So let's take a look at a few of those.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=533s",
        "start_time": "533.409"
    },
    {
        "id": "9bee6317",
        "text": "error goes down for quite a while and then it just like remains unchanged and then it actually starts to increase again, right? So this again is another indication that our model is hugely overfitting. So now the question is how do we solve this issue? Because obviously we want for our model to be able to generalize to data it has never seen before. Well, it turns out that there are a bunch of different techniques that we can use. So let's take a look at a few of those. So here we have five that I listed. So we have simpler architecture data augmentation, early stop in dropout and regularization.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=550s",
        "start_time": "550.0"
    },
    {
        "id": "6cc7e343",
        "text": "now the question is how do we solve this issue? Because obviously we want for our model to be able to generalize to data it has never seen before. Well, it turns out that there are a bunch of different techniques that we can use. So let's take a look at a few of those. So here we have five that I listed. So we have simpler architecture data augmentation, early stop in dropout and regularization. Now, uh I'm gonna implement in terms of flow only drop out and a regularization. Uh But I'm gonna talk about like all of them, right? So let's start from the, the first one which is also like the, the simplest, probably also like to understand",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=567s",
        "start_time": "567.559"
    },
    {
        "id": "750a5688",
        "text": "So here we have five that I listed. So we have simpler architecture data augmentation, early stop in dropout and regularization. Now, uh I'm gonna implement in terms of flow only drop out and a regularization. Uh But I'm gonna talk about like all of them, right? So let's start from the, the first one which is also like the, the simplest, probably also like to understand uh here. Uh The whole point is that uh if we have a model that's like overfitting quite a lot, perhaps what we want to do is to try having a simpler architecture. So, and how do we achieve that? Well, uh we can achieve that by doing a couple of things first, we can remove uh layers. So if we have, for example, like four or five hidden layers, we can go down to three or two",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=585s",
        "start_time": "585.59"
    },
    {
        "id": "146203e9",
        "text": "Now, uh I'm gonna implement in terms of flow only drop out and a regularization. Uh But I'm gonna talk about like all of them, right? So let's start from the, the first one which is also like the, the simplest, probably also like to understand uh here. Uh The whole point is that uh if we have a model that's like overfitting quite a lot, perhaps what we want to do is to try having a simpler architecture. So, and how do we achieve that? Well, uh we can achieve that by doing a couple of things first, we can remove uh layers. So if we have, for example, like four or five hidden layers, we can go down to three or two and then we can decrease the number of neurons that we have in each layer. And the reason behind this is that uh like the more complex the architecture and the more the architecture, uh the more the more the model is gonna be able like to actually interpret like all the patterns and getting like and learning everything also like beyond what's like Generali and all like the the I would say like also like the artifacts and",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=596s",
        "start_time": "596.359"
    },
    {
        "id": "c0c540a1",
        "text": "uh here. Uh The whole point is that uh if we have a model that's like overfitting quite a lot, perhaps what we want to do is to try having a simpler architecture. So, and how do we achieve that? Well, uh we can achieve that by doing a couple of things first, we can remove uh layers. So if we have, for example, like four or five hidden layers, we can go down to three or two and then we can decrease the number of neurons that we have in each layer. And the reason behind this is that uh like the more complex the architecture and the more the architecture, uh the more the more the model is gonna be able like to actually interpret like all the patterns and getting like and learning everything also like beyond what's like Generali and all like the the I would say like also like the artifacts and uh nuances of the trains set like itself. So by going with a simpler architecture, we kind of like remove all of that. And that basically means that the the model is gonna be uh probably like more likely to perform better like on more general data.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=613s",
        "start_time": "613.239"
    },
    {
        "id": "840a633a",
        "text": "and then we can decrease the number of neurons that we have in each layer. And the reason behind this is that uh like the more complex the architecture and the more the architecture, uh the more the more the model is gonna be able like to actually interpret like all the patterns and getting like and learning everything also like beyond what's like Generali and all like the the I would say like also like the artifacts and uh nuances of the trains set like itself. So by going with a simpler architecture, we kind of like remove all of that. And that basically means that the the model is gonna be uh probably like more likely to perform better like on more general data. So this is like the the the first uh option that we have to uh fight against overfitting now. Uh You might, you may be asking but uh how do I do that? So what, what, what, what is, what, what is a simple architecture? Well, there's really no",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=639s",
        "start_time": "639.179"
    },
    {
        "id": "9a55f348",
        "text": "uh nuances of the trains set like itself. So by going with a simpler architecture, we kind of like remove all of that. And that basically means that the the model is gonna be uh probably like more likely to perform better like on more general data. So this is like the the the first uh option that we have to uh fight against overfitting now. Uh You might, you may be asking but uh how do I do that? So what, what, what, what is, what, what is a simple architecture? Well, there's really no universal rule here. So you have just like to play around with stuff and, and see like what works for you. What I usually do is I start with relatively simple networks with few like layers and few neurons and uh then I just like add them up in order to have like a a model that's like more uh more powerful like and it's able like to express like and to learn the complexity of the data.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=668s",
        "start_time": "668.059"
    },
    {
        "id": "16725878",
        "text": "So this is like the the the first uh option that we have to uh fight against overfitting now. Uh You might, you may be asking but uh how do I do that? So what, what, what, what is, what, what is a simple architecture? Well, there's really no universal rule here. So you have just like to play around with stuff and, and see like what works for you. What I usually do is I start with relatively simple networks with few like layers and few neurons and uh then I just like add them up in order to have like a a model that's like more uh more powerful like and it's able like to express like and to learn the complexity of the data. Cool. So this is about using a simple architecture. Now we have another option which is called data augmentation. In our case, we're gonna do audio data augmentation. And here the whole idea is basically, it's quite simple, right? And it's basically like the more data you have and the better your uh model is gonna perform uh both on uh your screens set, but hopefully also like on your team,",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=685s",
        "start_time": "685.57"
    },
    {
        "id": "e2c3ad61",
        "text": "universal rule here. So you have just like to play around with stuff and, and see like what works for you. What I usually do is I start with relatively simple networks with few like layers and few neurons and uh then I just like add them up in order to have like a a model that's like more uh more powerful like and it's able like to express like and to learn the complexity of the data. Cool. So this is about using a simple architecture. Now we have another option which is called data augmentation. In our case, we're gonna do audio data augmentation. And here the whole idea is basically, it's quite simple, right? And it's basically like the more data you have and the better your uh model is gonna perform uh both on uh your screens set, but hopefully also like on your team, which is basically all the data that the model has never seen, right? But sometimes it's very difficult to get like uh extra data. So what we want to do is we want to artificially uh build new uh training uh samples, right?",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=702s",
        "start_time": "702.924"
    },
    {
        "id": "7072b318",
        "text": "Cool. So this is about using a simple architecture. Now we have another option which is called data augmentation. In our case, we're gonna do audio data augmentation. And here the whole idea is basically, it's quite simple, right? And it's basically like the more data you have and the better your uh model is gonna perform uh both on uh your screens set, but hopefully also like on your team, which is basically all the data that the model has never seen, right? But sometimes it's very difficult to get like uh extra data. So what we want to do is we want to artificially uh build new uh training uh samples, right? And for doing that, what we can do is we can apply transformations to uh our trains set to all of our training samples in the case of audio, what this basically means is to for example, uh apply certain transformations like pitch shifting. So where we just like move the pitch either up or down, we can also like time stretch. So we basically change",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=729s",
        "start_time": "729.799"
    },
    {
        "id": "a2f2a68a",
        "text": "which is basically all the data that the model has never seen, right? But sometimes it's very difficult to get like uh extra data. So what we want to do is we want to artificially uh build new uh training uh samples, right? And for doing that, what we can do is we can apply transformations to uh our trains set to all of our training samples in the case of audio, what this basically means is to for example, uh apply certain transformations like pitch shifting. So where we just like move the pitch either up or down, we can also like time stretch. So we basically change the speed like of the audio file or we can add background noise. And so in that way, we are able like to, to, to just like recreate artificial versions like of the original like training samples. And so that we're gonna have like more data, which hopefully is gonna kind of like prevent us from overfitting. Now,",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=758s",
        "start_time": "758.094"
    },
    {
        "id": "630eccbb",
        "text": "And for doing that, what we can do is we can apply transformations to uh our trains set to all of our training samples in the case of audio, what this basically means is to for example, uh apply certain transformations like pitch shifting. So where we just like move the pitch either up or down, we can also like time stretch. So we basically change the speed like of the audio file or we can add background noise. And so in that way, we are able like to, to, to just like recreate artificial versions like of the original like training samples. And so that we're gonna have like more data, which hopefully is gonna kind of like prevent us from overfitting. Now, there's a whole art and science about a audio data augmentation and we're not gonna get into the details, but the whole point is you apply a bunch of light transformations and obviously you can combine also those. So for example, you can combine pitch shifting with time stretching or you can add noise and then time stretch an audio file, right?",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=775s",
        "start_time": "775.929"
    },
    {
        "id": "c2585db4",
        "text": "the speed like of the audio file or we can add background noise. And so in that way, we are able like to, to, to just like recreate artificial versions like of the original like training samples. And so that we're gonna have like more data, which hopefully is gonna kind of like prevent us from overfitting. Now, there's a whole art and science about a audio data augmentation and we're not gonna get into the details, but the whole point is you apply a bunch of light transformations and obviously you can combine also those. So for example, you can combine pitch shifting with time stretching or you can add noise and then time stretch an audio file, right? But the whole point is to create a lot of like new data uh that is somehow related to the to the original one, but then it has uh like, I mean way more training samples. Now, the important thing to remember here is that when we do audio data augmentation, uh we want to use like the uh we want to do it like directly on the train set.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=801s",
        "start_time": "801.099"
    },
    {
        "id": "1874a053",
        "text": "there's a whole art and science about a audio data augmentation and we're not gonna get into the details, but the whole point is you apply a bunch of light transformations and obviously you can combine also those. So for example, you can combine pitch shifting with time stretching or you can add noise and then time stretch an audio file, right? But the whole point is to create a lot of like new data uh that is somehow related to the to the original one, but then it has uh like, I mean way more training samples. Now, the important thing to remember here is that when we do audio data augmentation, uh we want to use like the uh we want to do it like directly on the train set. So we don't want to do audio data augmentation, the whole data sets and then use the augmented data also for testing purposes because otherwise there you are cheating a little bit because I mean, at the end of the day, the transform data is somehow",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=826s",
        "start_time": "826.419"
    },
    {
        "id": "99394dd8",
        "text": "But the whole point is to create a lot of like new data uh that is somehow related to the to the original one, but then it has uh like, I mean way more training samples. Now, the important thing to remember here is that when we do audio data augmentation, uh we want to use like the uh we want to do it like directly on the train set. So we don't want to do audio data augmentation, the whole data sets and then use the augmented data also for testing purposes because otherwise there you are cheating a little bit because I mean, at the end of the day, the transform data is somehow related to the uh like original data, right? And so you want to keep the um augmented data only for training purposes. So that when you uh work on the test set, that is like a completely unseen batch of data for the model,",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=844s",
        "start_time": "844.63"
    },
    {
        "id": "aa022231",
        "text": "So we don't want to do audio data augmentation, the whole data sets and then use the augmented data also for testing purposes because otherwise there you are cheating a little bit because I mean, at the end of the day, the transform data is somehow related to the uh like original data, right? And so you want to keep the um augmented data only for training purposes. So that when you uh work on the test set, that is like a completely unseen batch of data for the model, right? So this is like the second technique we have, then we have a third one which is like quite heuristic based and quite simple to understand it's called early stopping. And so here the whole point is that we want to choose certain rules to stop the training. And so for doing that, let let's look at the error uh like plot down here, right? So in blue, we have the train error and then in orange and we have the test error, right.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=868s",
        "start_time": "868.08"
    },
    {
        "id": "eeb2f250",
        "text": "related to the uh like original data, right? And so you want to keep the um augmented data only for training purposes. So that when you uh work on the test set, that is like a completely unseen batch of data for the model, right? So this is like the second technique we have, then we have a third one which is like quite heuristic based and quite simple to understand it's called early stopping. And so here the whole point is that we want to choose certain rules to stop the training. And so for doing that, let let's look at the error uh like plot down here, right? So in blue, we have the train error and then in orange and we have the test error, right. So basically, uh with early stopping, for example, in this case, we could say, hey, if the uh test uh error doesn't improve after, I don't know, let's say uh seven",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=884s",
        "start_time": "884.039"
    },
    {
        "id": "30e2456c",
        "text": "right? So this is like the second technique we have, then we have a third one which is like quite heuristic based and quite simple to understand it's called early stopping. And so here the whole point is that we want to choose certain rules to stop the training. And so for doing that, let let's look at the error uh like plot down here, right? So in blue, we have the train error and then in orange and we have the test error, right. So basically, uh with early stopping, for example, in this case, we could say, hey, if the uh test uh error doesn't improve after, I don't know, let's say uh seven um iterations then just uh stop uh the training, right? And so here you can decide how many epics you want to wait uh before uh you stop. And obviously, you can also like decide how much like is the improvement that you want to like expect",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=901s",
        "start_time": "901.39"
    },
    {
        "id": "36b2e3e1",
        "text": "So basically, uh with early stopping, for example, in this case, we could say, hey, if the uh test uh error doesn't improve after, I don't know, let's say uh seven um iterations then just uh stop uh the training, right? And so here you can decide how many epics you want to wait uh before uh you stop. And obviously, you can also like decide how much like is the improvement that you want to like expect uh right. And so like in this case, as you can see, this is like very, very handy because we are stopping training before we start uh overfitting. Because as you see after that, the uh test error remains like more or less stable whereas the train error goes down, which is like a typical signal indication of overfitting, right.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=931s",
        "start_time": "931.059"
    },
    {
        "id": "1a73fa28",
        "text": "um iterations then just uh stop uh the training, right? And so here you can decide how many epics you want to wait uh before uh you stop. And obviously, you can also like decide how much like is the improvement that you want to like expect uh right. And so like in this case, as you can see, this is like very, very handy because we are stopping training before we start uh overfitting. Because as you see after that, the uh test error remains like more or less stable whereas the train error goes down, which is like a typical signal indication of overfitting, right. So this is for early stopping. Now we have another couple of very useful uh techniques to fight against overfitting. The first one is called drop out. So drop out is a technique that enables us",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=944s",
        "start_time": "944.359"
    },
    {
        "id": "d8d2f901",
        "text": "uh right. And so like in this case, as you can see, this is like very, very handy because we are stopping training before we start uh overfitting. Because as you see after that, the uh test error remains like more or less stable whereas the train error goes down, which is like a typical signal indication of overfitting, right. So this is for early stopping. Now we have another couple of very useful uh techniques to fight against overfitting. The first one is called drop out. So drop out is a technique that enables us to randomly drop neurons while training. And by doing that, we increase the network robustness. So how does that work? So here we have uh down here we have like uh our network, right? So now when we train say for example, we have like the the the the first like batch of data in. So we may decide to randomly drop certain neurons. And so here, for example, we have like these two neurons like in gray",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=961s",
        "start_time": "961.969"
    },
    {
        "id": "85272f18",
        "text": "So this is for early stopping. Now we have another couple of very useful uh techniques to fight against overfitting. The first one is called drop out. So drop out is a technique that enables us to randomly drop neurons while training. And by doing that, we increase the network robustness. So how does that work? So here we have uh down here we have like uh our network, right? So now when we train say for example, we have like the the the the first like batch of data in. So we may decide to randomly drop certain neurons. And so here, for example, we have like these two neurons like in gray which have been dropped. So all the connections like don't work for these neurons. And so the training just happens uh like through like the the the remaining part of the network, right? And so now like with the with the second batch of data, for example, we could just like change the neurons and this is gonna be done uh like randomly stochastically, right?",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=984s",
        "start_time": "984.88"
    },
    {
        "id": "8f37f141",
        "text": "to randomly drop neurons while training. And by doing that, we increase the network robustness. So how does that work? So here we have uh down here we have like uh our network, right? So now when we train say for example, we have like the the the the first like batch of data in. So we may decide to randomly drop certain neurons. And so here, for example, we have like these two neurons like in gray which have been dropped. So all the connections like don't work for these neurons. And so the training just happens uh like through like the the the remaining part of the network, right? And so now like with the with the second batch of data, for example, we could just like change the neurons and this is gonna be done uh like randomly stochastically, right? We have a certain uh probability of dropping neurons in a in a layer. So here for example, we drop this neuron in the second hidden layer and we restore like the previous neurons, for example, right?",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1002s",
        "start_time": "1002.71"
    },
    {
        "id": "1b4b610a",
        "text": "which have been dropped. So all the connections like don't work for these neurons. And so the training just happens uh like through like the the the remaining part of the network, right? And so now like with the with the second batch of data, for example, we could just like change the neurons and this is gonna be done uh like randomly stochastically, right? We have a certain uh probability of dropping neurons in a in a layer. So here for example, we drop this neuron in the second hidden layer and we restore like the previous neurons, for example, right? And the question here is why, why does this work? So what's the point of all of this? Well, it turns out that if we do this, we are increasing the network robustness because the network can't rely on uh specific neurons like too much. So all the neurons have to take somewhat responsibility of the uh prediction um process, right?",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1031s",
        "start_time": "1031.568"
    },
    {
        "id": "69807794",
        "text": "We have a certain uh probability of dropping neurons in a in a layer. So here for example, we drop this neuron in the second hidden layer and we restore like the previous neurons, for example, right? And the question here is why, why does this work? So what's the point of all of this? Well, it turns out that if we do this, we are increasing the network robustness because the network can't rely on uh specific neurons like too much. So all the neurons have to take somewhat responsibility of the uh prediction um process, right? Because sometimes like some of these neurons don't exist. And so like the neuron, uh the network has to kind of like reshape and reive responsibilities to all of the neurons so that none of them is uh like indispensable. Right? Cool. So now uh there's a hyper parameter here that's called the dropout like probability.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1054s",
        "start_time": "1054.04"
    },
    {
        "id": "850600d6",
        "text": "And the question here is why, why does this work? So what's the point of all of this? Well, it turns out that if we do this, we are increasing the network robustness because the network can't rely on uh specific neurons like too much. So all the neurons have to take somewhat responsibility of the uh prediction um process, right? Because sometimes like some of these neurons don't exist. And so like the neuron, uh the network has to kind of like reshape and reive responsibilities to all of the neurons so that none of them is uh like indispensable. Right? Cool. So now uh there's a hyper parameter here that's called the dropout like probability. And now again, here there's really no universal rule. And this uh so usually like what you would use like is anything between like 10 to 50% of drop in neurons like in, in different layers. Uh right. But again, it's somewhat uh",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1066s",
        "start_time": "1066.959"
    },
    {
        "id": "72871ce9",
        "text": "Because sometimes like some of these neurons don't exist. And so like the neuron, uh the network has to kind of like reshape and reive responsibilities to all of the neurons so that none of them is uh like indispensable. Right? Cool. So now uh there's a hyper parameter here that's called the dropout like probability. And now again, here there's really no universal rule. And this uh so usually like what you would use like is anything between like 10 to 50% of drop in neurons like in, in different layers. Uh right. But again, it's somewhat uh connected to like your problem. So you have to figure out like what works for your problem specifically, right. So this was a drop out. And now we have a final technique that's called uh regularization, right? And so this technique is um very interesting and very effective and it basically adds a penalty to the error function.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1094s",
        "start_time": "1094.14"
    },
    {
        "id": "74d69deb",
        "text": "And now again, here there's really no universal rule. And this uh so usually like what you would use like is anything between like 10 to 50% of drop in neurons like in, in different layers. Uh right. But again, it's somewhat uh connected to like your problem. So you have to figure out like what works for your problem specifically, right. So this was a drop out. And now we have a final technique that's called uh regularization, right? And so this technique is um very interesting and very effective and it basically adds a penalty to the error function. And basically, it's the whole point is that we want to punish large weights. And uh so the larger the weights and the, the higher like the, the penalties like that, we're gonna, that we're gonna give like to the error function.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1119s",
        "start_time": "1119.52"
    },
    {
        "id": "9a113140",
        "text": "connected to like your problem. So you have to figure out like what works for your problem specifically, right. So this was a drop out. And now we have a final technique that's called uh regularization, right? And so this technique is um very interesting and very effective and it basically adds a penalty to the error function. And basically, it's the whole point is that we want to punish large weights. And uh so the larger the weights and the, the higher like the, the penalties like that, we're gonna, that we're gonna give like to the error function. So here we have a couple of uh types of uh regularization that are usually used in deep learning L one and L two regularization. So let's take a look at them like in uh specifically one by one. OK. So here we have L one regularization.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1138s",
        "start_time": "1138.5"
    },
    {
        "id": "75fa1f38",
        "text": "And basically, it's the whole point is that we want to punish large weights. And uh so the larger the weights and the, the higher like the, the penalties like that, we're gonna, that we're gonna give like to the error function. So here we have a couple of uh types of uh regularization that are usually used in deep learning L one and L two regularization. So let's take a look at them like in uh specifically one by one. OK. So here we have L one regularization. So here, the whole point is that we want to minimize the absolute value of the weight. Now down here, you can recognize the error function quadratic function that we've used like so far, like in our theoretical um discussion of neural nets. Now these like",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1167s",
        "start_time": "1167.39"
    },
    {
        "id": "4887f1a4",
        "text": "So here we have a couple of uh types of uh regularization that are usually used in deep learning L one and L two regularization. So let's take a look at them like in uh specifically one by one. OK. So here we have L one regularization. So here, the whole point is that we want to minimize the absolute value of the weight. Now down here, you can recognize the error function quadratic function that we've used like so far, like in our theoretical um discussion of neural nets. Now these like uh we have like the quadratic error here and then we add this regular regularization uh thing. And now, as you can see here, we have like the, the sum of all like the uh the weights of the absolute value of the weights. And then we have LAMBDA which is like the term, the regularization like term. So the larger lambda and then the higher like the, the penalty that we give to the network, obviously lambda",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1183s",
        "start_time": "1183.119"
    },
    {
        "id": "a27b902b",
        "text": "So here, the whole point is that we want to minimize the absolute value of the weight. Now down here, you can recognize the error function quadratic function that we've used like so far, like in our theoretical um discussion of neural nets. Now these like uh we have like the quadratic error here and then we add this regular regularization uh thing. And now, as you can see here, we have like the, the sum of all like the uh the weights of the absolute value of the weights. And then we have LAMBDA which is like the term, the regularization like term. So the larger lambda and then the higher like the, the penalty that we give to the network, obviously lambda is another uh like hyper parameter that we need to tweak in order to like optimize our network, right. So L one regular regularization is really good because it's uh robust to uh our layers. And it kind of like generates a simple, simpler models like overall. Now we can also use L two regularization and the difference with L one regularization is that we minimize the squared",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1199s",
        "start_time": "1199.31"
    },
    {
        "id": "cadd25f4",
        "text": "uh we have like the quadratic error here and then we add this regular regularization uh thing. And now, as you can see here, we have like the, the sum of all like the uh the weights of the absolute value of the weights. And then we have LAMBDA which is like the term, the regularization like term. So the larger lambda and then the higher like the, the penalty that we give to the network, obviously lambda is another uh like hyper parameter that we need to tweak in order to like optimize our network, right. So L one regular regularization is really good because it's uh robust to uh our layers. And it kind of like generates a simple, simpler models like overall. Now we can also use L two regularization and the difference with L one regularization is that we minimize the squared uh value of the, of the weights. And you can see it here. And because we do that uh L two regularization is way less robust to outliers. But the great thing about L two regularization is that it can uh learn uh quite complex patterns. So I already know that what you want to ask. So should I use L one or L two regularization? Now, again, this is",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1217s",
        "start_time": "1217.68"
    },
    {
        "id": "9dbfb2a0",
        "text": "is another uh like hyper parameter that we need to tweak in order to like optimize our network, right. So L one regular regularization is really good because it's uh robust to uh our layers. And it kind of like generates a simple, simpler models like overall. Now we can also use L two regularization and the difference with L one regularization is that we minimize the squared uh value of the, of the weights. And you can see it here. And because we do that uh L two regularization is way less robust to outliers. But the great thing about L two regularization is that it can uh learn uh quite complex patterns. So I already know that what you want to ask. So should I use L one or L two regularization? Now, again, this is kind of like more an art than a science. But the overall like rule uh a thumb that I can give you here is that if you have like uh data kind of like relatively like simple to train, simple to learn data, probably you should go with L one. But if you have like data, it's kind of like, I don't know uh that's a little bit like more complex to learn. The patterns are more complex than go with L two.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1246s",
        "start_time": "1246.949"
    },
    {
        "id": "0de75102",
        "text": "uh value of the, of the weights. And you can see it here. And because we do that uh L two regularization is way less robust to outliers. But the great thing about L two regularization is that it can uh learn uh quite complex patterns. So I already know that what you want to ask. So should I use L one or L two regularization? Now, again, this is kind of like more an art than a science. But the overall like rule uh a thumb that I can give you here is that if you have like uh data kind of like relatively like simple to train, simple to learn data, probably you should go with L one. But if you have like data, it's kind of like, I don't know uh that's a little bit like more complex to learn. The patterns are more complex than go with L two. I would say like that in most audio and music based uh deep learning tasks you usually want to use L two regularization. Cool. So now we have an overview of all the different techniques that we can use for uh fighting over fitting. Now let's just go back to, to the code.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1275s",
        "start_time": "1275.65"
    },
    {
        "id": "ba284098",
        "text": "kind of like more an art than a science. But the overall like rule uh a thumb that I can give you here is that if you have like uh data kind of like relatively like simple to train, simple to learn data, probably you should go with L one. But if you have like data, it's kind of like, I don't know uh that's a little bit like more complex to learn. The patterns are more complex than go with L two. I would say like that in most audio and music based uh deep learning tasks you usually want to use L two regularization. Cool. So now we have an overview of all the different techniques that we can use for uh fighting over fitting. Now let's just go back to, to the code. So we can easily implement drop outs and regularization using tensorflow. Well, it's as as easily as it can be really.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1303s",
        "start_time": "1303.189"
    },
    {
        "id": "c3ffd117",
        "text": "I would say like that in most audio and music based uh deep learning tasks you usually want to use L two regularization. Cool. So now we have an overview of all the different techniques that we can use for uh fighting over fitting. Now let's just go back to, to the code. So we can easily implement drop outs and regularization using tensorflow. Well, it's as as easily as it can be really. So for drop outs, we are gonna implement it for all the hidden layers. So for doing that, we'll just do a Kas dot layers dot drop out and then we'll pass in the dropout probability which we set to 30%. So 0.3 good. So now",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1330s",
        "start_time": "1330.229"
    },
    {
        "id": "b6789f7e",
        "text": "So we can easily implement drop outs and regularization using tensorflow. Well, it's as as easily as it can be really. So for drop outs, we are gonna implement it for all the hidden layers. So for doing that, we'll just do a Kas dot layers dot drop out and then we'll pass in the dropout probability which we set to 30%. So 0.3 good. So now uh I'm gonna copy this and just paste it below the 2nd and 3rd hidden layer and drop out is done. Cool. What about regularization? Well, for regularization, we need to pass in an extra argument to this uh dense layers. So we'll do it for um hidden layer 12 and three cool. So the extra argument is called a kernel regularizer.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1351s",
        "start_time": "1351.609"
    },
    {
        "id": "bc63203e",
        "text": "So for drop outs, we are gonna implement it for all the hidden layers. So for doing that, we'll just do a Kas dot layers dot drop out and then we'll pass in the dropout probability which we set to 30%. So 0.3 good. So now uh I'm gonna copy this and just paste it below the 2nd and 3rd hidden layer and drop out is done. Cool. What about regularization? Well, for regularization, we need to pass in an extra argument to this uh dense layers. So we'll do it for um hidden layer 12 and three cool. So the extra argument is called a kernel regularizer. And we just need to call Kas dot regularizer dot uh We'll do an L two here. And if you guys remember L two has a hyper parameter called a Lambda, which is kind of like the, the penalty multiplier and we'll put it to no 0.001 cool. So now I'll just like take this and paste it for layer hidden layer two and three. Great.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1361s",
        "start_time": "1361.069"
    },
    {
        "id": "8e9dbdfa",
        "text": "uh I'm gonna copy this and just paste it below the 2nd and 3rd hidden layer and drop out is done. Cool. What about regularization? Well, for regularization, we need to pass in an extra argument to this uh dense layers. So we'll do it for um hidden layer 12 and three cool. So the extra argument is called a kernel regularizer. And we just need to call Kas dot regularizer dot uh We'll do an L two here. And if you guys remember L two has a hyper parameter called a Lambda, which is kind of like the, the penalty multiplier and we'll put it to no 0.001 cool. So now I'll just like take this and paste it for layer hidden layer two and three. Great. We've done our, yeah, basically, we, we've implemented our way of like solving overfitting for this music genre classifier. So we should just like see if this works. And in order for doing that, we need to re run the model and take a look at the plots of the accuracy and the error. So I'll see you in a second here. We",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1385s",
        "start_time": "1385.03"
    },
    {
        "id": "5f013309",
        "text": "And we just need to call Kas dot regularizer dot uh We'll do an L two here. And if you guys remember L two has a hyper parameter called a Lambda, which is kind of like the, the penalty multiplier and we'll put it to no 0.001 cool. So now I'll just like take this and paste it for layer hidden layer two and three. Great. We've done our, yeah, basically, we, we've implemented our way of like solving overfitting for this music genre classifier. So we should just like see if this works. And in order for doing that, we need to re run the model and take a look at the plots of the accuracy and the error. So I'll see you in a second here. We are with the results of our training process. And as you can see from the accuracy and error plots, things are going quite well. We've basically almost completely solved, prevented overfitting from happening. And so let's take a look at the uh error uh plot down here. So I'm just gonna zoom in uh to take a look at this. And uh as you can see here, uh the error of uh the test",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1415s",
        "start_time": "1415.0"
    },
    {
        "id": "6a69c860",
        "text": "We've done our, yeah, basically, we, we've implemented our way of like solving overfitting for this music genre classifier. So we should just like see if this works. And in order for doing that, we need to re run the model and take a look at the plots of the accuracy and the error. So I'll see you in a second here. We are with the results of our training process. And as you can see from the accuracy and error plots, things are going quite well. We've basically almost completely solved, prevented overfitting from happening. And so let's take a look at the uh error uh plot down here. So I'm just gonna zoom in uh to take a look at this. And uh as you can see here, uh the error of uh the test is quite comparable to uh the train error up until like I would say like, yeah, uh epoch uh number 70 then like the train error is going down, whereas like the test error is remaining more or less the same. And we can see that issue also in the accuracy. Uh So plot over here like around like,",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1445s",
        "start_time": "1445.119"
    },
    {
        "id": "457a692e",
        "text": "are with the results of our training process. And as you can see from the accuracy and error plots, things are going quite well. We've basically almost completely solved, prevented overfitting from happening. And so let's take a look at the uh error uh plot down here. So I'm just gonna zoom in uh to take a look at this. And uh as you can see here, uh the error of uh the test is quite comparable to uh the train error up until like I would say like, yeah, uh epoch uh number 70 then like the train error is going down, whereas like the test error is remaining more or less the same. And we can see that issue also in the accuracy. Uh So plot over here like around like, yeah, I would say like epoch 70 we start having the train accuracy going up and the test accuracy stabilizing, which basically means we are overfitting a little bit. But overall, we basically like so overfitting here, if we wanted to do like even a better job, we could use like some early stopping that probably would stop. Um uh like the training process around like epoch 17",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1470s",
        "start_time": "1470.145"
    },
    {
        "id": "8f4d007f",
        "text": "is quite comparable to uh the train error up until like I would say like, yeah, uh epoch uh number 70 then like the train error is going down, whereas like the test error is remaining more or less the same. And we can see that issue also in the accuracy. Uh So plot over here like around like, yeah, I would say like epoch 70 we start having the train accuracy going up and the test accuracy stabilizing, which basically means we are overfitting a little bit. But overall, we basically like so overfitting here, if we wanted to do like even a better job, we could use like some early stopping that probably would stop. Um uh like the training process around like epoch 17 cool. But this is great news cos now we know how to fight against all the fitting and as we've seen drop out and regularization work really, really well cool. So the next time we're gonna look into a more complex type of neural network",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1498s",
        "start_time": "1498.53"
    },
    {
        "id": "82171f05",
        "text": "yeah, I would say like epoch 70 we start having the train accuracy going up and the test accuracy stabilizing, which basically means we are overfitting a little bit. But overall, we basically like so overfitting here, if we wanted to do like even a better job, we could use like some early stopping that probably would stop. Um uh like the training process around like epoch 17 cool. But this is great news cos now we know how to fight against all the fitting and as we've seen drop out and regularization work really, really well cool. So the next time we're gonna look into a more complex type of neural network called a convolutional neural network that's been used like for uh like image data quite a lot. But it's also very, very useful on audio data. So stay tuned for that. I hope you've enjoyed this video. If that's the case, please like it and uh consider subscribing. So if you have any questions, please leave them in the comment section below and I hope I'll see you next time. Cheers.",
        "video": "14-  SOLVING OVERFITTING in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "Gf5DO6br0ts",
        "youtube_link": "https://www.youtube.com/watch?v=Gf5DO6br0ts&t=1519s",
        "start_time": "1519.93"
    },
    {
        "id": "c3708f1e",
        "text": "Hi, everybody and welcome to a new video in the Deep Learning for Radio with Python series. This time we're gonna start our projects, building a music genre classifier. Uh What we're gonna do today really is uh preparing uh it's basically like doing some preprocessing on a music data set. So what we want to do really is extract the inputs and the targets. So basically like the uh labels and the MFCC uh from music data set and store that in adjacent files. So that then we can use it when we actually train our neural network. So, first of all, what we need for a music genre classifier is a music data set. And uh luckily there's a great one on the Marias uh website where we have this uh data set for genre classification. So you can go to the website and download it here. I'm not gonna do that because I already have it. But uh",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "f822c795",
        "text": "uh from music data set and store that in adjacent files. So that then we can use it when we actually train our neural network. So, first of all, what we need for a music genre classifier is a music data set. And uh luckily there's a great one on the Marias uh website where we have this uh data set for genre classification. So you can go to the website and download it here. I'm not gonna do that because I already have it. But uh don't worry, I'm just gonna leave the um a link in the description below. So you can uh have access to the data set. Cool. So let's take a look at the uh at the data set. So it's divided into like 10 different uh folders and each folder is basically a different musical genre. So we have blues, classical",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=27s",
        "start_time": "27.465"
    },
    {
        "id": "bb52abab",
        "text": "So you can go to the website and download it here. I'm not gonna do that because I already have it. But uh don't worry, I'm just gonna leave the um a link in the description below. So you can uh have access to the data set. Cool. So let's take a look at the uh at the data set. So it's divided into like 10 different uh folders and each folder is basically a different musical genre. So we have blues, classical rock, reggae, like all sorts of genres. And inside each um genre folder, we have 100 different songs, but these are not like full songs. It's just like 30 seconds worth of song. That's, I guess like, because of like copyright uh issues. So we're gonna work with this for our music genre classifier. Cool. So now let's go uh and start uh building uh like the, this preprocessor, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=53s",
        "start_time": "53.659"
    },
    {
        "id": "f021fd7d",
        "text": "don't worry, I'm just gonna leave the um a link in the description below. So you can uh have access to the data set. Cool. So let's take a look at the uh at the data set. So it's divided into like 10 different uh folders and each folder is basically a different musical genre. So we have blues, classical rock, reggae, like all sorts of genres. And inside each um genre folder, we have 100 different songs, but these are not like full songs. It's just like 30 seconds worth of song. That's, I guess like, because of like copyright uh issues. So we're gonna work with this for our music genre classifier. Cool. So now let's go uh and start uh building uh like the, this preprocessor, right? OK. So what we want to do really is to define a high level function that we'll call save MFCC,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=60s",
        "start_time": "60.65"
    },
    {
        "id": "d12f377b",
        "text": "rock, reggae, like all sorts of genres. And inside each um genre folder, we have 100 different songs, but these are not like full songs. It's just like 30 seconds worth of song. That's, I guess like, because of like copyright uh issues. So we're gonna work with this for our music genre classifier. Cool. So now let's go uh and start uh building uh like the, this preprocessor, right? OK. So what we want to do really is to define a high level function that we'll call save MFCC, right? And so here we're going to have a bunch of different arguments. So first of all, we want the data set path,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=81s",
        "start_time": "81.584"
    },
    {
        "id": "2e6036f6",
        "text": "OK. So what we want to do really is to define a high level function that we'll call save MFCC, right? And so here we're going to have a bunch of different arguments. So first of all, we want the data set path, then we want the Jason Oops, not capital J Jason uh path. So data set path being obviously the, the path of the, of the data set itself. Jason path being the path of the adjacent file where we want to store like all the MF CCS and the labels, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=109s",
        "start_time": "109.809"
    },
    {
        "id": "d1ffa24b",
        "text": "right? And so here we're going to have a bunch of different arguments. So first of all, we want the data set path, then we want the Jason Oops, not capital J Jason uh path. So data set path being obviously the, the path of the, of the data set itself. Jason path being the path of the adjacent file where we want to store like all the MF CCS and the labels, right? OK. So data set path, Jason path. And then we need to pass in a lot of like values which are relative like to, to uh the MFCC feature uh itself. So we're, we're gonna do a number of MF CCS",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=119s",
        "start_time": "119.699"
    },
    {
        "id": "3b7f52a7",
        "text": "then we want the Jason Oops, not capital J Jason uh path. So data set path being obviously the, the path of the, of the data set itself. Jason path being the path of the adjacent file where we want to store like all the MF CCS and the labels, right? OK. So data set path, Jason path. And then we need to pass in a lot of like values which are relative like to, to uh the MFCC feature uh itself. So we're, we're gonna do a number of MF CCS and we'll default this to 13, then we'll do a number. Uh oh yeah, let's do number FFT and this is like the interval in number of samples that we are considering for our fourier transforms. And here we'll default this to 2048 then we'll have the hop length",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=129s",
        "start_time": "129.32"
    },
    {
        "id": "be38a43c",
        "text": "OK. So data set path, Jason path. And then we need to pass in a lot of like values which are relative like to, to uh the MFCC feature uh itself. So we're, we're gonna do a number of MF CCS and we'll default this to 13, then we'll do a number. Uh oh yeah, let's do number FFT and this is like the interval in number of samples that we are considering for our fourier transforms. And here we'll default this to 2048 then we'll have the hop length and we'll default this to 512. Now, if you don't know what these things are, don't worry because I have a couple of videos where I both explain the, the theory and uh the implementation like of these things like in Libera which by the way, uh is the library audio library that we're gonna use also like today. Uh Cool.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=148s",
        "start_time": "148.1"
    },
    {
        "id": "4d226e8f",
        "text": "and we'll default this to 13, then we'll do a number. Uh oh yeah, let's do number FFT and this is like the interval in number of samples that we are considering for our fourier transforms. And here we'll default this to 2048 then we'll have the hop length and we'll default this to 512. Now, if you don't know what these things are, don't worry because I have a couple of videos where I both explain the, the theory and uh the implementation like of these things like in Libera which by the way, uh is the library audio library that we're gonna use also like today. Uh Cool. OK. So beyond this, we want also another uh argument and we can call this num number of segments. And yeah, let's default this to, to five, for example. Uh OK. So why do we need number of segments? Well, it turns out that for training deep learning models, you need a lot of data.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=164s",
        "start_time": "164.779"
    },
    {
        "id": "4d33841a",
        "text": "and we'll default this to 512. Now, if you don't know what these things are, don't worry because I have a couple of videos where I both explain the, the theory and uh the implementation like of these things like in Libera which by the way, uh is the library audio library that we're gonna use also like today. Uh Cool. OK. So beyond this, we want also another uh argument and we can call this num number of segments. And yeah, let's default this to, to five, for example. Uh OK. So why do we need number of segments? Well, it turns out that for training deep learning models, you need a lot of data. So if you consider that here, we only have 100 data points really? So it's 100 tracks per genre that isn't really much. So what we wanna do is just like chop up the each track into like a number of different segments. And then instead of like saving uh like the inputs as like tracks, saving them like as different segments, right? So that we're gonna have like many more uh like input data.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=186s",
        "start_time": "186.6"
    },
    {
        "id": "0eb4806a",
        "text": "OK. So beyond this, we want also another uh argument and we can call this num number of segments. And yeah, let's default this to, to five, for example. Uh OK. So why do we need number of segments? Well, it turns out that for training deep learning models, you need a lot of data. So if you consider that here, we only have 100 data points really? So it's 100 tracks per genre that isn't really much. So what we wanna do is just like chop up the each track into like a number of different segments. And then instead of like saving uh like the inputs as like tracks, saving them like as different segments, right? So that we're gonna have like many more uh like input data. Cool. Uh OK. So this is like the, the the high level like function. So now we need to like start uh writing uh like what we need here. So the, the first thing that we want to do is to build a dictionary, dictionary, to store uh data, right? And so we'll call this not surprisingly data.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=209s",
        "start_time": "209.649"
    },
    {
        "id": "b6371051",
        "text": "So if you consider that here, we only have 100 data points really? So it's 100 tracks per genre that isn't really much. So what we wanna do is just like chop up the each track into like a number of different segments. And then instead of like saving uh like the inputs as like tracks, saving them like as different segments, right? So that we're gonna have like many more uh like input data. Cool. Uh OK. So this is like the, the the high level like function. So now we need to like start uh writing uh like what we need here. So the, the first thing that we want to do is to build a dictionary, dictionary, to store uh data, right? And so we'll call this not surprisingly data. And uh here we're gonna have a bunch of things. So if the first one is mapping and it's gonna be an array, then we'll have, oops, sorry, this is not correct. Here we go. So mapping, then we'll have the NFC C and then we'll have the uh labels down here. So what are all of these things? Right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=231s",
        "start_time": "231.69"
    },
    {
        "id": "b6b29d6a",
        "text": "Cool. Uh OK. So this is like the, the the high level like function. So now we need to like start uh writing uh like what we need here. So the, the first thing that we want to do is to build a dictionary, dictionary, to store uh data, right? And so we'll call this not surprisingly data. And uh here we're gonna have a bunch of things. So if the first one is mapping and it's gonna be an array, then we'll have, oops, sorry, this is not correct. Here we go. So mapping, then we'll have the NFC C and then we'll have the uh labels down here. So what are all of these things? Right? OK. So, mappings uh we need a way of mapping uh the different uh like genres, genre labels uh onto numbers. And so we're gonna use a list for doing that. So say, for example, we have this classical",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=261s",
        "start_time": "261.98"
    },
    {
        "id": "f596c4e5",
        "text": "And uh here we're gonna have a bunch of things. So if the first one is mapping and it's gonna be an array, then we'll have, oops, sorry, this is not correct. Here we go. So mapping, then we'll have the NFC C and then we'll have the uh labels down here. So what are all of these things? Right? OK. So, mappings uh we need a way of mapping uh the different uh like genres, genre labels uh onto numbers. And so we're gonna use a list for doing that. So say, for example, we have this classical and a blue blues,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=287s",
        "start_time": "287.98"
    },
    {
        "id": "3f26a452",
        "text": "OK. So, mappings uh we need a way of mapping uh the different uh like genres, genre labels uh onto numbers. And so we're gonna use a list for doing that. So say, for example, we have this classical and a blue blues, right? So we are basically mapping classical to zero, which is the index of the list uh it belongs to and blues uh to one. Then here we're gonna have like the MF CCS. And so basically, we're gonna have the MC CMFCC vectors for each of the uh segments. And so we're gonna have say we have like three segments, for example, here. So I'm not gonna uh fill this in. But uh you get the idea right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=313s",
        "start_time": "313.57"
    },
    {
        "id": "ef7ac3ad",
        "text": "and a blue blues, right? So we are basically mapping classical to zero, which is the index of the list uh it belongs to and blues uh to one. Then here we're gonna have like the MF CCS. And so basically, we're gonna have the MC CMFCC vectors for each of the uh segments. And so we're gonna have say we have like three segments, for example, here. So I'm not gonna uh fill this in. But uh you get the idea right? Uh We'll see this like uh then in action and then we have the, the labels down here and the labels are gonna be, for example, 00 and one. So the MF CCS are itself the training data. So the the training inputs, the labels are the, the outputs or the, the targets that we expect. So here, basically, we're saying that's for",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=330s",
        "start_time": "330.25"
    },
    {
        "id": "b3554256",
        "text": "right? So we are basically mapping classical to zero, which is the index of the list uh it belongs to and blues uh to one. Then here we're gonna have like the MF CCS. And so basically, we're gonna have the MC CMFCC vectors for each of the uh segments. And so we're gonna have say we have like three segments, for example, here. So I'm not gonna uh fill this in. But uh you get the idea right? Uh We'll see this like uh then in action and then we have the, the labels down here and the labels are gonna be, for example, 00 and one. So the MF CCS are itself the training data. So the the training inputs, the labels are the, the outputs or the, the targets that we expect. So here, basically, we're saying that's for this MFCC uh vector here. For this segment here, we, we expect this label zero which is classical uh same thing like for like the, the, this like second segment over here, we expect zero which is classical. And here uh for the third segment over here, we expect one which is a blues, right? So, so this is like a nice way of storing uh like information that we can use for training purposes,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=333s",
        "start_time": "333.679"
    },
    {
        "id": "503ac2af",
        "text": "Uh We'll see this like uh then in action and then we have the, the labels down here and the labels are gonna be, for example, 00 and one. So the MF CCS are itself the training data. So the the training inputs, the labels are the, the outputs or the, the targets that we expect. So here, basically, we're saying that's for this MFCC uh vector here. For this segment here, we, we expect this label zero which is classical uh same thing like for like the, the, this like second segment over here, we expect zero which is classical. And here uh for the third segment over here, we expect one which is a blues, right? So, so this is like a nice way of storing uh like information that we can use for training purposes, right? But now, obviously, we don't need any of this. We just need the, the schema uh overall and we're gonna just like then fill it in while we analyze stuff. So let's go and analyze things, right? So what we wanna do, first of all is we want to look through all the uh genres, right? So basically what I'm, what I'm saying here is that if we go back here,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=363s",
        "start_time": "363.049"
    },
    {
        "id": "083a1363",
        "text": "this MFCC uh vector here. For this segment here, we, we expect this label zero which is classical uh same thing like for like the, the, this like second segment over here, we expect zero which is classical. And here uh for the third segment over here, we expect one which is a blues, right? So, so this is like a nice way of storing uh like information that we can use for training purposes, right? But now, obviously, we don't need any of this. We just need the, the schema uh overall and we're gonna just like then fill it in while we analyze stuff. So let's go and analyze things, right? So what we wanna do, first of all is we want to look through all the uh genres, right? So basically what I'm, what I'm saying here is that if we go back here, so this is our data set. So what we wanna do is like loop through all of these folders and then analyze like this um uh songs like one by one. Cool. So how do we do that? Well, turns out it's quite simple if you have a uh if you rely on a um",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=389s",
        "start_time": "389.25"
    },
    {
        "id": "d1aa32f5",
        "text": "right? But now, obviously, we don't need any of this. We just need the, the schema uh overall and we're gonna just like then fill it in while we analyze stuff. So let's go and analyze things, right? So what we wanna do, first of all is we want to look through all the uh genres, right? So basically what I'm, what I'm saying here is that if we go back here, so this is our data set. So what we wanna do is like loop through all of these folders and then analyze like this um uh songs like one by one. Cool. So how do we do that? Well, turns out it's quite simple if you have a uh if you rely on a um function on a method that's in the OS uh module and the f the method it's called walk. So we'll do a four and we'll say here, dear path, dear names and file names",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=417s",
        "start_time": "417.089"
    },
    {
        "id": "58e21599",
        "text": "so this is our data set. So what we wanna do is like loop through all of these folders and then analyze like this um uh songs like one by one. Cool. So how do we do that? Well, turns out it's quite simple if you have a uh if you rely on a um function on a method that's in the OS uh module and the f the method it's called walk. So we'll do a four and we'll say here, dear path, dear names and file names in. We'll do O Os dot Walk and we'll pass in the data set path. Ok. So what's this? So, the deer path is the current, uh, folder is the path to the folder we're currently in.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=447s",
        "start_time": "447.339"
    },
    {
        "id": "14feb029",
        "text": "function on a method that's in the OS uh module and the f the method it's called walk. So we'll do a four and we'll say here, dear path, dear names and file names in. We'll do O Os dot Walk and we'll pass in the data set path. Ok. So what's this? So, the deer path is the current, uh, folder is the path to the folder we're currently in. The dear names are all the names of the sub folders in the deer path, uh, in the depth folder. And the file names are all the files that we have in dearth, right? So this is like very, very useful because then we can uh recursively going through uh all of our folders, all of our data sets recursively and, and to do that, we just use the, this OS dot Walk uh utility method which is quite cool.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=471s",
        "start_time": "471.239"
    },
    {
        "id": "dee5b34f",
        "text": "in. We'll do O Os dot Walk and we'll pass in the data set path. Ok. So what's this? So, the deer path is the current, uh, folder is the path to the folder we're currently in. The dear names are all the names of the sub folders in the deer path, uh, in the depth folder. And the file names are all the files that we have in dearth, right? So this is like very, very useful because then we can uh recursively going through uh all of our folders, all of our data sets recursively and, and to do that, we just use the, this OS dot Walk uh utility method which is quite cool. Ok. But uh we just don't want this information. We also want the count here and the count is needed because we're gonna use it for LA for like these labels here. So,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=488s",
        "start_time": "488.679"
    },
    {
        "id": "b9d49b3e",
        "text": "The dear names are all the names of the sub folders in the deer path, uh, in the depth folder. And the file names are all the files that we have in dearth, right? So this is like very, very useful because then we can uh recursively going through uh all of our folders, all of our data sets recursively and, and to do that, we just use the, this OS dot Walk uh utility method which is quite cool. Ok. But uh we just don't want this information. We also want the count here and the count is needed because we're gonna use it for LA for like these labels here. So, uh let's do that. So if we want also like the count, which is basically the number of um uh number of iterations we are currently in. We want to use uh enumerate. So we want to enumerate this Os dot Work in data set, uh data set path and this will unpack",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=508s",
        "start_time": "508.51"
    },
    {
        "id": "5ccd83b3",
        "text": "Ok. But uh we just don't want this information. We also want the count here and the count is needed because we're gonna use it for LA for like these labels here. So, uh let's do that. So if we want also like the count, which is basically the number of um uh number of iterations we are currently in. We want to use uh enumerate. So we want to enumerate this Os dot Work in data set, uh data set path and this will unpack this value giving us like the, the counts, the current loop, uh current iteration we are in plus their path, their names and, and file names.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=537s",
        "start_time": "537.409"
    },
    {
        "id": "7792fcde",
        "text": "uh let's do that. So if we want also like the count, which is basically the number of um uh number of iterations we are currently in. We want to use uh enumerate. So we want to enumerate this Os dot Work in data set, uh data set path and this will unpack this value giving us like the, the counts, the current loop, uh current iteration we are in plus their path, their names and, and file names. Cool.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=550s",
        "start_time": "550.26"
    },
    {
        "id": "bba014e7",
        "text": "this value giving us like the, the counts, the current loop, uh current iteration we are in plus their path, their names and, and file names. Cool. OK. So, uh what we wanna do here, first of all is we want to ensure uh that uh we are uh not uh at the",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=571s",
        "start_time": "571.179"
    },
    {
        "id": "eb90482a",
        "text": "Cool. OK. So, uh what we wanna do here, first of all is we want to ensure uh that uh we are uh not uh at the root level.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=579s",
        "start_time": "579.95"
    },
    {
        "id": "b204da0a",
        "text": "OK. So, uh what we wanna do here, first of all is we want to ensure uh that uh we are uh not uh at the root level. So we are not at the, at, at the data set level. Uh Right. So because we want to go through all the the folders uh sub folders here like blues classical, but Os dot walk uh will give us as during the first situation uh data set path itself, right? So it, it'll give us in their path, data set path. And we, we don't want that because like, we don't, we don't need it, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=581s",
        "start_time": "581.26"
    },
    {
        "id": "5c33ff42",
        "text": "root level. So we are not at the, at, at the data set level. Uh Right. So because we want to go through all the the folders uh sub folders here like blues classical, but Os dot walk uh will give us as during the first situation uh data set path itself, right? So it, it'll give us in their path, data set path. And we, we don't want that because like, we don't, we don't need it, right? And so we want to ensure that we are at the genre level, right? At the genres of folders. And so for doing that, we'll do if, um, we'll do deer path,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=595s",
        "start_time": "595.979"
    },
    {
        "id": "bd5be4c2",
        "text": "So we are not at the, at, at the data set level. Uh Right. So because we want to go through all the the folders uh sub folders here like blues classical, but Os dot walk uh will give us as during the first situation uh data set path itself, right? So it, it'll give us in their path, data set path. And we, we don't want that because like, we don't, we don't need it, right? And so we want to ensure that we are at the genre level, right? At the genres of folders. And so for doing that, we'll do if, um, we'll do deer path, uh, is not a dataset path and then you will write, uh, like our magic, right? Ok. So now let me do just like a thing here. Uh, so let me just write the data set path here like as a constant. And here we have,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=598s",
        "start_time": "598.69"
    },
    {
        "id": "3b55ca93",
        "text": "And so we want to ensure that we are at the genre level, right? At the genres of folders. And so for doing that, we'll do if, um, we'll do deer path, uh, is not a dataset path and then you will write, uh, like our magic, right? Ok. So now let me do just like a thing here. Uh, so let me just write the data set path here like as a constant. And here we have, uh, I created a reduced version of the, uh Marcia data set, which has only one song per genre. And I've done that because like it's gonna be like way faster, uh, to, to process everything. But then you can, you should use here like the, the path to the, the Marcia data set, right? To the full one.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=627s",
        "start_time": "627.25"
    },
    {
        "id": "d6dba407",
        "text": "uh, is not a dataset path and then you will write, uh, like our magic, right? Ok. So now let me do just like a thing here. Uh, so let me just write the data set path here like as a constant. And here we have, uh, I created a reduced version of the, uh Marcia data set, which has only one song per genre. And I've done that because like it's gonna be like way faster, uh, to, to process everything. But then you can, you should use here like the, the path to the, the Marcia data set, right? To the full one. So, and here we put the Jason path and here I'm gonna put in. Yeah, let's say data dot Jason, right? And obviously here uh I'm, I'll be saving stuff like in the, in the current uh like a working folder,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=640s",
        "start_time": "640.099"
    },
    {
        "id": "696d9a2e",
        "text": "uh, I created a reduced version of the, uh Marcia data set, which has only one song per genre. And I've done that because like it's gonna be like way faster, uh, to, to process everything. But then you can, you should use here like the, the path to the, the Marcia data set, right? To the full one. So, and here we put the Jason path and here I'm gonna put in. Yeah, let's say data dot Jason, right? And obviously here uh I'm, I'll be saving stuff like in the, in the current uh like a working folder, right? OK. So let's go back here. So we can say uh So here uh now what we uh want to do is to, first of all uh like save uh the uh semantic uh label and what do I mean by that? Well, I mean, I want to save in mappings over here.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=661s",
        "start_time": "661.679"
    },
    {
        "id": "6f719fbe",
        "text": "So, and here we put the Jason path and here I'm gonna put in. Yeah, let's say data dot Jason, right? And obviously here uh I'm, I'll be saving stuff like in the, in the current uh like a working folder, right? OK. So let's go back here. So we can say uh So here uh now what we uh want to do is to, first of all uh like save uh the uh semantic uh label and what do I mean by that? Well, I mean, I want to save in mappings over here. Uh, I want to say things like, uh, classical, for example, right? Or at the next situation blues, right? So how do we do that? Well, so",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=682s",
        "start_time": "682.739"
    },
    {
        "id": "bd2d2060",
        "text": "right? OK. So let's go back here. So we can say uh So here uh now what we uh want to do is to, first of all uh like save uh the uh semantic uh label and what do I mean by that? Well, I mean, I want to save in mappings over here. Uh, I want to say things like, uh, classical, for example, right? Or at the next situation blues, right? So how do we do that? Well, so we know that a deer path,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=701s",
        "start_time": "701.63"
    },
    {
        "id": "1c0721a6",
        "text": "Uh, I want to say things like, uh, classical, for example, right? Or at the next situation blues, right? So how do we do that? Well, so we know that a deer path, uh, deer path, uh, is the, uh, gives us like the, the path of the, uh, current directory. So, in our case, what we're looking at is we want to take deer path and we want to pass from the deer path to its components.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=728s",
        "start_time": "728.039"
    },
    {
        "id": "27b1ba91",
        "text": "we know that a deer path, uh, deer path, uh, is the, uh, gives us like the, the path of the, uh, current directory. So, in our case, what we're looking at is we want to take deer path and we want to pass from the deer path to its components. So, how do we do that? Well, we do a",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=739s",
        "start_time": "739.979"
    },
    {
        "id": "91cc2458",
        "text": "uh, deer path, uh, is the, uh, gives us like the, the path of the, uh, current directory. So, in our case, what we're looking at is we want to take deer path and we want to pass from the deer path to its components. So, how do we do that? Well, we do a dear,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=744s",
        "start_time": "744.4"
    },
    {
        "id": "ce879e84",
        "text": "So, how do we do that? Well, we do a dear, uh, dear path dot uh, we're gonna split it and we're gonna split it, uh, based on the, uh, slash. So this basically means that if we have a deer path, uh, which is, for example, genre slash, uh, blues,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=768s",
        "start_time": "768.03"
    },
    {
        "id": "4f564fe0",
        "text": "dear, uh, dear path dot uh, we're gonna split it and we're gonna split it, uh, based on the, uh, slash. So this basically means that if we have a deer path, uh, which is, for example, genre slash, uh, blues, this is gonna give us,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=772s",
        "start_time": "772.429"
    },
    {
        "id": "b77d32b2",
        "text": "uh, dear path dot uh, we're gonna split it and we're gonna split it, uh, based on the, uh, slash. So this basically means that if we have a deer path, uh, which is, for example, genre slash, uh, blues, this is gonna give us, right? This is gonna give us a list, uh, where we have a genre and blues.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=774s",
        "start_time": "774.2"
    },
    {
        "id": "a25515fb",
        "text": "this is gonna give us, right? This is gonna give us a list, uh, where we have a genre and blues. Right now, we are interested in this semantic label. So blues for our mapping and so",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=794s",
        "start_time": "794.25"
    },
    {
        "id": "79fbe207",
        "text": "right? This is gonna give us a list, uh, where we have a genre and blues. Right now, we are interested in this semantic label. So blues for our mapping and so we could isolate the semantic uh uh label",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=798s",
        "start_time": "798.729"
    },
    {
        "id": "7d1ea855",
        "text": "Right now, we are interested in this semantic label. So blues for our mapping and so we could isolate the semantic uh uh label doing a dip path components. And considering only the last uh the value like for the last index, which is basically this blues here. Nice. So now we have the semantic label. So what we wanna do is to a append it to the mapping.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=806s",
        "start_time": "806.9"
    },
    {
        "id": "a29c7b95",
        "text": "we could isolate the semantic uh uh label doing a dip path components. And considering only the last uh the value like for the last index, which is basically this blues here. Nice. So now we have the semantic label. So what we wanna do is to a append it to the mapping. So we'll do a data mapping dot uh append and we'll append the uh semantic label. Nice. So this is like the, the first path and the first thing, the first step, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=815s",
        "start_time": "815.2"
    },
    {
        "id": "12e66670",
        "text": "doing a dip path components. And considering only the last uh the value like for the last index, which is basically this blues here. Nice. So now we have the semantic label. So what we wanna do is to a append it to the mapping. So we'll do a data mapping dot uh append and we'll append the uh semantic label. Nice. So this is like the, the first path and the first thing, the first step, right? So now what we wanna do next is we want to go through all the files in the current deer path in the current, uh, genre folder. So, for doing that, uh, we, so, well, first of all, like, let's comment this. So let's say process,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=824s",
        "start_time": "824.0"
    },
    {
        "id": "0afe21d9",
        "text": "So we'll do a data mapping dot uh append and we'll append the uh semantic label. Nice. So this is like the, the first path and the first thing, the first step, right? So now what we wanna do next is we want to go through all the files in the current deer path in the current, uh, genre folder. So, for doing that, uh, we, so, well, first of all, like, let's comment this. So let's say process, uh, files for a specific, uh, genre. Ok? So, uh, we'll do a four loop and so we'll do a four F in file names.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=845s",
        "start_time": "845.58"
    },
    {
        "id": "954a9e82",
        "text": "So now what we wanna do next is we want to go through all the files in the current deer path in the current, uh, genre folder. So, for doing that, uh, we, so, well, first of all, like, let's comment this. So let's say process, uh, files for a specific, uh, genre. Ok? So, uh, we'll do a four loop and so we'll do a four F in file names. And here",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=860s",
        "start_time": "860.95"
    },
    {
        "id": "c4f5b16d",
        "text": "uh, files for a specific, uh, genre. Ok? So, uh, we'll do a four loop and so we'll do a four F in file names. And here we are, we, so, first of all, we need to, uh, get the five path,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=879s",
        "start_time": "879.7"
    },
    {
        "id": "646c0c49",
        "text": "And here we are, we, so, first of all, we need to, uh, get the five path, right? Because the file name itself. So this f gives us just like the, the name of the file. It's not the full path. And so we actually need the full path for loading the,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=896s",
        "start_time": "896.789"
    },
    {
        "id": "470900a6",
        "text": "we are, we, so, first of all, we need to, uh, get the five path, right? Because the file name itself. So this f gives us just like the, the name of the file. It's not the full path. And so we actually need the full path for loading the, uh, for, for loading the audio file, right? And so for, uh, arriving at the five path, we'll do an Os dot path",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=898s",
        "start_time": "898.52"
    },
    {
        "id": "1140bf58",
        "text": "right? Because the file name itself. So this f gives us just like the, the name of the file. It's not the full path. And so we actually need the full path for loading the, uh, for, for loading the audio file, right? And so for, uh, arriving at the five path, we'll do an Os dot path dot uh, join and we'll pass in, uh, initially the deer path",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=906s",
        "start_time": "906.229"
    },
    {
        "id": "9f15d849",
        "text": "uh, for, for loading the audio file, right? And so for, uh, arriving at the five path, we'll do an Os dot path dot uh, join and we'll pass in, uh, initially the deer path and the file name,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=921s",
        "start_time": "921.44"
    },
    {
        "id": "99acd260",
        "text": "dot uh, join and we'll pass in, uh, initially the deer path and the file name, which is F right. So why do we want to do that? Well, because we want to load the audio file. So now we have the, the, the, the file path. And the next thing for loading the audio file is",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=930s",
        "start_time": "930.51"
    },
    {
        "id": "c8def979",
        "text": "and the file name, which is F right. So why do we want to do that? Well, because we want to load the audio file. So now we have the, the, the, the file path. And the next thing for loading the audio file is importing Lib Rosa",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=937s",
        "start_time": "937.719"
    },
    {
        "id": "6ccc8185",
        "text": "which is F right. So why do we want to do that? Well, because we want to load the audio file. So now we have the, the, the, the file path. And the next thing for loading the audio file is importing Lib Rosa saying",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=940s",
        "start_time": "940.799"
    },
    {
        "id": "592c7a08",
        "text": "importing Lib Rosa saying uh in the previous video I showed how to use Li Breza, which is this great uh audio uh like processing uh library. So if you don't know uh if you haven't watched the video, just like, go back because they like that's quite uh detail about how to do a bunch of stuff with Libera.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=957s",
        "start_time": "957.489"
    },
    {
        "id": "4858ebff",
        "text": "saying uh in the previous video I showed how to use Li Breza, which is this great uh audio uh like processing uh library. So if you don't know uh if you haven't watched the video, just like, go back because they like that's quite uh detail about how to do a bunch of stuff with Libera. But uh let's go back here. So now what we want to do is load this file and so we are gonna have the signal and the uh sample rate. And uh here we'll do a li browser dot uh load and we need to pass the file and we'll pass the file path and then we'll need to specify the uh sample rate. And uh let's assume that we have a constant here for the sample rate,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=960s",
        "start_time": "960.52"
    },
    {
        "id": "3abc585f",
        "text": "uh in the previous video I showed how to use Li Breza, which is this great uh audio uh like processing uh library. So if you don't know uh if you haven't watched the video, just like, go back because they like that's quite uh detail about how to do a bunch of stuff with Libera. But uh let's go back here. So now what we want to do is load this file and so we are gonna have the signal and the uh sample rate. And uh here we'll do a li browser dot uh load and we need to pass the file and we'll pass the file path and then we'll need to specify the uh sample rate. And uh let's assume that we have a constant here for the sample rate, okay? And so we'll put it over here. And so we'll say SA Mle",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=962s",
        "start_time": "962.2"
    },
    {
        "id": "de526bc4",
        "text": "But uh let's go back here. So now what we want to do is load this file and so we are gonna have the signal and the uh sample rate. And uh here we'll do a li browser dot uh load and we need to pass the file and we'll pass the file path and then we'll need to specify the uh sample rate. And uh let's assume that we have a constant here for the sample rate, okay? And so we'll put it over here. And so we'll say SA Mle uh rates is equal to 2 22,000 uh 50 which is a customary uh value for sample rate when we do music processing. OK. And so now we've uh loaded the uh audio file, OK?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=979s",
        "start_time": "979.65"
    },
    {
        "id": "be80778b",
        "text": "okay? And so we'll put it over here. And so we'll say SA Mle uh rates is equal to 2 22,000 uh 50 which is a customary uh value for sample rate when we do music processing. OK. And so now we've uh loaded the uh audio file, OK? But now we can't just analyze and extract the MF CCS like at this level because uh we want to like analyze and extract MF CCS at the level of the segments, right? And so now we need to like divide like this signal into a bunch of different uh segments. And so what we need to do here",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1007s",
        "start_time": "1007.599"
    },
    {
        "id": "a5e362d3",
        "text": "uh rates is equal to 2 22,000 uh 50 which is a customary uh value for sample rate when we do music processing. OK. And so now we've uh loaded the uh audio file, OK? But now we can't just analyze and extract the MF CCS like at this level because uh we want to like analyze and extract MF CCS at the level of the segments, right? And so now we need to like divide like this signal into a bunch of different uh segments. And so what we need to do here is to process uh segments, extracting MFCC and uh storing",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1015s",
        "start_time": "1015.34"
    },
    {
        "id": "1df3a38a",
        "text": "But now we can't just analyze and extract the MF CCS like at this level because uh we want to like analyze and extract MF CCS at the level of the segments, right? And so now we need to like divide like this signal into a bunch of different uh segments. And so what we need to do here is to process uh segments, extracting MFCC and uh storing uh yeah, the data, right.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1032s",
        "start_time": "1032.06"
    },
    {
        "id": "6fc1e472",
        "text": "is to process uh segments, extracting MFCC and uh storing uh yeah, the data, right. Uh And yeah, storing data, process segments, extracting and storing data. Yes. OK. So what we'll do here is another for loop, nested loop. And so here we'll do for s in a range, a number of segments. And so we're going through like all the segments. And here what we wanna do",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1056s",
        "start_time": "1056.189"
    },
    {
        "id": "12114dfa",
        "text": "uh yeah, the data, right. Uh And yeah, storing data, process segments, extracting and storing data. Yes. OK. So what we'll do here is another for loop, nested loop. And so here we'll do for s in a range, a number of segments. And so we're going through like all the segments. And here what we wanna do is to have a for each segment, we need",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1067s",
        "start_time": "1067.25"
    },
    {
        "id": "28829b60",
        "text": "Uh And yeah, storing data, process segments, extracting and storing data. Yes. OK. So what we'll do here is another for loop, nested loop. And so here we'll do for s in a range, a number of segments. And so we're going through like all the segments. And here what we wanna do is to have a for each segment, we need a, a start uh sample in the signal and we need a um finish sample,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1070s",
        "start_time": "1070.17"
    },
    {
        "id": "739fcd4c",
        "text": "is to have a for each segment, we need a, a start uh sample in the signal and we need a um finish sample, right. OK. And so the, the start sample is gonna be given by an bear a second uh with makers, this is gonna be a little bit like convoluted. So here we want the number",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1092s",
        "start_time": "1092.13"
    },
    {
        "id": "e6637c8c",
        "text": "a, a start uh sample in the signal and we need a um finish sample, right. OK. And so the, the start sample is gonna be given by an bear a second uh with makers, this is gonna be a little bit like convoluted. So here we want the number uh of samples",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1099s",
        "start_time": "1099.569"
    },
    {
        "id": "1f83cef5",
        "text": "right. OK. And so the, the start sample is gonna be given by an bear a second uh with makers, this is gonna be a little bit like convoluted. So here we want the number uh of samples P",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1111s",
        "start_time": "1111.04"
    },
    {
        "id": "3d205f53",
        "text": "uh of samples P segment",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1126s",
        "start_time": "1126.979"
    },
    {
        "id": "aabafcbe",
        "text": "P segment multiplied",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1130s",
        "start_time": "1130.589"
    },
    {
        "id": "2e0f8f07",
        "text": "segment multiplied uh by S which is like the, the current segment uh we are in and now the finish sample is gonna be the start sample plus the number of samples per uh segment.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1133s",
        "start_time": "1133.219"
    },
    {
        "id": "58d35ba3",
        "text": "multiplied uh by S which is like the, the current segment uh we are in and now the finish sample is gonna be the start sample plus the number of samples per uh segment. OK. So uh le let's move on and then I'll create uh uh I'll derive like this variable here. Cool. So basically what we wanna do here is to get the MF CCS and for doing that, we'll do a lib Rosa dot uh feature dot MFCC. And here we need to pass the signal in. But here you'll see that we'll,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1135s",
        "start_time": "1135.579"
    },
    {
        "id": "032bfbce",
        "text": "uh by S which is like the, the current segment uh we are in and now the finish sample is gonna be the start sample plus the number of samples per uh segment. OK. So uh le let's move on and then I'll create uh uh I'll derive like this variable here. Cool. So basically what we wanna do here is to get the MF CCS and for doing that, we'll do a lib Rosa dot uh feature dot MFCC. And here we need to pass the signal in. But here you'll see that we'll, we, we don't want to analyze the whole signal, but we want to analyze a slice of that. And so the slice is gonna be between the start sample and the finish sample, right? And then we need to pass in the uh sample rate and the sample rate is gonna be equal to SR and then we want to pass in",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1138s",
        "start_time": "1138.459"
    },
    {
        "id": "57329213",
        "text": "OK. So uh le let's move on and then I'll create uh uh I'll derive like this variable here. Cool. So basically what we wanna do here is to get the MF CCS and for doing that, we'll do a lib Rosa dot uh feature dot MFCC. And here we need to pass the signal in. But here you'll see that we'll, we, we don't want to analyze the whole signal, but we want to analyze a slice of that. And so the slice is gonna be between the start sample and the finish sample, right? And then we need to pass in the uh sample rate and the sample rate is gonna be equal to SR and then we want to pass in all of these values here, right. So the number of MF CCS, the number uh the interval that we are considering for the four transfer and the hop lamp, the sliding window uh right. And so we'll do any NFFT is equal to N",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1156s",
        "start_time": "1156.829"
    },
    {
        "id": "edb4cedb",
        "text": "we, we don't want to analyze the whole signal, but we want to analyze a slice of that. And so the slice is gonna be between the start sample and the finish sample, right? And then we need to pass in the uh sample rate and the sample rate is gonna be equal to SR and then we want to pass in all of these values here, right. So the number of MF CCS, the number uh the interval that we are considering for the four transfer and the hop lamp, the sliding window uh right. And so we'll do any NFFT is equal to N FFT, then we'll do a um mm NMFCC is equal to",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1183s",
        "start_time": "1183.979"
    },
    {
        "id": "540cdbdd",
        "text": "all of these values here, right. So the number of MF CCS, the number uh the interval that we are considering for the four transfer and the hop lamp, the sliding window uh right. And so we'll do any NFFT is equal to N FFT, then we'll do a um mm NMFCC is equal to NMFC over here.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1207s",
        "start_time": "1207.29"
    },
    {
        "id": "6bc1a70d",
        "text": "FFT, then we'll do a um mm NMFCC is equal to NMFC over here. And then we'll do a",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1225s",
        "start_time": "1225.619"
    },
    {
        "id": "56ac0c8a",
        "text": "NMFC over here. And then we'll do a H length which is again equal to help length. And these are all values that we got from uh the arguments of the, of the function itself, right? Cool. OK. So now, as you can see here,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1235s",
        "start_time": "1235.88"
    },
    {
        "id": "80c106db",
        "text": "And then we'll do a H length which is again equal to help length. And these are all values that we got from uh the arguments of the, of the function itself, right? Cool. OK. So now, as you can see here, we, we are just analyzing uh a slice of the signal which is the slice which is irrelevant for the current segment. And so for the start sample and the finish sample, as we said, we need the overall number of samples per uh segment. So let's calculate this. And we given like this is like a something that remains like unchanged throughout. So we could um calculate that in here.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1238s",
        "start_time": "1238.27"
    },
    {
        "id": "bfeaf508",
        "text": "H length which is again equal to help length. And these are all values that we got from uh the arguments of the, of the function itself, right? Cool. OK. So now, as you can see here, we, we are just analyzing uh a slice of the signal which is the slice which is irrelevant for the current segment. And so for the start sample and the finish sample, as we said, we need the overall number of samples per uh segment. So let's calculate this. And we given like this is like a something that remains like unchanged throughout. So we could um calculate that in here. And so the overall number of samples per segment is given by the number.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1241s",
        "start_time": "1241.189"
    },
    {
        "id": "4d8cb865",
        "text": "we, we are just analyzing uh a slice of the signal which is the slice which is irrelevant for the current segment. And so for the start sample and the finish sample, as we said, we need the overall number of samples per uh segment. So let's calculate this. And we given like this is like a something that remains like unchanged throughout. So we could um calculate that in here. And so the overall number of samples per segment is given by the number. Uh So it's samples",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1257s",
        "start_time": "1257.4"
    },
    {
        "id": "0a1bc761",
        "text": "And so the overall number of samples per segment is given by the number. Uh So it's samples per",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1286s",
        "start_time": "1286.29"
    },
    {
        "id": "6ee23d2d",
        "text": "Uh So it's samples per truck",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1295s",
        "start_time": "1295.359"
    },
    {
        "id": "5f23cf3d",
        "text": "per truck divided by the number of segments, right? So now this samples per truck is the overall number of samples in a, in a, in a truck, in a sample, right? Uh And we can do, we should do like a an inch of this. So we are casting this like in, right? OK. So obviously we don't have this samples per truck and this is a constant and we need to like create it over here.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1300s",
        "start_time": "1300.239"
    },
    {
        "id": "a55a4e66",
        "text": "truck divided by the number of segments, right? So now this samples per truck is the overall number of samples in a, in a, in a truck, in a sample, right? Uh And we can do, we should do like a an inch of this. So we are casting this like in, right? OK. So obviously we don't have this samples per truck and this is a constant and we need to like create it over here. So, and the Sa Mples per truck is given by the Sa Mle rate multiplied by the",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1301s",
        "start_time": "1301.91"
    },
    {
        "id": "02a42409",
        "text": "divided by the number of segments, right? So now this samples per truck is the overall number of samples in a, in a, in a truck, in a sample, right? Uh And we can do, we should do like a an inch of this. So we are casting this like in, right? OK. So obviously we don't have this samples per truck and this is a constant and we need to like create it over here. So, and the Sa Mples per truck is given by the Sa Mle rate multiplied by the duration, right? And we know that with the Mariah data set, the duration is a 30 it's given uh in uh seconds, right? So,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1303s",
        "start_time": "1303.64"
    },
    {
        "id": "e6c82917",
        "text": "So, and the Sa Mples per truck is given by the Sa Mle rate multiplied by the duration, right? And we know that with the Mariah data set, the duration is a 30 it's given uh in uh seconds, right? So, OK, so let's recap this because this was quite, quite the jump, right? OK. So we have the sample rate which is 22,050 the duration of each um audio file is 30 seconds. So the overall number of samples per track is given by the sample rate multiplied by the duration right",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1332s",
        "start_time": "1332.52"
    },
    {
        "id": "369a5ffd",
        "text": "duration, right? And we know that with the Mariah data set, the duration is a 30 it's given uh in uh seconds, right? So, OK, so let's recap this because this was quite, quite the jump, right? OK. So we have the sample rate which is 22,050 the duration of each um audio file is 30 seconds. So the overall number of samples per track is given by the sample rate multiplied by the duration right now. Uh We are interested in the number of samples for each segment and this is obviously given by the overall number of samples per track divided by the number of segments, right? And",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1340s",
        "start_time": "1340.88"
    },
    {
        "id": "cf85f68e",
        "text": "OK, so let's recap this because this was quite, quite the jump, right? OK. So we have the sample rate which is 22,050 the duration of each um audio file is 30 seconds. So the overall number of samples per track is given by the sample rate multiplied by the duration right now. Uh We are interested in the number of samples for each segment and this is obviously given by the overall number of samples per track divided by the number of segments, right? And now when we go down here, the start sample for each sample uh for sorry for each segment for each song is given by the number of samples per segment multiplied by uh the, the, the current segment we are in. OK. So uh let's for uh s equal to zero, for example. So we are basically at the, at the first segment,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1356s",
        "start_time": "1356.949"
    },
    {
        "id": "eba03e45",
        "text": "now. Uh We are interested in the number of samples for each segment and this is obviously given by the overall number of samples per track divided by the number of segments, right? And now when we go down here, the start sample for each sample uh for sorry for each segment for each song is given by the number of samples per segment multiplied by uh the, the, the current segment we are in. OK. So uh let's for uh s equal to zero, for example. So we are basically at the, at the first segment, this is equal to zero, right? Yeah, because we are starting at zero. And here the, the finish sample for S equal to zero is as we expect equal to",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1376s",
        "start_time": "1376.819"
    },
    {
        "id": "7263e9f4",
        "text": "now when we go down here, the start sample for each sample uh for sorry for each segment for each song is given by the number of samples per segment multiplied by uh the, the, the current segment we are in. OK. So uh let's for uh s equal to zero, for example. So we are basically at the, at the first segment, this is equal to zero, right? Yeah, because we are starting at zero. And here the, the finish sample for S equal to zero is as we expect equal to the number of samples per, per segment, right? Because we are doing like a whole uh interval which is like the whole number of samples per segment. And then we just like slide and add uh like we just like a slide to the right, adding the number of samples per segment for each um uh segment we are calculating, right. OK. So now we have the MFCC but we need to do one thing",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1390s",
        "start_time": "1390.75"
    },
    {
        "id": "1c55c45d",
        "text": "this is equal to zero, right? Yeah, because we are starting at zero. And here the, the finish sample for S equal to zero is as we expect equal to the number of samples per, per segment, right? Because we are doing like a whole uh interval which is like the whole number of samples per segment. And then we just like slide and add uh like we just like a slide to the right, adding the number of samples per segment for each um uh segment we are calculating, right. OK. So now we have the MFCC but we need to do one thing which is MFCC dot T. So we, we want to let the transfers uh like this because like it's gonna be like",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1419s",
        "start_time": "1419.599"
    },
    {
        "id": "bfaa5548",
        "text": "the number of samples per, per segment, right? Because we are doing like a whole uh interval which is like the whole number of samples per segment. And then we just like slide and add uh like we just like a slide to the right, adding the number of samples per segment for each um uh segment we are calculating, right. OK. So now we have the MFCC but we need to do one thing which is MFCC dot T. So we, we want to let the transfers uh like this because like it's gonna be like uh yeah, nicer to work with this. Now, uh there's one thing that we would uh would need to specify here. So sometimes it turns out that uh the uh audio files don't have, yeah, the, the expected like an overall like number of samples because like the duration is slightly like more or less like that what we would expect,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1436s",
        "start_time": "1436.349"
    },
    {
        "id": "6a4b5da6",
        "text": "which is MFCC dot T. So we, we want to let the transfers uh like this because like it's gonna be like uh yeah, nicer to work with this. Now, uh there's one thing that we would uh would need to specify here. So sometimes it turns out that uh the uh audio files don't have, yeah, the, the expected like an overall like number of samples because like the duration is slightly like more or less like that what we would expect, which basically means that when we do like the MFCC, we may have like more uh vectors, more or less vectors like than expected. And we don't want to uh include those like in our data set because when we pass uh like this MF CCS as training data, we need uh like for the training data to have all the same shape, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1463s",
        "start_time": "1463.92"
    },
    {
        "id": "48a45ca5",
        "text": "uh yeah, nicer to work with this. Now, uh there's one thing that we would uh would need to specify here. So sometimes it turns out that uh the uh audio files don't have, yeah, the, the expected like an overall like number of samples because like the duration is slightly like more or less like that what we would expect, which basically means that when we do like the MFCC, we may have like more uh vectors, more or less vectors like than expected. And we don't want to uh include those like in our data set because when we pass uh like this MF CCS as training data, we need uh like for the training data to have all the same shape, right? And we need to ensure that we have like the same number of uh MF CCS vectors for each segment. OK. And so what we want to do here is we want to first of all calculate the expected",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1472s",
        "start_time": "1472.77"
    },
    {
        "id": "eb6c12e6",
        "text": "which basically means that when we do like the MFCC, we may have like more uh vectors, more or less vectors like than expected. And we don't want to uh include those like in our data set because when we pass uh like this MF CCS as training data, we need uh like for the training data to have all the same shape, right? And we need to ensure that we have like the same number of uh MF CCS vectors for each segment. OK. And so what we want to do here is we want to first of all calculate the expected number",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1495s",
        "start_time": "1495.04"
    },
    {
        "id": "7ff28c80",
        "text": "And we need to ensure that we have like the same number of uh MF CCS vectors for each segment. OK. And so what we want to do here is we want to first of all calculate the expected number NFCC vectors",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1518s",
        "start_time": "1518.329"
    },
    {
        "id": "2cae13ed",
        "text": "number NFCC vectors per",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1537s",
        "start_time": "1537.68"
    },
    {
        "id": "2c7bfe05",
        "text": "NFCC vectors per segment,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1539s",
        "start_time": "1539.729"
    },
    {
        "id": "6e19ebdb",
        "text": "per segment, right? So this is a ridiculously long variable, but I hope it's quite clear, right? And so in this case, uh this, this value is given by the number of uh samples",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1544s",
        "start_time": "1544.829"
    },
    {
        "id": "0bf60325",
        "text": "segment, right? So this is a ridiculously long variable, but I hope it's quite clear, right? And so in this case, uh this, this value is given by the number of uh samples per segment",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1546s",
        "start_time": "1546.609"
    },
    {
        "id": "5caf61a4",
        "text": "right? So this is a ridiculously long variable, but I hope it's quite clear, right? And so in this case, uh this, this value is given by the number of uh samples per segment and uh it's divided by",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1548s",
        "start_time": "1548.28"
    },
    {
        "id": "650c2e75",
        "text": "per segment and uh it's divided by the hot length.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1566s",
        "start_time": "1566.469"
    },
    {
        "id": "cb749d0f",
        "text": "and uh it's divided by the hot length. So now I'm not gonna explain like into the details why this is the case. And but like if you go back uh to my video like on the fourier transform and on the MFCC, you will understand why that's the case. But that's because like we are doing uh uh",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1568s",
        "start_time": "1568.459"
    },
    {
        "id": "09358dc3",
        "text": "the hot length. So now I'm not gonna explain like into the details why this is the case. And but like if you go back uh to my video like on the fourier transform and on the MFCC, you will understand why that's the case. But that's because like we are doing uh uh like many, we are calculating the MF CCS basically like at each hop length. And so like when we want to have like the overall expected number of MFCC vectors per segment, we need just need to get all the number of samples per segment and divided it by like the, the H length.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1574s",
        "start_time": "1574.569"
    },
    {
        "id": "89033050",
        "text": "So now I'm not gonna explain like into the details why this is the case. And but like if you go back uh to my video like on the fourier transform and on the MFCC, you will understand why that's the case. But that's because like we are doing uh uh like many, we are calculating the MF CCS basically like at each hop length. And so like when we want to have like the overall expected number of MFCC vectors per segment, we need just need to get all the number of samples per segment and divided it by like the, the H length. But now this number uh could uh potentially be a value of like a float like this, right? 1.2 for example, right? Uh But uh what we actually want to do is around uh the number like to the higher integer there, like in this case like two. And so for doing that, what we wanna do is import the math",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1577s",
        "start_time": "1577.93"
    },
    {
        "id": "2c7e0649",
        "text": "like many, we are calculating the MF CCS basically like at each hop length. And so like when we want to have like the overall expected number of MFCC vectors per segment, we need just need to get all the number of samples per segment and divided it by like the, the H length. But now this number uh could uh potentially be a value of like a float like this, right? 1.2 for example, right? Uh But uh what we actually want to do is around uh the number like to the higher integer there, like in this case like two. And so for doing that, what we wanna do is import the math uh module",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1597s",
        "start_time": "1597.55"
    },
    {
        "id": "32ee359a",
        "text": "But now this number uh could uh potentially be a value of like a float like this, right? 1.2 for example, right? Uh But uh what we actually want to do is around uh the number like to the higher integer there, like in this case like two. And so for doing that, what we wanna do is import the math uh module and I use a nice function",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1617s",
        "start_time": "1617.8"
    },
    {
        "id": "24d0cc70",
        "text": "uh module and I use a nice function here that's called seal.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1647s",
        "start_time": "1647.39"
    },
    {
        "id": "d2e54424",
        "text": "and I use a nice function here that's called seal. So we'll seal this value.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1649s",
        "start_time": "1649.01"
    },
    {
        "id": "17e4dee5",
        "text": "here that's called seal. So we'll seal this value. And which basically means if we ever get something like 1.2 this value is going to be",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1652s",
        "start_time": "1652.439"
    },
    {
        "id": "39ec9073",
        "text": "So we'll seal this value. And which basically means if we ever get something like 1.2 this value is going to be uh two.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1655s",
        "start_time": "1655.189"
    },
    {
        "id": "41c74cc3",
        "text": "And which basically means if we ever get something like 1.2 this value is going to be uh two. And this is like how the NFC C like itself work, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1659s",
        "start_time": "1659.3"
    },
    {
        "id": "9b072919",
        "text": "uh two. And this is like how the NFC C like itself work, right? Um OK. So now let's take like this monster variable here. And let's specify here that here we want to uh let's write the comment first. So we want to store",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1665s",
        "start_time": "1665.68"
    },
    {
        "id": "3fc5e9af",
        "text": "And this is like how the NFC C like itself work, right? Um OK. So now let's take like this monster variable here. And let's specify here that here we want to uh let's write the comment first. So we want to store uh MFCC",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1667s",
        "start_time": "1667.849"
    },
    {
        "id": "65cdbd89",
        "text": "Um OK. So now let's take like this monster variable here. And let's specify here that here we want to uh let's write the comment first. So we want to store uh MFCC for segment, if it has the expected",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1672s",
        "start_time": "1672.349"
    },
    {
        "id": "cf8175ca",
        "text": "uh MFCC for segment, if it has the expected uh length, we could put, put it like this, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1689s",
        "start_time": "1689.17"
    },
    {
        "id": "18f9f108",
        "text": "for segment, if it has the expected uh length, we could put, put it like this, right? And so we can't say",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1692s",
        "start_time": "1692.859"
    },
    {
        "id": "be0686d6",
        "text": "uh length, we could put, put it like this, right? And so we can't say um",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1701s",
        "start_time": "1701.04"
    },
    {
        "id": "a61c20bc",
        "text": "And so we can't say um so if",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1705s",
        "start_time": "1705.04"
    },
    {
        "id": "ef0e68f9",
        "text": "um so if uh and so here we should say if the length",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1707s",
        "start_time": "1707.9"
    },
    {
        "id": "9d044368",
        "text": "so if uh and so here we should say if the length of the",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1709s",
        "start_time": "1709.92"
    },
    {
        "id": "17e59489",
        "text": "uh and so here we should say if the length of the MFC",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1715s",
        "start_time": "1715.369"
    },
    {
        "id": "e2a9d558",
        "text": "of the MFC is equal to this expected, then",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1721s",
        "start_time": "1721.27"
    },
    {
        "id": "e29e008f",
        "text": "MFC is equal to this expected, then we can do some stuff. But now, obviously we need to have like this MFCC value over here uh in order to uh like do some logic with it. So we need just to, to, to bring that up.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1724s",
        "start_time": "1724.3"
    },
    {
        "id": "0eddcbe2",
        "text": "is equal to this expected, then we can do some stuff. But now, obviously we need to have like this MFCC value over here uh in order to uh like do some logic with it. So we need just to, to, to bring that up. And so here we store uh like these values only if like the, the, we have like the expected number of like MFCC vectors in each segment, right. OK. So how do we do that? Well, we should do",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1728s",
        "start_time": "1728.52"
    },
    {
        "id": "136d2f3c",
        "text": "we can do some stuff. But now, obviously we need to have like this MFCC value over here uh in order to uh like do some logic with it. So we need just to, to, to bring that up. And so here we store uh like these values only if like the, the, we have like the expected number of like MFCC vectors in each segment, right. OK. So how do we do that? Well, we should do so we here, we should take the data and we'll take the MS CC and we'll do a, an append and we'll pass in the uh MFCC. But now we can just pass in the MS CC because this is, this is an NP array and we need to uh like cast that to a list because otherwise we're not gonna be able to save it uh as adjacent file.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1734s",
        "start_time": "1734.969"
    },
    {
        "id": "5fc6aa16",
        "text": "And so here we store uh like these values only if like the, the, we have like the expected number of like MFCC vectors in each segment, right. OK. So how do we do that? Well, we should do so we here, we should take the data and we'll take the MS CC and we'll do a, an append and we'll pass in the uh MFCC. But now we can just pass in the MS CC because this is, this is an NP array and we need to uh like cast that to a list because otherwise we're not gonna be able to save it uh as adjacent file. And uh then we also want to save the labels. So",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1748s",
        "start_time": "1748.459"
    },
    {
        "id": "d42e800e",
        "text": "so we here, we should take the data and we'll take the MS CC and we'll do a, an append and we'll pass in the uh MFCC. But now we can just pass in the MS CC because this is, this is an NP array and we need to uh like cast that to a list because otherwise we're not gonna be able to save it uh as adjacent file. And uh then we also want to save the labels. So data labels and here A P and here the labels is gonna be I minus one. So do you guys remember I, and it was over here and this is like why we use a numerate like in the first place, right? Because at each iteration, we are in a uh different uh genre folder like at this like higher level, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1766s",
        "start_time": "1766.739"
    },
    {
        "id": "8a4dac9e",
        "text": "And uh then we also want to save the labels. So data labels and here A P and here the labels is gonna be I minus one. So do you guys remember I, and it was over here and this is like why we use a numerate like in the first place, right? Because at each iteration, we are in a uh different uh genre folder like at this like higher level, right? And so we can associate uh a value which is equal to the count of the iterations we are in to each genre. But remember the first iteration was for the, the, the data set path itself. So we're ignoring that. So that's why we, we need to uh um do like a subtraction with minus one, right? OK.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1790s",
        "start_time": "1790.78"
    },
    {
        "id": "366ab228",
        "text": "data labels and here A P and here the labels is gonna be I minus one. So do you guys remember I, and it was over here and this is like why we use a numerate like in the first place, right? Because at each iteration, we are in a uh different uh genre folder like at this like higher level, right? And so we can associate uh a value which is equal to the count of the iterations we are in to each genre. But remember the first iteration was for the, the, the data set path itself. So we're ignoring that. So that's why we, we need to uh um do like a subtraction with minus one, right? OK. And so by doing this, what we are doing is storing the MC C and labels for each uh segment which is great. And uh if you, if we look at this basically here uh at the end of this uh quite big uh like four loop with many nested loops. We are basically gonna have like the, the mappings. So we're gonna have like all the genre um",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1797s",
        "start_time": "1797.54"
    },
    {
        "id": "e39171aa",
        "text": "And so we can associate uh a value which is equal to the count of the iterations we are in to each genre. But remember the first iteration was for the, the, the data set path itself. So we're ignoring that. So that's why we, we need to uh um do like a subtraction with minus one, right? OK. And so by doing this, what we are doing is storing the MC C and labels for each uh segment which is great. And uh if you, if we look at this basically here uh at the end of this uh quite big uh like four loop with many nested loops. We are basically gonna have like the, the mappings. So we're gonna have like all the genre um uh labels here in the mapping. Then we are gonna have like this MF CCS for each segment and the labels for each uh segment as a number, right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1827s",
        "start_time": "1827.3"
    },
    {
        "id": "588913c9",
        "text": "And so by doing this, what we are doing is storing the MC C and labels for each uh segment which is great. And uh if you, if we look at this basically here uh at the end of this uh quite big uh like four loop with many nested loops. We are basically gonna have like the, the mappings. So we're gonna have like all the genre um uh labels here in the mapping. Then we are gonna have like this MF CCS for each segment and the labels for each uh segment as a number, right? Uh Cool. OK. So what, what I want to do here uh is to do a print and so we can do a print like this And we could say um yeah, we could just like put the, the file name here and uh we'll put the, the segment here",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1851s",
        "start_time": "1851.599"
    },
    {
        "id": "61d9b0eb",
        "text": "uh labels here in the mapping. Then we are gonna have like this MF CCS for each segment and the labels for each uh segment as a number, right? Uh Cool. OK. So what, what I want to do here uh is to do a print and so we can do a print like this And we could say um yeah, we could just like put the, the file name here and uh we'll put the, the segment here and here we'll do a dot format. And um here this, this should be f the, the name like or the, well,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1879s",
        "start_time": "1879.189"
    },
    {
        "id": "f768b33d",
        "text": "Uh Cool. OK. So what, what I want to do here uh is to do a print and so we can do a print like this And we could say um yeah, we could just like put the, the file name here and uh we'll put the, the segment here and here we'll do a dot format. And um here this, this should be f the, the name like or the, well, we could put in like file path, right? So it's the whole file path. And then here we also like specify which segment we are processing. Cool. And then I wanted to do like also like another print uh at this",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1889s",
        "start_time": "1889.229"
    },
    {
        "id": "32fd62dd",
        "text": "and here we'll do a dot format. And um here this, this should be f the, the name like or the, well, we could put in like file path, right? So it's the whole file path. And then here we also like specify which segment we are processing. Cool. And then I wanted to do like also like another print uh at this level here.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1908s",
        "start_time": "1908.719"
    },
    {
        "id": "d500fd38",
        "text": "we could put in like file path, right? So it's the whole file path. And then here we also like specify which segment we are processing. Cool. And then I wanted to do like also like another print uh at this level here. So here, if you guys remember, we are at the level of the, of the genre of the folder. And so uh here we could do a uh print where we say uh say",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1920s",
        "start_time": "1920.609"
    },
    {
        "id": "f538ff0b",
        "text": "level here. So here, if you guys remember, we are at the level of the, of the genre of the folder. And so uh here we could do a uh print where we say uh say we, we do a new line and then we say uh processing",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1937s",
        "start_time": "1937.16"
    },
    {
        "id": "a9dbe63f",
        "text": "So here, if you guys remember, we are at the level of the, of the genre of the folder. And so uh here we could do a uh print where we say uh say we, we do a new line and then we say uh processing and we'll pass in uh the semantic label. So here we'll, we'll get like processing blues processing classical. So just like to, to keep track of this when we are running the script, right? OK. So now we have uh all we need uh uh to store like all the, the training data in our dictionary. Now, the next step and the final step, uh it's that of",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1940s",
        "start_time": "1940.28"
    },
    {
        "id": "a5f9b6db",
        "text": "we, we do a new line and then we say uh processing and we'll pass in uh the semantic label. So here we'll, we'll get like processing blues processing classical. So just like to, to keep track of this when we are running the script, right? OK. So now we have uh all we need uh uh to store like all the, the training data in our dictionary. Now, the next step and the final step, uh it's that of saving",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1953s",
        "start_time": "1953.619"
    },
    {
        "id": "938bf28c",
        "text": "and we'll pass in uh the semantic label. So here we'll, we'll get like processing blues processing classical. So just like to, to keep track of this when we are running the script, right? OK. So now we have uh all we need uh uh to store like all the, the training data in our dictionary. Now, the next step and the final step, uh it's that of saving everything",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1958s",
        "start_time": "1958.969"
    },
    {
        "id": "d1868c14",
        "text": "saving everything as a uh as a Jason file. So, what we'll do here is with, we'll do uh an open and here we'll pass in the Jason path and we'll uh open like this file to, to write basically how to create this file and we'll do an as",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1986s",
        "start_time": "1986.969"
    },
    {
        "id": "0951b1f4",
        "text": "everything as a uh as a Jason file. So, what we'll do here is with, we'll do uh an open and here we'll pass in the Jason path and we'll uh open like this file to, to write basically how to create this file and we'll do an as FP.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1989s",
        "start_time": "1989.089"
    },
    {
        "id": "9c36e104",
        "text": "as a uh as a Jason file. So, what we'll do here is with, we'll do uh an open and here we'll pass in the Jason path and we'll uh open like this file to, to write basically how to create this file and we'll do an as FP. And now",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=1990s",
        "start_time": "1990.64"
    },
    {
        "id": "0f1ba583",
        "text": "FP. And now here, what we want to do is a Jason",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2012s",
        "start_time": "2012.729"
    },
    {
        "id": "5c4ffe7d",
        "text": "And now here, what we want to do is a Jason dots uh dump.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2014s",
        "start_time": "2014.459"
    },
    {
        "id": "908a4a4b",
        "text": "here, what we want to do is a Jason dots uh dump. Now, we don't have Jason here and",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2016s",
        "start_time": "2016.14"
    },
    {
        "id": "378c198f",
        "text": "dots uh dump. Now, we don't have Jason here and we need to import it. So we'll do an import",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2020s",
        "start_time": "2020.4"
    },
    {
        "id": "d974afcc",
        "text": "Now, we don't have Jason here and we need to import it. So we'll do an import Jason there. OK? So we'll do Jason uh uh dot uh dump and then we'll pass in the data. So our dictionary then uh we'll say that we want to like, write the dictionary here like in this file",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2023s",
        "start_time": "2023.77"
    },
    {
        "id": "fc44f644",
        "text": "we need to import it. So we'll do an import Jason there. OK? So we'll do Jason uh uh dot uh dump and then we'll pass in the data. So our dictionary then uh we'll say that we want to like, write the dictionary here like in this file and then we'll pass in a nice um argument which is the indent. So we want to like the do like a, a four indent uh like four edge thing that we are writing to this file so that it becomes like more readable. Nice. So now we have like the whole, the whole function that's I wanna be able like to, to save everything.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2028s",
        "start_time": "2028.369"
    },
    {
        "id": "b8b18762",
        "text": "Jason there. OK? So we'll do Jason uh uh dot uh dump and then we'll pass in the data. So our dictionary then uh we'll say that we want to like, write the dictionary here like in this file and then we'll pass in a nice um argument which is the indent. So we want to like the do like a, a four indent uh like four edge thing that we are writing to this file so that it becomes like more readable. Nice. So now we have like the whole, the whole function that's I wanna be able like to, to save everything. So what remains to do is just that to run it. So we'll do as usual and if name is equal to main,",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2033s",
        "start_time": "2033.969"
    },
    {
        "id": "303847c1",
        "text": "and then we'll pass in a nice um argument which is the indent. So we want to like the do like a, a four indent uh like four edge thing that we are writing to this file so that it becomes like more readable. Nice. So now we have like the whole, the whole function that's I wanna be able like to, to save everything. So what remains to do is just that to run it. So we'll do as usual and if name is equal to main, then we'll do a safe MFCC and then we'll pass in uh the data set path, but not this one, we'll pass in this and we'll pass in the, the Jason path. And now let's say that we're gonna have, I don't know. Yeah, we, we could say like 10 segments, for example. Right?",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2051s",
        "start_time": "2051.52"
    },
    {
        "id": "a6d54b8b",
        "text": "So what remains to do is just that to run it. So we'll do as usual and if name is equal to main, then we'll do a safe MFCC and then we'll pass in uh the data set path, but not this one, we'll pass in this and we'll pass in the, the Jason path. And now let's say that we're gonna have, I don't know. Yeah, we, we could say like 10 segments, for example. Right? Uh OK. So now everything should be in place and now let's see if this works, if there are no mistakes. So, yeah, it's working nice. And so",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2075s",
        "start_time": "2075.239"
    },
    {
        "id": "0f2a0dc9",
        "text": "then we'll do a safe MFCC and then we'll pass in uh the data set path, but not this one, we'll pass in this and we'll pass in the, the Jason path. And now let's say that we're gonna have, I don't know. Yeah, we, we could say like 10 segments, for example. Right? Uh OK. So now everything should be in place and now let's see if this works, if there are no mistakes. So, yeah, it's working nice. And so we basically went through all the different genres. So we processed uh disco and as you can see here, we, we got like this file and we segmented it into uh like 10 different segments. Well, there's a, there's a minor mistake here. So it says segment 01234. Yeah. So to avoid having that and, and starting from one, we do here in the print and S plus one. But for the rest, like it's all good.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2085s",
        "start_time": "2085.638"
    },
    {
        "id": "7b3b0df5",
        "text": "Uh OK. So now everything should be in place and now let's see if this works, if there are no mistakes. So, yeah, it's working nice. And so we basically went through all the different genres. So we processed uh disco and as you can see here, we, we got like this file and we segmented it into uh like 10 different segments. Well, there's a, there's a minor mistake here. So it says segment 01234. Yeah. So to avoid having that and, and starting from one, we do here in the print and S plus one. But for the rest, like it's all good. So as you can see here, so then we are processing reggae rock pop. OK? So now let's take a look at the results of this.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2111s",
        "start_time": "2111.489"
    },
    {
        "id": "e718948d",
        "text": "we basically went through all the different genres. So we processed uh disco and as you can see here, we, we got like this file and we segmented it into uh like 10 different segments. Well, there's a, there's a minor mistake here. So it says segment 01234. Yeah. So to avoid having that and, and starting from one, we do here in the print and S plus one. But for the rest, like it's all good. So as you can see here, so then we are processing reggae rock pop. OK? So now let's take a look at the results of this. So we're doing that. Uh We'll see that here in our current uh working uh directory. Uh We have this data dot Jason, which is like this new file that we've just built and let's take a look at it. Nice.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2128s",
        "start_time": "2128.179"
    },
    {
        "id": "b9e19372",
        "text": "So as you can see here, so then we are processing reggae rock pop. OK? So now let's take a look at the results of this. So we're doing that. Uh We'll see that here in our current uh working uh directory. Uh We have this data dot Jason, which is like this new file that we've just built and let's take a look at it. Nice. OK. So, and as you can see here, we have the mapping and the mapping is given by these guys. So disco, reggae rock and as I said, disco is gonna be equal to zero, reggae equal to one and so on and so forth. Then we have MF CCS and we have like a bunch of values down here. And then uh as you can see down here, we should have like all the labels",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2156s",
        "start_time": "2156.709"
    },
    {
        "id": "d106a276",
        "text": "So we're doing that. Uh We'll see that here in our current uh working uh directory. Uh We have this data dot Jason, which is like this new file that we've just built and let's take a look at it. Nice. OK. So, and as you can see here, we have the mapping and the mapping is given by these guys. So disco, reggae rock and as I said, disco is gonna be equal to zero, reggae equal to one and so on and so forth. Then we have MF CCS and we have like a bunch of values down here. And then uh as you can see down here, we should have like all the labels and the labels are correct because we expect uh 10 zeros, then 10 ones, 10 twos and so on and so forth. Cool. This is great news because now we have our Jason file uh with all the uh all the training data.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2166s",
        "start_time": "2166.61"
    },
    {
        "id": "8cdc0297",
        "text": "OK. So, and as you can see here, we have the mapping and the mapping is given by these guys. So disco, reggae rock and as I said, disco is gonna be equal to zero, reggae equal to one and so on and so forth. Then we have MF CCS and we have like a bunch of values down here. And then uh as you can see down here, we should have like all the labels and the labels are correct because we expect uh 10 zeros, then 10 ones, 10 twos and so on and so forth. Cool. This is great news because now we have our Jason file uh with all the uh all the training data. So next time we are gonna use like this uh training data and we are going to build our network, our music genre classifier and we start with a with an M LP. So a multi perception and then down the line, we",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2184s",
        "start_time": "2184.979"
    },
    {
        "id": "ba3e72a7",
        "text": "and the labels are correct because we expect uh 10 zeros, then 10 ones, 10 twos and so on and so forth. Cool. This is great news because now we have our Jason file uh with all the uh all the training data. So next time we are gonna use like this uh training data and we are going to build our network, our music genre classifier and we start with a with an M LP. So a multi perception and then down the line, we upgrade that to a convolutional neural network. But uh this is it for today. I hope you've enjoyed the video. If that's the case, please remember to subscribe and definitely hit the notification bell. So you'll never miss a video and I'll see you next time. Cheers.",
        "video": "12- Music genre classification: Preparing the dataset",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "szyGiObZymo",
        "youtube_link": "https://www.youtube.com/watch?v=szyGiObZymo&t=2210s",
        "start_time": "2210.149"
    },
    {
        "id": "7e03acca",
        "text": "Hi, everybody and welcome to a new exciting video in the Deep Learning for Rode with Python series. This time we're gonna talk about competition in neural networks. I'm gonna introduce you the multi-layered perception, which is a simple neural network and we're gonna see how it works and the math behind it. So to get started, let's take a look at an old friend, the artificial neuron. So if you remember, we, we said that an artificial neuron is a computational unit that's able to process some information and do some transformation on it. Well, the question is, so if the artificial neuron is a computational unit, why do we need a neural network? Isn't an artificial neuron good enough by itself? Well, it turns out that if you want to work on quite complex problems, you want to scale up to networks of neurons or neural networks because a single neuron is really good if you are",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=0s",
        "start_time": "0.91"
    },
    {
        "id": "c338717f",
        "text": "if you remember, we, we said that an artificial neuron is a computational unit that's able to process some information and do some transformation on it. Well, the question is, so if the artificial neuron is a computational unit, why do we need a neural network? Isn't an artificial neuron good enough by itself? Well, it turns out that if you want to work on quite complex problems, you want to scale up to networks of neurons or neural networks because a single neuron is really good if you are dealing with linear problems. But if you're dealing with more complex problems like real world problems or where there's a lot of like non linearity in it. Well, a single neuron, it's not going to cut it. You're gonna need a network of neurons which work together.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=24s",
        "start_time": "24.575"
    },
    {
        "id": "c2b830f4",
        "text": "good enough by itself? Well, it turns out that if you want to work on quite complex problems, you want to scale up to networks of neurons or neural networks because a single neuron is really good if you are dealing with linear problems. But if you're dealing with more complex problems like real world problems or where there's a lot of like non linearity in it. Well, a single neuron, it's not going to cut it. You're gonna need a network of neurons which work together. And so artificial neural networks can reproduce highly nonlinear functions and can help us solve highly nonlinear problems.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=48s",
        "start_time": "48.549"
    },
    {
        "id": "361a5513",
        "text": "dealing with linear problems. But if you're dealing with more complex problems like real world problems or where there's a lot of like non linearity in it. Well, a single neuron, it's not going to cut it. You're gonna need a network of neurons which work together. And so artificial neural networks can reproduce highly nonlinear functions and can help us solve highly nonlinear problems. Cool. So let's take a look at the fundamental components of an artificial neuron network. So we have obviously the neurons and we've seen the math of a single neuron and then these neurons are organized together into layers and we have different types of layers. So we have the input layer which is the layer where we provide",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=66s",
        "start_time": "66.675"
    },
    {
        "id": "7a8dc2aa",
        "text": "And so artificial neural networks can reproduce highly nonlinear functions and can help us solve highly nonlinear problems. Cool. So let's take a look at the fundamental components of an artificial neuron network. So we have obviously the neurons and we've seen the math of a single neuron and then these neurons are organized together into layers and we have different types of layers. So we have the input layer which is the layer where we provide information signal into the network. First, then we have a bunch of hidden layers. And so these hidden layers are layers of neurons organized and which are hidden because they are just like at the center within like the topology of the neural network. And then we have an output layer which is the",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=85s",
        "start_time": "85.0"
    },
    {
        "id": "54916e23",
        "text": "Cool. So let's take a look at the fundamental components of an artificial neuron network. So we have obviously the neurons and we've seen the math of a single neuron and then these neurons are organized together into layers and we have different types of layers. So we have the input layer which is the layer where we provide information signal into the network. First, then we have a bunch of hidden layers. And so these hidden layers are layers of neurons organized and which are hidden because they are just like at the center within like the topology of the neural network. And then we have an output layer which is the layer that we use to get information out of the network. And this one which we like um this network that we have here also has a lot of weighted connections. And the weighted connections are these links here that we have among all of these neurons. And in this case, we have a fully",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=96s",
        "start_time": "96.989"
    },
    {
        "id": "5a771188",
        "text": "information signal into the network. First, then we have a bunch of hidden layers. And so these hidden layers are layers of neurons organized and which are hidden because they are just like at the center within like the topology of the neural network. And then we have an output layer which is the layer that we use to get information out of the network. And this one which we like um this network that we have here also has a lot of weighted connections. And the weighted connections are these links here that we have among all of these neurons. And in this case, we have a fully connected network because each neuron is connected to all other neurons in subsequent layers. As you can see it, for example, like this neuron here is connected to this neuron here, to this neuron here and to this neuron down here. So this is a fully connected neuron network.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=121s",
        "start_time": "121.319"
    },
    {
        "id": "c63fdc7a",
        "text": "layer that we use to get information out of the network. And this one which we like um this network that we have here also has a lot of weighted connections. And the weighted connections are these links here that we have among all of these neurons. And in this case, we have a fully connected network because each neuron is connected to all other neurons in subsequent layers. As you can see it, for example, like this neuron here is connected to this neuron here, to this neuron here and to this neuron down here. So this is a fully connected neuron network. And then another component that we saw already when we talked about uh artificial neurons are the activation functions. And these functions are fundamental to process the incoming information that gets into the neurons. So",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=145s",
        "start_time": "145.649"
    },
    {
        "id": "9ebfa58d",
        "text": "connected network because each neuron is connected to all other neurons in subsequent layers. As you can see it, for example, like this neuron here is connected to this neuron here, to this neuron here and to this neuron down here. So this is a fully connected neuron network. And then another component that we saw already when we talked about uh artificial neurons are the activation functions. And these functions are fundamental to process the incoming information that gets into the neurons. So a specific type of neural network uh which is like very, very common. And it was like one of the first neural networks that was somehow engineered back in the day, it's called the multi layer perception. Here. The the topology of the network is quite simple because you have a bunch of uh layers. And so you have an input layer, you have, you can have one or multiple",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=170s",
        "start_time": "170.19"
    },
    {
        "id": "9f59ef0b",
        "text": "And then another component that we saw already when we talked about uh artificial neurons are the activation functions. And these functions are fundamental to process the incoming information that gets into the neurons. So a specific type of neural network uh which is like very, very common. And it was like one of the first neural networks that was somehow engineered back in the day, it's called the multi layer perception. Here. The the topology of the network is quite simple because you have a bunch of uh layers. And so you have an input layer, you have, you can have one or multiple uh hidden layers and then you have an output layer. And the idea basically is that the information uh travels from left to right. And this is a type of feed forward network because the information, as I said",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=190s",
        "start_time": "190.44"
    },
    {
        "id": "5bace48c",
        "text": "a specific type of neural network uh which is like very, very common. And it was like one of the first neural networks that was somehow engineered back in the day, it's called the multi layer perception. Here. The the topology of the network is quite simple because you have a bunch of uh layers. And so you have an input layer, you have, you can have one or multiple uh hidden layers and then you have an output layer. And the idea basically is that the information uh travels from left to right. And this is a type of feed forward network because the information, as I said uh gets to the input layer and then travels all the way back to the output layer. So let's take a look at the competition in multi layer perception. So as we said, information gets in through the input layer gets fed into the network, then it travels all the way through the hidden layers. Here, we have only one. If we have multiple, then it travels through all of the um different hidden layers, one by one",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=208s",
        "start_time": "208.38"
    },
    {
        "id": "b0db9913",
        "text": "uh hidden layers and then you have an output layer. And the idea basically is that the information uh travels from left to right. And this is a type of feed forward network because the information, as I said uh gets to the input layer and then travels all the way back to the output layer. So let's take a look at the competition in multi layer perception. So as we said, information gets in through the input layer gets fed into the network, then it travels all the way through the hidden layers. Here, we have only one. If we have multiple, then it travels through all of the um different hidden layers, one by one and then it gets out through the output layer. So we have this left to right information uh processing",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=236s",
        "start_time": "236.71"
    },
    {
        "id": "ba4c7eed",
        "text": "uh gets to the input layer and then travels all the way back to the output layer. So let's take a look at the competition in multi layer perception. So as we said, information gets in through the input layer gets fed into the network, then it travels all the way through the hidden layers. Here, we have only one. If we have multiple, then it travels through all of the um different hidden layers, one by one and then it gets out through the output layer. So we have this left to right information uh processing right. So in uh the multi layer of perception, like the fundamental aspects of competition are the weights the net inputs, which are like the sum of the weighted inputs as we saw in the video about the artificial neurons. And then we have the activations which are those functions which transform the incoming information and produce",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=253s",
        "start_time": "253.75"
    },
    {
        "id": "53fd02f5",
        "text": "and then it gets out through the output layer. So we have this left to right information uh processing right. So in uh the multi layer of perception, like the fundamental aspects of competition are the weights the net inputs, which are like the sum of the weighted inputs as we saw in the video about the artificial neurons. And then we have the activations which are those functions which transform the incoming information and produce an output uh that the neurons provide to the next layer. So let's see them in context. So here we have our nice uh multi-layered perception neural network. And here, as you can see, we have this uppercase W one and uppercase W-2, these are the weights in the, in the case of W one,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=283s",
        "start_time": "283.739"
    },
    {
        "id": "a4f535fc",
        "text": "right. So in uh the multi layer of perception, like the fundamental aspects of competition are the weights the net inputs, which are like the sum of the weighted inputs as we saw in the video about the artificial neurons. And then we have the activations which are those functions which transform the incoming information and produce an output uh that the neurons provide to the next layer. So let's see them in context. So here we have our nice uh multi-layered perception neural network. And here, as you can see, we have this uppercase W one and uppercase W-2, these are the weights in the, in the case of W one, we have these are like the weights for all the connections between the uh inputs. So X one and X two and the hidden layer. So the S one S two and S A three neurons. And as you can see here, we can indicate these uh connections, these weights uh using lowercase W",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=294s",
        "start_time": "294.619"
    },
    {
        "id": "c3232e27",
        "text": "an output uh that the neurons provide to the next layer. So let's see them in context. So here we have our nice uh multi-layered perception neural network. And here, as you can see, we have this uppercase W one and uppercase W-2, these are the weights in the, in the case of W one, we have these are like the weights for all the connections between the uh inputs. So X one and X two and the hidden layer. So the S one S two and S A three neurons. And as you can see here, we can indicate these uh connections, these weights uh using lowercase W 11, lower case W 12 and so on and so forth. But how do we conveniently represent all of this information all of these ways? Well, we can use our old friends the matrices. So W one can be written in this case as a two by three metrics. So we have two rows and three columns here.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=322s",
        "start_time": "322.17"
    },
    {
        "id": "5abd0afd",
        "text": "we have these are like the weights for all the connections between the uh inputs. So X one and X two and the hidden layer. So the S one S two and S A three neurons. And as you can see here, we can indicate these uh connections, these weights uh using lowercase W 11, lower case W 12 and so on and so forth. But how do we conveniently represent all of this information all of these ways? Well, we can use our old friends the matrices. So W one can be written in this case as a two by three metrics. So we have two rows and three columns here. And W 11 represents the connection, the weight of the connection between the input X one and the uh first neuron in the hidden layer. So S one, then we have W 12 and that is the connection between X one and S two, W 13, it's the connection between X one and",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=348s",
        "start_time": "348.01"
    },
    {
        "id": "0cf08957",
        "text": "11, lower case W 12 and so on and so forth. But how do we conveniently represent all of this information all of these ways? Well, we can use our old friends the matrices. So W one can be written in this case as a two by three metrics. So we have two rows and three columns here. And W 11 represents the connection, the weight of the connection between the input X one and the uh first neuron in the hidden layer. So S one, then we have W 12 and that is the connection between X one and S two, W 13, it's the connection between X one and free. And as you can see here, we can look at these matrices. And if we consider only the rows, we're basically looking at all of the connections of one neuron from the initial uh layer into the second layer. Then if we look down here,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=370s",
        "start_time": "370.839"
    },
    {
        "id": "c636e7b4",
        "text": "And W 11 represents the connection, the weight of the connection between the input X one and the uh first neuron in the hidden layer. So S one, then we have W 12 and that is the connection between X one and S two, W 13, it's the connection between X one and free. And as you can see here, we can look at these matrices. And if we consider only the rows, we're basically looking at all of the connections of one neuron from the initial uh layer into the second layer. Then if we look down here, we'll see that uh on the second row, we have all the connections of X two with S one S two and S3. But now if we look at this matrix column wise, we can see that W 11 and W-2 1 are the incoming connections of S one with regard to X one and X two to basically all of the uh incoming inputs",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=398s",
        "start_time": "398.5"
    },
    {
        "id": "92215c2f",
        "text": "free. And as you can see here, we can look at these matrices. And if we consider only the rows, we're basically looking at all of the connections of one neuron from the initial uh layer into the second layer. Then if we look down here, we'll see that uh on the second row, we have all the connections of X two with S one S two and S3. But now if we look at this matrix column wise, we can see that W 11 and W-2 1 are the incoming connections of S one with regard to X one and X two to basically all of the uh incoming inputs good. So as you see here, uh for the first time metrics are very important for describing how neural networks work because it's a very convenient way of packaging up a lot of information about weights and a bunch of like other uh parameters as well. Cool.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=421s",
        "start_time": "421.065"
    },
    {
        "id": "e356c04a",
        "text": "we'll see that uh on the second row, we have all the connections of X two with S one S two and S3. But now if we look at this matrix column wise, we can see that W 11 and W-2 1 are the incoming connections of S one with regard to X one and X two to basically all of the uh incoming inputs good. So as you see here, uh for the first time metrics are very important for describing how neural networks work because it's a very convenient way of packaging up a lot of information about weights and a bunch of like other uh parameters as well. Cool. So we said weights are like the, the first very important element of a neural network. And specifically in this case of a multi layer perception, then we also have the net inputs. Now we saw the net inputs in the case of a single artificial neuron. So what happens uh the question is what happens with a layer of neurons then",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=444s",
        "start_time": "444.679"
    },
    {
        "id": "a5902622",
        "text": "good. So as you see here, uh for the first time metrics are very important for describing how neural networks work because it's a very convenient way of packaging up a lot of information about weights and a bunch of like other uh parameters as well. Cool. So we said weights are like the, the first very important element of a neural network. And specifically in this case of a multi layer perception, then we also have the net inputs. Now we saw the net inputs in the case of a single artificial neuron. So what happens uh the question is what happens with a layer of neurons then in order to get all the net inputs, we want to multiply in the case of H two, which is the net input for the second layer. For the hidden layer. Here, we need to do a matrix multiplication between the input vector and the weight uh one matrix. And this is like the",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=474s",
        "start_time": "474.69"
    },
    {
        "id": "3e4bdc8a",
        "text": "So we said weights are like the, the first very important element of a neural network. And specifically in this case of a multi layer perception, then we also have the net inputs. Now we saw the net inputs in the case of a single artificial neuron. So what happens uh the question is what happens with a layer of neurons then in order to get all the net inputs, we want to multiply in the case of H two, which is the net input for the second layer. For the hidden layer. Here, we need to do a matrix multiplication between the input vector and the weight uh one matrix. And this is like the uh we can rewrite this uh equation here like this. And so we have a row vector X one X two which is like this two guys here. So it's basically the input vector. And here we have our nice uh weight matrix. Now, if we perform the matrix multiplication there,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=497s",
        "start_time": "497.42"
    },
    {
        "id": "93c39213",
        "text": "in order to get all the net inputs, we want to multiply in the case of H two, which is the net input for the second layer. For the hidden layer. Here, we need to do a matrix multiplication between the input vector and the weight uh one matrix. And this is like the uh we can rewrite this uh equation here like this. And so we have a row vector X one X two which is like this two guys here. So it's basically the input vector. And here we have our nice uh weight matrix. Now, if we perform the matrix multiplication there, we get a row vector, a three dimensional row vector where the first element here X one, W one plus X 2 W-2 1 is the net input H one. So it's basically the net input for the first neuron uh S one. And then we have the second element here. So X one, W 12 plus X 2 W-2 2, it's,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=521s",
        "start_time": "521.158"
    },
    {
        "id": "d784e176",
        "text": "uh we can rewrite this uh equation here like this. And so we have a row vector X one X two which is like this two guys here. So it's basically the input vector. And here we have our nice uh weight matrix. Now, if we perform the matrix multiplication there, we get a row vector, a three dimensional row vector where the first element here X one, W one plus X 2 W-2 1 is the net input H one. So it's basically the net input for the first neuron uh S one. And then we have the second element here. So X one, W 12 plus X 2 W-2 2, it's, it corresponds to H two which is the net input for S two and same thing for H three here. That's given by this uh noise like formula down here. Now,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=546s",
        "start_time": "546.969"
    },
    {
        "id": "0a15197a",
        "text": "we get a row vector, a three dimensional row vector where the first element here X one, W one plus X 2 W-2 1 is the net input H one. So it's basically the net input for the first neuron uh S one. And then we have the second element here. So X one, W 12 plus X 2 W-2 2, it's, it corresponds to H two which is the net input for S two and same thing for H three here. That's given by this uh noise like formula down here. Now, if you don't know how to perform um matrix multiplications, don't worry, just go back to the previous video. Cos I went into some details into battery matrix operations. So just like refer back to that, I'm not gonna repeat myself. But if you're following it along uh until now, so you should know how to perform uh matrix multiplication span. Now.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=566s",
        "start_time": "566.7"
    },
    {
        "id": "28381015",
        "text": "it corresponds to H two which is the net input for S two and same thing for H three here. That's given by this uh noise like formula down here. Now, if you don't know how to perform um matrix multiplications, don't worry, just go back to the previous video. Cos I went into some details into battery matrix operations. So just like refer back to that, I'm not gonna repeat myself. But if you're following it along uh until now, so you should know how to perform uh matrix multiplication span. Now. Cool. So this was for net inputs. And then we have the activations. So if you remember when we studied the artificial neuron as an individual computational unit, we said that the artificial neuron does two things. The first part is performing the net input, the weighted sun",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=595s",
        "start_time": "595.919"
    },
    {
        "id": "dd36772a",
        "text": "if you don't know how to perform um matrix multiplications, don't worry, just go back to the previous video. Cos I went into some details into battery matrix operations. So just like refer back to that, I'm not gonna repeat myself. But if you're following it along uh until now, so you should know how to perform uh matrix multiplication span. Now. Cool. So this was for net inputs. And then we have the activations. So if you remember when we studied the artificial neuron as an individual computational unit, we said that the artificial neuron does two things. The first part is performing the net input, the weighted sun as we saw it. And then the second part is the activation itself, which is basically modulating the net input using a function. And we can easily rewrite the activation uh for all of the neurons in a layer by using like this simple formula here. So the activation",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=608s",
        "start_time": "608.46"
    },
    {
        "id": "8df466e9",
        "text": "Cool. So this was for net inputs. And then we have the activations. So if you remember when we studied the artificial neuron as an individual computational unit, we said that the artificial neuron does two things. The first part is performing the net input, the weighted sun as we saw it. And then the second part is the activation itself, which is basically modulating the net input using a function. And we can easily rewrite the activation uh for all of the neurons in a layer by using like this simple formula here. So the activation of the uh second layer here which is a vector because it is a uh a vector with like three values. So there's one activation for S 11 for S two and one for S3 is given by the activation function F which in this case is quite abstract.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=631s",
        "start_time": "631.219"
    },
    {
        "id": "364dc30c",
        "text": "as we saw it. And then the second part is the activation itself, which is basically modulating the net input using a function. And we can easily rewrite the activation uh for all of the neurons in a layer by using like this simple formula here. So the activation of the uh second layer here which is a vector because it is a uh a vector with like three values. So there's one activation for S 11 for S two and one for S3 is given by the activation function F which in this case is quite abstract. Uh But we'll see uh later on uh a, an example where we have a natural activation function which is the sigmoid function. And we want to pass H two to the net input for net input vector. For the second layer down here, we want to pass it to the activation function.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=654s",
        "start_time": "654.179"
    },
    {
        "id": "bb530bef",
        "text": "of the uh second layer here which is a vector because it is a uh a vector with like three values. So there's one activation for S 11 for S two and one for S3 is given by the activation function F which in this case is quite abstract. Uh But we'll see uh later on uh a, an example where we have a natural activation function which is the sigmoid function. And we want to pass H two to the net input for net input vector. For the second layer down here, we want to pass it to the activation function. So right, so let's just recap. So we have the basic um elements of a neural network uh for like its computations are the weights, the net inputs and the activations. So",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=679s",
        "start_time": "679.33"
    },
    {
        "id": "958ba17e",
        "text": "Uh But we'll see uh later on uh a, an example where we have a natural activation function which is the sigmoid function. And we want to pass H two to the net input for net input vector. For the second layer down here, we want to pass it to the activation function. So right, so let's just recap. So we have the basic um elements of a neural network uh for like its computations are the weights, the net inputs and the activations. So now let's take a look at the computation in a multi layer perception step by step using some kind of like abstract notation first and then we'll move on and see an example just like to clarify everything. So as we said, the information travels from left to right, so we start with X which is the input vector. So it's like these two values here, X one, X two. And then we,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=700s",
        "start_time": "700.02"
    },
    {
        "id": "b4ed979a",
        "text": "So right, so let's just recap. So we have the basic um elements of a neural network uh for like its computations are the weights, the net inputs and the activations. So now let's take a look at the computation in a multi layer perception step by step using some kind of like abstract notation first and then we'll move on and see an example just like to clarify everything. So as we said, the information travels from left to right, so we start with X which is the input vector. So it's like these two values here, X one, X two. And then we, the information uh the signal moves to the second layer and the second layer performs two things as we. So we have the net input first and then the activation. So the net input is given by the matrix multiplication between the vector input X and W one which is the",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=721s",
        "start_time": "721.02"
    },
    {
        "id": "1b921807",
        "text": "now let's take a look at the computation in a multi layer perception step by step using some kind of like abstract notation first and then we'll move on and see an example just like to clarify everything. So as we said, the information travels from left to right, so we start with X which is the input vector. So it's like these two values here, X one, X two. And then we, the information uh the signal moves to the second layer and the second layer performs two things as we. So we have the net input first and then the activation. So the net input is given by the matrix multiplication between the vector input X and W one which is the uh weight matrix for the connections between layer one and layer two. Then when we have H two, we can plug it into the activation function F and we get the activations vector for um the second layer which is basically the output of the second layer.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=736s",
        "start_time": "736.83"
    },
    {
        "id": "70e6b9f6",
        "text": "the information uh the signal moves to the second layer and the second layer performs two things as we. So we have the net input first and then the activation. So the net input is given by the matrix multiplication between the vector input X and W one which is the uh weight matrix for the connections between layer one and layer two. Then when we have H two, we can plug it into the activation function F and we get the activations vector for um the second layer which is basically the output of the second layer. Now we want to uh pass this information to the third layer which in our case is the output layer. So H three is again the net input. But this time for the third layer and we can cut,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=766s",
        "start_time": "766.38"
    },
    {
        "id": "a81a1a75",
        "text": "uh weight matrix for the connections between layer one and layer two. Then when we have H two, we can plug it into the activation function F and we get the activations vector for um the second layer which is basically the output of the second layer. Now we want to uh pass this information to the third layer which in our case is the output layer. So H three is again the net input. But this time for the third layer and we can cut, calculate that uh multiplying the activation vector for the second layer with W-2, which is the weight matrix for uh the connections between the second layer and the output layer. Now that we have H three,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=787s",
        "start_time": "787.219"
    },
    {
        "id": "3702e99f",
        "text": "Now we want to uh pass this information to the third layer which in our case is the output layer. So H three is again the net input. But this time for the third layer and we can cut, calculate that uh multiplying the activation vector for the second layer with W-2, which is the weight matrix for uh the connections between the second layer and the output layer. Now that we have H three, we can plug H three into the activation function F. And what we get is Y which is the output. Now, I know that this was like very abstract. But I wanted to give you an idea of like the not",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=809s",
        "start_time": "809.609"
    },
    {
        "id": "451db4a4",
        "text": "calculate that uh multiplying the activation vector for the second layer with W-2, which is the weight matrix for uh the connections between the second layer and the output layer. Now that we have H three, we can plug H three into the activation function F. And what we get is Y which is the output. Now, I know that this was like very abstract. But I wanted to give you an idea of like the not that we use to understanding to, to just like explain how the competition works in a multi layer perception. But now we look at an example so that we'll clarify out like all the things that we've studied so far.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=824s",
        "start_time": "824.7"
    },
    {
        "id": "a1437a6a",
        "text": "we can plug H three into the activation function F. And what we get is Y which is the output. Now, I know that this was like very abstract. But I wanted to give you an idea of like the not that we use to understanding to, to just like explain how the competition works in a multi layer perception. But now we look at an example so that we'll clarify out like all the things that we've studied so far. So here we have the very same uh network, but now we have a bunch of uh parameters in. so 0.8 here and one are the two inputs. And then we have all of these weights. So like 1.2 no, no 0.71 and so on. For uh the, the, the weights here, the, the, the W one matrix. And here we have",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=846s",
        "start_time": "846.14"
    },
    {
        "id": "bcb6bac3",
        "text": "that we use to understanding to, to just like explain how the competition works in a multi layer perception. But now we look at an example so that we'll clarify out like all the things that we've studied so far. So here we have the very same uh network, but now we have a bunch of uh parameters in. so 0.8 here and one are the two inputs. And then we have all of these weights. So like 1.2 no, no 0.71 and so on. For uh the, the, the weights here, the, the, the W one matrix. And here we have these weights for a W-2. So the, the weights relative to the connections between",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=861s",
        "start_time": "861.525"
    },
    {
        "id": "1c16ac66",
        "text": "So here we have the very same uh network, but now we have a bunch of uh parameters in. so 0.8 here and one are the two inputs. And then we have all of these weights. So like 1.2 no, no 0.71 and so on. For uh the, the, the weights here, the, the, the W one matrix. And here we have these weights for a W-2. So the, the weights relative to the connections between the second layer and the output layer. Now, as you can see here, once again, I want to say that this is a fully connected matrix uh sorry, fully connected neural network or multi layer perception in this case, uh because we have that uh all neurons are connected uh from a layer connected with all the other neurons in the subsequent layer, right? So let's get started. So the",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=877s",
        "start_time": "877.289"
    },
    {
        "id": "24a45f13",
        "text": "these weights for a W-2. So the, the weights relative to the connections between the second layer and the output layer. Now, as you can see here, once again, I want to say that this is a fully connected matrix uh sorry, fully connected neural network or multi layer perception in this case, uh because we have that uh all neurons are connected uh from a layer connected with all the other neurons in the subsequent layer, right? So let's get started. So the um input vector is just a row vector and in this case is its values are 0.8 and one. It's these two guys here which we can call X one and X two, right? So now what do we want? To do, we want to calculate um the uh the competition here in the second layer. And",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=904s",
        "start_time": "904.01"
    },
    {
        "id": "c66979fe",
        "text": "the second layer and the output layer. Now, as you can see here, once again, I want to say that this is a fully connected matrix uh sorry, fully connected neural network or multi layer perception in this case, uh because we have that uh all neurons are connected uh from a layer connected with all the other neurons in the subsequent layer, right? So let's get started. So the um input vector is just a row vector and in this case is its values are 0.8 and one. It's these two guys here which we can call X one and X two, right? So now what do we want? To do, we want to calculate um the uh the competition here in the second layer. And let's just like reiterate that. And the computation does two things there. So the first part is calculating the net input. And the second part is the activation which again is the output to the subsequent layer. And so in this case, H two is calculated through a matrix multiplication between the input and the um weight matrix one,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=911s",
        "start_time": "911.58"
    },
    {
        "id": "b94b57c5",
        "text": "um input vector is just a row vector and in this case is its values are 0.8 and one. It's these two guys here which we can call X one and X two, right? So now what do we want? To do, we want to calculate um the uh the competition here in the second layer. And let's just like reiterate that. And the computation does two things there. So the first part is calculating the net input. And the second part is the activation which again is the output to the subsequent layer. And so in this case, H two is calculated through a matrix multiplication between the input and the um weight matrix one, right. So now let's plug in the numbers. And so here we have uh the row vector which is the input vector here. So 0.8 and one. And here we have the uh weight matrix. So this guy here. So all of these weights organized in a weight matrix and now we can perform a matrix multiplication. So just like very briefly what we are basically doing here is we're",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=941s",
        "start_time": "941.159"
    },
    {
        "id": "dd9ea690",
        "text": "let's just like reiterate that. And the computation does two things there. So the first part is calculating the net input. And the second part is the activation which again is the output to the subsequent layer. And so in this case, H two is calculated through a matrix multiplication between the input and the um weight matrix one, right. So now let's plug in the numbers. And so here we have uh the row vector which is the input vector here. So 0.8 and one. And here we have the uh weight matrix. So this guy here. So all of these weights organized in a weight matrix and now we can perform a matrix multiplication. So just like very briefly what we are basically doing here is we're uh calculating the dot product between this row vector here and this column vector here. And here we have the results",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=964s",
        "start_time": "964.429"
    },
    {
        "id": "4f62e1a1",
        "text": "right. So now let's plug in the numbers. And so here we have uh the row vector which is the input vector here. So 0.8 and one. And here we have the uh weight matrix. So this guy here. So all of these weights organized in a weight matrix and now we can perform a matrix multiplication. So just like very briefly what we are basically doing here is we're uh calculating the dot product between this row vector here and this column vector here. And here we have the results uh for like the first neuron here. And then we shift and we have the results for the second neuron and the results for the third neuron. Now, if we run all of the math there, we come up with these results. And as you can see here, I've just like filled it into",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=990s",
        "start_time": "990.859"
    },
    {
        "id": "43148754",
        "text": "uh calculating the dot product between this row vector here and this column vector here. And here we have the results uh for like the first neuron here. And then we shift and we have the results for the second neuron and the results for the third neuron. Now, if we run all of the math there, we come up with these results. And as you can see here, I've just like filled it into the uh the graph uh the diagram here. And so 2.96 is the act the net input for the first neuron of the second layer 1.56 is the net input for the second neuron in the second layer. And 1.8 is the uh net input for the third neuron in the second layer, right? So we've done the net inputs. Now, what should we do?",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1019s",
        "start_time": "1019.799"
    },
    {
        "id": "b1888d03",
        "text": "uh for like the first neuron here. And then we shift and we have the results for the second neuron and the results for the third neuron. Now, if we run all of the math there, we come up with these results. And as you can see here, I've just like filled it into the uh the graph uh the diagram here. And so 2.96 is the act the net input for the first neuron of the second layer 1.56 is the net input for the second neuron in the second layer. And 1.8 is the uh net input for the third neuron in the second layer, right? So we've done the net inputs. Now, what should we do? You know it by now? It's the activations, right? So how do we calculate the activations? It's very, very simple. In this case, we're gonna use the sigmoid function. So the sigmoid function, we've already seen it in a previous e uh video on the artificial neurons. But it's quite simple. It's one, it's a function that it's, it's basically 1/1 plus the exponential to the minus X. And",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1028s",
        "start_time": "1028.77"
    },
    {
        "id": "ee3bbb4d",
        "text": "the uh the graph uh the diagram here. And so 2.96 is the act the net input for the first neuron of the second layer 1.56 is the net input for the second neuron in the second layer. And 1.8 is the uh net input for the third neuron in the second layer, right? So we've done the net inputs. Now, what should we do? You know it by now? It's the activations, right? So how do we calculate the activations? It's very, very simple. In this case, we're gonna use the sigmoid function. So the sigmoid function, we've already seen it in a previous e uh video on the artificial neurons. But it's quite simple. It's one, it's a function that it's, it's basically 1/1 plus the exponential to the minus X. And instead of X here we pass in",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1047s",
        "start_time": "1047.219"
    },
    {
        "id": "89a50d9b",
        "text": "You know it by now? It's the activations, right? So how do we calculate the activations? It's very, very simple. In this case, we're gonna use the sigmoid function. So the sigmoid function, we've already seen it in a previous e uh video on the artificial neurons. But it's quite simple. It's one, it's a function that it's, it's basically 1/1 plus the exponential to the minus X. And instead of X here we pass in H two. So we pass in all the net inputs. And if we run the math here we get these results. So we are basically passing",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1077s",
        "start_time": "1077.439"
    },
    {
        "id": "a2ae68e7",
        "text": "instead of X here we pass in H two. So we pass in all the net inputs. And if we run the math here we get these results. So we are basically passing the net inputs through the sigmoid function, which is our activation function of choice. And we get back uh back in a activation vector where each of these items in the vector corresponds to the activations of the neurons here in the second layer that we have no 0.95 no 0.83 no 0.86. OK,",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1107s",
        "start_time": "1107.31"
    },
    {
        "id": "05dd339f",
        "text": "H two. So we pass in all the net inputs. And if we run the math here we get these results. So we are basically passing the net inputs through the sigmoid function, which is our activation function of choice. And we get back uh back in a activation vector where each of these items in the vector corresponds to the activations of the neurons here in the second layer that we have no 0.95 no 0.83 no 0.86. OK, great. So we now have the output of the second layer and we are ready to process the third layer to get the net input. For the third layer. We do a matrix multiplication between the activations from the second layer and the weight matrix between for the connections between the second and the third layer. So",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1110s",
        "start_time": "1110.55"
    },
    {
        "id": "34bc7e73",
        "text": "the net inputs through the sigmoid function, which is our activation function of choice. And we get back uh back in a activation vector where each of these items in the vector corresponds to the activations of the neurons here in the second layer that we have no 0.95 no 0.83 no 0.86. OK, great. So we now have the output of the second layer and we are ready to process the third layer to get the net input. For the third layer. We do a matrix multiplication between the activations from the second layer and the weight matrix between for the connections between the second and the third layer. So let's plug in the numbers. And as you can see here, there's an interesting thing which is that the W-2 matrix is actually a column vector. So it's a free buy one matrix. And that's because we only have one output uh neuron. And so basically, we only have one incoming um a connection for each of the neurons in the second layer.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1123s",
        "start_time": "1123.689"
    },
    {
        "id": "1d6eb0fb",
        "text": "great. So we now have the output of the second layer and we are ready to process the third layer to get the net input. For the third layer. We do a matrix multiplication between the activations from the second layer and the weight matrix between for the connections between the second and the third layer. So let's plug in the numbers. And as you can see here, there's an interesting thing which is that the W-2 matrix is actually a column vector. So it's a free buy one matrix. And that's because we only have one output uh neuron. And so basically, we only have one incoming um a connection for each of the neurons in the second layer. So if we perform a dot product here, we end up with 2.99 which is the net input. And now if we want to get to the output",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1150s",
        "start_time": "1150.65"
    },
    {
        "id": "10cc9911",
        "text": "let's plug in the numbers. And as you can see here, there's an interesting thing which is that the W-2 matrix is actually a column vector. So it's a free buy one matrix. And that's because we only have one output uh neuron. And so basically, we only have one incoming um a connection for each of the neurons in the second layer. So if we perform a dot product here, we end up with 2.99 which is the net input. And now if we want to get to the output of all of this computation, we need to plug that number into the activation function, which in our case is the sigmoid function. So if we do that, basically we end up with no 0.95. And so this is the result of the computation of this uh neural network given all of this",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1174s",
        "start_time": "1174.145"
    },
    {
        "id": "b56a11f9",
        "text": "So if we perform a dot product here, we end up with 2.99 which is the net input. And now if we want to get to the output of all of this computation, we need to plug that number into the activation function, which in our case is the sigmoid function. So if we do that, basically we end up with no 0.95. And so this is the result of the computation of this uh neural network given all of this parameters. So right, no 0.4 95. And it's been like a quite long uh process. But at the same time, I think like it was quite instructive because you really want to wanted to understand how a neuron network like processes the information on a very detailed level. So before finishing off this video, I want to leave you with a few takeaway points. So",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1203s",
        "start_time": "1203.329"
    },
    {
        "id": "5edf2f39",
        "text": "of all of this computation, we need to plug that number into the activation function, which in our case is the sigmoid function. So if we do that, basically we end up with no 0.95. And so this is the result of the computation of this uh neural network given all of this parameters. So right, no 0.4 95. And it's been like a quite long uh process. But at the same time, I think like it was quite instructive because you really want to wanted to understand how a neuron network like processes the information on a very detailed level. So before finishing off this video, I want to leave you with a few takeaway points. So we said first of all that, we want to use artificial neural networks when we are dealing with complex problems or problems that have nonlinear solutions. Because uh the way artificial neural networks are arranged with all of these connections and all of this distributed structure with many neurons connected together is very good for reproducing reproducing",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1215s",
        "start_time": "1215.069"
    },
    {
        "id": "0d5b40c3",
        "text": "parameters. So right, no 0.4 95. And it's been like a quite long uh process. But at the same time, I think like it was quite instructive because you really want to wanted to understand how a neuron network like processes the information on a very detailed level. So before finishing off this video, I want to leave you with a few takeaway points. So we said first of all that, we want to use artificial neural networks when we are dealing with complex problems or problems that have nonlinear solutions. Because uh the way artificial neural networks are arranged with all of these connections and all of this distributed structure with many neurons connected together is very good for reproducing reproducing uh highly nonlinear functions. And as we saw throughout the video, the computation that happens in a neural network and in a multi layer perception in this case are distributed. So we have single neurons that do",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1240s",
        "start_time": "1240.079"
    },
    {
        "id": "52bc6677",
        "text": "we said first of all that, we want to use artificial neural networks when we are dealing with complex problems or problems that have nonlinear solutions. Because uh the way artificial neural networks are arranged with all of these connections and all of this distributed structure with many neurons connected together is very good for reproducing reproducing uh highly nonlinear functions. And as we saw throughout the video, the computation that happens in a neural network and in a multi layer perception in this case are distributed. So we have single neurons that do uh individual uh computations. And then all of this information is passed on to like other neurons. So another important thing that we saw here is that in a feed forward network, like a multi layer perception, the signal moves from left to right. And finally, uh I want just like",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1265s",
        "start_time": "1265.319"
    },
    {
        "id": "b209516e",
        "text": "uh highly nonlinear functions. And as we saw throughout the video, the computation that happens in a neural network and in a multi layer perception in this case are distributed. So we have single neurons that do uh individual uh computations. And then all of this information is passed on to like other neurons. So another important thing that we saw here is that in a feed forward network, like a multi layer perception, the signal moves from left to right. And finally, uh I want just like to reiterate the fact that the important elements of a computation in a neural network are the weights which are represented with matrices, the net inputs which are represented as row vectors and activations which again are represented as row vectors and are the outputs of a layer onto the next layer. Cool. So this was it for this video.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1291s",
        "start_time": "1291.93"
    },
    {
        "id": "cd2d29b5",
        "text": "uh individual uh computations. And then all of this information is passed on to like other neurons. So another important thing that we saw here is that in a feed forward network, like a multi layer perception, the signal moves from left to right. And finally, uh I want just like to reiterate the fact that the important elements of a computation in a neural network are the weights which are represented with matrices, the net inputs which are represented as row vectors and activations which again are represented as row vectors and are the outputs of a layer onto the next layer. Cool. So this was it for this video. And you may be wondering so what's up next? Well, we've done the theory behind um multi layer perceptions and neural networks. So now uh we are going to have the fun part which is coding a multi layer perception from scratch only using Python.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1309s",
        "start_time": "1309.0"
    },
    {
        "id": "389956e6",
        "text": "to reiterate the fact that the important elements of a computation in a neural network are the weights which are represented with matrices, the net inputs which are represented as row vectors and activations which again are represented as row vectors and are the outputs of a layer onto the next layer. Cool. So this was it for this video. And you may be wondering so what's up next? Well, we've done the theory behind um multi layer perceptions and neural networks. So now uh we are going to have the fun part which is coding a multi layer perception from scratch only using Python. Cool. So I hope you enjoyed the video if that's the case just uh leave a like you can subscribe and if you have any questions, please feel free to leave a comment in the comments section below and that's it for now and I hope I'll see you next time. Cheers.",
        "video": "5-  Computation in neural networks",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "QUCzvlgvk6I",
        "youtube_link": "https://www.youtube.com/watch?v=QUCzvlgvk6I&t=1331s",
        "start_time": "1331.614"
    },
    {
        "id": "64f029d5",
        "text": "Hi, everybody and welcome to a new video in the Deep Learning for Radio with Python series. This time we're gonna talk about training a neural network. And specifically, we're gonna look at the theory behind it. So we'll look at two very important algorithms backward propagation and gradient the sound cool. So let's get started with the idea of training a neural network. So the high level overview. So what do we do when we train a network? Well, we basically trick all the weights of the connections among different neurons so that we can have predictions that are really good. So how do we do that? Well, we basically uh feed some training data. So inputs and target values to the network, then we look at the predictions and we calculate the error between the predictions and the expected outcome. And then we use that information for iteratively adjusting the weights I know like this feels like very high level. So we're gonna now look at how that's done precisely cool. So again, from a quite high level overview, there are two main steps here. So when we train a network, we pass a sample, an example",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=0s",
        "start_time": "0.0"
    },
    {
        "id": "a0ae13fb",
        "text": "So what do we do when we train a network? Well, we basically trick all the weights of the connections among different neurons so that we can have predictions that are really good. So how do we do that? Well, we basically uh feed some training data. So inputs and target values to the network, then we look at the predictions and we calculate the error between the predictions and the expected outcome. And then we use that information for iteratively adjusting the weights I know like this feels like very high level. So we're gonna now look at how that's done precisely cool. So again, from a quite high level overview, there are two main steps here. So when we train a network, we pass a sample, an example uh input and that input uh does we use it like for doing two things? So we have the input traveling all the way through to the rightmost uh layer and we get the prediction there. So we have a feed forward propagation forward pass",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=25s",
        "start_time": "25.049"
    },
    {
        "id": "9e91fe9c",
        "text": "And then we use that information for iteratively adjusting the weights I know like this feels like very high level. So we're gonna now look at how that's done precisely cool. So again, from a quite high level overview, there are two main steps here. So when we train a network, we pass a sample, an example uh input and that input uh does we use it like for doing two things? So we have the input traveling all the way through to the rightmost uh layer and we get the prediction there. So we have a feed forward propagation forward pass and then we calculate an error, we use that information and we back propagate that signal error signal back to the uh initial layers. So from right to left cool. So now let's break down this two big steps. So forward propagation and back propagation into it's like subst steps. So",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=53s",
        "start_time": "53.389"
    },
    {
        "id": "c6005b6b",
        "text": "uh input and that input uh does we use it like for doing two things? So we have the input traveling all the way through to the rightmost uh layer and we get the prediction there. So we have a feed forward propagation forward pass and then we calculate an error, we use that information and we back propagate that signal error signal back to the uh initial layers. So from right to left cool. So now let's break down this two big steps. So forward propagation and back propagation into it's like subst steps. So when we have this forward propagation, what we do basically is we get the prediction and in the end we calculate the error,",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=79s",
        "start_time": "79.819"
    },
    {
        "id": "0b930b21",
        "text": "and then we calculate an error, we use that information and we back propagate that signal error signal back to the uh initial layers. So from right to left cool. So now let's break down this two big steps. So forward propagation and back propagation into it's like subst steps. So when we have this forward propagation, what we do basically is we get the prediction and in the end we calculate the error, then when we have the uh error traveling back. So the back propagation side, we do two things. We calculate the gradient of the error function uh with regard to the weight. And we use that information to update the uh parameters because the gradient is gonna tell us in which direction we should go",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=101s",
        "start_time": "101.11"
    },
    {
        "id": "992341d4",
        "text": "when we have this forward propagation, what we do basically is we get the prediction and in the end we calculate the error, then when we have the uh error traveling back. So the back propagation side, we do two things. We calculate the gradient of the error function uh with regard to the weight. And we use that information to update the uh parameters because the gradient is gonna tell us in which direction we should go uh in which direction we should treat the weights in order to improve the predictions. Cool. So now what I'm gonna do is I'm gonna go one by one through all of these steps and explain them and explain the math behind it. So get prediction.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=126s",
        "start_time": "126.94"
    },
    {
        "id": "64533201",
        "text": "then when we have the uh error traveling back. So the back propagation side, we do two things. We calculate the gradient of the error function uh with regard to the weight. And we use that information to update the uh parameters because the gradient is gonna tell us in which direction we should go uh in which direction we should treat the weights in order to improve the predictions. Cool. So now what I'm gonna do is I'm gonna go one by one through all of these steps and explain them and explain the math behind it. So get prediction. Uh we, we saw this already. So uh we have a previous video where I went through uh how to calculate uh competition in neural networks. So if you're interested in the specifics of this just go",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=135s",
        "start_time": "135.289"
    },
    {
        "id": "0faf66e6",
        "text": "uh in which direction we should treat the weights in order to improve the predictions. Cool. So now what I'm gonna do is I'm gonna go one by one through all of these steps and explain them and explain the math behind it. So get prediction. Uh we, we saw this already. So uh we have a previous video where I went through uh how to calculate uh competition in neural networks. So if you're interested in the specifics of this just go uh back to that one and watch that video cool. But in a nutshell, we have inputs, these inputs get propagated to uh uh the subsequent layers like on um the right and and then until they arrive like at the last layer where we get a prediction cool. So then we have the next step which is calculating the error. And for calculating the error, we need an error function.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=160s",
        "start_time": "160.559"
    },
    {
        "id": "c1473b7c",
        "text": "Uh we, we saw this already. So uh we have a previous video where I went through uh how to calculate uh competition in neural networks. So if you're interested in the specifics of this just go uh back to that one and watch that video cool. But in a nutshell, we have inputs, these inputs get propagated to uh uh the subsequent layers like on um the right and and then until they arrive like at the last layer where we get a prediction cool. So then we have the next step which is calculating the error. And for calculating the error, we need an error function. Uh The error function is a function of two variables. So the prediction itself and why, which in this case indicates the expected uh outcome. Cool.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=180s",
        "start_time": "180.35"
    },
    {
        "id": "32682620",
        "text": "uh back to that one and watch that video cool. But in a nutshell, we have inputs, these inputs get propagated to uh uh the subsequent layers like on um the right and and then until they arrive like at the last layer where we get a prediction cool. So then we have the next step which is calculating the error. And for calculating the error, we need an error function. Uh The error function is a function of two variables. So the prediction itself and why, which in this case indicates the expected uh outcome. Cool. So uh be aware that many people instead of using the expression error function, use loss function. So you can use them like interchangeably I prefer using error because I feel like it's more like it's more it's clear like to understand what we're talking about. Cool. So we've calculated the error. So now we need to go to the next step.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=194s",
        "start_time": "194.24"
    },
    {
        "id": "223a756c",
        "text": "Uh The error function is a function of two variables. So the prediction itself and why, which in this case indicates the expected uh outcome. Cool. So uh be aware that many people instead of using the expression error function, use loss function. So you can use them like interchangeably I prefer using error because I feel like it's more like it's more it's clear like to understand what we're talking about. Cool. So we've calculated the error. So now we need to go to the next step. But before we go to the next step, I wanted to mention that like in this example, we are using the um quadratic uh error function which is given by this nice formula over here. Nice. OK. So now we can move to the next step which is calculating the gradient of the error function. So what we want to do is calculate the derivative of e of the error function with regard to",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=220s",
        "start_time": "220.899"
    },
    {
        "id": "223149a9",
        "text": "So uh be aware that many people instead of using the expression error function, use loss function. So you can use them like interchangeably I prefer using error because I feel like it's more like it's more it's clear like to understand what we're talking about. Cool. So we've calculated the error. So now we need to go to the next step. But before we go to the next step, I wanted to mention that like in this example, we are using the um quadratic uh error function which is given by this nice formula over here. Nice. OK. So now we can move to the next step which is calculating the gradient of the error function. So what we want to do is calculate the derivative of e of the error function with regard to uh w so all the weights. So, and if you guys remember W 1 W-2, these are the weight um uh mattresses. And so these are the weights for the connections between like different neurons in different layers. Nice.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=234s",
        "start_time": "234.07"
    },
    {
        "id": "c8faaa30",
        "text": "But before we go to the next step, I wanted to mention that like in this example, we are using the um quadratic uh error function which is given by this nice formula over here. Nice. OK. So now we can move to the next step which is calculating the gradient of the error function. So what we want to do is calculate the derivative of e of the error function with regard to uh w so all the weights. So, and if you guys remember W 1 W-2, these are the weight um uh mattresses. And so these are the weights for the connections between like different neurons in different layers. Nice. Cool. So, but how do we do that? Well, first, we should think of the uh of a neural network as a very complex function uh which is dependent on two variables. So the XS which are like this input vector here. Uh X so the inputs and W so all the weights. So",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=257s",
        "start_time": "257.399"
    },
    {
        "id": "aea30678",
        "text": "uh w so all the weights. So, and if you guys remember W 1 W-2, these are the weight um uh mattresses. And so these are the weights for the connections between like different neurons in different layers. Nice. Cool. So, but how do we do that? Well, first, we should think of the uh of a neural network as a very complex function uh which is dependent on two variables. So the XS which are like this input vector here. Uh X so the inputs and W so all the weights. So if you remember the error function is a function of P and Y so P being the prediction, but the prediction obviously is a function is the results of the um neural network. So we can substitute this F of XW uh for P. And by doing so we find out that E",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=285s",
        "start_time": "285.32"
    },
    {
        "id": "c47e149c",
        "text": "Cool. So, but how do we do that? Well, first, we should think of the uh of a neural network as a very complex function uh which is dependent on two variables. So the XS which are like this input vector here. Uh X so the inputs and W so all the weights. So if you remember the error function is a function of P and Y so P being the prediction, but the prediction obviously is a function is the results of the um neural network. So we can substitute this F of XW uh for P. And by doing so we find out that E the error function is a function of the weight itself. So it makes sense to calculate the, the gradient of the error function of the weights because it depends on the weight cool. So having like all of this knowledge we can move on and just like review all the elements of a neural network. I hope by now you're very familiar with all of this like elements. But if you're not, I'll give you a quick overview. So we have the",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=300s",
        "start_time": "300.22"
    },
    {
        "id": "fec0e82a",
        "text": "if you remember the error function is a function of P and Y so P being the prediction, but the prediction obviously is a function is the results of the um neural network. So we can substitute this F of XW uh for P. And by doing so we find out that E the error function is a function of the weight itself. So it makes sense to calculate the, the gradient of the error function of the weights because it depends on the weight cool. So having like all of this knowledge we can move on and just like review all the elements of a neural network. I hope by now you're very familiar with all of this like elements. But if you're not, I'll give you a quick overview. So we have the axis which are obviously the inputs, then we have like the activations. A one is equal to the inputs. W one we show this, it's the, the weights for the first layer H two is the net input vector.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=321s",
        "start_time": "321.079"
    },
    {
        "id": "7ad13e8e",
        "text": "the error function is a function of the weight itself. So it makes sense to calculate the, the gradient of the error function of the weights because it depends on the weight cool. So having like all of this knowledge we can move on and just like review all the elements of a neural network. I hope by now you're very familiar with all of this like elements. But if you're not, I'll give you a quick overview. So we have the axis which are obviously the inputs, then we have like the activations. A one is equal to the inputs. W one we show this, it's the, the weights for the first layer H two is the net input vector. So it's all the net inputs for layer two. And then we just repeat a 2 W-2 H three. And then finally, a three in this case, which is the prediction itself nice, we have this information we know about the error function and its dependencies. And so what we can do is we can calculate the derivative of E with respect to W-2.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=342s",
        "start_time": "342.38"
    },
    {
        "id": "69746409",
        "text": "axis which are obviously the inputs, then we have like the activations. A one is equal to the inputs. W one we show this, it's the, the weights for the first layer H two is the net input vector. So it's all the net inputs for layer two. And then we just repeat a 2 W-2 H three. And then finally, a three in this case, which is the prediction itself nice, we have this information we know about the error function and its dependencies. And so what we can do is we can calculate the derivative of E with respect to W-2. And we can use a very nice rule from calculus which I'm not gonna um review here, but it's called the chain rule. And if we use the chain rule, uh we can come up with this formula",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=370s",
        "start_time": "370.269"
    },
    {
        "id": "8af9b1ac",
        "text": "So it's all the net inputs for layer two. And then we just repeat a 2 W-2 H three. And then finally, a three in this case, which is the prediction itself nice, we have this information we know about the error function and its dependencies. And so what we can do is we can calculate the derivative of E with respect to W-2. And we can use a very nice rule from calculus which I'm not gonna um review here, but it's called the chain rule. And if we use the chain rule, uh we can come up with this formula uh which rewrites the derivative of the uh E uh error function with regard to W-2. So we have three blocks, three derivatives for coming up with the, with this main derivative here of the error over W-2. And he, yeah, this feels like a little bit intimidating. And I know and as I said, like this is gonna be a quick math heavy",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=385s",
        "start_time": "385.769"
    },
    {
        "id": "1bb7d9ac",
        "text": "And we can use a very nice rule from calculus which I'm not gonna um review here, but it's called the chain rule. And if we use the chain rule, uh we can come up with this formula uh which rewrites the derivative of the uh E uh error function with regard to W-2. So we have three blocks, three derivatives for coming up with the, with this main derivative here of the error over W-2. And he, yeah, this feels like a little bit intimidating. And I know and as I said, like this is gonna be a quick math heavy uh video, but uh I'm gonna try to make this uh calculations as clear and as simple as possible. So you can follow and understand what's going on. Cool. So let's calculate this three derivatives here. Let's start from the first one, the derivative of E over a three. Let's remember that we are using the",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=409s",
        "start_time": "409.989"
    },
    {
        "id": "f96a9d66",
        "text": "uh which rewrites the derivative of the uh E uh error function with regard to W-2. So we have three blocks, three derivatives for coming up with the, with this main derivative here of the error over W-2. And he, yeah, this feels like a little bit intimidating. And I know and as I said, like this is gonna be a quick math heavy uh video, but uh I'm gonna try to make this uh calculations as clear and as simple as possible. So you can follow and understand what's going on. Cool. So let's calculate this three derivatives here. Let's start from the first one, the derivative of E over a three. Let's remember that we are using the uh quadratic error uh function here as the error function. And so if we perform the derivative of E uh with respect to a three, we end up with these results. So it's basically a three minus Y. So it's the uh prediction uh minus the actual expected outcome",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=424s",
        "start_time": "424.35"
    },
    {
        "id": "4471d26e",
        "text": "uh video, but uh I'm gonna try to make this uh calculations as clear and as simple as possible. So you can follow and understand what's going on. Cool. So let's calculate this three derivatives here. Let's start from the first one, the derivative of E over a three. Let's remember that we are using the uh quadratic error uh function here as the error function. And so if we perform the derivative of E uh with respect to a three, we end up with these results. So it's basically a three minus Y. So it's the uh prediction uh minus the actual expected outcome good. So that's the first block, second block, the derivative of A three over H three. Now, for the activations uh in this example, and as like in the examples, for the previous video, we are gonna use the sigmoid function. And so the A three, the mm uh the prediction here a three is gonna be the sigmoid function calculated in H three",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=449s",
        "start_time": "449.399"
    },
    {
        "id": "e5b5b5d1",
        "text": "uh quadratic error uh function here as the error function. And so if we perform the derivative of E uh with respect to a three, we end up with these results. So it's basically a three minus Y. So it's the uh prediction uh minus the actual expected outcome good. So that's the first block, second block, the derivative of A three over H three. Now, for the activations uh in this example, and as like in the examples, for the previous video, we are gonna use the sigmoid function. And so the A three, the mm uh the prediction here a three is gonna be the sigmoid function calculated in H three cool. So now if we want to calculate the derivative of A three over H three, we're gonna end up with the first derivative of the sigmoid function",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=471s",
        "start_time": "471.279"
    },
    {
        "id": "d587e734",
        "text": "good. So that's the first block, second block, the derivative of A three over H three. Now, for the activations uh in this example, and as like in the examples, for the previous video, we are gonna use the sigmoid function. And so the A three, the mm uh the prediction here a three is gonna be the sigmoid function calculated in H three cool. So now if we want to calculate the derivative of A three over H three, we're gonna end up with the first derivative of the sigmoid function calculated in H free. I'm using this Sigma uh symbol here uh to indicate the sigmoid function and the derivative of the sigmoid function is given by this formula here. So it's the Sigma function itself by one minus the sigmoid function. And in this case, it's calculated in H three nice.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=495s",
        "start_time": "495.929"
    },
    {
        "id": "6b4e8984",
        "text": "cool. So now if we want to calculate the derivative of A three over H three, we're gonna end up with the first derivative of the sigmoid function calculated in H free. I'm using this Sigma uh symbol here uh to indicate the sigmoid function and the derivative of the sigmoid function is given by this formula here. So it's the Sigma function itself by one minus the sigmoid function. And in this case, it's calculated in H three nice. So we have the second element, let's move on to the third element. So it's the derivative of H three over W-2. So H three, you guys should know this by now it's given by the matrix multiplication of the uh activation vector by the uh weight matrix uh of the second layer",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=522s",
        "start_time": "522.51"
    },
    {
        "id": "824df1b1",
        "text": "calculated in H free. I'm using this Sigma uh symbol here uh to indicate the sigmoid function and the derivative of the sigmoid function is given by this formula here. So it's the Sigma function itself by one minus the sigmoid function. And in this case, it's calculated in H three nice. So we have the second element, let's move on to the third element. So it's the derivative of H three over W-2. So H three, you guys should know this by now it's given by the matrix multiplication of the uh activation vector by the uh weight matrix uh of the second layer nice. So this is basically a linear transformation. And so if we do a derivative of H three over W-2, we are gonna end up with A two with the activations cool. Now we have all the three elements that we need to rewrite uh the uh derivative of E over W-2. So now let's rewrite this, let's fill this up. So first elements,",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=534s",
        "start_time": "534.849"
    },
    {
        "id": "2db45ea2",
        "text": "So we have the second element, let's move on to the third element. So it's the derivative of H three over W-2. So H three, you guys should know this by now it's given by the matrix multiplication of the uh activation vector by the uh weight matrix uh of the second layer nice. So this is basically a linear transformation. And so if we do a derivative of H three over W-2, we are gonna end up with A two with the activations cool. Now we have all the three elements that we need to rewrite uh the uh derivative of E over W-2. So now let's rewrite this, let's fill this up. So first elements, so the first derivative here is a three minus Y, then the derivative of the A three over H three is basically the derivative of the Sigma function calculated in H three. And finally, the last block is equal to a two nice. We have our first um element to calculate like the gradient. So we have our error function,",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=557s",
        "start_time": "557.28"
    },
    {
        "id": "b78bd515",
        "text": "nice. So this is basically a linear transformation. And so if we do a derivative of H three over W-2, we are gonna end up with A two with the activations cool. Now we have all the three elements that we need to rewrite uh the uh derivative of E over W-2. So now let's rewrite this, let's fill this up. So first elements, so the first derivative here is a three minus Y, then the derivative of the A three over H three is basically the derivative of the Sigma function calculated in H three. And finally, the last block is equal to a two nice. We have our first um element to calculate like the gradient. So we have our error function, uh the derivative of the error function calculated in W-2. Now, what should we do next? Well, we should calculate the error, the derivative of the error function with respect to W one. So we are basically moving from the second layer back towards the first layer. So back propagation, we are going back from right to left. Now",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=578s",
        "start_time": "578.09"
    },
    {
        "id": "e128aed4",
        "text": "so the first derivative here is a three minus Y, then the derivative of the A three over H three is basically the derivative of the Sigma function calculated in H three. And finally, the last block is equal to a two nice. We have our first um element to calculate like the gradient. So we have our error function, uh the derivative of the error function calculated in W-2. Now, what should we do next? Well, we should calculate the error, the derivative of the error function with respect to W one. So we are basically moving from the second layer back towards the first layer. So back propagation, we are going back from right to left. Now um the derivative of E with respect to W one is equal to this other be here. But it's not too different like from what we've seen like before, we've just like reused the, the same like um expansion rules, rewriting rules using the chain rules uh chain rule from calculus of the E of the derivative of E calculated with respect to W-2",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=604s",
        "start_time": "604.049"
    },
    {
        "id": "b054766c",
        "text": "uh the derivative of the error function calculated in W-2. Now, what should we do next? Well, we should calculate the error, the derivative of the error function with respect to W one. So we are basically moving from the second layer back towards the first layer. So back propagation, we are going back from right to left. Now um the derivative of E with respect to W one is equal to this other be here. But it's not too different like from what we've seen like before, we've just like reused the, the same like um expansion rules, rewriting rules using the chain rules uh chain rule from calculus of the E of the derivative of E calculated with respect to W-2 cool. So we can get these first element here. So the derivative of E with respect to A two and using uh the chain rule again, we can rewrite it as this formula down here with this three blocks. Now, if we take all of this and we plug in",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=633s",
        "start_time": "633.349"
    },
    {
        "id": "890e429a",
        "text": "um the derivative of E with respect to W one is equal to this other be here. But it's not too different like from what we've seen like before, we've just like reused the, the same like um expansion rules, rewriting rules using the chain rules uh chain rule from calculus of the E of the derivative of E calculated with respect to W-2 cool. So we can get these first element here. So the derivative of E with respect to A two and using uh the chain rule again, we can rewrite it as this formula down here with this three blocks. Now, if we take all of this and we plug in in here, we're gonna end up with this beast formula for the derivative of V uh with regards to W one which is given by all of these five elements. Now don't be scared. So it's quite easy to calculate. And good news is that we've already calculated these first two elements when we calculated the derivative of V with respect to W-2.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=656s",
        "start_time": "656.94"
    },
    {
        "id": "763100d2",
        "text": "cool. So we can get these first element here. So the derivative of E with respect to A two and using uh the chain rule again, we can rewrite it as this formula down here with this three blocks. Now, if we take all of this and we plug in in here, we're gonna end up with this beast formula for the derivative of V uh with regards to W one which is given by all of these five elements. Now don't be scared. So it's quite easy to calculate. And good news is that we've already calculated these first two elements when we calculated the derivative of V with respect to W-2. So we should now focus on this three blocks, three derivatives now here. So let's",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=681s",
        "start_time": "681.859"
    },
    {
        "id": "3de9bc28",
        "text": "in here, we're gonna end up with this beast formula for the derivative of V uh with regards to W one which is given by all of these five elements. Now don't be scared. So it's quite easy to calculate. And good news is that we've already calculated these first two elements when we calculated the derivative of V with respect to W-2. So we should now focus on this three blocks, three derivatives now here. So let's uh let's start calculating the first one of this three. So the derivative of H three with respect to A two. So H three again is given by uh the matrix multiplication of A two and W-2. And when we calculate the derivative here of H three over A two, we end up with only W-2. So the weight matrix for layer two, now let's move on next derivative. So it's a derivative of a two calculated over H two.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=705s",
        "start_time": "705.869"
    },
    {
        "id": "b288ea45",
        "text": "So we should now focus on this three blocks, three derivatives now here. So let's uh let's start calculating the first one of this three. So the derivative of H three with respect to A two. So H three again is given by uh the matrix multiplication of A two and W-2. And when we calculate the derivative here of H three over A two, we end up with only W-2. So the weight matrix for layer two, now let's move on next derivative. So it's a derivative of a two calculated over H two. So again, a two is the basically given by the sigmoid function uh calculated in H two. And so when we do the derivative of A two over H two, again, we have the first derivative of the sigmoid function. But this time calculated in H two",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=731s",
        "start_time": "731.96"
    },
    {
        "id": "fbdcd515",
        "text": "uh let's start calculating the first one of this three. So the derivative of H three with respect to A two. So H three again is given by uh the matrix multiplication of A two and W-2. And when we calculate the derivative here of H three over A two, we end up with only W-2. So the weight matrix for layer two, now let's move on next derivative. So it's a derivative of a two calculated over H two. So again, a two is the basically given by the sigmoid function uh calculated in H two. And so when we do the derivative of A two over H two, again, we have the first derivative of the sigmoid function. But this time calculated in H two good. Now final element. So the derivative of H two calculated over W one. So H two is given by the matrix multiplication of the input vector X with the uh weight matrix one nice. So the derivative of H two over W one is just the input vector X",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=739s",
        "start_time": "739.38"
    },
    {
        "id": "fc507b6a",
        "text": "So again, a two is the basically given by the sigmoid function uh calculated in H two. And so when we do the derivative of A two over H two, again, we have the first derivative of the sigmoid function. But this time calculated in H two good. Now final element. So the derivative of H two calculated over W one. So H two is given by the matrix multiplication of the input vector X with the uh weight matrix one nice. So the derivative of H two over W one is just the input vector X cool. We have all the five elements to rewrite the derivative of E over W uh over W one. So let's rewrite that",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=769s",
        "start_time": "769.25"
    },
    {
        "id": "7bb77fd8",
        "text": "good. Now final element. So the derivative of H two calculated over W one. So H two is given by the matrix multiplication of the input vector X with the uh weight matrix one nice. So the derivative of H two over W one is just the input vector X cool. We have all the five elements to rewrite the derivative of E over W uh over W one. So let's rewrite that first element here. So the derivative of the E over A three can be rewritten as a three minus Y. Then we have the second element here. So which is the derivative of a three over H three. And this is the",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=788s",
        "start_time": "788.299"
    },
    {
        "id": "806dbf08",
        "text": "cool. We have all the five elements to rewrite the derivative of E over W uh over W one. So let's rewrite that first element here. So the derivative of the E over A three can be rewritten as a three minus Y. Then we have the second element here. So which is the derivative of a three over H three. And this is the uh first derivative of the Sigma function calculated in H three. So the third block is basically just W-2. The fourth block is again the uh the first derivative of the Sigma function calculated in H two. And finally, we have for the fifth block our input vector X excellent. So now we have all the elements for our gradient which are the",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=813s",
        "start_time": "813.539"
    },
    {
        "id": "af78521d",
        "text": "first element here. So the derivative of the E over A three can be rewritten as a three minus Y. Then we have the second element here. So which is the derivative of a three over H three. And this is the uh first derivative of the Sigma function calculated in H three. So the third block is basically just W-2. The fourth block is again the uh the first derivative of the Sigma function calculated in H two. And finally, we have for the fifth block our input vector X excellent. So now we have all the elements for our gradient which are the derivative of the E with respect to W-2 and the derivative of the E with respect to W one. So do you see a pattern emerging here? Yeah, I guess like you can see something. So look at these two elements here. A three minus Y and multiplied by Sigma, the first derivative of the Sigma F function calculated in H three. So they are basically the same as the ones we have here.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=825s",
        "start_time": "825.599"
    },
    {
        "id": "7231ef6d",
        "text": "uh first derivative of the Sigma function calculated in H three. So the third block is basically just W-2. The fourth block is again the uh the first derivative of the Sigma function calculated in H two. And finally, we have for the fifth block our input vector X excellent. So now we have all the elements for our gradient which are the derivative of the E with respect to W-2 and the derivative of the E with respect to W one. So do you see a pattern emerging here? Yeah, I guess like you can see something. So look at these two elements here. A three minus Y and multiplied by Sigma, the first derivative of the Sigma F function calculated in H three. So they are basically the same as the ones we have here. This is great news because it means that we for calculating um the derivatives of previous layers here, we can reuse some stuff that we've calculated uh in layers that are more like towards like the right towards the end of the network. And this is the whole point of like a back propagation. So we for calculating um",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=840s",
        "start_time": "840.679"
    },
    {
        "id": "c9798820",
        "text": "derivative of the E with respect to W-2 and the derivative of the E with respect to W one. So do you see a pattern emerging here? Yeah, I guess like you can see something. So look at these two elements here. A three minus Y and multiplied by Sigma, the first derivative of the Sigma F function calculated in H three. So they are basically the same as the ones we have here. This is great news because it means that we for calculating um the derivatives of previous layers here, we can reuse some stuff that we've calculated uh in layers that are more like towards like the right towards the end of the network. And this is the whole point of like a back propagation. So we for calculating um uh the the the derivatives of like the error function in previous",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=868s",
        "start_time": "868.559"
    },
    {
        "id": "81bf3003",
        "text": "This is great news because it means that we for calculating um the derivatives of previous layers here, we can reuse some stuff that we've calculated uh in layers that are more like towards like the right towards the end of the network. And this is the whole point of like a back propagation. So we for calculating um uh the the the derivatives of like the error function in previous uh layers with respect to the weights, we can use elements that we've already calculated. So let's see this like in action again now that we know like all the pieces of the puzzle cool. So we said that the first phase here for doing back prop is to calculate the error, then once we have the errors, we use those errors to calculate the first derivative of the error with respect to W-2.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=897s",
        "start_time": "897.83"
    },
    {
        "id": "d8bbaf15",
        "text": "uh the the the derivatives of like the error function in previous uh layers with respect to the weights, we can use elements that we've already calculated. So let's see this like in action again now that we know like all the pieces of the puzzle cool. So we said that the first phase here for doing back prop is to calculate the error, then once we have the errors, we use those errors to calculate the first derivative of the error with respect to W-2. And then we back propagate that we use that information to calculate the derivative of the error function with respect to W one. So we're basically moving all the way backwards from right to left. And so this is the whole point of",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=926s",
        "start_time": "926.63"
    },
    {
        "id": "06798a77",
        "text": "uh layers with respect to the weights, we can use elements that we've already calculated. So let's see this like in action again now that we know like all the pieces of the puzzle cool. So we said that the first phase here for doing back prop is to calculate the error, then once we have the errors, we use those errors to calculate the first derivative of the error with respect to W-2. And then we back propagate that we use that information to calculate the derivative of the error function with respect to W one. So we're basically moving all the way backwards from right to left. And so this is the whole point of propagating back the error signal. Nice. So now we have an understanding of back propagation. So you should congratulate yourself because I feel there are not many people out there also machine learning practitioners who really understands like the math behind uh back propagation. Now you do nice.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=933s",
        "start_time": "933.28"
    },
    {
        "id": "1b0242e1",
        "text": "And then we back propagate that we use that information to calculate the derivative of the error function with respect to W one. So we're basically moving all the way backwards from right to left. And so this is the whole point of propagating back the error signal. Nice. So now we have an understanding of back propagation. So you should congratulate yourself because I feel there are not many people out there also machine learning practitioners who really understands like the math behind uh back propagation. Now you do nice. So let's go back, let's say the higher level perspective. So the the training steps. So we've seen the first three steps. So we get the prop predictions, we calculate the error and then we calculate the gradient of the error function over the weight. Now, the last thing that remains to do is to actually update the parameters to update the weight. So how do we do that? Well, for doing that, we need",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=959s",
        "start_time": "959.38"
    },
    {
        "id": "e45df34b",
        "text": "propagating back the error signal. Nice. So now we have an understanding of back propagation. So you should congratulate yourself because I feel there are not many people out there also machine learning practitioners who really understands like the math behind uh back propagation. Now you do nice. So let's go back, let's say the higher level perspective. So the the training steps. So we've seen the first three steps. So we get the prop predictions, we calculate the error and then we calculate the gradient of the error function over the weight. Now, the last thing that remains to do is to actually update the parameters to update the weight. So how do we do that? Well, for doing that, we need a very important algorithm that's called gradient descent that's also used in traditional machine learning. So how does gradient descent work? Well with gradient descent, we take a step in the opposite direction to the gradient.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=979s",
        "start_time": "979.59"
    },
    {
        "id": "ce232456",
        "text": "So let's go back, let's say the higher level perspective. So the the training steps. So we've seen the first three steps. So we get the prop predictions, we calculate the error and then we calculate the gradient of the error function over the weight. Now, the last thing that remains to do is to actually update the parameters to update the weight. So how do we do that? Well, for doing that, we need a very important algorithm that's called gradient descent that's also used in traditional machine learning. So how does gradient descent work? Well with gradient descent, we take a step in the opposite direction to the gradient. So the, the, the, the step or the size of the step is basically the learning rate and the learning rate is a parameter uh that we use. And we can trick when we do training with our neural networks to obtain better results. OK. But",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1002s",
        "start_time": "1002.89"
    },
    {
        "id": "349bcda5",
        "text": "a very important algorithm that's called gradient descent that's also used in traditional machine learning. So how does gradient descent work? Well with gradient descent, we take a step in the opposite direction to the gradient. So the, the, the, the step or the size of the step is basically the learning rate and the learning rate is a parameter uh that we use. And we can trick when we do training with our neural networks to obtain better results. OK. But we now have an understanding like a back propagation and like of gradient descent. But how does gradient descent like really work? So what what, what, what does it mean to take a step in the opposite direction to the gradient? Cool for doing that? We need this uh graph down here. So on the X axis, we have the weights",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1029s",
        "start_time": "1029.359"
    },
    {
        "id": "0ae4b09c",
        "text": "So the, the, the, the step or the size of the step is basically the learning rate and the learning rate is a parameter uh that we use. And we can trick when we do training with our neural networks to obtain better results. OK. But we now have an understanding like a back propagation and like of gradient descent. But how does gradient descent like really work? So what what, what, what does it mean to take a step in the opposite direction to the gradient? Cool for doing that? We need this uh graph down here. So on the X axis, we have the weights and on the y axis we have the errors nice. And here we have the error function which is E as a function of W obviously. And it's this orange curve here. So let's say we start from this position here. So after",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1047s",
        "start_time": "1047.41"
    },
    {
        "id": "9bc1ac47",
        "text": "we now have an understanding like a back propagation and like of gradient descent. But how does gradient descent like really work? So what what, what, what does it mean to take a step in the opposite direction to the gradient? Cool for doing that? We need this uh graph down here. So on the X axis, we have the weights and on the y axis we have the errors nice. And here we have the error function which is E as a function of W obviously. And it's this orange curve here. So let's say we start from this position here. So after uh we've uh trained uh like our network, uh for example, we've passed in like the first sample we've calculated the gradient and we've, and we know that we are like at this point like with the errors and with the weight cool. So here we need to calculate the gradient and the gradient is basically this purple line here. And the gradient what does intuitively is to give us information the direction",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1067s",
        "start_time": "1067.079"
    },
    {
        "id": "15a92b5a",
        "text": "and on the y axis we have the errors nice. And here we have the error function which is E as a function of W obviously. And it's this orange curve here. So let's say we start from this position here. So after uh we've uh trained uh like our network, uh for example, we've passed in like the first sample we've calculated the gradient and we've, and we know that we are like at this point like with the errors and with the weight cool. So here we need to calculate the gradient and the gradient is basically this purple line here. And the gradient what does intuitively is to give us information the direction uh in which the function uh increases the fastest right cool. So if we know the gradient, what we want to do is go in the opposite direction to the gradient because we want to go down in uh the, the function because we want to minimize this function. So we want to go down here to this global minimum where the error of our network is minimized. And so we take the gradient",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1089s",
        "start_time": "1089.959"
    },
    {
        "id": "2216de2c",
        "text": "uh we've uh trained uh like our network, uh for example, we've passed in like the first sample we've calculated the gradient and we've, and we know that we are like at this point like with the errors and with the weight cool. So here we need to calculate the gradient and the gradient is basically this purple line here. And the gradient what does intuitively is to give us information the direction uh in which the function uh increases the fastest right cool. So if we know the gradient, what we want to do is go in the opposite direction to the gradient because we want to go down in uh the, the function because we want to minimize this function. So we want to go down here to this global minimum where the error of our network is minimized. And so we take the gradient and we basically uh take a step in the opposite direction to the gradient where we have like the gradient calculated for all the weights in the network. And so we take a step that's given by the learning rate. So",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1106s",
        "start_time": "1106.189"
    },
    {
        "id": "9ba3c845",
        "text": "uh in which the function uh increases the fastest right cool. So if we know the gradient, what we want to do is go in the opposite direction to the gradient because we want to go down in uh the, the function because we want to minimize this function. So we want to go down here to this global minimum where the error of our network is minimized. And so we take the gradient and we basically uh take a step in the opposite direction to the gradient where we have like the gradient calculated for all the weights in the network. And so we take a step that's given by the learning rate. So uh we calculate the gradient here like when we are like at this 0.1 and we jump down here to, to this point. Now we we have like uh another like sample in uh like the, the uh the network for example. And it does like forward propagation, like backward propagation. And then when we have like the gradient uh of the of the net of the weight of the network again,",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1135s",
        "start_time": "1135.599"
    },
    {
        "id": "0f593c63",
        "text": "and we basically uh take a step in the opposite direction to the gradient where we have like the gradient calculated for all the weights in the network. And so we take a step that's given by the learning rate. So uh we calculate the gradient here like when we are like at this 0.1 and we jump down here to, to this point. Now we we have like uh another like sample in uh like the, the uh the network for example. And it does like forward propagation, like backward propagation. And then when we have like the gradient uh of the of the net of the weight of the network again, uh we just like take a step in the opposite direction and we go down to three, down to 456 until we reach the global minimum. And so this is the whole point of training. So we want to",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1163s",
        "start_time": "1163.619"
    },
    {
        "id": "b85a885a",
        "text": "uh we calculate the gradient here like when we are like at this 0.1 and we jump down here to, to this point. Now we we have like uh another like sample in uh like the, the uh the network for example. And it does like forward propagation, like backward propagation. And then when we have like the gradient uh of the of the net of the weight of the network again, uh we just like take a step in the opposite direction and we go down to three, down to 456 until we reach the global minimum. And so this is the whole point of training. So we want to uh tweak the weights in such a way that we can get the minimum possible error. And when we have the minimum possible error, it means that we have good predictions because the predictions are quite similar to the act actual outputs to the correct outputs",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1180s",
        "start_time": "1180.81"
    },
    {
        "id": "b9c47fa3",
        "text": "uh we just like take a step in the opposite direction and we go down to three, down to 456 until we reach the global minimum. And so this is the whole point of training. So we want to uh tweak the weights in such a way that we can get the minimum possible error. And when we have the minimum possible error, it means that we have good predictions because the predictions are quite similar to the act actual outputs to the correct outputs good. So this was like quite intense. But now I hope you have understanding of both back propagation and grand in descent and how we use them for neural networks. Cool. So what remains to do? So what should we do",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1206s",
        "start_time": "1206.56"
    },
    {
        "id": "7fce1c37",
        "text": "uh tweak the weights in such a way that we can get the minimum possible error. And when we have the minimum possible error, it means that we have good predictions because the predictions are quite similar to the act actual outputs to the correct outputs good. So this was like quite intense. But now I hope you have understanding of both back propagation and grand in descent and how we use them for neural networks. Cool. So what remains to do? So what should we do like next in the next video? So we'll take all of this theoretical knowledge and we're gonna turn it into implementation. So we're gonna implement back propagation and grid in the center and we're gonna expand the multi-layered perception class that we've already built so that we can train our network from scratch good.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1219s",
        "start_time": "1219.01"
    },
    {
        "id": "9e952d2d",
        "text": "good. So this was like quite intense. But now I hope you have understanding of both back propagation and grand in descent and how we use them for neural networks. Cool. So what remains to do? So what should we do like next in the next video? So we'll take all of this theoretical knowledge and we're gonna turn it into implementation. So we're gonna implement back propagation and grid in the center and we're gonna expand the multi-layered perception class that we've already built so that we can train our network from scratch good. So this was the video for today. So I hope you enjoyed it. And if that's the case, please subscribe and hit the notification bell if you have any questions or doubts uh about like the content of this video because it was like quite tough. Please leave a comment, leave a questions in the comments section below and I hope I'll see you next time. Cheers.",
        "video": "7- Training a neural network: Backward propagation and gradient descent",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "ScL18goxsSg",
        "youtube_link": "https://www.youtube.com/watch?v=ScL18goxsSg&t=1238s",
        "start_time": "1238.29"
    },
    {
        "id": "45b8c114",
        "text": "Do you really need to know math in order to work with deep learning? An argument goes that no, you just need to know high level libraries like py torch or tensorflow. But in reality, if you really want to understand how neurons work, then yes, you need to know a bit of linear algebra. So that's why in this video we're going to talk about vector and matrix operations. So let's get started with vectors. What are vectors? Well, vectors are just a ray of numbers. We notate vectors with small caps bolts, a letter like this a vector here. And then we can arrange a vector that's a collection of different items both horizontally. And then we have a row vector or vertically and we have a column vector. So on vectors, we can perform a number of different operations",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=0s",
        "start_time": "0.129"
    },
    {
        "id": "346006a8",
        "text": "in this video we're going to talk about vector and matrix operations. So let's get started with vectors. What are vectors? Well, vectors are just a ray of numbers. We notate vectors with small caps bolts, a letter like this a vector here. And then we can arrange a vector that's a collection of different items both horizontally. And then we have a row vector or vertically and we have a column vector. So on vectors, we can perform a number of different operations some that come to mind and are the simplest ones are scaler operations. These operations involve a vector and the scaler which is basically a number like two or 3.5 for example. And we have scr additions, subtraction, multiplication",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=19s",
        "start_time": "19.879"
    },
    {
        "id": "41640527",
        "text": "a letter like this a vector here. And then we can arrange a vector that's a collection of different items both horizontally. And then we have a row vector or vertically and we have a column vector. So on vectors, we can perform a number of different operations some that come to mind and are the simplest ones are scaler operations. These operations involve a vector and the scaler which is basically a number like two or 3.5 for example. And we have scr additions, subtraction, multiplication and division. So let's take a look at scalar edition for example. And the argument goes with all of the other scalar operations as well. So here we have the vector A and we add the scalar N.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=36s",
        "start_time": "36.055"
    },
    {
        "id": "3bcb84eb",
        "text": "some that come to mind and are the simplest ones are scaler operations. These operations involve a vector and the scaler which is basically a number like two or 3.5 for example. And we have scr additions, subtraction, multiplication and division. So let's take a look at scalar edition for example. And the argument goes with all of the other scalar operations as well. So here we have the vector A and we add the scalar N. And so, as you can see here, uh the result is this vector here and the vector is given by the A one which is the first element of the vector plus N which is the scalar and then a two plus N A three plus N. So basically we add the scaler to all the items of the vector. So uh let's take an example here. So we have a vector",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=57s",
        "start_time": "57.02"
    },
    {
        "id": "4e5c9d3b",
        "text": "and division. So let's take a look at scalar edition for example. And the argument goes with all of the other scalar operations as well. So here we have the vector A and we add the scalar N. And so, as you can see here, uh the result is this vector here and the vector is given by the A one which is the first element of the vector plus N which is the scalar and then a two plus N A three plus N. So basically we add the scaler to all the items of the vector. So uh let's take an example here. So we have a vector uh this column vector which is 1 to 2 and we want to add one and the resulting vector is 233, we just added 1 to 1 here, 1 to 2 here and 1 to 2 here. It's very simple, isn't it?",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=75s",
        "start_time": "75.489"
    },
    {
        "id": "7fd3cab9",
        "text": "And so, as you can see here, uh the result is this vector here and the vector is given by the A one which is the first element of the vector plus N which is the scalar and then a two plus N A three plus N. So basically we add the scaler to all the items of the vector. So uh let's take an example here. So we have a vector uh this column vector which is 1 to 2 and we want to add one and the resulting vector is 233, we just added 1 to 1 here, 1 to 2 here and 1 to 2 here. It's very simple, isn't it? Then we have other types of um operations like vector addition and subtraction. So here the operations are performed between two vectors and that's a constraint here. So the two vectors must have the same dimension. So, and what's a vector dimension? Well, it's very simple. It's the number of items that we have in a vector. So",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=90s",
        "start_time": "90.699"
    },
    {
        "id": "606ab948",
        "text": "uh this column vector which is 1 to 2 and we want to add one and the resulting vector is 233, we just added 1 to 1 here, 1 to 2 here and 1 to 2 here. It's very simple, isn't it? Then we have other types of um operations like vector addition and subtraction. So here the operations are performed between two vectors and that's a constraint here. So the two vectors must have the same dimension. So, and what's a vector dimension? Well, it's very simple. It's the number of items that we have in a vector. So uh in this way, for example, here, uh we are adding up a, the vector A plus the vector B. And as you can see both vectors have the same number of items. So three items for the A vector and three items for the B vector.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=116s",
        "start_time": "116.36"
    },
    {
        "id": "2589d32b",
        "text": "Then we have other types of um operations like vector addition and subtraction. So here the operations are performed between two vectors and that's a constraint here. So the two vectors must have the same dimension. So, and what's a vector dimension? Well, it's very simple. It's the number of items that we have in a vector. So uh in this way, for example, here, uh we are adding up a, the vector A plus the vector B. And as you can see both vectors have the same number of items. So three items for the A vector and three items for the B vector. So A vector addition and subtraction are element wise operations as you can see here. So what this basically means is that we add the first elements of the first vector to the first element of the second vector. And this is gonna be the first element of the, some vector in this case,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=133s",
        "start_time": "133.919"
    },
    {
        "id": "55194508",
        "text": "uh in this way, for example, here, uh we are adding up a, the vector A plus the vector B. And as you can see both vectors have the same number of items. So three items for the A vector and three items for the B vector. So A vector addition and subtraction are element wise operations as you can see here. So what this basically means is that we add the first elements of the first vector to the first element of the second vector. And this is gonna be the first element of the, some vector in this case, then we do the same thing with the second items. So A two plus B two gives us the uh second element of the uh of the sum vector. And finally, here a three plus B three is the third element of the uh sum uh vector over here. So",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=161s",
        "start_time": "161.63"
    },
    {
        "id": "59bda780",
        "text": "So A vector addition and subtraction are element wise operations as you can see here. So what this basically means is that we add the first elements of the first vector to the first element of the second vector. And this is gonna be the first element of the, some vector in this case, then we do the same thing with the second items. So A two plus B two gives us the uh second element of the uh of the sum vector. And finally, here a three plus B three is the third element of the uh sum uh vector over here. So again, let's take a look at, at an example, just like to see this more clearly. So here we have two vectors. So 123 and 356. So we want to add them up together. And so the sum vector over here is 479. So one plus 32 plus five that gives us seven and three plus six that gives us nine.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=177s",
        "start_time": "177.91"
    },
    {
        "id": "a674bf30",
        "text": "then we do the same thing with the second items. So A two plus B two gives us the uh second element of the uh of the sum vector. And finally, here a three plus B three is the third element of the uh sum uh vector over here. So again, let's take a look at, at an example, just like to see this more clearly. So here we have two vectors. So 123 and 356. So we want to add them up together. And so the sum vector over here is 479. So one plus 32 plus five that gives us seven and three plus six that gives us nine. So this is very simple, isn't it? Now, we have another operation which is super important as we'll see in a second for neural networks as well. And it's called the dot product. Again, the dot product is performed between two vectors and the result this time is a skier. So it's a number. So it's not another vector.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=199s",
        "start_time": "199.029"
    },
    {
        "id": "9507dc88",
        "text": "again, let's take a look at, at an example, just like to see this more clearly. So here we have two vectors. So 123 and 356. So we want to add them up together. And so the sum vector over here is 479. So one plus 32 plus five that gives us seven and three plus six that gives us nine. So this is very simple, isn't it? Now, we have another operation which is super important as we'll see in a second for neural networks as well. And it's called the dot product. Again, the dot product is performed between two vectors and the result this time is a skier. So it's a number. So it's not another vector. So how do we notate a dot product? Well, and not surprisingly, we noted dot product with the dots and as you can see here, so we have the dot product that's given by um so that we can perform between two vectors over here. And the result of the dot vector over here is a one",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=220s",
        "start_time": "220.059"
    },
    {
        "id": "e1b99ec1",
        "text": "So this is very simple, isn't it? Now, we have another operation which is super important as we'll see in a second for neural networks as well. And it's called the dot product. Again, the dot product is performed between two vectors and the result this time is a skier. So it's a number. So it's not another vector. So how do we notate a dot product? Well, and not surprisingly, we noted dot product with the dots and as you can see here, so we have the dot product that's given by um so that we can perform between two vectors over here. And the result of the dot vector over here is a one uh multiplied by B one plus A two multiplied by B two plus A three, multiplied by uh B three. So how do we get to this formula over here? Well, it's quite simple. So we take the first um item of the first vector and we multiply that with the first item of the second vector. And then we add to this, the second",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=246s",
        "start_time": "246.539"
    },
    {
        "id": "0f648eb8",
        "text": "So how do we notate a dot product? Well, and not surprisingly, we noted dot product with the dots and as you can see here, so we have the dot product that's given by um so that we can perform between two vectors over here. And the result of the dot vector over here is a one uh multiplied by B one plus A two multiplied by B two plus A three, multiplied by uh B three. So how do we get to this formula over here? Well, it's quite simple. So we take the first um item of the first vector and we multiply that with the first item of the second vector. And then we add to this, the second item of the first uh vector multiplied by the second item of the second vector and so on and so forth. So now let's take a look at an example here. So we want to perform the dot product between two vectors. So 123 and four minus 21.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=271s",
        "start_time": "271.359"
    },
    {
        "id": "c5195a69",
        "text": "uh multiplied by B one plus A two multiplied by B two plus A three, multiplied by uh B three. So how do we get to this formula over here? Well, it's quite simple. So we take the first um item of the first vector and we multiply that with the first item of the second vector. And then we add to this, the second item of the first uh vector multiplied by the second item of the second vector and so on and so forth. So now let's take a look at an example here. So we want to perform the dot product between two vectors. So 123 and four minus 21. So the result here is one by four. And we've reached this by multiplying the first element of the first vector with the first element of the second vector. So one by four plus two by minus two. So multiplying the T the, the second items from the first and the second vector plus three by one.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=295s",
        "start_time": "295.73"
    },
    {
        "id": "fb414875",
        "text": "item of the first uh vector multiplied by the second item of the second vector and so on and so forth. So now let's take a look at an example here. So we want to perform the dot product between two vectors. So 123 and four minus 21. So the result here is one by four. And we've reached this by multiplying the first element of the first vector with the first element of the second vector. So one by four plus two by minus two. So multiplying the T the, the second items from the first and the second vector plus three by one. And we achieve this by multiplying the third item from the first vector by the third item uh from the second vector. So now if we run the math here we come up with a number that's three. So the dot product in this case, uh is three.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=325s",
        "start_time": "325.72"
    },
    {
        "id": "362c7478",
        "text": "So the result here is one by four. And we've reached this by multiplying the first element of the first vector with the first element of the second vector. So one by four plus two by minus two. So multiplying the T the, the second items from the first and the second vector plus three by one. And we achieve this by multiplying the third item from the first vector by the third item uh from the second vector. So now if we run the math here we come up with a number that's three. So the dot product in this case, uh is three. Nice. So I just said before that the dot product is super important for neural nets. So let's really research something that we've seen in the previous video. So if you remember, so this is an artificial neuron, this X one X two X three are the inputs W 1 W-2 W three and the weights. And here we have the neuron that does two things. So one is the weighted sum or the net input.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=346s",
        "start_time": "346.049"
    },
    {
        "id": "2f199bc5",
        "text": "And we achieve this by multiplying the third item from the first vector by the third item uh from the second vector. So now if we run the math here we come up with a number that's three. So the dot product in this case, uh is three. Nice. So I just said before that the dot product is super important for neural nets. So let's really research something that we've seen in the previous video. So if you remember, so this is an artificial neuron, this X one X two X three are the inputs W 1 W-2 W three and the weights. And here we have the neuron that does two things. So one is the weighted sum or the net input. And then we have the activation. So let's just focus on the net input. Now. So if you remember the net input age is the weighted sum over all the inputs. In other words, is X one input, one by weight one plus input, two by weight two plus input three by weight three. Now if you look at these,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=372s",
        "start_time": "372.1"
    },
    {
        "id": "b44ae36c",
        "text": "Nice. So I just said before that the dot product is super important for neural nets. So let's really research something that we've seen in the previous video. So if you remember, so this is an artificial neuron, this X one X two X three are the inputs W 1 W-2 W three and the weights. And here we have the neuron that does two things. So one is the weighted sum or the net input. And then we have the activation. So let's just focus on the net input. Now. So if you remember the net input age is the weighted sum over all the inputs. In other words, is X one input, one by weight one plus input, two by weight two plus input three by weight three. Now if you look at these, you may uh just think that we can rewrite that using the dot product because it's basically the, the very same result. And indeed, we can rewrite H as the dot product of X and",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=394s",
        "start_time": "394.63"
    },
    {
        "id": "6cccb0f0",
        "text": "And then we have the activation. So let's just focus on the net input. Now. So if you remember the net input age is the weighted sum over all the inputs. In other words, is X one input, one by weight one plus input, two by weight two plus input three by weight three. Now if you look at these, you may uh just think that we can rewrite that using the dot product because it's basically the, the very same result. And indeed, we can rewrite H as the dot product of X and W where X is the X one XGX three vector, which is basically the, it's called the input vector. And it puts together this X one input XG input and X three inp input. And then we have this W 1 W-2 W-2 W three vector, that's the weights vector.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=424s",
        "start_time": "424.299"
    },
    {
        "id": "6ceaf950",
        "text": "you may uh just think that we can rewrite that using the dot product because it's basically the, the very same result. And indeed, we can rewrite H as the dot product of X and W where X is the X one XGX three vector, which is basically the, it's called the input vector. And it puts together this X one input XG input and X three inp input. And then we have this W 1 W-2 W-2 W three vector, that's the weights vector. So in other words, we can rewrite the net input as the dot product between",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=451s",
        "start_time": "451.869"
    },
    {
        "id": "ed53373d",
        "text": "W where X is the X one XGX three vector, which is basically the, it's called the input vector. And it puts together this X one input XG input and X three inp input. And then we have this W 1 W-2 W-2 W three vector, that's the weights vector. So in other words, we can rewrite the net input as the dot product between uh the inputs and the weights. And as you can see, obviously the result here X one W one plus X 2 W-2 plus X three, W three is equal to the net input as we calculated it last time.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=470s",
        "start_time": "470.149"
    },
    {
        "id": "a5df60bb",
        "text": "So in other words, we can rewrite the net input as the dot product between uh the inputs and the weights. And as you can see, obviously the result here X one W one plus X 2 W-2 plus X three, W three is equal to the net input as we calculated it last time. So how's this interesting? So why can't we just use this notation? Well, it turns out that this is like way more elegant. So working with vector and as we'll see with mattresses, like it's way more elegant and way faster in terms of notation than just using like all of these like uh symbols like in an extended form. So this is why uh we basically use a lot of like doll products and linear algebra",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=497s",
        "start_time": "497.179"
    },
    {
        "id": "d567c062",
        "text": "uh the inputs and the weights. And as you can see, obviously the result here X one W one plus X 2 W-2 plus X three, W three is equal to the net input as we calculated it last time. So how's this interesting? So why can't we just use this notation? Well, it turns out that this is like way more elegant. So working with vector and as we'll see with mattresses, like it's way more elegant and way faster in terms of notation than just using like all of these like uh symbols like in an extended form. So this is why uh we basically use a lot of like doll products and linear algebra uh for uh notating uh neural nets. And that's very, very convenient. So this was it like for uh the basic operations for vectors. Now let's move on to matrices. So what's a matrix? So a matrix",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=505s",
        "start_time": "505.07"
    },
    {
        "id": "7ebebe1e",
        "text": "So how's this interesting? So why can't we just use this notation? Well, it turns out that this is like way more elegant. So working with vector and as we'll see with mattresses, like it's way more elegant and way faster in terms of notation than just using like all of these like uh symbols like in an extended form. So this is why uh we basically use a lot of like doll products and linear algebra uh for uh notating uh neural nets. And that's very, very convenient. So this was it like for uh the basic operations for vectors. Now let's move on to matrices. So what's a matrix? So a matrix is a rectangular grid of numbers. It's like a spreadsheet really? So you have a spreadsheet with a lot of like variables or numbers and you can store all of that information in this kind of like squared rectangular uh grid. So",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=522s",
        "start_time": "522.34"
    },
    {
        "id": "4cae9c9d",
        "text": "uh for uh notating uh neural nets. And that's very, very convenient. So this was it like for uh the basic operations for vectors. Now let's move on to matrices. So what's a matrix? So a matrix is a rectangular grid of numbers. It's like a spreadsheet really? So you have a spreadsheet with a lot of like variables or numbers and you can store all of that information in this kind of like squared rectangular uh grid. So we notate a matrix using an uppercase letter. So just like here a uppercase A and then we have these two weird indexes here. Indexes. So I and J so, so what do they represent? Well, I represents the rows, indexes and J represents the colon indexes. So let's take a look at these like in actions.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=550s",
        "start_time": "550.7"
    },
    {
        "id": "056af517",
        "text": "is a rectangular grid of numbers. It's like a spreadsheet really? So you have a spreadsheet with a lot of like variables or numbers and you can store all of that information in this kind of like squared rectangular uh grid. So we notate a matrix using an uppercase letter. So just like here a uppercase A and then we have these two weird indexes here. Indexes. So I and J so, so what do they represent? Well, I represents the rows, indexes and J represents the colon indexes. So let's take a look at these like in actions. So a 11 is the the first item we have here. And as you can see this one in uh this like a red square represents, indicates the fact that we are on row number one. Whereas the second one over here represents the fact that we are on column number one. So this element here is the element that stays in a row one, column one.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=568s",
        "start_time": "568.424"
    },
    {
        "id": "1b82ebd4",
        "text": "we notate a matrix using an uppercase letter. So just like here a uppercase A and then we have these two weird indexes here. Indexes. So I and J so, so what do they represent? Well, I represents the rows, indexes and J represents the colon indexes. So let's take a look at these like in actions. So a 11 is the the first item we have here. And as you can see this one in uh this like a red square represents, indicates the fact that we are on row number one. Whereas the second one over here represents the fact that we are on column number one. So this element here is the element that stays in a row one, column one. So now let's move on to a 12.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=587s",
        "start_time": "587.4"
    },
    {
        "id": "b8ce9aca",
        "text": "So a 11 is the the first item we have here. And as you can see this one in uh this like a red square represents, indicates the fact that we are on row number one. Whereas the second one over here represents the fact that we are on column number one. So this element here is the element that stays in a row one, column one. So now let's move on to a 12. The one again here represents the fact that we are on row number one, but the two represents the fact that we are on column number two.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=617s",
        "start_time": "617.53"
    },
    {
        "id": "728ced26",
        "text": "So now let's move on to a 12. The one again here represents the fact that we are on row number one, but the two represents the fact that we are on column number two. Now 81",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=647s",
        "start_time": "647.159"
    },
    {
        "id": "7df4dbcb",
        "text": "The one again here represents the fact that we are on row number one, but the two represents the fact that we are on column number two. Now 81 uh here we have uh row number two and then one indicates that we are on column number one. So this way we can identify the position of all the elements of a matrix just by using two indexes I which stands for",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=650s",
        "start_time": "650.349"
    },
    {
        "id": "f32fa3b4",
        "text": "Now 81 uh here we have uh row number two and then one indicates that we are on column number one. So this way we can identify the position of all the elements of a matrix just by using two indexes I which stands for um rows and J which stands for uh columns. It's as simple as that. So a very important element of a matrix is its dimension. So, and the dimension is indicated by the number of rows and columns. So here we have two matrixes here. So like on the left,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=659s",
        "start_time": "659.119"
    },
    {
        "id": "1f5645bc",
        "text": "uh here we have uh row number two and then one indicates that we are on column number one. So this way we can identify the position of all the elements of a matrix just by using two indexes I which stands for um rows and J which stands for uh columns. It's as simple as that. So a very important element of a matrix is its dimension. So, and the dimension is indicated by the number of rows and columns. So here we have two matrixes here. So like on the left, uh the dimension of this matrix is three by two and it's three because it has three rows and it's two because it has two columns. Whereas the uh matrix on the right is two by three and two is the number of rows we have and three is the number of columns we have.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=661s",
        "start_time": "661.94"
    },
    {
        "id": "ded84f2c",
        "text": "um rows and J which stands for uh columns. It's as simple as that. So a very important element of a matrix is its dimension. So, and the dimension is indicated by the number of rows and columns. So here we have two matrixes here. So like on the left, uh the dimension of this matrix is three by two and it's three because it has three rows and it's two because it has two columns. Whereas the uh matrix on the right is two by three and two is the number of rows we have and three is the number of columns we have. So remember dimension of matrix is indicated by number of rows first and then number of columns.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=683s",
        "start_time": "683.69"
    },
    {
        "id": "3eafede0",
        "text": "uh the dimension of this matrix is three by two and it's three because it has three rows and it's two because it has two columns. Whereas the uh matrix on the right is two by three and two is the number of rows we have and three is the number of columns we have. So remember dimension of matrix is indicated by number of rows first and then number of columns. So there's a specific case of, of matrix uh which can be used to indicate vectors and we can indicate vectors as a row. Uh So we can,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=710s",
        "start_time": "710.049"
    },
    {
        "id": "1abedca7",
        "text": "So remember dimension of matrix is indicated by number of rows first and then number of columns. So there's a specific case of, of matrix uh which can be used to indicate vectors and we can indicate vectors as a row. Uh So we can, uh as we mentioned, we have both like a row vectors and column vectors and a row vector is indicated by a one by N matrix. And the column vector is indicated by an N by one Metrix. Let's just take a look at an example, cos it's gonna make things like way clearer. So here on the left, we have this uh row vector. So it's 143.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=733s",
        "start_time": "733.65"
    },
    {
        "id": "24dc216d",
        "text": "So there's a specific case of, of matrix uh which can be used to indicate vectors and we can indicate vectors as a row. Uh So we can, uh as we mentioned, we have both like a row vectors and column vectors and a row vector is indicated by a one by N matrix. And the column vector is indicated by an N by one Metrix. Let's just take a look at an example, cos it's gonna make things like way clearer. So here on the left, we have this uh row vector. So it's 143. And as we can see here, it's a one by three metrics. It's one because it has only one row and it's three in terms of columns because obviously it has three columns.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=742s",
        "start_time": "742.89"
    },
    {
        "id": "4fab5825",
        "text": "uh as we mentioned, we have both like a row vectors and column vectors and a row vector is indicated by a one by N matrix. And the column vector is indicated by an N by one Metrix. Let's just take a look at an example, cos it's gonna make things like way clearer. So here on the left, we have this uh row vector. So it's 143. And as we can see here, it's a one by three metrics. It's one because it has only one row and it's three in terms of columns because obviously it has three columns. And then here on the right, we have a column vector and this column vector can be uh fought as a matrix. And it's a four by one matrix. It's four rows here and it's only one column. So this is a nice way of thinking uh vectors as uh matrices,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=757s",
        "start_time": "757.45"
    },
    {
        "id": "99a5678c",
        "text": "And as we can see here, it's a one by three metrics. It's one because it has only one row and it's three in terms of columns because obviously it has three columns. And then here on the right, we have a column vector and this column vector can be uh fought as a matrix. And it's a four by one matrix. It's four rows here and it's only one column. So this is a nice way of thinking uh vectors as uh matrices, an important operation that we have with a mattress, it's called transposition. So with trans transposition, what we do, it's quite simple. So basically we switch rows and columns. So let's take a look at this",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=783s",
        "start_time": "783.71"
    },
    {
        "id": "bf21dbdc",
        "text": "And then here on the right, we have a column vector and this column vector can be uh fought as a matrix. And it's a four by one matrix. It's four rows here and it's only one column. So this is a nice way of thinking uh vectors as uh matrices, an important operation that we have with a mattress, it's called transposition. So with trans transposition, what we do, it's quite simple. So basically we switch rows and columns. So let's take a look at this metrics for example. So here we have this matrix which is 123 minus 40.51 and then it says three by two matrix. Now, the transpose matrix which is indicated with",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=797s",
        "start_time": "797.96"
    },
    {
        "id": "a7a2825e",
        "text": "an important operation that we have with a mattress, it's called transposition. So with trans transposition, what we do, it's quite simple. So basically we switch rows and columns. So let's take a look at this metrics for example. So here we have this matrix which is 123 minus 40.51 and then it says three by two matrix. Now, the transpose matrix which is indicated with a superscript capital T which stands for uh transpose, it's basically the same metrics. But as you can see here, the columns have become rows. So here we have the in the first column, 130.5. And here",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=823s",
        "start_time": "823.159"
    },
    {
        "id": "e52e7f93",
        "text": "metrics for example. So here we have this matrix which is 123 minus 40.51 and then it says three by two matrix. Now, the transpose matrix which is indicated with a superscript capital T which stands for uh transpose, it's basically the same metrics. But as you can see here, the columns have become rows. So here we have the in the first column, 130.5. And here uh the, the 130.5 has become the first row. So we've basically switched rows and columns. So it's all you have it here. And a simple way of notating this in a very condensed manner is this down here. So the transposed Metrix A",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=839s",
        "start_time": "839.9"
    },
    {
        "id": "70e38571",
        "text": "a superscript capital T which stands for uh transpose, it's basically the same metrics. But as you can see here, the columns have become rows. So here we have the in the first column, 130.5. And here uh the, the 130.5 has become the first row. So we've basically switched rows and columns. So it's all you have it here. And a simple way of notating this in a very condensed manner is this down here. So the transposed Metrix A uh of IJ is basically a of J I. So we've inverted the uh two indexes for the columns and the rows down here.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=857s",
        "start_time": "857.95"
    },
    {
        "id": "bcafdda0",
        "text": "uh the, the 130.5 has become the first row. So we've basically switched rows and columns. So it's all you have it here. And a simple way of notating this in a very condensed manner is this down here. So the transposed Metrix A uh of IJ is basically a of J I. So we've inverted the uh two indexes for the columns and the rows down here. OK. So now let's take a look at scalar operations because we also have scalar operations for matrices. And so here we have addition, subtraction, multiplication and division of matrices with a number. And they work in a similar manner to",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=881s",
        "start_time": "881.2"
    },
    {
        "id": "95a938d8",
        "text": "uh of IJ is basically a of J I. So we've inverted the uh two indexes for the columns and the rows down here. OK. So now let's take a look at scalar operations because we also have scalar operations for matrices. And so here we have addition, subtraction, multiplication and division of matrices with a number. And they work in a similar manner to a scalar operations with vectors. So here, for example, you can see a scalar multiplication. So we have a scalar which is this N it could be like any number. So say for example, three and we want to multiply it by this matrix A and the result over here, it's quite simple. So we multiply every index,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=902s",
        "start_time": "902.809"
    },
    {
        "id": "9b3f6553",
        "text": "OK. So now let's take a look at scalar operations because we also have scalar operations for matrices. And so here we have addition, subtraction, multiplication and division of matrices with a number. And they work in a similar manner to a scalar operations with vectors. So here, for example, you can see a scalar multiplication. So we have a scalar which is this N it could be like any number. So say for example, three and we want to multiply it by this matrix A and the result over here, it's quite simple. So we multiply every index, every element in the matrix by N. So the new matrix is, for example, here in the first uh in the index like in column of row one, column one is N by a 11, then we have N by a 12 and so on and so forth. So it's super simple. Same thing with subtraction, multi um addition and division. We do the very same thing here. So",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=918s",
        "start_time": "918.179"
    },
    {
        "id": "ad8d7f97",
        "text": "a scalar operations with vectors. So here, for example, you can see a scalar multiplication. So we have a scalar which is this N it could be like any number. So say for example, three and we want to multiply it by this matrix A and the result over here, it's quite simple. So we multiply every index, every element in the matrix by N. So the new matrix is, for example, here in the first uh in the index like in column of row one, column one is N by a 11, then we have N by a 12 and so on and so forth. So it's super simple. Same thing with subtraction, multi um addition and division. We do the very same thing here. So another type uh of operation is addition and subtraction. So here we have two mattresses and here we also have a constraint. So the two mattresses must have the same dimension. So they must have the very same number of rows and columns. And this is an element wise operation because we,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=937s",
        "start_time": "937.21"
    },
    {
        "id": "a7fd451b",
        "text": "every element in the matrix by N. So the new matrix is, for example, here in the first uh in the index like in column of row one, column one is N by a 11, then we have N by a 12 and so on and so forth. So it's super simple. Same thing with subtraction, multi um addition and division. We do the very same thing here. So another type uh of operation is addition and subtraction. So here we have two mattresses and here we also have a constraint. So the two mattresses must have the same dimension. So they must have the very same number of rows and columns. And this is an element wise operation because we, in the case of addition, for example, adds up the",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=963s",
        "start_time": "963.21"
    },
    {
        "id": "8d2263dc",
        "text": "another type uh of operation is addition and subtraction. So here we have two mattresses and here we also have a constraint. So the two mattresses must have the same dimension. So they must have the very same number of rows and columns. And this is an element wise operation because we, in the case of addition, for example, adds up the elements with the same indexes together from the first and the second matrix. So let's clarify this by taking uh a look at this example down here. So we have this matrix ABC D and we want to add a second matrix which is 1234. And as you can see here, the sum matrix here is given by the addition",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=992s",
        "start_time": "992.669"
    },
    {
        "id": "97eb4a1c",
        "text": "in the case of addition, for example, adds up the elements with the same indexes together from the first and the second matrix. So let's clarify this by taking uh a look at this example down here. So we have this matrix ABC D and we want to add a second matrix which is 1234. And as you can see here, the sum matrix here is given by the addition of the elements of the first matrix with the respective elements of the second matrix. So for element 11, so row one, column one, we are adding a and one together. So a being the first uh being element um in row one column one from the first matrix and one being element uh 11",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1019s",
        "start_time": "1019.0"
    },
    {
        "id": "48730274",
        "text": "elements with the same indexes together from the first and the second matrix. So let's clarify this by taking uh a look at this example down here. So we have this matrix ABC D and we want to add a second matrix which is 1234. And as you can see here, the sum matrix here is given by the addition of the elements of the first matrix with the respective elements of the second matrix. So for element 11, so row one, column one, we are adding a and one together. So a being the first uh being element um in row one column one from the first matrix and one being element uh 11 uh from the second matrix. And then we, we do the same thing for element um 21. So row two, column one and here we add B plus two and as you can see here, so here we have B and here we have two, then we do C plus three and D plus four. So it's very simple. And in this way, we can calculate super easily, both addition and subtraction. So now",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1023s",
        "start_time": "1023.549"
    },
    {
        "id": "8f2b6b20",
        "text": "of the elements of the first matrix with the respective elements of the second matrix. So for element 11, so row one, column one, we are adding a and one together. So a being the first uh being element um in row one column one from the first matrix and one being element uh 11 uh from the second matrix. And then we, we do the same thing for element um 21. So row two, column one and here we add B plus two and as you can see here, so here we have B and here we have two, then we do C plus three and D plus four. So it's very simple. And in this way, we can calculate super easily, both addition and subtraction. So now the problems I would say like start to come in when we talk about matrix multiplication, this is a little bit more complicated than mm um all the other operations that we've seen so far. So first of all, there are certain weird constraints on when we can perform matrix multiplication.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1052s",
        "start_time": "1052.02"
    },
    {
        "id": "fea11dd1",
        "text": "uh from the second matrix. And then we, we do the same thing for element um 21. So row two, column one and here we add B plus two and as you can see here, so here we have B and here we have two, then we do C plus three and D plus four. So it's very simple. And in this way, we can calculate super easily, both addition and subtraction. So now the problems I would say like start to come in when we talk about matrix multiplication, this is a little bit more complicated than mm um all the other operations that we've seen so far. So first of all, there are certain weird constraints on when we can perform matrix multiplication. So we have again two different matrices and we want to multiply them. But then in order to multiply them, we need to have that the number of columns of the first matrix should be equal to the number of rows of the second matrix. Now let's take a look at this example here. So we have this three by two",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1077s",
        "start_time": "1077.839"
    },
    {
        "id": "ef768935",
        "text": "the problems I would say like start to come in when we talk about matrix multiplication, this is a little bit more complicated than mm um all the other operations that we've seen so far. So first of all, there are certain weird constraints on when we can perform matrix multiplication. So we have again two different matrices and we want to multiply them. But then in order to multiply them, we need to have that the number of columns of the first matrix should be equal to the number of rows of the second matrix. Now let's take a look at this example here. So we have this three by two matrix and this two by two matrix here. Now we can multiply these two matrices together because as we said, the number of columns of the first matrix which in this case is two and you can see it here is equal to the number of rows of the second matrix here, which is two. Now the result of this multiplication is a new matrix itself where",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1106s",
        "start_time": "1106.14"
    },
    {
        "id": "3654a6fa",
        "text": "So we have again two different matrices and we want to multiply them. But then in order to multiply them, we need to have that the number of columns of the first matrix should be equal to the number of rows of the second matrix. Now let's take a look at this example here. So we have this three by two matrix and this two by two matrix here. Now we can multiply these two matrices together because as we said, the number of columns of the first matrix which in this case is two and you can see it here is equal to the number of rows of the second matrix here, which is two. Now the result of this multiplication is a new matrix itself where uh the dimension of this new matrix is given by the rows of the first matrix that we want to multiply and the number of columns of the second matrix that we multiply. So in this case, given",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1129s",
        "start_time": "1129.13"
    },
    {
        "id": "1a8e7d4e",
        "text": "matrix and this two by two matrix here. Now we can multiply these two matrices together because as we said, the number of columns of the first matrix which in this case is two and you can see it here is equal to the number of rows of the second matrix here, which is two. Now the result of this multiplication is a new matrix itself where uh the dimension of this new matrix is given by the rows of the first matrix that we want to multiply and the number of columns of the second matrix that we multiply. So in this case, given uh in this multiplication, uh in this example, we have here, given we have the first matrix which is three by two and the second ma matrix which is two by two. Now the product matrix here is gonna be three by two,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1157s",
        "start_time": "1157.54"
    },
    {
        "id": "844b2462",
        "text": "uh the dimension of this new matrix is given by the rows of the first matrix that we want to multiply and the number of columns of the second matrix that we multiply. So in this case, given uh in this multiplication, uh in this example, we have here, given we have the first matrix which is three by two and the second ma matrix which is two by two. Now the product matrix here is gonna be three by two, right. So you may be wondering how do we get to these results? So let's just like perform this mat multiplication here step by step.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1187s",
        "start_time": "1187.449"
    },
    {
        "id": "807671f2",
        "text": "uh in this multiplication, uh in this example, we have here, given we have the first matrix which is three by two and the second ma matrix which is two by two. Now the product matrix here is gonna be three by two, right. So you may be wondering how do we get to these results? So let's just like perform this mat multiplication here step by step. OK. So as you can see here,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1208s",
        "start_time": "1208.959"
    },
    {
        "id": "59843c71",
        "text": "right. So you may be wondering how do we get to these results? So let's just like perform this mat multiplication here step by step. OK. So as you can see here, the first element here, so the element 11, so row one, column one is given by the dot product of the first row of the first matrix with the first column of the second matrix that we are multiplying. So another way of um visualizing this is just by writing it like this. So we have the",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1225s",
        "start_time": "1225.229"
    },
    {
        "id": "7a2567ba",
        "text": "OK. So as you can see here, the first element here, so the element 11, so row one, column one is given by the dot product of the first row of the first matrix with the first column of the second matrix that we are multiplying. So another way of um visualizing this is just by writing it like this. So we have the uh this vector 12 and we do the dot product with AC and the result now we should know it because we know the dot product is one A plus two C. So, and this is like what we are performing here and this result gets logged in position +11 because it comes from row one and column one.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1238s",
        "start_time": "1238.06"
    },
    {
        "id": "9645b599",
        "text": "the first element here, so the element 11, so row one, column one is given by the dot product of the first row of the first matrix with the first column of the second matrix that we are multiplying. So another way of um visualizing this is just by writing it like this. So we have the uh this vector 12 and we do the dot product with AC and the result now we should know it because we know the dot product is one A plus two C. So, and this is like what we are performing here and this result gets logged in position +11 because it comes from row one and column one. So now let's move on with this very simple algorithm to understand how to perform all of the um uh perform multiplication and create the new matrix and all of its elements. So now we can shift",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1241s",
        "start_time": "1241.479"
    },
    {
        "id": "5b35347c",
        "text": "uh this vector 12 and we do the dot product with AC and the result now we should know it because we know the dot product is one A plus two C. So, and this is like what we are performing here and this result gets logged in position +11 because it comes from row one and column one. So now let's move on with this very simple algorithm to understand how to perform all of the um uh perform multiplication and create the new matrix and all of its elements. So now we can shift uh the column vector here. And so we have here the dot product between the first row of the first matrix and the second column of the second matrix. And",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1270s",
        "start_time": "1270.13"
    },
    {
        "id": "cb98dbdb",
        "text": "So now let's move on with this very simple algorithm to understand how to perform all of the um uh perform multiplication and create the new matrix and all of its elements. So now we can shift uh the column vector here. And so we have here the dot product between the first row of the first matrix and the second column of the second matrix. And we log these uh dot products here in the element 12. So row one column two. Now, not surprisingly, you'll see that we'll switch down to the second row here and we'll do the dot product of this second row with the um first column of the second matrix. And this is a three A plus four C and we lock that in element +21,",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1297s",
        "start_time": "1297.979"
    },
    {
        "id": "2d0ecf75",
        "text": "uh the column vector here. And so we have here the dot product between the first row of the first matrix and the second column of the second matrix. And we log these uh dot products here in the element 12. So row one column two. Now, not surprisingly, you'll see that we'll switch down to the second row here and we'll do the dot product of this second row with the um first column of the second matrix. And this is a three A plus four C and we lock that in element +21, we'll switch and we'll have element 22 and then we'll switch again and then we'll move to uh the third row. And here we have the third row by the first column uh with the dot product. And here we log this in element 31 and then the final element will be the third row here dot products. Second column. And here the result is logged in element 32.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1315s",
        "start_time": "1315.119"
    },
    {
        "id": "46994672",
        "text": "we log these uh dot products here in the element 12. So row one column two. Now, not surprisingly, you'll see that we'll switch down to the second row here and we'll do the dot product of this second row with the um first column of the second matrix. And this is a three A plus four C and we lock that in element +21, we'll switch and we'll have element 22 and then we'll switch again and then we'll move to uh the third row. And here we have the third row by the first column uh with the dot product. And here we log this in element 31 and then the final element will be the third row here dot products. Second column. And here the result is logged in element 32. Cool.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1331s",
        "start_time": "1331.969"
    },
    {
        "id": "558d2808",
        "text": "we'll switch and we'll have element 22 and then we'll switch again and then we'll move to uh the third row. And here we have the third row by the first column uh with the dot product. And here we log this in element 31 and then the final element will be the third row here dot products. Second column. And here the result is logged in element 32. Cool. Sorry. So we can",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1361s",
        "start_time": "1361.819"
    },
    {
        "id": "db0d95b1",
        "text": "Cool. Sorry. So we can um rewrite um a matrix multiplication in a way like that. That's easier like to, to remember. So again, if you think of like for example, like this 12 as a, a row vector and it's row vector A one because like this is matrix A and this is matrix B. So this is a one. And then we take a look at this second row vector here and we indicate it as A two.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1392s",
        "start_time": "1392.52"
    },
    {
        "id": "f56a34d9",
        "text": "Sorry. So we can um rewrite um a matrix multiplication in a way like that. That's easier like to, to remember. So again, if you think of like for example, like this 12 as a, a row vector and it's row vector A one because like this is matrix A and this is matrix B. So this is a one. And then we take a look at this second row vector here and we indicate it as A two. And we look at this B matrix here and we call this column, the column vector B one and the second column, the column vector B two. Then the matrix multiplication of it here is given by A one dot Predators B one for element 11, then A one B two for element 12.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1394s",
        "start_time": "1394.26"
    },
    {
        "id": "d83c9fa4",
        "text": "um rewrite um a matrix multiplication in a way like that. That's easier like to, to remember. So again, if you think of like for example, like this 12 as a, a row vector and it's row vector A one because like this is matrix A and this is matrix B. So this is a one. And then we take a look at this second row vector here and we indicate it as A two. And we look at this B matrix here and we call this column, the column vector B one and the second column, the column vector B two. Then the matrix multiplication of it here is given by A one dot Predators B one for element 11, then A one B two for element 12. And then uh the dot product between the A two row vector and the B one column vector for element to one and A two",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1397s",
        "start_time": "1397.56"
    },
    {
        "id": "700d22b2",
        "text": "And we look at this B matrix here and we call this column, the column vector B one and the second column, the column vector B two. Then the matrix multiplication of it here is given by A one dot Predators B one for element 11, then A one B two for element 12. And then uh the dot product between the A two row vector and the B one column vector for element to one and A two uh dot products B two for element 22. So this is like a very simple way of remembering how a matrix multiplication works. So this was a little bit like weirder than all the other um operations that we've seen so far. But as you'll see, this is very, very important because we can use all of this notation to",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1427s",
        "start_time": "1427.689"
    },
    {
        "id": "791e03fe",
        "text": "And then uh the dot product between the A two row vector and the B one column vector for element to one and A two uh dot products B two for element 22. So this is like a very simple way of remembering how a matrix multiplication works. So this was a little bit like weirder than all the other um operations that we've seen so far. But as you'll see, this is very, very important because we can use all of this notation to easily capture how a neuron network performs its computations. And that's what we're gonna see in the next video. So we're gonna look at neuron networks and specifically at multi layer perceptions and see how they compute the information when the signal moves from left to right.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1455s",
        "start_time": "1455.349"
    },
    {
        "id": "381ef873",
        "text": "uh dot products B two for element 22. So this is like a very simple way of remembering how a matrix multiplication works. So this was a little bit like weirder than all the other um operations that we've seen so far. But as you'll see, this is very, very important because we can use all of this notation to easily capture how a neuron network performs its computations. And that's what we're gonna see in the next video. So we're gonna look at neuron networks and specifically at multi layer perceptions and see how they compute the information when the signal moves from left to right. So this was it for this very quick, I would say like very quick introduction to linear algebra. I hope you enjoyed this. And if you like the video, please subscribe. If you have any questions, just leave a comment in the comment section below. And if you like the video again, just like like it. So I hope I'll see you next time. So, bye for now.",
        "video": "4- Vector and matrix operations",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "FmD1S5yP_os",
        "youtube_link": "https://www.youtube.com/watch?v=FmD1S5yP_os&t=1466s",
        "start_time": "1466.189"
    },
    {
        "id": "6cd8063d",
        "text": "Hi, everybody and welcome to the last video in the Deep Learning for Rode with Python series. This time we're gonna implement an RNNLSDM network for music genre classification. Now we've already built a convolutional neural network that can do, can perform a music genre classification. So we're basically gonna use that code as a basis and we're gonna just change a few things around that makes sense. So to have an RNNLSDM instead of a CNN, right? OK. So uh let's see like what the code that we already have and we'll start from the main and if you guys like have followed along in the series, you probably can recognize most of this code because we've already done this like in previous videos. So over here, uh the first thing that we'll do is we'll get a train validation and",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=0s",
        "start_time": "0.31"
    },
    {
        "id": "ca0db7c7",
        "text": "Now we've already built a convolutional neural network that can do, can perform a music genre classification. So we're basically gonna use that code as a basis and we're gonna just change a few things around that makes sense. So to have an RNNLSDM instead of a CNN, right? OK. So uh let's see like what the code that we already have and we'll start from the main and if you guys like have followed along in the series, you probably can recognize most of this code because we've already done this like in previous videos. So over here, uh the first thing that we'll do is we'll get a train validation and splits starting from the MF CCS that we've extracted in a previous video where we pre processed a data, data set for um the, the for the music genre classification task, right? So once we have this data set, so uh for train validation and test. So what what we do next is we create the network",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=12s",
        "start_time": "12.739"
    },
    {
        "id": "0eaad719",
        "text": "and if you guys like have followed along in the series, you probably can recognize most of this code because we've already done this like in previous videos. So over here, uh the first thing that we'll do is we'll get a train validation and splits starting from the MF CCS that we've extracted in a previous video where we pre processed a data, data set for um the, the for the music genre classification task, right? So once we have this data set, so uh for train validation and test. So what what we do next is we create the network and uh When we create the network over here, we have this build a model function that we need to change for obvious reasons. Because build model in this code is gonna build a CNN and we want to convert that to an RNN, then we compile the model, then we get like a summary of the model. Then we train the model with model dot fits. Uh Then we want to plot the accuracy and error for training and validation.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=41s",
        "start_time": "41.79"
    },
    {
        "id": "8d76d096",
        "text": "splits starting from the MF CCS that we've extracted in a previous video where we pre processed a data, data set for um the, the for the music genre classification task, right? So once we have this data set, so uh for train validation and test. So what what we do next is we create the network and uh When we create the network over here, we have this build a model function that we need to change for obvious reasons. Because build model in this code is gonna build a CNN and we want to convert that to an RNN, then we compile the model, then we get like a summary of the model. Then we train the model with model dot fits. Uh Then we want to plot the accuracy and error for training and validation. And we have this function here that does that. And finally, we want to evaluate the model on, on the test set to see how well it does and how capable it is to generalize. OK. So let's get started and see like what we need to change here. So the first thing is this prepare a data sets and here we have a couple of arguments that we pass. So no 0.25 in this case indicates that we use 1/4 of the data",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=57s",
        "start_time": "57.575"
    },
    {
        "id": "42549517",
        "text": "and uh When we create the network over here, we have this build a model function that we need to change for obvious reasons. Because build model in this code is gonna build a CNN and we want to convert that to an RNN, then we compile the model, then we get like a summary of the model. Then we train the model with model dot fits. Uh Then we want to plot the accuracy and error for training and validation. And we have this function here that does that. And finally, we want to evaluate the model on, on the test set to see how well it does and how capable it is to generalize. OK. So let's get started and see like what we need to change here. So the first thing is this prepare a data sets and here we have a couple of arguments that we pass. So no 0.25 in this case indicates that we use 1/4 of the data set for testing purposes. And this no 0.2 over here says that of the remaining 75% we want to use 20% of that for the validation split. Cool. But what should we change here? Well, let's go and figure that out.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=83s",
        "start_time": "83.98"
    },
    {
        "id": "139799c6",
        "text": "And we have this function here that does that. And finally, we want to evaluate the model on, on the test set to see how well it does and how capable it is to generalize. OK. So let's get started and see like what we need to change here. So the first thing is this prepare a data sets and here we have a couple of arguments that we pass. So no 0.25 in this case indicates that we use 1/4 of the data set for testing purposes. And this no 0.2 over here says that of the remaining 75% we want to use 20% of that for the validation split. Cool. But what should we change here? Well, let's go and figure that out. So here in prepared data sets, uh we just like load uh the data and then obviously we load both the inputs and uh the the targets, what the whys and then we create train validation and test split. And he",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=113s",
        "start_time": "113.62"
    },
    {
        "id": "cb71c003",
        "text": "set for testing purposes. And this no 0.2 over here says that of the remaining 75% we want to use 20% of that for the validation split. Cool. But what should we change here? Well, let's go and figure that out. So here in prepared data sets, uh we just like load uh the data and then obviously we load both the inputs and uh the the targets, what the whys and then we create train validation and test split. And he here, once we have that uh for CNN, we had to add an extra access to the input sets. And you have it here, for example. So X strain is equal to X strain with this like weird like three dots comma",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=142s",
        "start_time": "142.339"
    },
    {
        "id": "56dee841",
        "text": "So here in prepared data sets, uh we just like load uh the data and then obviously we load both the inputs and uh the the targets, what the whys and then we create train validation and test split. And he here, once we have that uh for CNN, we had to add an extra access to the input sets. And you have it here, for example. So X strain is equal to X strain with this like weird like three dots comma NPNP dot New access. So we are adding a third dimension and this is because in a CNN tensorflow expects three dimensions. And in this case, the first one, the first dimension represents the number of steps that we have.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=159s",
        "start_time": "159.649"
    },
    {
        "id": "6431d42c",
        "text": "here, once we have that uh for CNN, we had to add an extra access to the input sets. And you have it here, for example. So X strain is equal to X strain with this like weird like three dots comma NPNP dot New access. So we are adding a third dimension and this is because in a CNN tensorflow expects three dimensions. And in this case, the first one, the first dimension represents the number of steps that we have. So the number of uh MFCC slices that we take. The second dimension is 13 in this case, which is the number of MF CCS that we are actually taking at each snapshot. And the third dimension is equal to the depth and the depth with any audio data is usually just one because we have like only one dimension go",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=177s",
        "start_time": "177.315"
    },
    {
        "id": "ed99eff4",
        "text": "NPNP dot New access. So we are adding a third dimension and this is because in a CNN tensorflow expects three dimensions. And in this case, the first one, the first dimension represents the number of steps that we have. So the number of uh MFCC slices that we take. The second dimension is 13 in this case, which is the number of MF CCS that we are actually taking at each snapshot. And the third dimension is equal to the depth and the depth with any audio data is usually just one because we have like only one dimension go so to prepare the data set for an R and N, we don't need this third dimension. So we'll just drop that. And so with that, we should have done all that's required for prepared data sets. So now let's go back to",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=195s",
        "start_time": "195.119"
    },
    {
        "id": "8c31ddbe",
        "text": "So the number of uh MFCC slices that we take. The second dimension is 13 in this case, which is the number of MF CCS that we are actually taking at each snapshot. And the third dimension is equal to the depth and the depth with any audio data is usually just one because we have like only one dimension go so to prepare the data set for an R and N, we don't need this third dimension. So we'll just drop that. And so with that, we should have done all that's required for prepared data sets. So now let's go back to domain over here. So now uh after uh like this uh line of instruction, so we should have the train validation and test splits. And now we should create the network. Now we should change another thing here. So it's the input shape. So the input shape. So the shape that the RNN expects in these cases, we already said that is by dimensions, it's two dimensional. So, but if you see here,",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=209s",
        "start_time": "209.169"
    },
    {
        "id": "77c51ab9",
        "text": "so to prepare the data set for an R and N, we don't need this third dimension. So we'll just drop that. And so with that, we should have done all that's required for prepared data sets. So now let's go back to domain over here. So now uh after uh like this uh line of instruction, so we should have the train validation and test splits. And now we should create the network. Now we should change another thing here. So it's the input shape. So the input shape. So the shape that the RNN expects in these cases, we already said that is by dimensions, it's two dimensional. So, but if you see here, uh like in this line, you see that we have like an input shape that's equal, it's a three dimensional, right? And we want to drop this third dimension over here. So which is like the depth and we remain only with two dimensions. And specifically, I believe that the first dimension is equal to 100 and 30 which is the number of slices or time steps",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=232s",
        "start_time": "232.57"
    },
    {
        "id": "255a0840",
        "text": "domain over here. So now uh after uh like this uh line of instruction, so we should have the train validation and test splits. And now we should create the network. Now we should change another thing here. So it's the input shape. So the input shape. So the shape that the RNN expects in these cases, we already said that is by dimensions, it's two dimensional. So, but if you see here, uh like in this line, you see that we have like an input shape that's equal, it's a three dimensional, right? And we want to drop this third dimension over here. So which is like the depth and we remain only with two dimensions. And specifically, I believe that the first dimension is equal to 100 and 30 which is the number of slices or time steps at which we take uh the MF CCS. And then the second dimension is equal to 13, which is the actual coefficients that we extract from or we've extracted when pre when processing the data, right?",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=248s",
        "start_time": "248.134"
    },
    {
        "id": "79098324",
        "text": "uh like in this line, you see that we have like an input shape that's equal, it's a three dimensional, right? And we want to drop this third dimension over here. So which is like the depth and we remain only with two dimensions. And specifically, I believe that the first dimension is equal to 100 and 30 which is the number of slices or time steps at which we take uh the MF CCS. And then the second dimension is equal to 13, which is the actual coefficients that we extract from or we've extracted when pre when processing the data, right? And then we build the model perfect. Now we, we need to like take a look at this because this is the, the core uh of like the stuff that we have to change over here. So this is the function that generates a CNN model. Obviously, we don't want a CNN model here. Rather we want a, an R and N uh LSDM uh model. And so I'm gonna change that also here.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=275s",
        "start_time": "275.7"
    },
    {
        "id": "567f2dbb",
        "text": "at which we take uh the MF CCS. And then the second dimension is equal to 13, which is the actual coefficients that we extract from or we've extracted when pre when processing the data, right? And then we build the model perfect. Now we, we need to like take a look at this because this is the, the core uh of like the stuff that we have to change over here. So this is the function that generates a CNN model. Obviously, we don't want a CNN model here. Rather we want a, an R and N uh LSDM uh model. And so I'm gonna change that also here. Cool. OK. So let's see the network topology over here. So we have like we, we just like build uh this like sequential model and then we have a bunch of convolutional layers. And obviously, we want to drop all of these convolutional layers and we'll just leave the output layer over here uh which is a soft max layer with 10 neurons and this 10 neurons correspond with the 10 different genres that we want to predict.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=297s",
        "start_time": "297.929"
    },
    {
        "id": "7e181399",
        "text": "And then we build the model perfect. Now we, we need to like take a look at this because this is the, the core uh of like the stuff that we have to change over here. So this is the function that generates a CNN model. Obviously, we don't want a CNN model here. Rather we want a, an R and N uh LSDM uh model. And so I'm gonna change that also here. Cool. OK. So let's see the network topology over here. So we have like we, we just like build uh this like sequential model and then we have a bunch of convolutional layers. And obviously, we want to drop all of these convolutional layers and we'll just leave the output layer over here uh which is a soft max layer with 10 neurons and this 10 neurons correspond with the 10 different genres that we want to predict. Good. OK. So uh what we need to build here is a couple of uh LSDM layers. So we'll build two LSTM uh layers, right? OK. So how do we do that? Well, that is as usual with uh sensor flu and carers. Very, very simple. So we do a model dot art and then we want to add analys TM layer",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=311s",
        "start_time": "311.72"
    },
    {
        "id": "a4cf493e",
        "text": "Cool. OK. So let's see the network topology over here. So we have like we, we just like build uh this like sequential model and then we have a bunch of convolutional layers. And obviously, we want to drop all of these convolutional layers and we'll just leave the output layer over here uh which is a soft max layer with 10 neurons and this 10 neurons correspond with the 10 different genres that we want to predict. Good. OK. So uh what we need to build here is a couple of uh LSDM layers. So we'll build two LSTM uh layers, right? OK. So how do we do that? Well, that is as usual with uh sensor flu and carers. Very, very simple. So we do a model dot art and then we want to add analys TM layer uh and to do that, not surprisingly, we do Kous dot Layers dot LSTM good. OK. So here we should specify a few things. So the first one is the number of units that we want for this LSTM layer and this is equal to 64. So we're gonna have 64 units. Then we need to specify the input shape and the input shape is gonna be equal to input shape, which is this",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=339s",
        "start_time": "339.66"
    },
    {
        "id": "37784951",
        "text": "Good. OK. So uh what we need to build here is a couple of uh LSDM layers. So we'll build two LSTM uh layers, right? OK. So how do we do that? Well, that is as usual with uh sensor flu and carers. Very, very simple. So we do a model dot art and then we want to add analys TM layer uh and to do that, not surprisingly, we do Kous dot Layers dot LSTM good. OK. So here we should specify a few things. So the first one is the number of units that we want for this LSTM layer and this is equal to 64. So we're gonna have 64 units. Then we need to specify the input shape and the input shape is gonna be equal to input shape, which is this um uh argument that we are passing to build model to, to this function. And then we need a, an extra um",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=369s",
        "start_time": "369.809"
    },
    {
        "id": "6edd6d89",
        "text": "uh and to do that, not surprisingly, we do Kous dot Layers dot LSTM good. OK. So here we should specify a few things. So the first one is the number of units that we want for this LSTM layer and this is equal to 64. So we're gonna have 64 units. Then we need to specify the input shape and the input shape is gonna be equal to input shape, which is this um uh argument that we are passing to build model to, to this function. And then we need a, an extra um an extra argument here which is really like a, a boolean argument that's called a return sequences and we need to set this to true. So what is this? Well, if you guys remember from my video on R and MS and if you haven't watched that, you should go like uh uh and watch that out because like it will give you like a more profound understanding of what we are doing here.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=398s",
        "start_time": "398.47"
    },
    {
        "id": "290fe674",
        "text": "um uh argument that we are passing to build model to, to this function. And then we need a, an extra um an extra argument here which is really like a, a boolean argument that's called a return sequences and we need to set this to true. So what is this? Well, if you guys remember from my video on R and MS and if you haven't watched that, you should go like uh uh and watch that out because like it will give you like a more profound understanding of what we are doing here. But if you remember that we have two types of RNM uh layers. So one is called sequence to sequence. And so basically, we pass in a sequence to an RNN layer and then we get back a, a sequence. Uh And there's another one that's called a sequence to vector. So we pass in as input as sequence.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=427s",
        "start_time": "427.929"
    },
    {
        "id": "1868f241",
        "text": "an extra argument here which is really like a, a boolean argument that's called a return sequences and we need to set this to true. So what is this? Well, if you guys remember from my video on R and MS and if you haven't watched that, you should go like uh uh and watch that out because like it will give you like a more profound understanding of what we are doing here. But if you remember that we have two types of RNM uh layers. So one is called sequence to sequence. And so basically, we pass in a sequence to an RNN layer and then we get back a, a sequence. Uh And there's another one that's called a sequence to vector. So we pass in as input as sequence. Uh So for example, a time series, but then we don't get a sequence as an output but just like the final step. So the final prediction, it's as if like in a in a melody, I pass, I pass like 10 notes and then I expect only like the the new notes like the prediction for the 11th notes, right? OK. So here we want to return a sequence because we want a sequence to sequence uh layer",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=438s",
        "start_time": "438.17"
    },
    {
        "id": "ffd87751",
        "text": "But if you remember that we have two types of RNM uh layers. So one is called sequence to sequence. And so basically, we pass in a sequence to an RNN layer and then we get back a, a sequence. Uh And there's another one that's called a sequence to vector. So we pass in as input as sequence. Uh So for example, a time series, but then we don't get a sequence as an output but just like the final step. So the final prediction, it's as if like in a in a melody, I pass, I pass like 10 notes and then I expect only like the the new notes like the prediction for the 11th notes, right? OK. So here we want to return a sequence because we want a sequence to sequence uh layer and so we have to set written sequences to true. Now why do, why do we do that well? Because we want to pass that sequence into the second LSDM layer. And so the second LSDM layer uh is gonna be equal to Model dot art. And then we pass in KIS uh layers LSDM.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=463s",
        "start_time": "463.69"
    },
    {
        "id": "0b568899",
        "text": "Uh So for example, a time series, but then we don't get a sequence as an output but just like the final step. So the final prediction, it's as if like in a in a melody, I pass, I pass like 10 notes and then I expect only like the the new notes like the prediction for the 11th notes, right? OK. So here we want to return a sequence because we want a sequence to sequence uh layer and so we have to set written sequences to true. Now why do, why do we do that well? Because we want to pass that sequence into the second LSDM layer. And so the second LSDM layer uh is gonna be equal to Model dot art. And then we pass in KIS uh layers LSDM. And now all we need to pass in is the number of units which again, I'm gonna set to 64 good. So now we have two LSDM layers.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=483s",
        "start_time": "483.929"
    },
    {
        "id": "108adbb2",
        "text": "and so we have to set written sequences to true. Now why do, why do we do that well? Because we want to pass that sequence into the second LSDM layer. And so the second LSDM layer uh is gonna be equal to Model dot art. And then we pass in KIS uh layers LSDM. And now all we need to pass in is the number of units which again, I'm gonna set to 64 good. So now we have two LSDM layers. So now I'm gonna have another layer that's a dense layer. So in in order to do that, so I'll do a Model dot Art again,",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=512s",
        "start_time": "512.0"
    },
    {
        "id": "85e523a7",
        "text": "And now all we need to pass in is the number of units which again, I'm gonna set to 64 good. So now we have two LSDM layers. So now I'm gonna have another layer that's a dense layer. So in in order to do that, so I'll do a Model dot Art again, So I hope that by now, you you just like are able just like to predict all of these instructions because I mean, we've seen them quite a lot so far and then they are very, very intuitive. And this is like the great thing about carers. So it makes things and building like this network also very complex network, quite easy to do. So again, we want to dance layers, so we'll do a Kous dot Layers dot uh dance,",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=537s",
        "start_time": "537.719"
    },
    {
        "id": "51cf7661",
        "text": "So now I'm gonna have another layer that's a dense layer. So in in order to do that, so I'll do a Model dot Art again, So I hope that by now, you you just like are able just like to predict all of these instructions because I mean, we've seen them quite a lot so far and then they are very, very intuitive. And this is like the great thing about carers. So it makes things and building like this network also very complex network, quite easy to do. So again, we want to dance layers, so we'll do a Kous dot Layers dot uh dance, right? And so here we need to pass in a couple of arguments. So the first one being the number of units which we set to 64 again, and then we need to specify the activation function that we wanna use. And in this case, I'm gonna use recti rectified linear unit or R good. And I want to add also another layer that's a drop out layer. And",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=550s",
        "start_time": "550.89"
    },
    {
        "id": "b6909f1f",
        "text": "So I hope that by now, you you just like are able just like to predict all of these instructions because I mean, we've seen them quite a lot so far and then they are very, very intuitive. And this is like the great thing about carers. So it makes things and building like this network also very complex network, quite easy to do. So again, we want to dance layers, so we'll do a Kous dot Layers dot uh dance, right? And so here we need to pass in a couple of arguments. So the first one being the number of units which we set to 64 again, and then we need to specify the activation function that we wanna use. And in this case, I'm gonna use recti rectified linear unit or R good. And I want to add also another layer that's a drop out layer. And uh I'm gonna add this just like to, to avoid uh overfitting or just like to mitigate uh the issue of overfitting. Now, if you don't remember what dropout is, again, I have a video on that uh on overfitting and how to solve that. And you should check that out and it should be like above. And so just like click that",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=563s",
        "start_time": "563.53"
    },
    {
        "id": "59ec33b1",
        "text": "right? And so here we need to pass in a couple of arguments. So the first one being the number of units which we set to 64 again, and then we need to specify the activation function that we wanna use. And in this case, I'm gonna use recti rectified linear unit or R good. And I want to add also another layer that's a drop out layer. And uh I'm gonna add this just like to, to avoid uh overfitting or just like to mitigate uh the issue of overfitting. Now, if you don't remember what dropout is, again, I have a video on that uh on overfitting and how to solve that. And you should check that out and it should be like above. And so just like click that cool. OK. So uh so we were saying we want this drop out layer. So we'll do Kous dot uh again, layers dot uh drop out and then we'll set the dropout probability to uh no 0.3 or 30% good. So now I believe that we have built the whole model, the whole R and N long shot and memory",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=591s",
        "start_time": "591.099"
    },
    {
        "id": "0915cacb",
        "text": "uh I'm gonna add this just like to, to avoid uh overfitting or just like to mitigate uh the issue of overfitting. Now, if you don't remember what dropout is, again, I have a video on that uh on overfitting and how to solve that. And you should check that out and it should be like above. And so just like click that cool. OK. So uh so we were saying we want this drop out layer. So we'll do Kous dot uh again, layers dot uh drop out and then we'll set the dropout probability to uh no 0.3 or 30% good. So now I believe that we have built the whole model, the whole R and N long shot and memory network good. So just like let's revise this like very quickly. So first of all, we build, we get like this, we create this sequential model. Then we add a couple of LSDM layers. The",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=617s",
        "start_time": "617.44"
    },
    {
        "id": "ea858794",
        "text": "cool. OK. So uh so we were saying we want this drop out layer. So we'll do Kous dot uh again, layers dot uh drop out and then we'll set the dropout probability to uh no 0.3 or 30% good. So now I believe that we have built the whole model, the whole R and N long shot and memory network good. So just like let's revise this like very quickly. So first of all, we build, we get like this, we create this sequential model. Then we add a couple of LSDM layers. The first which is a sequence to sequence um a layer. The second one is just a sequence to vector. And I don't need to specify that because the default is return sequences equal to fault in carers.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=639s",
        "start_time": "639.64"
    },
    {
        "id": "2da6b6c5",
        "text": "network good. So just like let's revise this like very quickly. So first of all, we build, we get like this, we create this sequential model. Then we add a couple of LSDM layers. The first which is a sequence to sequence um a layer. The second one is just a sequence to vector. And I don't need to specify that because the default is return sequences equal to fault in carers. And then I've added a dense layer. And finally, uh the dense layer uh gets input into like the output layer which is a soft max classifier with 10 neurons. And uh the 10 neurons represent like the 10 different musical genres that we want to predict. Cool. So now let's go back to the um main.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=665s",
        "start_time": "665.26"
    },
    {
        "id": "68091e55",
        "text": "first which is a sequence to sequence um a layer. The second one is just a sequence to vector. And I don't need to specify that because the default is return sequences equal to fault in carers. And then I've added a dense layer. And finally, uh the dense layer uh gets input into like the output layer which is a soft max classifier with 10 neurons. And uh the 10 neurons represent like the 10 different musical genres that we want to predict. Cool. So now let's go back to the um main. And so, and we'll see that all the rest over here should be fine and good to go. So we are here. So we've just like created uh the, the network, we've built the model, then we're gonna compile the model and for compiling the model, we're gonna just use like this very same setting. So I'm gonna use AAM as the optimizer with the learning rate on 0.0001.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=680s",
        "start_time": "680.27"
    },
    {
        "id": "710e9d26",
        "text": "And then I've added a dense layer. And finally, uh the dense layer uh gets input into like the output layer which is a soft max classifier with 10 neurons. And uh the 10 neurons represent like the 10 different musical genres that we want to predict. Cool. So now let's go back to the um main. And so, and we'll see that all the rest over here should be fine and good to go. So we are here. So we've just like created uh the, the network, we've built the model, then we're gonna compile the model and for compiling the model, we're gonna just use like this very same setting. So I'm gonna use AAM as the optimizer with the learning rate on 0.0001. And then we'll compile uh the model and which we, we'll use like as the error function this past category called Kenty and as the metrics, we're gonna track accuracy. Uh I'll just like uh run the script here and uh see how that, how that goes. Cool. OK.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=695s",
        "start_time": "695.59"
    },
    {
        "id": "24141916",
        "text": "And so, and we'll see that all the rest over here should be fine and good to go. So we are here. So we've just like created uh the, the network, we've built the model, then we're gonna compile the model and for compiling the model, we're gonna just use like this very same setting. So I'm gonna use AAM as the optimizer with the learning rate on 0.0001. And then we'll compile uh the model and which we, we'll use like as the error function this past category called Kenty and as the metrics, we're gonna track accuracy. Uh I'll just like uh run the script here and uh see how that, how that goes. Cool. OK. So as usually it's gonna take like some time to like train the whole thing. So I'm just gonna post the video now and uh just like get back like once we have uh the results. So the training process has finished and we got the results here and the test accuracy is 64% which is a quite decent result given we have 10 different um genres.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=721s",
        "start_time": "721.34"
    },
    {
        "id": "f966b06e",
        "text": "And then we'll compile uh the model and which we, we'll use like as the error function this past category called Kenty and as the metrics, we're gonna track accuracy. Uh I'll just like uh run the script here and uh see how that, how that goes. Cool. OK. So as usually it's gonna take like some time to like train the whole thing. So I'm just gonna post the video now and uh just like get back like once we have uh the results. So the training process has finished and we got the results here and the test accuracy is 64% which is a quite decent result given we have 10 different um genres. And yeah, and I think like it's probably close to the result that we also got with the CNN. Good. So yeah, I guess like this is like it for this video now, you know how to build an RNN long short term memory network, which is great. Cool. So at the same time, this is the end of the deep learning for audio with Python series. And by now if you followed it,",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=745s",
        "start_time": "745.82"
    },
    {
        "id": "c0a9748d",
        "text": "So as usually it's gonna take like some time to like train the whole thing. So I'm just gonna post the video now and uh just like get back like once we have uh the results. So the training process has finished and we got the results here and the test accuracy is 64% which is a quite decent result given we have 10 different um genres. And yeah, and I think like it's probably close to the result that we also got with the CNN. Good. So yeah, I guess like this is like it for this video now, you know how to build an RNN long short term memory network, which is great. Cool. So at the same time, this is the end of the deep learning for audio with Python series. And by now if you followed it, you should be able to like build your own models and carers, be able to uh process all your data extract MF CS, perform fourier transforms and do like a bunch more things and have an understanding like of all your data more in general.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=769s",
        "start_time": "769.44"
    },
    {
        "id": "d8ab8464",
        "text": "And yeah, and I think like it's probably close to the result that we also got with the CNN. Good. So yeah, I guess like this is like it for this video now, you know how to build an RNN long short term memory network, which is great. Cool. So at the same time, this is the end of the deep learning for audio with Python series. And by now if you followed it, you should be able to like build your own models and carers, be able to uh process all your data extract MF CS, perform fourier transforms and do like a bunch more things and have an understanding like of all your data more in general. Cool. I hope you really enjoyed this uh series for me. It's been like a very, very nice uh journey and if that's the case, please consider subscribing. And yeah, so uh another thing that uh would be great is if you could just le leave a comment in the comment section below and let me know what you'd like to learn next in the A I music A I audio space.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=796s",
        "start_time": "796.469"
    },
    {
        "id": "dbfe0e32",
        "text": "you should be able to like build your own models and carers, be able to uh process all your data extract MF CS, perform fourier transforms and do like a bunch more things and have an understanding like of all your data more in general. Cool. I hope you really enjoyed this uh series for me. It's been like a very, very nice uh journey and if that's the case, please consider subscribing. And yeah, so uh another thing that uh would be great is if you could just le leave a comment in the comment section below and let me know what you'd like to learn next in the A I music A I audio space. That's all for today. So I hope you enjoyed all of this and if that's the case, I'll see you next time. Cheers.",
        "video": "19- How to Implement an RNN-LSTM Network for Music Genre Classification",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "4nXI0h2sq2I",
        "youtube_link": "https://www.youtube.com/watch?v=4nXI0h2sq2I&t=819s",
        "start_time": "819.7"
    },
    {
        "id": "9a2c8766",
        "text": "Hi, everybody and welcome to a new video in the Deep Learning for audio with Python series. This time we're gonna introduce basic concepts about audio data and signal processing. Specifically, we're gonna look into waveforms, sound concepts like pitch loudness and things that are a little bit more advanced, like spectrograms, fourier transform and MF CCS. And we're gonna need all of these elements because these are the bases we'll need for implementing audio and music, deep learning models. Cool. So a disclaimer is needed here. So I'm not, this is not a comprehensive video on audio, digital signal processing rather. It, it will give you just like the the basic foundations you'll need for like deep learning in this field. But if you want to know more about this fascinating topic, like let me know in the comments section and I may make a few videos on the topic moving forward. Cool. So let's get started. So first question. So what sound well,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=0s",
        "start_time": "0.23"
    },
    {
        "id": "fd6e6b5e",
        "text": "Specifically, we're gonna look into waveforms, sound concepts like pitch loudness and things that are a little bit more advanced, like spectrograms, fourier transform and MF CCS. And we're gonna need all of these elements because these are the bases we'll need for implementing audio and music, deep learning models. Cool. So a disclaimer is needed here. So I'm not, this is not a comprehensive video on audio, digital signal processing rather. It, it will give you just like the the basic foundations you'll need for like deep learning in this field. But if you want to know more about this fascinating topic, like let me know in the comments section and I may make a few videos on the topic moving forward. Cool. So let's get started. So first question. So what sound well, sound is produced when there's an object that vibrates and these vibrations and determine the oscillation of air molecules, which basically creates an alternation of air pressure. And this high pressure alternated with low pressure causes a wave and we can represent this wave using a nice wave form.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=11s",
        "start_time": "11.5"
    },
    {
        "id": "483a7833",
        "text": "So a disclaimer is needed here. So I'm not, this is not a comprehensive video on audio, digital signal processing rather. It, it will give you just like the the basic foundations you'll need for like deep learning in this field. But if you want to know more about this fascinating topic, like let me know in the comments section and I may make a few videos on the topic moving forward. Cool. So let's get started. So first question. So what sound well, sound is produced when there's an object that vibrates and these vibrations and determine the oscillation of air molecules, which basically creates an alternation of air pressure. And this high pressure alternated with low pressure causes a wave and we can represent this wave using a nice wave form. And in this case, we have like a very nice wave that oscillates. And we can represent it using an amplitude and a time because at the end of the day, this is a wave is just like a, a point that oscillates with different like amplitude in different points cool. So",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=38s",
        "start_time": "38.159"
    },
    {
        "id": "92e094a9",
        "text": "sound is produced when there's an object that vibrates and these vibrations and determine the oscillation of air molecules, which basically creates an alternation of air pressure. And this high pressure alternated with low pressure causes a wave and we can represent this wave using a nice wave form. And in this case, we have like a very nice wave that oscillates. And we can represent it using an amplitude and a time because at the end of the day, this is a wave is just like a, a point that oscillates with different like amplitude in different points cool. So uh there are like a few important uh elements of a wave or a sound wave. So one is the period and the period uh gives us an idea when we have like the same, the starts like of the same uh wave. So for example, here, like we have a peak and then we go back like to the next peak. And this is like the, the period which is like the interval before like we see uh that peak again.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=67s",
        "start_time": "67.51"
    },
    {
        "id": "c5ca9786",
        "text": "And in this case, we have like a very nice wave that oscillates. And we can represent it using an amplitude and a time because at the end of the day, this is a wave is just like a, a point that oscillates with different like amplitude in different points cool. So uh there are like a few important uh elements of a wave or a sound wave. So one is the period and the period uh gives us an idea when we have like the same, the starts like of the same uh wave. So for example, here, like we have a peak and then we go back like to the next peak. And this is like the, the period which is like the interval before like we see uh that peak again. Now um the period is strictly correlated with a frequency. Indeed a frequency is the inverse of period. So the higher uh the period, the lower the frequency and the lower the period, the higher uh the frequency",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=94s",
        "start_time": "94.36"
    },
    {
        "id": "12a77a6f",
        "text": "uh there are like a few important uh elements of a wave or a sound wave. So one is the period and the period uh gives us an idea when we have like the same, the starts like of the same uh wave. So for example, here, like we have a peak and then we go back like to the next peak. And this is like the, the period which is like the interval before like we see uh that peak again. Now um the period is strictly correlated with a frequency. Indeed a frequency is the inverse of period. So the higher uh the period, the lower the frequency and the lower the period, the higher uh the frequency now to uh describe uh a sound wave, we also need another uh information about another thing which is indeed a amplitude and amplitude is given by the distance uh of a point uh from like zero amplitude, right? In this case, we can represent this sound wave as with a, with a sine function.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=117s",
        "start_time": "117.319"
    },
    {
        "id": "9c964fed",
        "text": "Now um the period is strictly correlated with a frequency. Indeed a frequency is the inverse of period. So the higher uh the period, the lower the frequency and the lower the period, the higher uh the frequency now to uh describe uh a sound wave, we also need another uh information about another thing which is indeed a amplitude and amplitude is given by the distance uh of a point uh from like zero amplitude, right? In this case, we can represent this sound wave as with a, with a sine function. And here we have a mathematical representation of this a sound wave and it's given by the A, by A which is the amplitude multiplied by the sine function uh calculated in two pi F which stands for frequency time",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=145s",
        "start_time": "145.32"
    },
    {
        "id": "6436b90b",
        "text": "now to uh describe uh a sound wave, we also need another uh information about another thing which is indeed a amplitude and amplitude is given by the distance uh of a point uh from like zero amplitude, right? In this case, we can represent this sound wave as with a, with a sine function. And here we have a mathematical representation of this a sound wave and it's given by the A, by A which is the amplitude multiplied by the sine function uh calculated in two pi F which stands for frequency time uh plus P. This phi is a Greek letter which stands for phase well phase. Uh what phase does to a waveform? It's basically like it shifts it to the right or to the left",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=164s",
        "start_time": "164.11"
    },
    {
        "id": "0858e5ca",
        "text": "And here we have a mathematical representation of this a sound wave and it's given by the A, by A which is the amplitude multiplied by the sine function uh calculated in two pi F which stands for frequency time uh plus P. This phi is a Greek letter which stands for phase well phase. Uh what phase does to a waveform? It's basically like it shifts it to the right or to the left uh cool. OK. So now we have like a simple mathematical representation of a of a, of a waveform. So now let's look at how like sound wave uh like these two fundamental elements like of sound waves are like frequency and amplitude are connected with pitch and loudness. So frequency and pitch are connected together.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=189s",
        "start_time": "189.6"
    },
    {
        "id": "d01d8986",
        "text": "uh plus P. This phi is a Greek letter which stands for phase well phase. Uh what phase does to a waveform? It's basically like it shifts it to the right or to the left uh cool. OK. So now we have like a simple mathematical representation of a of a, of a waveform. So now let's look at how like sound wave uh like these two fundamental elements like of sound waves are like frequency and amplitude are connected with pitch and loudness. So frequency and pitch are connected together. Uh basically what happens is that higher frequencies are perceived as higher pitch, but pitch is not a physical uh observable. It is like a perceptual is the way we perceive uh like a frequency and we process it right. And so uh basically, the idea here is that, and you can see it here with this two sound waves like in red.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=206s",
        "start_time": "206.179"
    },
    {
        "id": "542b854a",
        "text": "uh cool. OK. So now we have like a simple mathematical representation of a of a, of a waveform. So now let's look at how like sound wave uh like these two fundamental elements like of sound waves are like frequency and amplitude are connected with pitch and loudness. So frequency and pitch are connected together. Uh basically what happens is that higher frequencies are perceived as higher pitch, but pitch is not a physical uh observable. It is like a perceptual is the way we perceive uh like a frequency and we process it right. And so uh basically, the idea here is that, and you can see it here with this two sound waves like in red. So when you have uh like a longer periods, you have basically lower frequencies and here with shorter periods, you have like higher frequencies. So basically, we would perceive these um sound wave on the bottom left as higher pitch than the one like on uh like on the top part.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=220s",
        "start_time": "220.33"
    },
    {
        "id": "ad703d2f",
        "text": "Uh basically what happens is that higher frequencies are perceived as higher pitch, but pitch is not a physical uh observable. It is like a perceptual is the way we perceive uh like a frequency and we process it right. And so uh basically, the idea here is that, and you can see it here with this two sound waves like in red. So when you have uh like a longer periods, you have basically lower frequencies and here with shorter periods, you have like higher frequencies. So basically, we would perceive these um sound wave on the bottom left as higher pitch than the one like on uh like on the top part. OK. So now let's move on to amplitude and loudness. Well, um there's a correlation, obviously, there's a connection between amplitude and loudness, but it's by no means like linear and it's very complicated. But all in all uh larger amplitudes uh are perceived as louder, right? So for example, if we compare uh these two sound waves like on the right. So these two blue sound waves like the one on the top",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=245s",
        "start_time": "245.339"
    },
    {
        "id": "88d21fb7",
        "text": "So when you have uh like a longer periods, you have basically lower frequencies and here with shorter periods, you have like higher frequencies. So basically, we would perceive these um sound wave on the bottom left as higher pitch than the one like on uh like on the top part. OK. So now let's move on to amplitude and loudness. Well, um there's a correlation, obviously, there's a connection between amplitude and loudness, but it's by no means like linear and it's very complicated. But all in all uh larger amplitudes uh are perceived as louder, right? So for example, if we compare uh these two sound waves like on the right. So these two blue sound waves like the one on the top uh is quieter than the one like on the bottom.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=272s",
        "start_time": "272.119"
    },
    {
        "id": "b09bd560",
        "text": "OK. So now let's move on to amplitude and loudness. Well, um there's a correlation, obviously, there's a connection between amplitude and loudness, but it's by no means like linear and it's very complicated. But all in all uh larger amplitudes uh are perceived as louder, right? So for example, if we compare uh these two sound waves like on the right. So these two blue sound waves like the one on the top uh is quieter than the one like on the bottom. Cool. So now a thing that I think like it's important to strike here is that when we talk about uh like acoustic sound waves. So for example, like the sound of my voice or the sound of a of a piano playing, these are continuous wave forms, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=295s",
        "start_time": "295.619"
    },
    {
        "id": "281112da",
        "text": "uh is quieter than the one like on the bottom. Cool. So now a thing that I think like it's important to strike here is that when we talk about uh like acoustic sound waves. So for example, like the sound of my voice or the sound of a of a piano playing, these are continuous wave forms, right? So, so they are analog waveforms, but obviously, we can't really store analog waveforms. We need a way of digitalis those. And for doing that, we have this analog digital conversion uh process or a DC. So when we do analog digital conversion,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=325s",
        "start_time": "325.559"
    },
    {
        "id": "acda75d3",
        "text": "Cool. So now a thing that I think like it's important to strike here is that when we talk about uh like acoustic sound waves. So for example, like the sound of my voice or the sound of a of a piano playing, these are continuous wave forms, right? So, so they are analog waveforms, but obviously, we can't really store analog waveforms. We need a way of digitalis those. And for doing that, we have this analog digital conversion uh process or a DC. So when we do analog digital conversion, uh we basically do perform two steps. The first one is called uh sampling and the second one is called quantization. So during a sampling, uh what we do is we just like sample the signal at specific uh time intervals and then we quantize the amplitude given and, and we represent that with a limited number of bits. So let's see this in action with this example here.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=330s",
        "start_time": "330.5"
    },
    {
        "id": "76ad36e5",
        "text": "So, so they are analog waveforms, but obviously, we can't really store analog waveforms. We need a way of digitalis those. And for doing that, we have this analog digital conversion uh process or a DC. So when we do analog digital conversion, uh we basically do perform two steps. The first one is called uh sampling and the second one is called quantization. So during a sampling, uh what we do is we just like sample the signal at specific uh time intervals and then we quantize the amplitude given and, and we represent that with a limited number of bits. So let's see this in action with this example here. So as you'll see, we haven't read a nice continuous analog uh sound wave and now we're gonna sample it at these like blue points here which are all at the same interval. And the interval is given by the sample rate, which is basically uh the amount of like samples that we have like in a uh in a second. Cool. Uh So",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=347s",
        "start_time": "347.779"
    },
    {
        "id": "4f9fe2fb",
        "text": "uh we basically do perform two steps. The first one is called uh sampling and the second one is called quantization. So during a sampling, uh what we do is we just like sample the signal at specific uh time intervals and then we quantize the amplitude given and, and we represent that with a limited number of bits. So let's see this in action with this example here. So as you'll see, we haven't read a nice continuous analog uh sound wave and now we're gonna sample it at these like blue points here which are all at the same interval. And the interval is given by the sample rate, which is basically uh the amount of like samples that we have like in a uh in a second. Cool. Uh So um what we do like at each uh sample is we project the, the value of the amplitude of the analog uh sound wave to the closer quantized uh bit we have here like on the left, right.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=370s",
        "start_time": "370.959"
    },
    {
        "id": "62907c35",
        "text": "So as you'll see, we haven't read a nice continuous analog uh sound wave and now we're gonna sample it at these like blue points here which are all at the same interval. And the interval is given by the sample rate, which is basically uh the amount of like samples that we have like in a uh in a second. Cool. Uh So um what we do like at each uh sample is we project the, the value of the amplitude of the analog uh sound wave to the closer quantized uh bit we have here like on the left, right. So for example, if you, if you look at this point here,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=399s",
        "start_time": "399.23"
    },
    {
        "id": "36fb3fbb",
        "text": "um what we do like at each uh sample is we project the, the value of the amplitude of the analog uh sound wave to the closer quantized uh bit we have here like on the left, right. So for example, if you, if you look at this point here, so as you'll see, so like the amplitude, it's probably like around 6.6 something like that. But we, we don't have 6.6 right? And so we're just gonna project that to the closer bit we have, which is seven. And so we'll store this information here like with, with a seven, right. So now the",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=427s",
        "start_time": "427.38"
    },
    {
        "id": "d731c427",
        "text": "So for example, if you, if you look at this point here, so as you'll see, so like the amplitude, it's probably like around 6.6 something like that. But we, we don't have 6.6 right? And so we're just gonna project that to the closer bit we have, which is seven. And so we'll store this information here like with, with a seven, right. So now the uh obviously, as you'll see here, there are, there are gonna be like quite some like errors that accumulate throughout uh like the A DC process because of like the, the sampling process itself and the quantization. But the more bits we have to store the amplitude and the better the quality of the sound uh will be. So we have two,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=446s",
        "start_time": "446.589"
    },
    {
        "id": "10d54ab8",
        "text": "so as you'll see, so like the amplitude, it's probably like around 6.6 something like that. But we, we don't have 6.6 right? And so we're just gonna project that to the closer bit we have, which is seven. And so we'll store this information here like with, with a seven, right. So now the uh obviously, as you'll see here, there are, there are gonna be like quite some like errors that accumulate throughout uh like the A DC process because of like the, the sampling process itself and the quantization. But the more bits we have to store the amplitude and the better the quality of the sound uh will be. So we have two, I would say like matrix here when we do a DC. So one is called sample rate. And the other one is called bit F. For example, with the CD RM, we have a sample rate of 44,000 and 100 heads, which basically means that we take more than 40,000 amplitude points in a second, right? And the bit depth is given by 16 bits uh for each channel, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=450s",
        "start_time": "450.619"
    },
    {
        "id": "70b726a1",
        "text": "uh obviously, as you'll see here, there are, there are gonna be like quite some like errors that accumulate throughout uh like the A DC process because of like the, the sampling process itself and the quantization. But the more bits we have to store the amplitude and the better the quality of the sound uh will be. So we have two, I would say like matrix here when we do a DC. So one is called sample rate. And the other one is called bit F. For example, with the CD RM, we have a sample rate of 44,000 and 100 heads, which basically means that we take more than 40,000 amplitude points in a second, right? And the bit depth is given by 16 bits uh for each channel, right? Uh So the more like nostalgic uh like if you guys are who like really love like video game music or video games, like retro games may remember the so called eight bit music like Super Mario or Final Fantasy, like the first ones, right? And so that music is called eight bit because the bit depth was eight bit.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=472s",
        "start_time": "472.769"
    },
    {
        "id": "90aa2e7e",
        "text": "I would say like matrix here when we do a DC. So one is called sample rate. And the other one is called bit F. For example, with the CD RM, we have a sample rate of 44,000 and 100 heads, which basically means that we take more than 40,000 amplitude points in a second, right? And the bit depth is given by 16 bits uh for each channel, right? Uh So the more like nostalgic uh like if you guys are who like really love like video game music or video games, like retro games may remember the so called eight bit music like Super Mario or Final Fantasy, like the first ones, right? And so that music is called eight bit because the bit depth was eight bit. And obviously the quality of that sound was kind of like not that that great compared to what with what we have now, but still like it was like really, really nice, cool. So this is a DC. So now let's move on and let's take a look at Real World sound waves. So",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=498s",
        "start_time": "498.2"
    },
    {
        "id": "9a1bab78",
        "text": "Uh So the more like nostalgic uh like if you guys are who like really love like video game music or video games, like retro games may remember the so called eight bit music like Super Mario or Final Fantasy, like the first ones, right? And so that music is called eight bit because the bit depth was eight bit. And obviously the quality of that sound was kind of like not that that great compared to what with what we have now, but still like it was like really, really nice, cool. So this is a DC. So now let's move on and let's take a look at Real World sound waves. So it turns out the Real World sound waves are not as simple as like the sine wave that we've seen before. So here, for example, we have a, a waveform for a piano key. So we just like strike uh a piano key and we, we wait until like the sound fades out basically here after nine seconds. Cool. So this is like a messy uh sound",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=527s",
        "start_time": "527.94"
    },
    {
        "id": "325a543e",
        "text": "And obviously the quality of that sound was kind of like not that that great compared to what with what we have now, but still like it was like really, really nice, cool. So this is a DC. So now let's move on and let's take a look at Real World sound waves. So it turns out the Real World sound waves are not as simple as like the sine wave that we've seen before. So here, for example, we have a, a waveform for a piano key. So we just like strike uh a piano key and we, we wait until like the sound fades out basically here after nine seconds. Cool. So this is like a messy uh sound that there's a lot of like complexity in it. So the question we could ask and which is like super legitimate is like, what can we know about this sound cause like,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=551s",
        "start_time": "551.609"
    },
    {
        "id": "2c24f80f",
        "text": "it turns out the Real World sound waves are not as simple as like the sine wave that we've seen before. So here, for example, we have a, a waveform for a piano key. So we just like strike uh a piano key and we, we wait until like the sound fades out basically here after nine seconds. Cool. So this is like a messy uh sound that there's a lot of like complexity in it. So the question we could ask and which is like super legitimate is like, what can we know about this sound cause like, I mean, it doesn't seem like we can know much, but actually, it turns out that nature has given us like an incredible way of knowing quite a lot about complex sounds. And that's given through a fourier transform.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=570s",
        "start_time": "570.869"
    },
    {
        "id": "c4a40c99",
        "text": "that there's a lot of like complexity in it. So the question we could ask and which is like super legitimate is like, what can we know about this sound cause like, I mean, it doesn't seem like we can know much, but actually, it turns out that nature has given us like an incredible way of knowing quite a lot about complex sounds. And that's given through a fourier transform. And basically what we do with a fourier transform is like the process of decomposing a periodic sound into a sum of sine waves which all vibrate oscillate like at different frequencies.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=593s",
        "start_time": "593.83"
    },
    {
        "id": "1dbef76e",
        "text": "I mean, it doesn't seem like we can know much, but actually, it turns out that nature has given us like an incredible way of knowing quite a lot about complex sounds. And that's given through a fourier transform. And basically what we do with a fourier transform is like the process of decomposing a periodic sound into a sum of sine waves which all vibrate oscillate like at different frequencies. So think about that, this is like quite incredible. So we can describe a very complex sound as long as it's periodic",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=604s",
        "start_time": "604.559"
    },
    {
        "id": "5282e1a8",
        "text": "And basically what we do with a fourier transform is like the process of decomposing a periodic sound into a sum of sine waves which all vibrate oscillate like at different frequencies. So think about that, this is like quite incredible. So we can describe a very complex sound as long as it's periodic as a sum as the super imposition of a bunch of different sine waves and different frequencies. Like it's quite remarkable, isn't it cool? So, but let's like try to like uh visualize this because this could feel I know a little bit abstract. So let's start like with this uh sound wave over here.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=620s",
        "start_time": "620.799"
    },
    {
        "id": "d3bcbd2e",
        "text": "So think about that, this is like quite incredible. So we can describe a very complex sound as long as it's periodic as a sum as the super imposition of a bunch of different sine waves and different frequencies. Like it's quite remarkable, isn't it cool? So, but let's like try to like uh visualize this because this could feel I know a little bit abstract. So let's start like with this uh sound wave over here. Now this uh sound wave is given by the super imposition of these two sine waves, right? So if we sum them, we're gonna get this, which is quite cool. So let's see this like uh mathematically.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=637s",
        "start_time": "637.82"
    },
    {
        "id": "9f1b1c53",
        "text": "as a sum as the super imposition of a bunch of different sine waves and different frequencies. Like it's quite remarkable, isn't it cool? So, but let's like try to like uh visualize this because this could feel I know a little bit abstract. So let's start like with this uh sound wave over here. Now this uh sound wave is given by the super imposition of these two sine waves, right? So if we sum them, we're gonna get this, which is quite cool. So let's see this like uh mathematically. So over here, so if we call this sound wave, uh the red sound wave s then we see that that is given by the uh the sine wave relative like to, to this guy here plus the sound wave, the wave like relative to this",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=648s",
        "start_time": "648.469"
    },
    {
        "id": "f70bdaef",
        "text": "Now this uh sound wave is given by the super imposition of these two sine waves, right? So if we sum them, we're gonna get this, which is quite cool. So let's see this like uh mathematically. So over here, so if we call this sound wave, uh the red sound wave s then we see that that is given by the uh the sine wave relative like to, to this guy here plus the sound wave, the wave like relative to this guy over here,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=666s",
        "start_time": "666.94"
    },
    {
        "id": "bfb94056",
        "text": "So over here, so if we call this sound wave, uh the red sound wave s then we see that that is given by the uh the sine wave relative like to, to this guy here plus the sound wave, the wave like relative to this guy over here, which is quite cool. And if we, if we take a look over here. So we can describe, as we said before, like this two sine waves like with the amplitude with the frequency and with the phase which in this case is zero cool. But uh when we do uh a fourier transform, we are particularly interested in the amplitudes themselves. And why is that the case? Because like the amplitude tells us how much",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=684s",
        "start_time": "684.08"
    },
    {
        "id": "89c37aa1",
        "text": "guy over here, which is quite cool. And if we, if we take a look over here. So we can describe, as we said before, like this two sine waves like with the amplitude with the frequency and with the phase which in this case is zero cool. But uh when we do uh a fourier transform, we are particularly interested in the amplitudes themselves. And why is that the case? Because like the amplitude tells us how much uh a specific frequency contributes to the complex sound, right? So the higher the amplitude and the more I know that that specific uh frequency is contributing to the complex sound, I want to decompose, right. So in this case, we see that uh the frequency uh 1.5 is the one that contributes the most to this sound over here.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=702s",
        "start_time": "702.64"
    },
    {
        "id": "be833182",
        "text": "which is quite cool. And if we, if we take a look over here. So we can describe, as we said before, like this two sine waves like with the amplitude with the frequency and with the phase which in this case is zero cool. But uh when we do uh a fourier transform, we are particularly interested in the amplitudes themselves. And why is that the case? Because like the amplitude tells us how much uh a specific frequency contributes to the complex sound, right? So the higher the amplitude and the more I know that that specific uh frequency is contributing to the complex sound, I want to decompose, right. So in this case, we see that uh the frequency uh 1.5 is the one that contributes the most to this sound over here. Uh which is because like the amplitude is 1.5 which is way more than 0.5 like for the case of frequency four, right? And so, so you may be wondering, but what's the great thing about this? Well, it, it's fantastic because now we know like the different",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=704s",
        "start_time": "704.549"
    },
    {
        "id": "d1a82ea5",
        "text": "uh a specific frequency contributes to the complex sound, right? So the higher the amplitude and the more I know that that specific uh frequency is contributing to the complex sound, I want to decompose, right. So in this case, we see that uh the frequency uh 1.5 is the one that contributes the most to this sound over here. Uh which is because like the amplitude is 1.5 which is way more than 0.5 like for the case of frequency four, right? And so, so you may be wondering, but what's the great thing about this? Well, it, it's fantastic because now we know like the different elements which uh kind of like contribute to create a complex sound. It's as if like you think of like for example, like a dish say for example, like you have a uh some pasta spaghetti like tomato spaghetti, right? So the waveform is just like that",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=735s",
        "start_time": "735.419"
    },
    {
        "id": "6b762765",
        "text": "Uh which is because like the amplitude is 1.5 which is way more than 0.5 like for the case of frequency four, right? And so, so you may be wondering, but what's the great thing about this? Well, it, it's fantastic because now we know like the different elements which uh kind of like contribute to create a complex sound. It's as if like you think of like for example, like a dish say for example, like you have a uh some pasta spaghetti like tomato spaghetti, right? So the waveform is just like that dish in itself. And it's difficult like to understand like all the different uh parts of it. But then with a fourier transform, we can divide those um elements, those ingredients and understand that probably we had like 200 g of spaghetti. Then we had like a little bit of garlic. We had",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=765s",
        "start_time": "765.479"
    },
    {
        "id": "fbfd261a",
        "text": "elements which uh kind of like contribute to create a complex sound. It's as if like you think of like for example, like a dish say for example, like you have a uh some pasta spaghetti like tomato spaghetti, right? So the waveform is just like that dish in itself. And it's difficult like to understand like all the different uh parts of it. But then with a fourier transform, we can divide those um elements, those ingredients and understand that probably we had like 200 g of spaghetti. Then we had like a little bit of garlic. We had uh five leaves for example of uh basilica, a basil, right? And we had 100 g of tomatoes, right? So basically, we can decompose the whole dish in it, in its ingredients. And it's the same thing we can do with the fourier transform, we can decompose a complex sound and understand how different frequencies are contributing to that,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=785s",
        "start_time": "785.32"
    },
    {
        "id": "f7e50558",
        "text": "dish in itself. And it's difficult like to understand like all the different uh parts of it. But then with a fourier transform, we can divide those um elements, those ingredients and understand that probably we had like 200 g of spaghetti. Then we had like a little bit of garlic. We had uh five leaves for example of uh basilica, a basil, right? And we had 100 g of tomatoes, right? So basically, we can decompose the whole dish in it, in its ingredients. And it's the same thing we can do with the fourier transform, we can decompose a complex sound and understand how different frequencies are contributing to that, right? So let's go back to the waveform of our piano key and perform a fourier transform here. And so what you'll get if you perform a fourier transform is this guy over here which is called a power spectrum. So and the spectrum basically gives us the magnitude as a function of frequency. So here we know that there's a peak",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=804s",
        "start_time": "804.799"
    },
    {
        "id": "18c8ad4b",
        "text": "uh five leaves for example of uh basilica, a basil, right? And we had 100 g of tomatoes, right? So basically, we can decompose the whole dish in it, in its ingredients. And it's the same thing we can do with the fourier transform, we can decompose a complex sound and understand how different frequencies are contributing to that, right? So let's go back to the waveform of our piano key and perform a fourier transform here. And so what you'll get if you perform a fourier transform is this guy over here which is called a power spectrum. So and the spectrum basically gives us the magnitude as a function of frequency. So here we know that there's a peak of like magnitude of power around 3000. Yeah, I would say like 500 like over here. So the uh the uh this frequency is very much represented like in this sound over here, right? So now,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=829s",
        "start_time": "829.239"
    },
    {
        "id": "f9bb553c",
        "text": "right? So let's go back to the waveform of our piano key and perform a fourier transform here. And so what you'll get if you perform a fourier transform is this guy over here which is called a power spectrum. So and the spectrum basically gives us the magnitude as a function of frequency. So here we know that there's a peak of like magnitude of power around 3000. Yeah, I would say like 500 like over here. So the uh the uh this frequency is very much represented like in this sound over here, right? So now, so when we do like this fourier transfer, basically we move from the time domain towards the frequency domain. What does that mean? Well, if you take a look at this waveform here, so you'll see that here we have the amplitude as a function of time. So we are in the time domain. But then when we apply the fourier transform, we move in the frequency domain because here we have on the x axis, the frequency",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=856s",
        "start_time": "856.63"
    },
    {
        "id": "251ec459",
        "text": "of like magnitude of power around 3000. Yeah, I would say like 500 like over here. So the uh the uh this frequency is very much represented like in this sound over here, right? So now, so when we do like this fourier transfer, basically we move from the time domain towards the frequency domain. What does that mean? Well, if you take a look at this waveform here, so you'll see that here we have the amplitude as a function of time. So we are in the time domain. But then when we apply the fourier transform, we move in the frequency domain because here we have on the x axis, the frequency and the magnitude is a function of the frequency itself, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=880s",
        "start_time": "880.659"
    },
    {
        "id": "744a656e",
        "text": "so when we do like this fourier transfer, basically we move from the time domain towards the frequency domain. What does that mean? Well, if you take a look at this waveform here, so you'll see that here we have the amplitude as a function of time. So we are in the time domain. But then when we apply the fourier transform, we move in the frequency domain because here we have on the x axis, the frequency and the magnitude is a function of the frequency itself, right? Cool. And so if, because this happens, we lose information about uh time. So it's as if uh this spectrum power spectrum here was a snapshot of all the elements uh which concur to form this sound over this like nine seconds, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=898s",
        "start_time": "898.099"
    },
    {
        "id": "ba789a18",
        "text": "and the magnitude is a function of the frequency itself, right? Cool. And so if, because this happens, we lose information about uh time. So it's as if uh this spectrum power spectrum here was a snapshot of all the elements uh which concur to form this sound over this like nine seconds, right? And so basically what this uh spectrum is telling us is telling us that uh these different frequencies have different uh powers. But throughout all of the, all of the um all of the sound here. So it's a snapshot, it's static which could be seen like as a problem because obviously audio and music data like it is a time series, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=924s",
        "start_time": "924.89"
    },
    {
        "id": "eee698c8",
        "text": "Cool. And so if, because this happens, we lose information about uh time. So it's as if uh this spectrum power spectrum here was a snapshot of all the elements uh which concur to form this sound over this like nine seconds, right? And so basically what this uh spectrum is telling us is telling us that uh these different frequencies have different uh powers. But throughout all of the, all of the um all of the sound here. So it's a snapshot, it's static which could be seen like as a problem because obviously audio and music data like it is a time series, right? So things change in time. And so we want to know about how things change in time and it seems that with the fourier transform, we, we can't really do that. So we are missing on a lot of information, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=931s",
        "start_time": "931.099"
    },
    {
        "id": "f0cabbf3",
        "text": "And so basically what this uh spectrum is telling us is telling us that uh these different frequencies have different uh powers. But throughout all of the, all of the um all of the sound here. So it's a snapshot, it's static which could be seen like as a problem because obviously audio and music data like it is a time series, right? So things change in time. And so we want to know about how things change in time and it seems that with the fourier transform, we, we can't really do that. So we are missing on a lot of information, right? But obviously, there's a solution to that and the solution it's called the short time fourier transform or SDFT. And so what the short time fourier transform does, it computes several fourier transforms at different intervals. And in doing so it preserves information about uh time and the way",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=955s",
        "start_time": "955.83"
    },
    {
        "id": "8ca352b6",
        "text": "So things change in time. And so we want to know about how things change in time and it seems that with the fourier transform, we, we can't really do that. So we are missing on a lot of information, right? But obviously, there's a solution to that and the solution it's called the short time fourier transform or SDFT. And so what the short time fourier transform does, it computes several fourier transforms at different intervals. And in doing so it preserves information about uh time and the way uh sounds uh like evolves like over time, right? And so the different intervals at which we perform uh the fourier transform uh is given by the frame size. And so a frame is a bunch of samples. And so we fix the number of samples. And we say, OK, so let's consider only like for example, 202 hun 2048 samples",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=984s",
        "start_time": "984.479"
    },
    {
        "id": "3f4bb783",
        "text": "But obviously, there's a solution to that and the solution it's called the short time fourier transform or SDFT. And so what the short time fourier transform does, it computes several fourier transforms at different intervals. And in doing so it preserves information about uh time and the way uh sounds uh like evolves like over time, right? And so the different intervals at which we perform uh the fourier transform uh is given by the frame size. And so a frame is a bunch of samples. And so we fix the number of samples. And we say, OK, so let's consider only like for example, 202 hun 2048 samples and do the, the fourier transform there. And then let's move on to let's shift and move on like to, to the rest like of the waveform. And what happens here is that we are given a spectrogram which is",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=997s",
        "start_time": "997.15"
    },
    {
        "id": "c0c233c6",
        "text": "uh sounds uh like evolves like over time, right? And so the different intervals at which we perform uh the fourier transform uh is given by the frame size. And so a frame is a bunch of samples. And so we fix the number of samples. And we say, OK, so let's consider only like for example, 202 hun 2048 samples and do the, the fourier transform there. And then let's move on to let's shift and move on like to, to the rest like of the waveform. And what happens here is that we are given a spectrogram which is a representation that gives us information about uh like magnitude as a, as a function of frequency and time. So let's take a look at the spectrogram of the um piano key that we, we say like uh before, right? So like that sound wave cool. So here, as you can see we are",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1024s",
        "start_time": "1024.3"
    },
    {
        "id": "50ee3f0c",
        "text": "and do the, the fourier transform there. And then let's move on to let's shift and move on like to, to the rest like of the waveform. And what happens here is that we are given a spectrogram which is a representation that gives us information about uh like magnitude as a, as a function of frequency and time. So let's take a look at the spectrogram of the um piano key that we, we say like uh before, right? So like that sound wave cool. So here, as you can see we are back in business because we have time now here on the, on the uh x axis. But we also have frequency on the y axis and we have a third axis which is basically given by the color and the color is telling us how much a given frequence like is present uh in uh the sound at a given time. So for example, here we see that like towards like the beginning",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1051s",
        "start_time": "1051.489"
    },
    {
        "id": "3f7e8168",
        "text": "a representation that gives us information about uh like magnitude as a, as a function of frequency and time. So let's take a look at the spectrogram of the um piano key that we, we say like uh before, right? So like that sound wave cool. So here, as you can see we are back in business because we have time now here on the, on the uh x axis. But we also have frequency on the y axis and we have a third axis which is basically given by the color and the color is telling us how much a given frequence like is present uh in uh the sound at a given time. So for example, here we see that like towards like the beginning uh above like 4000 Hertz, we really don't have like much uh uh contribution like at all. So it seems like that the, the energy is kind of like um",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1067s",
        "start_time": "1067.31"
    },
    {
        "id": "26891aa9",
        "text": "back in business because we have time now here on the, on the uh x axis. But we also have frequency on the y axis and we have a third axis which is basically given by the color and the color is telling us how much a given frequence like is present uh in uh the sound at a given time. So for example, here we see that like towards like the beginning uh above like 4000 Hertz, we really don't have like much uh uh contribution like at all. So it seems like that the, the energy is kind of like um I would say like uh around",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1089s",
        "start_time": "1089.469"
    },
    {
        "id": "6d353371",
        "text": "uh above like 4000 Hertz, we really don't have like much uh uh contribution like at all. So it seems like that the, the energy is kind of like um I would say like uh around a very like low frequency here could be like 500 something like that like Hertz, right? And as you see here, uh this uh spectrogram resembles a little bit of the waveform of the piano key as well because as you see, like in time, like all of these frequencies are kind of like fading out like the energy for these frequencies because like the time the the sound",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1118s",
        "start_time": "1118.619"
    },
    {
        "id": "fc34aae3",
        "text": "I would say like uh around a very like low frequency here could be like 500 something like that like Hertz, right? And as you see here, uh this uh spectrogram resembles a little bit of the waveform of the piano key as well because as you see, like in time, like all of these frequencies are kind of like fading out like the energy for these frequencies because like the time the the sound of the piano key is just like fading out uh all the time cool. OK. So now we have like an idea of like what a spectrogram is, but let's see how we perform a short time fourier transform a little bit like more in detail. Um just like to understand like how this works. So here we start obviously from a um a waveform like the the mm",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1133s",
        "start_time": "1133.27"
    },
    {
        "id": "ae22d86d",
        "text": "a very like low frequency here could be like 500 something like that like Hertz, right? And as you see here, uh this uh spectrogram resembles a little bit of the waveform of the piano key as well because as you see, like in time, like all of these frequencies are kind of like fading out like the energy for these frequencies because like the time the the sound of the piano key is just like fading out uh all the time cool. OK. So now we have like an idea of like what a spectrogram is, but let's see how we perform a short time fourier transform a little bit like more in detail. Um just like to understand like how this works. So here we start obviously from a um a waveform like the the mm sound wave. And then what we do next is we basically like focus only on uh one frame which is given by a number of samples here. And then what we do next is we calculate the, the fourier transform over here. And then we take this information and we just project it in the spectrogram.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1137s",
        "start_time": "1137.709"
    },
    {
        "id": "4a101b37",
        "text": "of the piano key is just like fading out uh all the time cool. OK. So now we have like an idea of like what a spectrogram is, but let's see how we perform a short time fourier transform a little bit like more in detail. Um just like to understand like how this works. So here we start obviously from a um a waveform like the the mm sound wave. And then what we do next is we basically like focus only on uh one frame which is given by a number of samples here. And then what we do next is we calculate the, the fourier transform over here. And then we take this information and we just project it in the spectrogram. So we, we put it like in the, in the first interval here in the spectrogram uh at, at time zero basically. And then we, we pass like the, the frequency here and then we pass the, the magnitude here. Uh And basically, we, we can visualize it like as a, as a function of, of color.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1166s",
        "start_time": "1166.66"
    },
    {
        "id": "a2b2beeb",
        "text": "sound wave. And then what we do next is we basically like focus only on uh one frame which is given by a number of samples here. And then what we do next is we calculate the, the fourier transform over here. And then we take this information and we just project it in the spectrogram. So we, we put it like in the, in the first interval here in the spectrogram uh at, at time zero basically. And then we, we pass like the, the frequency here and then we pass the, the magnitude here. Uh And basically, we, we can visualize it like as a, as a function of, of color. So now two things here. So here I'm using like decibels and uh like with decibels, basically, we, we apply like a logarithmic like a function like to, to the magnitude itself.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1195s",
        "start_time": "1195.469"
    },
    {
        "id": "1aaeffcb",
        "text": "So we, we put it like in the, in the first interval here in the spectrogram uh at, at time zero basically. And then we, we pass like the, the frequency here and then we pass the, the magnitude here. Uh And basically, we, we can visualize it like as a, as a function of, of color. So now two things here. So here I'm using like decibels and uh like with decibels, basically, we, we apply like a logarithmic like a function like to, to the magnitude itself. And uh the other thing that I wanted to say is that uh when we uh perform the fourier transform, what we actually perform is the FFT which is the fast fourier transform, which is a kind of like a uh a variation of the fourier transform which is used for performing um like the fourier transform like way faster, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1222s",
        "start_time": "1222.579"
    },
    {
        "id": "dece09f9",
        "text": "So now two things here. So here I'm using like decibels and uh like with decibels, basically, we, we apply like a logarithmic like a function like to, to the magnitude itself. And uh the other thing that I wanted to say is that uh when we uh perform the fourier transform, what we actually perform is the FFT which is the fast fourier transform, which is a kind of like a uh a variation of the fourier transform which is used for performing um like the fourier transform like way faster, right? OK. So now we are at the end of the, of the first uh iteration. So what do we do next? Well, we just like slide here um like on the, on the sound wave. And now we take like the second like the same like uh the same uh kind of like interval but the second frame. So we are like shifting to the right.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1245s",
        "start_time": "1245.63"
    },
    {
        "id": "7a68f260",
        "text": "And uh the other thing that I wanted to say is that uh when we uh perform the fourier transform, what we actually perform is the FFT which is the fast fourier transform, which is a kind of like a uh a variation of the fourier transform which is used for performing um like the fourier transform like way faster, right? OK. So now we are at the end of the, of the first uh iteration. So what do we do next? Well, we just like slide here um like on the, on the sound wave. And now we take like the second like the same like uh the same uh kind of like interval but the second frame. So we are like shifting to the right. And now we do the same thing. So we calculate the spectrum there through the fast fourier transform and then we project that into the spectrogram and then we move on. So we have the third, the fourth until we get to the end. And once we are at the end, we have our own spectrogram which is fantastic news.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1262s",
        "start_time": "1262.489"
    },
    {
        "id": "817971c6",
        "text": "OK. So now we are at the end of the, of the first uh iteration. So what do we do next? Well, we just like slide here um like on the, on the sound wave. And now we take like the second like the same like uh the same uh kind of like interval but the second frame. So we are like shifting to the right. And now we do the same thing. So we calculate the spectrum there through the fast fourier transform and then we project that into the spectrogram and then we move on. So we have the third, the fourth until we get to the end. And once we are at the end, we have our own spectrogram which is fantastic news. Cool. So now you may be wondering well, but why did we have to learn about uh like spectrograms like in all of this? Well, it turns out like that spectrograms are fundamental uh for uh performing like deep learning uh like applications like on audio data",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1287s",
        "start_time": "1287.77"
    },
    {
        "id": "855fb5f1",
        "text": "And now we do the same thing. So we calculate the spectrum there through the fast fourier transform and then we project that into the spectrogram and then we move on. So we have the third, the fourth until we get to the end. And once we are at the end, we have our own spectrogram which is fantastic news. Cool. So now you may be wondering well, but why did we have to learn about uh like spectrograms like in all of this? Well, it turns out like that spectrograms are fundamental uh for uh performing like deep learning uh like applications like on audio data well. And actually uh like the whole preprocessing pipeline for audio data for deep learning is based on spectrograms. So not surprisingly, uh when we have a data set, we start with a bunch of wave files. So basically of like a wave forms or like sound waves, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1312s",
        "start_time": "1312.459"
    },
    {
        "id": "c631f6e7",
        "text": "Cool. So now you may be wondering well, but why did we have to learn about uh like spectrograms like in all of this? Well, it turns out like that spectrograms are fundamental uh for uh performing like deep learning uh like applications like on audio data well. And actually uh like the whole preprocessing pipeline for audio data for deep learning is based on spectrograms. So not surprisingly, uh when we have a data set, we start with a bunch of wave files. So basically of like a wave forms or like sound waves, right? And then we pass those in into a short time fourier transform and we get a spectrogram and then we use that spectrogram as an input for our deep learning model over here. Nice. So this is a super nice way of getting uh like a valuable representation for our like deep learning uh model. So we just like focus on the spectrogram. Now",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1335s",
        "start_time": "1335.91"
    },
    {
        "id": "ed69ba2b",
        "text": "well. And actually uh like the whole preprocessing pipeline for audio data for deep learning is based on spectrograms. So not surprisingly, uh when we have a data set, we start with a bunch of wave files. So basically of like a wave forms or like sound waves, right? And then we pass those in into a short time fourier transform and we get a spectrogram and then we use that spectrogram as an input for our deep learning model over here. Nice. So this is a super nice way of getting uh like a valuable representation for our like deep learning uh model. So we just like focus on the spectrogram. Now this is like very different from uh what used to happen in the past when we were focusing more on uh traditional machine learning um like algorithms like a logistic regression or super machines, like in those cases, like the preprocessing pipeline for audio data was quite different. So let's take a look at that. So the whole thing was way more about feature engineering. So",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1357s",
        "start_time": "1357.63"
    },
    {
        "id": "fd8005ba",
        "text": "And then we pass those in into a short time fourier transform and we get a spectrogram and then we use that spectrogram as an input for our deep learning model over here. Nice. So this is a super nice way of getting uh like a valuable representation for our like deep learning uh model. So we just like focus on the spectrogram. Now this is like very different from uh what used to happen in the past when we were focusing more on uh traditional machine learning um like algorithms like a logistic regression or super machines, like in those cases, like the preprocessing pipeline for audio data was quite different. So let's take a look at that. So the whole thing was way more about feature engineering. So it turns out that from a from uh like a waveform, you can take a lot of like different features like uh from those. And so what we used to do before was like taking like those features. And um and for doing that, uh we would basically use either a directly like a waveform or perform a fourier transform. And then using like the the spectrum",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1377s",
        "start_time": "1377.42"
    },
    {
        "id": "70d36afc",
        "text": "this is like very different from uh what used to happen in the past when we were focusing more on uh traditional machine learning um like algorithms like a logistic regression or super machines, like in those cases, like the preprocessing pipeline for audio data was quite different. So let's take a look at that. So the whole thing was way more about feature engineering. So it turns out that from a from uh like a waveform, you can take a lot of like different features like uh from those. And so what we used to do before was like taking like those features. And um and for doing that, uh we would basically use either a directly like a waveform or perform a fourier transform. And then using like the the spectrum and you would use like a waveform for extracting a time domain features. And you would use a spectrogram for extracting uh frequency uh domain features. An example of like a time domain feature is amplitude envelope. An example of like frequency domain feature like spectral center or like spectral flux.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1406s",
        "start_time": "1406.79"
    },
    {
        "id": "84f4d41b",
        "text": "it turns out that from a from uh like a waveform, you can take a lot of like different features like uh from those. And so what we used to do before was like taking like those features. And um and for doing that, uh we would basically use either a directly like a waveform or perform a fourier transform. And then using like the the spectrum and you would use like a waveform for extracting a time domain features. And you would use a spectrogram for extracting uh frequency uh domain features. An example of like a time domain feature is amplitude envelope. An example of like frequency domain feature like spectral center or like spectral flux. But the point that I want to make here is that we start from a waveform and then we, we get a bunch of features we and we should decide which features we want to get. And then we combine these features, we aggregate them somehow using like some statistical means or even like some unsupervised learning uh like models. And then we, we use those",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1434s",
        "start_time": "1434.219"
    },
    {
        "id": "6935dfa5",
        "text": "and you would use like a waveform for extracting a time domain features. And you would use a spectrogram for extracting uh frequency uh domain features. An example of like a time domain feature is amplitude envelope. An example of like frequency domain feature like spectral center or like spectral flux. But the point that I want to make here is that we start from a waveform and then we, we get a bunch of features we and we should decide which features we want to get. And then we combine these features, we aggregate them somehow using like some statistical means or even like some unsupervised learning uh like models. And then we, we use those and we fed those features into an ML algorithm like logistic regression or like super vector machine. So with the advance of like deep learning, the whole process became a little bit more straightforward because we don't need to uh like a concern that much about feature engineering because we just use the spectrum right. And so we don't use like all of these other things. This is why like a deep learning",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1463s",
        "start_time": "1463.209"
    },
    {
        "id": "324648fb",
        "text": "But the point that I want to make here is that we start from a waveform and then we, we get a bunch of features we and we should decide which features we want to get. And then we combine these features, we aggregate them somehow using like some statistical means or even like some unsupervised learning uh like models. And then we, we use those and we fed those features into an ML algorithm like logistic regression or like super vector machine. So with the advance of like deep learning, the whole process became a little bit more straightforward because we don't need to uh like a concern that much about feature engineering because we just use the spectrum right. And so we don't use like all of these other things. This is why like a deep learning uh model uh like for audio, like in this case, it's called like an end to end model because basically you just use some basic information without",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1482s",
        "start_time": "1482.625"
    },
    {
        "id": "5d080392",
        "text": "and we fed those features into an ML algorithm like logistic regression or like super vector machine. So with the advance of like deep learning, the whole process became a little bit more straightforward because we don't need to uh like a concern that much about feature engineering because we just use the spectrum right. And so we don't use like all of these other things. This is why like a deep learning uh model uh like for audio, like in this case, it's called like an end to end model because basically you just use some basic information without uh worrying too much about extracting specific features. Cool. Now I want to introduce the another feature which is fundamental for deep learning. And this is like as important if not more important than the spectrogram itself. And this feature is called the mal frequency subs subs coefficients. Now",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1506s",
        "start_time": "1506.42"
    },
    {
        "id": "32239cdf",
        "text": "uh model uh like for audio, like in this case, it's called like an end to end model because basically you just use some basic information without uh worrying too much about extracting specific features. Cool. Now I want to introduce the another feature which is fundamental for deep learning. And this is like as important if not more important than the spectrogram itself. And this feature is called the mal frequency subs subs coefficients. Now uh to extract like this features like it's quite complicated and I'm not gonna get like into the details because again, we we don't need them. So we need to understand like the intuition between like a spectrogram and MF CCS. But we don't need to like understand the implementation details here. But what we need to do however, is to",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1533s",
        "start_time": "1533.689"
    },
    {
        "id": "2bffcb0b",
        "text": "uh worrying too much about extracting specific features. Cool. Now I want to introduce the another feature which is fundamental for deep learning. And this is like as important if not more important than the spectrogram itself. And this feature is called the mal frequency subs subs coefficients. Now uh to extract like this features like it's quite complicated and I'm not gonna get like into the details because again, we we don't need them. So we need to understand like the intuition between like a spectrogram and MF CCS. But we don't need to like understand the implementation details here. But what we need to do however, is to understands like what uh the MF CCS are and how we can like use them from a very high level perspective. Well, MF CCS capture timbrel and textural aspects of sound. So if you have, for example, like a piano and a violin uh playing like the same melody, uh you would have potentially like the, the same like peach content, the same frequency and same like rhythm",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1542s",
        "start_time": "1542.979"
    },
    {
        "id": "b2435a99",
        "text": "uh to extract like this features like it's quite complicated and I'm not gonna get like into the details because again, we we don't need them. So we need to understand like the intuition between like a spectrogram and MF CCS. But we don't need to like understand the implementation details here. But what we need to do however, is to understands like what uh the MF CCS are and how we can like use them from a very high level perspective. Well, MF CCS capture timbrel and textural aspects of sound. So if you have, for example, like a piano and a violin uh playing like the same melody, uh you would have potentially like the, the same like peach content, the same frequency and same like rhythm um more or less there depending on the performance. But what would change is like timbre the quality of sound and the MF CCS are capable of capturing that information.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1565s",
        "start_time": "1565.67"
    },
    {
        "id": "c13ff130",
        "text": "understands like what uh the MF CCS are and how we can like use them from a very high level perspective. Well, MF CCS capture timbrel and textural aspects of sound. So if you have, for example, like a piano and a violin uh playing like the same melody, uh you would have potentially like the, the same like peach content, the same frequency and same like rhythm um more or less there depending on the performance. But what would change is like timbre the quality of sound and the MF CCS are capable of capturing that information. And for uh extracting MF CCS, we, we perform uh a fourier transform and we move like from the time domain into like the frequency domain. So MF CCS are basically like frequency domain uh feature. But the great thing, the great advantage of MF CCS over spectrograms is that they approximate the human auditory system that try to model the way",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1587s",
        "start_time": "1587.859"
    },
    {
        "id": "8c157b79",
        "text": "um more or less there depending on the performance. But what would change is like timbre the quality of sound and the MF CCS are capable of capturing that information. And for uh extracting MF CCS, we, we perform uh a fourier transform and we move like from the time domain into like the frequency domain. So MF CCS are basically like frequency domain uh feature. But the great thing, the great advantage of MF CCS over spectrograms is that they approximate the human auditory system that try to model the way we perceive like frequency, right? And so that's like very important. Like if you then want to do like deep learning stuff to have like some data that represents the way we uh kind of like process like audio.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1615s",
        "start_time": "1615.91"
    },
    {
        "id": "3fe2382f",
        "text": "And for uh extracting MF CCS, we, we perform uh a fourier transform and we move like from the time domain into like the frequency domain. So MF CCS are basically like frequency domain uh feature. But the great thing, the great advantage of MF CCS over spectrograms is that they approximate the human auditory system that try to model the way we perceive like frequency, right? And so that's like very important. Like if you then want to do like deep learning stuff to have like some data that represents the way we uh kind of like process like audio. Now, uh the results like of uh extracting MF CCS is a bunch of coefficients. It's an MFCC vector. And so you can specify a number of like different coefficients. Usually in all your music applications you want to use between 13 to 40 coefficients. And then again, you are gonna calculate all of these coefficients at each frame. So that you have um an idea of how like the MF CCS are evolving over time,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1628s",
        "start_time": "1628.42"
    },
    {
        "id": "5cb8388e",
        "text": "we perceive like frequency, right? And so that's like very important. Like if you then want to do like deep learning stuff to have like some data that represents the way we uh kind of like process like audio. Now, uh the results like of uh extracting MF CCS is a bunch of coefficients. It's an MFCC vector. And so you can specify a number of like different coefficients. Usually in all your music applications you want to use between 13 to 40 coefficients. And then again, you are gonna calculate all of these coefficients at each frame. So that you have um an idea of how like the MF CCS are evolving over time, right? Cool. So now let's take a look at uh like uh an MFCC like representation here and this is like very similar to a spectrogram, isn't it? Right? So here on uh the AX ax axis, we have time. And then again, this is the MF CCS calculated for the same uh sound wave like that we, so up until now, so like the, the uh the piano key, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1656s",
        "start_time": "1656.76"
    },
    {
        "id": "7f5a53a6",
        "text": "Now, uh the results like of uh extracting MF CCS is a bunch of coefficients. It's an MFCC vector. And so you can specify a number of like different coefficients. Usually in all your music applications you want to use between 13 to 40 coefficients. And then again, you are gonna calculate all of these coefficients at each frame. So that you have um an idea of how like the MF CCS are evolving over time, right? Cool. So now let's take a look at uh like uh an MFCC like representation here and this is like very similar to a spectrogram, isn't it? Right? So here on uh the AX ax axis, we have time. And then again, this is the MF CCS calculated for the same uh sound wave like that we, so up until now, so like the, the uh the piano key, right? OK. So here we have time. So it is nine seconds. And here on the Y axis, we have the different MFCC coefficients. And here you see, so this is one, this is two, this is three. And here I think I have like 13 coefficients. And now again, like the value of the coefficient is represented with this",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1670s",
        "start_time": "1670.43"
    },
    {
        "id": "8b91d6d8",
        "text": "right? Cool. So now let's take a look at uh like uh an MFCC like representation here and this is like very similar to a spectrogram, isn't it? Right? So here on uh the AX ax axis, we have time. And then again, this is the MF CCS calculated for the same uh sound wave like that we, so up until now, so like the, the uh the piano key, right? OK. So here we have time. So it is nine seconds. And here on the Y axis, we have the different MFCC coefficients. And here you see, so this is one, this is two, this is three. And here I think I have like 13 coefficients. And now again, like the value of the coefficient is represented with this uh like uh colors over here and as usual, like the, the more red, the redder, the um a value like over here and like the, the, the, the, the bigger, like the value itself, right? Cool. So",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1700s",
        "start_time": "1700.66"
    },
    {
        "id": "967cd216",
        "text": "OK. So here we have time. So it is nine seconds. And here on the Y axis, we have the different MFCC coefficients. And here you see, so this is one, this is two, this is three. And here I think I have like 13 coefficients. And now again, like the value of the coefficient is represented with this uh like uh colors over here and as usual, like the, the more red, the redder, the um a value like over here and like the, the, the, the, the bigger, like the value itself, right? Cool. So uh what can we say about MF CCS? So where do we use them? Well, it turns out that uh MF CCS are fantastic for a number of different audio applications. So they've originally been introduced for speech recognition and they are still used like for speech recognition today quite extensively. And but around the 2000, they've also been introduced in music analysis. And so",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1725s",
        "start_time": "1725.939"
    },
    {
        "id": "08d1e3e3",
        "text": "uh like uh colors over here and as usual, like the, the more red, the redder, the um a value like over here and like the, the, the, the, the bigger, like the value itself, right? Cool. So uh what can we say about MF CCS? So where do we use them? Well, it turns out that uh MF CCS are fantastic for a number of different audio applications. So they've originally been introduced for speech recognition and they are still used like for speech recognition today quite extensively. And but around the 2000, they've also been introduced in music analysis. And so uh we can use MF CCS for music genre classification and music instrument classification as well. So regarding music genre classification in a few videos, I think a couple of videos we are gonna use MF CCS for classifying uh a bunch of like different uh tracks that we'll have and decide which genres like they are and we'll use MF CCS for that.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1744s",
        "start_time": "1744.38"
    },
    {
        "id": "68d8cf19",
        "text": "uh what can we say about MF CCS? So where do we use them? Well, it turns out that uh MF CCS are fantastic for a number of different audio applications. So they've originally been introduced for speech recognition and they are still used like for speech recognition today quite extensively. And but around the 2000, they've also been introduced in music analysis. And so uh we can use MF CCS for music genre classification and music instrument classification as well. So regarding music genre classification in a few videos, I think a couple of videos we are gonna use MF CCS for classifying uh a bunch of like different uh tracks that we'll have and decide which genres like they are and we'll use MF CCS for that. Cool. So now again, so as you've seen, I haven't gone into like the deep mathematical details or implementation details, both of like the fourier transform and MF CCS because we don't need them. But again, if you are like interested in knowing more, just like let me know in the comments section. And once I'm done like with the series,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1763s",
        "start_time": "1763.68"
    },
    {
        "id": "3a12ac52",
        "text": "uh we can use MF CCS for music genre classification and music instrument classification as well. So regarding music genre classification in a few videos, I think a couple of videos we are gonna use MF CCS for classifying uh a bunch of like different uh tracks that we'll have and decide which genres like they are and we'll use MF CCS for that. Cool. So now again, so as you've seen, I haven't gone into like the deep mathematical details or implementation details, both of like the fourier transform and MF CCS because we don't need them. But again, if you are like interested in knowing more, just like let me know in the comments section. And once I'm done like with the series, uh I could actually like make a few videos about this and if you are very interested in that I could actually create a series about all your digital signal processing, which is a fascinating topic in itself and very complex.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1791s",
        "start_time": "1791.64"
    },
    {
        "id": "07a3e7ee",
        "text": "Cool. So now again, so as you've seen, I haven't gone into like the deep mathematical details or implementation details, both of like the fourier transform and MF CCS because we don't need them. But again, if you are like interested in knowing more, just like let me know in the comments section. And once I'm done like with the series, uh I could actually like make a few videos about this and if you are very interested in that I could actually create a series about all your digital signal processing, which is a fascinating topic in itself and very complex. Cool. OK. So now how do we use like MF CCS like in the uh preprocessing pipeline? Like for a data? Well, it's basically the same thing that we do like for a spectrogram. So we start from a, a waveform, a sound form",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1816s",
        "start_time": "1816.67"
    },
    {
        "id": "0eeda75b",
        "text": "uh I could actually like make a few videos about this and if you are very interested in that I could actually create a series about all your digital signal processing, which is a fascinating topic in itself and very complex. Cool. OK. So now how do we use like MF CCS like in the uh preprocessing pipeline? Like for a data? Well, it's basically the same thing that we do like for a spectrogram. So we start from a, a waveform, a sound form a sound wave, then we extract MF CCS. Now we have these MF CCS and we pass the MF CCS directly into our deep learning um uh network, right?",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1838s",
        "start_time": "1838.26"
    },
    {
        "id": "8149e568",
        "text": "Cool. OK. So now how do we use like MF CCS like in the uh preprocessing pipeline? Like for a data? Well, it's basically the same thing that we do like for a spectrogram. So we start from a, a waveform, a sound form a sound wave, then we extract MF CCS. Now we have these MF CCS and we pass the MF CCS directly into our deep learning um uh network, right? So again, using MF CCS is another way of like doing end to end uh deep learning with audio and it's like a very efficient one, effective one cool. So this was it for this video. So what's up next? Well,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1851s",
        "start_time": "1851.88"
    },
    {
        "id": "74e82e38",
        "text": "a sound wave, then we extract MF CCS. Now we have these MF CCS and we pass the MF CCS directly into our deep learning um uh network, right? So again, using MF CCS is another way of like doing end to end uh deep learning with audio and it's like a very efficient one, effective one cool. So this was it for this video. So what's up next? Well, uh it turns out that like in this video, we spent quite a lot of time talking about theoretical stuff. And now as usual, so as we usually do, we, we want just like to turn like this theoretical uh information into like implementation. So in the next video, we'll perform fast fourier transforms a short term,",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1871s",
        "start_time": "1871.89"
    },
    {
        "id": "c188c033",
        "text": "So again, using MF CCS is another way of like doing end to end uh deep learning with audio and it's like a very efficient one, effective one cool. So this was it for this video. So what's up next? Well, uh it turns out that like in this video, we spent quite a lot of time talking about theoretical stuff. And now as usual, so as we usually do, we, we want just like to turn like this theoretical uh information into like implementation. So in the next video, we'll perform fast fourier transforms a short term, short time via transform. Uh with Python, we'll look at spectrograms at spectra and we'll extract MF CCS. But again, we won't implement uh these extractors like from scratch. But R will get familiar with a fantastic audio library in Python that's called uh Li Brosa. And that's like a library that you really want to know if you want to use if you want to like do stuff like in um audio",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1886s",
        "start_time": "1886.0"
    },
    {
        "id": "48765e3e",
        "text": "uh it turns out that like in this video, we spent quite a lot of time talking about theoretical stuff. And now as usual, so as we usually do, we, we want just like to turn like this theoretical uh information into like implementation. So in the next video, we'll perform fast fourier transforms a short term, short time via transform. Uh with Python, we'll look at spectrograms at spectra and we'll extract MF CCS. But again, we won't implement uh these extractors like from scratch. But R will get familiar with a fantastic audio library in Python that's called uh Li Brosa. And that's like a library that you really want to know if you want to use if you want to like do stuff like in um audio with deep learning, right? To prepare your data. Cool. So this is it for this video? Yeah, I really hope you enjoyed it. And if that's the case, as usual, just uh subscribe and hit the notification bell and if you have any questions, feel free like to to post them in the comments section below and I'll see you the next time. Cheers.",
        "video": "10 - Understanding audio data for deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "m3XbqfIij_Y",
        "youtube_link": "https://www.youtube.com/watch?v=m3XbqfIij_Y&t=1902s",
        "start_time": "1902.27"
    },
    {
        "id": "28c11836",
        "text": "Hi, everybody and welcome to a new video in the Deep Learning for Rode with Python series. In this video, we're gonna have an overview at artificial intelligence, machine learning and deep learning. These are some of the uh topics that we're gonna be covering during the video. So we're gonna define artificial intelligence and see the relationship with machine learning and deep learning. And then we're going to talk about a number of different machine learning flavors or paradigms like supervised, unsupervised and reinforcement learning. Then we're gonna get into de defining deep learning and then we're gonna see the difference between traditional machine learning and deep learning. And finally, we're gonna be talking a little bit about possible applications of deep learning in audio and music technology. Cool. So let's get started with uh the relationship between artificial intelligence, machine learning and deep learning. Usually",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=7s",
        "start_time": "7.73"
    },
    {
        "id": "61664dd0",
        "text": "define artificial intelligence and see the relationship with machine learning and deep learning. And then we're going to talk about a number of different machine learning flavors or paradigms like supervised, unsupervised and reinforcement learning. Then we're gonna get into de defining deep learning and then we're gonna see the difference between traditional machine learning and deep learning. And finally, we're gonna be talking a little bit about possible applications of deep learning in audio and music technology. Cool. So let's get started with uh the relationship between artificial intelligence, machine learning and deep learning. Usually uh you'll hear like this terms or sometimes you'll hear these terms like interchangeably like artificial intelligence or machine learning, but actually artificial intelligence like is the, the bigger set machine learning is just a part of the A I and then deep learning is a subset of machine learning. So A I like is the big thing",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=24s",
        "start_time": "24.459"
    },
    {
        "id": "f7d2d266",
        "text": "and then we're gonna see the difference between traditional machine learning and deep learning. And finally, we're gonna be talking a little bit about possible applications of deep learning in audio and music technology. Cool. So let's get started with uh the relationship between artificial intelligence, machine learning and deep learning. Usually uh you'll hear like this terms or sometimes you'll hear these terms like interchangeably like artificial intelligence or machine learning, but actually artificial intelligence like is the, the bigger set machine learning is just a part of the A I and then deep learning is a subset of machine learning. So A I like is the big thing machine learning is but one technique or one series of algorithms that you use inside the A I field, which is like bigger than machine learning. And then you have deep learning that then again is one subset of machine learning. So, but how do we define artificial intelligence? Well, there's a fantastic definition",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=44s",
        "start_time": "44.38"
    },
    {
        "id": "bf519e7c",
        "text": "uh you'll hear like this terms or sometimes you'll hear these terms like interchangeably like artificial intelligence or machine learning, but actually artificial intelligence like is the, the bigger set machine learning is just a part of the A I and then deep learning is a subset of machine learning. So A I like is the big thing machine learning is but one technique or one series of algorithms that you use inside the A I field, which is like bigger than machine learning. And then you have deep learning that then again is one subset of machine learning. So, but how do we define artificial intelligence? Well, there's a fantastic definition uh given by Russell and Norvig in a book that's basically a Bible for artificial intelligence. And the book is called Artificial Intelligence and modern approach. And so the two scholars define A I as the the",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=68s",
        "start_time": "68.98"
    },
    {
        "id": "6590fcef",
        "text": "machine learning is but one technique or one series of algorithms that you use inside the A I field, which is like bigger than machine learning. And then you have deep learning that then again is one subset of machine learning. So, but how do we define artificial intelligence? Well, there's a fantastic definition uh given by Russell and Norvig in a book that's basically a Bible for artificial intelligence. And the book is called Artificial Intelligence and modern approach. And so the two scholars define A I as the the designing and building of intelligent agents that receive percept from the environment and take actions that affect the environment. So basically for the, for these guys for Russell and Norvig A I is just like the",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=91s",
        "start_time": "91.36"
    },
    {
        "id": "765758d3",
        "text": "uh given by Russell and Norvig in a book that's basically a Bible for artificial intelligence. And the book is called Artificial Intelligence and modern approach. And so the two scholars define A I as the the designing and building of intelligent agents that receive percept from the environment and take actions that affect the environment. So basically for the, for these guys for Russell and Norvig A I is just like the the creation, the science and the art of building like these intelligent agents. But then obviously one could ask but wait a minute, what's an intelligent agent? So here it seems that we are shifting the definition of artificial intelligence towards that of like an intelligence",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=113s",
        "start_time": "113.93"
    },
    {
        "id": "b70aa659",
        "text": "designing and building of intelligent agents that receive percept from the environment and take actions that affect the environment. So basically for the, for these guys for Russell and Norvig A I is just like the the creation, the science and the art of building like these intelligent agents. But then obviously one could ask but wait a minute, what's an intelligent agent? So here it seems that we are shifting the definition of artificial intelligence towards that of like an intelligence agent. So what's an intelligent agent? And so an intelligent agent in the in the field of A I is basically a rational agent, right? But now it seems like that we are still shifting like the definition of like A I once more. OK. So an intelligent agent is a rational agent. But what's rationality, what's a rational agent? Well,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=130s",
        "start_time": "130.104"
    },
    {
        "id": "a763a399",
        "text": "the creation, the science and the art of building like these intelligent agents. But then obviously one could ask but wait a minute, what's an intelligent agent? So here it seems that we are shifting the definition of artificial intelligence towards that of like an intelligence agent. So what's an intelligent agent? And so an intelligent agent in the in the field of A I is basically a rational agent, right? But now it seems like that we are still shifting like the definition of like A I once more. OK. So an intelligent agent is a rational agent. But what's rationality, what's a rational agent? Well, rationality basically means that you are, if you are rational, you are being, you are acting to achieve your own goals, given your beliefs. So you're basically building like the best strategies possible to achieve your goals, given like your",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=147s",
        "start_time": "147.059"
    },
    {
        "id": "d7769fce",
        "text": "agent. So what's an intelligent agent? And so an intelligent agent in the in the field of A I is basically a rational agent, right? But now it seems like that we are still shifting like the definition of like A I once more. OK. So an intelligent agent is a rational agent. But what's rationality, what's a rational agent? Well, rationality basically means that you are, if you are rational, you are being, you are acting to achieve your own goals, given your beliefs. So you're basically building like the best strategies possible to achieve your goals, given like your beliefs. So basically all like the situation and the context like you find yourself like in so in that sense, a rational agent is an agent that's able to optimize his actions in order to achieve its goals, right?",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=163s",
        "start_time": "163.529"
    },
    {
        "id": "feb7ae40",
        "text": "rationality basically means that you are, if you are rational, you are being, you are acting to achieve your own goals, given your beliefs. So you're basically building like the best strategies possible to achieve your goals, given like your beliefs. So basically all like the situation and the context like you find yourself like in so in that sense, a rational agent is an agent that's able to optimize his actions in order to achieve its goals, right? So we were saying that artificial intelligence is more than machine learning. So there are many A I techniques and algorithms out there. So here I want just to mention just like a few but there are way more than this. So for example, evolutionary algorithms that exploit ideas that come a little bit I would say like from biology and it's kind of like using like genetic algorithms or like",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=188s",
        "start_time": "188.699"
    },
    {
        "id": "c209d28c",
        "text": "beliefs. So basically all like the situation and the context like you find yourself like in so in that sense, a rational agent is an agent that's able to optimize his actions in order to achieve its goals, right? So we were saying that artificial intelligence is more than machine learning. So there are many A I techniques and algorithms out there. So here I want just to mention just like a few but there are way more than this. So for example, evolutionary algorithms that exploit ideas that come a little bit I would say like from biology and it's kind of like using like genetic algorithms or like things that come from biology but using them like in a a kind of like computer science in their context, then we have expert systems which are systems that use rules uh to, to, to make decisions and just like to solve problems. Then we have like other types of like algorithms like search algorithms, like one that's like very popular, it's called a star. And this is used",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=206s",
        "start_time": "206.22"
    },
    {
        "id": "d36d15af",
        "text": "So we were saying that artificial intelligence is more than machine learning. So there are many A I techniques and algorithms out there. So here I want just to mention just like a few but there are way more than this. So for example, evolutionary algorithms that exploit ideas that come a little bit I would say like from biology and it's kind of like using like genetic algorithms or like things that come from biology but using them like in a a kind of like computer science in their context, then we have expert systems which are systems that use rules uh to, to, to make decisions and just like to solve problems. Then we have like other types of like algorithms like search algorithms, like one that's like very popular, it's called a star. And this is used a lot for example, in video games like for path finding, so to just like move uh characters around, for example, and for solving problems as well. And then obviously we have machine learning, which is one of this but then again, artificial intelligence has loads and loads of different techniques and algorithms and machine learning is one category like it's one subset of artificial intelligence, right.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=225s",
        "start_time": "225.0"
    },
    {
        "id": "550d2903",
        "text": "things that come from biology but using them like in a a kind of like computer science in their context, then we have expert systems which are systems that use rules uh to, to, to make decisions and just like to solve problems. Then we have like other types of like algorithms like search algorithms, like one that's like very popular, it's called a star. And this is used a lot for example, in video games like for path finding, so to just like move uh characters around, for example, and for solving problems as well. And then obviously we have machine learning, which is one of this but then again, artificial intelligence has loads and loads of different techniques and algorithms and machine learning is one category like it's one subset of artificial intelligence, right. So now let's focus on machine learning or ML. So what's machine learning? Well, in machine learning, you have a computer that performs task without using explicit instructions. So basically the computer is able to learn from data. So as developers or as researchers, we don't instruct the computer with specific rules to solve a problem. Rather we let the computer or like the program,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=250s",
        "start_time": "250.24"
    },
    {
        "id": "e1d74869",
        "text": "a lot for example, in video games like for path finding, so to just like move uh characters around, for example, and for solving problems as well. And then obviously we have machine learning, which is one of this but then again, artificial intelligence has loads and loads of different techniques and algorithms and machine learning is one category like it's one subset of artificial intelligence, right. So now let's focus on machine learning or ML. So what's machine learning? Well, in machine learning, you have a computer that performs task without using explicit instructions. So basically the computer is able to learn from data. So as developers or as researchers, we don't instruct the computer with specific rules to solve a problem. Rather we let the computer or like the program, uh the ML program to figure the rules by itself using data.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=275s",
        "start_time": "275.48"
    },
    {
        "id": "03bd1a93",
        "text": "So now let's focus on machine learning or ML. So what's machine learning? Well, in machine learning, you have a computer that performs task without using explicit instructions. So basically the computer is able to learn from data. So as developers or as researchers, we don't instruct the computer with specific rules to solve a problem. Rather we let the computer or like the program, uh the ML program to figure the rules by itself using data. Now, let's take a look at the difference between like machine learning and other types of like A I systems, more traditional systems like expert systems. And so if we're doing this, we're gonna look at a task that uh is, is used like in audio um like analysis or like A I music. It's, it's, it's quite common and it's called onset detection.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=300s",
        "start_time": "300.89"
    },
    {
        "id": "3b9346bd",
        "text": "uh the ML program to figure the rules by itself using data. Now, let's take a look at the difference between like machine learning and other types of like A I systems, more traditional systems like expert systems. And so if we're doing this, we're gonna look at a task that uh is, is used like in audio um like analysis or like A I music. It's, it's, it's quite common and it's called onset detection. And basically the onset detection task is quite simple to understand because all you need to do is to identify the start of a musical note. Imagine you have a melody, so a series of different musical notes and you want to identify when that melody starts. And here you have a waveform and it's kind of",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=330s",
        "start_time": "330.459"
    },
    {
        "id": "0683ea4a",
        "text": "Now, let's take a look at the difference between like machine learning and other types of like A I systems, more traditional systems like expert systems. And so if we're doing this, we're gonna look at a task that uh is, is used like in audio um like analysis or like A I music. It's, it's, it's quite common and it's called onset detection. And basically the onset detection task is quite simple to understand because all you need to do is to identify the start of a musical note. Imagine you have a melody, so a series of different musical notes and you want to identify when that melody starts. And here you have a waveform and it's kind of like very simple to understand where, like where we have like all the different nodes because it's basically a burst in energy which is like a P that you have like on the y axis. So here, obviously, we have a waveform and then here you can see that like in with these picks on the y axis you have and on such, you have a note as remember here, like on the X axis, you have time. Whereas on the y axis, you have the amplitude or like loudness if you will",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=336s",
        "start_time": "336.649"
    },
    {
        "id": "63aee632",
        "text": "And basically the onset detection task is quite simple to understand because all you need to do is to identify the start of a musical note. Imagine you have a melody, so a series of different musical notes and you want to identify when that melody starts. And here you have a waveform and it's kind of like very simple to understand where, like where we have like all the different nodes because it's basically a burst in energy which is like a P that you have like on the y axis. So here, obviously, we have a waveform and then here you can see that like in with these picks on the y axis you have and on such, you have a note as remember here, like on the X axis, you have time. Whereas on the y axis, you have the amplitude or like loudness if you will and then when you have like this burst uh in energy here, it means that, that there's an event. And so there's an onset more likely. So uh let's take a look at how you, we would solve this problem with machine learning and expert system. So identifying the uh the onsets like in this wave form, right? Let's start with the expert system. So for an expert system, what we would do is like we would",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=362s",
        "start_time": "362.279"
    },
    {
        "id": "3c33603a",
        "text": "like very simple to understand where, like where we have like all the different nodes because it's basically a burst in energy which is like a P that you have like on the y axis. So here, obviously, we have a waveform and then here you can see that like in with these picks on the y axis you have and on such, you have a note as remember here, like on the X axis, you have time. Whereas on the y axis, you have the amplitude or like loudness if you will and then when you have like this burst uh in energy here, it means that, that there's an event. And so there's an onset more likely. So uh let's take a look at how you, we would solve this problem with machine learning and expert system. So identifying the uh the onsets like in this wave form, right? Let's start with the expert system. So for an expert system, what we would do is like we would uh hardwire certain rules into the uh the system. And then the system would be able to identify the onsets by relying on rules that we as developers have provided it with, right. So in this case, so what could be like a rule that we could use here? Right.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=384s",
        "start_time": "384.049"
    },
    {
        "id": "c29964c6",
        "text": "and then when you have like this burst uh in energy here, it means that, that there's an event. And so there's an onset more likely. So uh let's take a look at how you, we would solve this problem with machine learning and expert system. So identifying the uh the onsets like in this wave form, right? Let's start with the expert system. So for an expert system, what we would do is like we would uh hardwire certain rules into the uh the system. And then the system would be able to identify the onsets by relying on rules that we as developers have provided it with, right. So in this case, so what could be like a rule that we could use here? Right. So if you look at this, you could say, well, if we have here like on the y axis, so like the amplitude is more than 0.2 or less than minus 0.2 then probably have an onset, right? And so we could use just like this very basic rule and then we would end up with this uh results. So we would be able to identify all of these guys like here,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=413s",
        "start_time": "413.459"
    },
    {
        "id": "c135e8c3",
        "text": "uh hardwire certain rules into the uh the system. And then the system would be able to identify the onsets by relying on rules that we as developers have provided it with, right. So in this case, so what could be like a rule that we could use here? Right. So if you look at this, you could say, well, if we have here like on the y axis, so like the amplitude is more than 0.2 or less than minus 0.2 then probably have an onset, right? And so we could use just like this very basic rule and then we would end up with this uh results. So we would be able to identify all of these guys like here, like vertical bars in red. And obviously, as you can see here, there are certain things that uh can't be identified and are like this smaller",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=440s",
        "start_time": "440.23"
    },
    {
        "id": "507aed4e",
        "text": "So if you look at this, you could say, well, if we have here like on the y axis, so like the amplitude is more than 0.2 or less than minus 0.2 then probably have an onset, right? And so we could use just like this very basic rule and then we would end up with this uh results. So we would be able to identify all of these guys like here, like vertical bars in red. And obviously, as you can see here, there are certain things that uh can't be identified and are like this smaller uh outbursts of energy here because like they are less than 0.2 or uh like in, in terms of like amplitude. And so we, we don't cover those with our simple rule, right? But this is like a an expert based approach. So we provided the rules to the system and the system has learned uh basically just like by what we we told it, told it to do,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=462s",
        "start_time": "462.029"
    },
    {
        "id": "d29045b1",
        "text": "like vertical bars in red. And obviously, as you can see here, there are certain things that uh can't be identified and are like this smaller uh outbursts of energy here because like they are less than 0.2 or uh like in, in terms of like amplitude. And so we, we don't cover those with our simple rule, right? But this is like a an expert based approach. So we provided the rules to the system and the system has learned uh basically just like by what we we told it, told it to do, right. So now let's take another way to solving the same problem. Let's take the machine learning uh route here. So, and how do we approach this problem? Well, it's kind of relatively simple, like on a very high level here. So, because basically the idea is that we provide to the machine learning system, which is like this nice robot over here,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=490s",
        "start_time": "490.489"
    },
    {
        "id": "d85ac15e",
        "text": "uh outbursts of energy here because like they are less than 0.2 or uh like in, in terms of like amplitude. And so we, we don't cover those with our simple rule, right? But this is like a an expert based approach. So we provided the rules to the system and the system has learned uh basically just like by what we we told it, told it to do, right. So now let's take another way to solving the same problem. Let's take the machine learning uh route here. So, and how do we approach this problem? Well, it's kind of relatively simple, like on a very high level here. So, because basically the idea is that we provide to the machine learning system, which is like this nice robot over here, we we provide a bunch of uh data with samples regarding um um where we have a lot of like this wave forms with um onsets that have been identified. So they are labeled. So we know that we have onsets uh like for example, here, here, here and here. And so we pass all of this information to the",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=504s",
        "start_time": "504.44"
    },
    {
        "id": "57cbdeed",
        "text": "right. So now let's take another way to solving the same problem. Let's take the machine learning uh route here. So, and how do we approach this problem? Well, it's kind of relatively simple, like on a very high level here. So, because basically the idea is that we provide to the machine learning system, which is like this nice robot over here, we we provide a bunch of uh data with samples regarding um um where we have a lot of like this wave forms with um onsets that have been identified. So they are labeled. So we know that we have onsets uh like for example, here, here, here and here. And so we pass all of this information to the machine learning uh system and then the machine learning system is able to abstract from all of that data and derive the rules by itself. So basically, we're not uh providing the machine learning system any rule uh beforehand. Rather, we're passing only a bunch of data. And we hope that the machine learning uh system is going to be able to recognize those rules.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=531s",
        "start_time": "531.45"
    },
    {
        "id": "7c641b71",
        "text": "we we provide a bunch of uh data with samples regarding um um where we have a lot of like this wave forms with um onsets that have been identified. So they are labeled. So we know that we have onsets uh like for example, here, here, here and here. And so we pass all of this information to the machine learning uh system and then the machine learning system is able to abstract from all of that data and derive the rules by itself. So basically, we're not uh providing the machine learning system any rule uh beforehand. Rather, we're passing only a bunch of data. And we hope that the machine learning uh system is going to be able to recognize those rules. And so this process is called learning. And we, and it's only like uh half of the whole like machine learning, like process and workflow. So we train a machine learning",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=554s",
        "start_time": "554.71"
    },
    {
        "id": "329e8fa5",
        "text": "machine learning uh system and then the machine learning system is able to abstract from all of that data and derive the rules by itself. So basically, we're not uh providing the machine learning system any rule uh beforehand. Rather, we're passing only a bunch of data. And we hope that the machine learning uh system is going to be able to recognize those rules. And so this process is called learning. And we, and it's only like uh half of the whole like machine learning, like process and workflow. So we train a machine learning uh system first and then we use it for inference. And then in, in a kind of like inference setting, what we do is we provide the system with the new waveform that the uh machine learning system has never seen before. And then hopefully the machine learning model will be able to identify in this case, all the onsets that we uh need, right? So",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=581s",
        "start_time": "581.494"
    },
    {
        "id": "a795c6f6",
        "text": "And so this process is called learning. And we, and it's only like uh half of the whole like machine learning, like process and workflow. So we train a machine learning uh system first and then we use it for inference. And then in, in a kind of like inference setting, what we do is we provide the system with the new waveform that the uh machine learning system has never seen before. And then hopefully the machine learning model will be able to identify in this case, all the onsets that we uh need, right? So this is like more or less like the the difference between a machine learning based approach and a more traditional A I approach based on uh expert rules. But now in machine learning, we have a bunch of different paradigms or flavors if you will. So here, I'm gonna just like refer to three,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=608s",
        "start_time": "608.539"
    },
    {
        "id": "3659a167",
        "text": "uh system first and then we use it for inference. And then in, in a kind of like inference setting, what we do is we provide the system with the new waveform that the uh machine learning system has never seen before. And then hopefully the machine learning model will be able to identify in this case, all the onsets that we uh need, right? So this is like more or less like the the difference between a machine learning based approach and a more traditional A I approach based on uh expert rules. But now in machine learning, we have a bunch of different paradigms or flavors if you will. So here, I'm gonna just like refer to three, but there are a little bit more. So like the first one is obviously like supervised learning. And it's the one that will be mainly focusing on throughout the whole series. Then we have unsupervised learning and then a reinforcement learning. So let's get started from uh supervised learning.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=620s",
        "start_time": "620.369"
    },
    {
        "id": "6ed058d7",
        "text": "this is like more or less like the the difference between a machine learning based approach and a more traditional A I approach based on uh expert rules. But now in machine learning, we have a bunch of different paradigms or flavors if you will. So here, I'm gonna just like refer to three, but there are a little bit more. So like the first one is obviously like supervised learning. And it's the one that will be mainly focusing on throughout the whole series. Then we have unsupervised learning and then a reinforcement learning. So let's get started from uh supervised learning. So in supervised learning, so when we provide the training data set, so the the the data that the machine learning model will will learn from, we pass it with lab labeled data. So basically what we give the",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=650s",
        "start_time": "650.26"
    },
    {
        "id": "eb378766",
        "text": "but there are a little bit more. So like the first one is obviously like supervised learning. And it's the one that will be mainly focusing on throughout the whole series. Then we have unsupervised learning and then a reinforcement learning. So let's get started from uh supervised learning. So in supervised learning, so when we provide the training data set, so the the the data that the machine learning model will will learn from, we pass it with lab labeled data. So basically what we give the uh the system is a bunch of data where all the uh in this case, the onsets are annotated. So there's a solution there to to the problem along with the data itself. So we pass the waveform and we also pass all the onsets so that",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=669s",
        "start_time": "669.14"
    },
    {
        "id": "6ab810e8",
        "text": "So in supervised learning, so when we provide the training data set, so the the the data that the machine learning model will will learn from, we pass it with lab labeled data. So basically what we give the uh the system is a bunch of data where all the uh in this case, the onsets are annotated. So there's a solution there to to the problem along with the data itself. So we pass the waveform and we also pass all the onsets so that the machine learning system is able like to match the row data against the solution or the identification of the different onsets. And so this way, the system is able to, to learn the rules and to tweak all of its weights uh based on uh samples, right. So this is supervised learning in a nutshell. And as I said, we'll be focusing a lot uh on this uh paradigm.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=689s",
        "start_time": "689.405"
    },
    {
        "id": "f1792e31",
        "text": "uh the system is a bunch of data where all the uh in this case, the onsets are annotated. So there's a solution there to to the problem along with the data itself. So we pass the waveform and we also pass all the onsets so that the machine learning system is able like to match the row data against the solution or the identification of the different onsets. And so this way, the system is able to, to learn the rules and to tweak all of its weights uh based on uh samples, right. So this is supervised learning in a nutshell. And as I said, we'll be focusing a lot uh on this uh paradigm. But then you have another paradigm that's very hand handy when you don't have uh label data and that's called unsupervised learning. And in this case, the model draws inferences from unstructured data or uh data that's not labeled. So say for example, here we had a data set where we have both like apples like in oranges, obviously like this is like a an abstract, like an example. But uh you,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=710s",
        "start_time": "710.09"
    },
    {
        "id": "0d7bfc35",
        "text": "the machine learning system is able like to match the row data against the solution or the identification of the different onsets. And so this way, the system is able to, to learn the rules and to tweak all of its weights uh based on uh samples, right. So this is supervised learning in a nutshell. And as I said, we'll be focusing a lot uh on this uh paradigm. But then you have another paradigm that's very hand handy when you don't have uh label data and that's called unsupervised learning. And in this case, the model draws inferences from unstructured data or uh data that's not labeled. So say for example, here we had a data set where we have both like apples like in oranges, obviously like this is like a an abstract, like an example. But uh you, yeah, but it's easy like to understand. So the algorithm is uh only past all of this data. So both like apples and oranges, but we don't say we don't tell the algorithm uh that uh like these red fruits here are apples, whereas the orange ones are oranges, right?",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=730s",
        "start_time": "730.955"
    },
    {
        "id": "bcb7c313",
        "text": "But then you have another paradigm that's very hand handy when you don't have uh label data and that's called unsupervised learning. And in this case, the model draws inferences from unstructured data or uh data that's not labeled. So say for example, here we had a data set where we have both like apples like in oranges, obviously like this is like a an abstract, like an example. But uh you, yeah, but it's easy like to understand. So the algorithm is uh only past all of this data. So both like apples and oranges, but we don't say we don't tell the algorithm uh that uh like these red fruits here are apples, whereas the orange ones are oranges, right? And so using unser certain unsupervised uh algorithms, uh the system will be able to divide up this unstructured data and hopefully create clusters, create uh like differentiations between apples and oranges. So in other words, the",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=760s",
        "start_time": "760.4"
    },
    {
        "id": "6af06949",
        "text": "yeah, but it's easy like to understand. So the algorithm is uh only past all of this data. So both like apples and oranges, but we don't say we don't tell the algorithm uh that uh like these red fruits here are apples, whereas the orange ones are oranges, right? And so using unser certain unsupervised uh algorithms, uh the system will be able to divide up this unstructured data and hopefully create clusters, create uh like differentiations between apples and oranges. So in other words, the um the algorithm is able to learn to separate uh this data without knowing the uh the labels like of this different data beforehand. And this can be like very handy if you have a lot of data, but this data like it is not labeled, right. So, and a third type of machine learning is the so called reinforcement learning. In this case, we have",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=787s",
        "start_time": "787.38"
    },
    {
        "id": "2cadf065",
        "text": "And so using unser certain unsupervised uh algorithms, uh the system will be able to divide up this unstructured data and hopefully create clusters, create uh like differentiations between apples and oranges. So in other words, the um the algorithm is able to learn to separate uh this data without knowing the uh the labels like of this different data beforehand. And this can be like very handy if you have a lot of data, but this data like it is not labeled, right. So, and a third type of machine learning is the so called reinforcement learning. In this case, we have an agent, a rational agent as we defined it that takes actions in virtual environments and learns through rewards and punishment. So in other words, like here, we have an agent that's um put like in a virtual environment and that goes through a series of uh long simulations. So, and to understand a little bit more about reinforcement learning,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=810s",
        "start_time": "810.27"
    },
    {
        "id": "431c4afd",
        "text": "um the algorithm is able to learn to separate uh this data without knowing the uh the labels like of this different data beforehand. And this can be like very handy if you have a lot of data, but this data like it is not labeled, right. So, and a third type of machine learning is the so called reinforcement learning. In this case, we have an agent, a rational agent as we defined it that takes actions in virtual environments and learns through rewards and punishment. So in other words, like here, we have an agent that's um put like in a virtual environment and that goes through a series of uh long simulations. So, and to understand a little bit more about reinforcement learning, uh we can look at alpha zero which is an amazing rein reinforcement learning model that has been developed by Google Deep mind. And the the fantastic thing about this model is that they trained this model only playing chess or shoy or go against itself. So you have the agent that plays against itself and then",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=833s",
        "start_time": "833.09"
    },
    {
        "id": "91cd0671",
        "text": "an agent, a rational agent as we defined it that takes actions in virtual environments and learns through rewards and punishment. So in other words, like here, we have an agent that's um put like in a virtual environment and that goes through a series of uh long simulations. So, and to understand a little bit more about reinforcement learning, uh we can look at alpha zero which is an amazing rein reinforcement learning model that has been developed by Google Deep mind. And the the fantastic thing about this model is that they trained this model only playing chess or shoy or go against itself. So you have the agent that plays against itself and then by playing loads and loads and loads of uh chess games, for example, uh against itself. Uh the system has been able to learn chess from scratch without giving it any knowledge about chess, apart from how the different pieces move. And then",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=862s",
        "start_time": "862.5"
    },
    {
        "id": "9658c010",
        "text": "uh we can look at alpha zero which is an amazing rein reinforcement learning model that has been developed by Google Deep mind. And the the fantastic thing about this model is that they trained this model only playing chess or shoy or go against itself. So you have the agent that plays against itself and then by playing loads and loads and loads of uh chess games, for example, uh against itself. Uh the system has been able to learn chess from scratch without giving it any knowledge about chess, apart from how the different pieces move. And then uh in a matter of like probably less than a day or something like that, the alpha zero was able to master chess to superhuman level. And the amazing thing is that the very same algorithm has been used by deep mind for cracking chess shoy. That's also like another uh board game and go. And as you can see here, you have a little bit like of the results of the",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=890s",
        "start_time": "890.88"
    },
    {
        "id": "90e48ed1",
        "text": "by playing loads and loads and loads of uh chess games, for example, uh against itself. Uh the system has been able to learn chess from scratch without giving it any knowledge about chess, apart from how the different pieces move. And then uh in a matter of like probably less than a day or something like that, the alpha zero was able to master chess to superhuman level. And the amazing thing is that the very same algorithm has been used by deep mind for cracking chess shoy. That's also like another uh board game and go. And as you can see here, you have a little bit like of the results of the um of Alpha zero competing against other top notch A I systems that play chess shoy and go. And for example, here in chess, you can see that Alpha zero completely destroyed stockfish. That was the at the time I believe the the um the world chess champion like for uh for software",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=918s",
        "start_time": "918.659"
    },
    {
        "id": "3fcfcfde",
        "text": "uh in a matter of like probably less than a day or something like that, the alpha zero was able to master chess to superhuman level. And the amazing thing is that the very same algorithm has been used by deep mind for cracking chess shoy. That's also like another uh board game and go. And as you can see here, you have a little bit like of the results of the um of Alpha zero competing against other top notch A I systems that play chess shoy and go. And for example, here in chess, you can see that Alpha zero completely destroyed stockfish. That was the at the time I believe the the um the world chess champion like for uh for software uh like playing chess, right? So if you want to know more about alpha zero, you can take a look at this article. It's great shedding new light on chess shoy and go",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=938s",
        "start_time": "938.4"
    },
    {
        "id": "4d301f6c",
        "text": "um of Alpha zero competing against other top notch A I systems that play chess shoy and go. And for example, here in chess, you can see that Alpha zero completely destroyed stockfish. That was the at the time I believe the the um the world chess champion like for uh for software uh like playing chess, right? So if you want to know more about alpha zero, you can take a look at this article. It's great shedding new light on chess shoy and go and you have a link to this article in the description below. But basically the takeaway point here is that with reinforcement learning, you have an agent, the agent uh is trained in an environment. And in order to learn how to behave to in order to learn like the different strategies to use the",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=967s",
        "start_time": "967.58"
    },
    {
        "id": "9e279f08",
        "text": "uh like playing chess, right? So if you want to know more about alpha zero, you can take a look at this article. It's great shedding new light on chess shoy and go and you have a link to this article in the description below. But basically the takeaway point here is that with reinforcement learning, you have an agent, the agent uh is trained in an environment. And in order to learn how to behave to in order to learn like the different strategies to use the the the agent uh goes through a series of rewards if it does well and of punishment if it does like uh poorly, right? But as I said, we will be focusing on a supervised learning mainly,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=995s",
        "start_time": "995.13"
    },
    {
        "id": "867390a5",
        "text": "and you have a link to this article in the description below. But basically the takeaway point here is that with reinforcement learning, you have an agent, the agent uh is trained in an environment. And in order to learn how to behave to in order to learn like the different strategies to use the the the agent uh goes through a series of rewards if it does well and of punishment if it does like uh poorly, right? But as I said, we will be focusing on a supervised learning mainly, right? So machine learning itself uh has a bunch of different algorithms so that there's really a ton of these. But like some of the ones like that are are I would say like the most popular are logistic regression, linear regression random forest KNN or K nearest neighbor and support veteran machines. And obviously",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1007s",
        "start_time": "1007.19"
    },
    {
        "id": "052e0f4c",
        "text": "the the agent uh goes through a series of rewards if it does well and of punishment if it does like uh poorly, right? But as I said, we will be focusing on a supervised learning mainly, right? So machine learning itself uh has a bunch of different algorithms so that there's really a ton of these. But like some of the ones like that are are I would say like the most popular are logistic regression, linear regression random forest KNN or K nearest neighbor and support veteran machines. And obviously uh another algorithm that's like very, very fashionable, like at this moment in time, it's uh neural networks or artificial neural networks I I should say, right? So in this case, a neural network is a an algorithm",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1029s",
        "start_time": "1029.65"
    },
    {
        "id": "708bfb8b",
        "text": "right? So machine learning itself uh has a bunch of different algorithms so that there's really a ton of these. But like some of the ones like that are are I would say like the most popular are logistic regression, linear regression random forest KNN or K nearest neighbor and support veteran machines. And obviously uh another algorithm that's like very, very fashionable, like at this moment in time, it's uh neural networks or artificial neural networks I I should say, right? So in this case, a neural network is a an algorithm uh that relies on a bunch of neurons. We I won't be getting into the details because like we'll, we'll, we'll do that like over the next few videos. But for the time being, what it, what it's important for you to understand is that a neural network is structured in a series of layer um neuronal layers",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1046s",
        "start_time": "1046.448"
    },
    {
        "id": "f54b9354",
        "text": "uh another algorithm that's like very, very fashionable, like at this moment in time, it's uh neural networks or artificial neural networks I I should say, right? So in this case, a neural network is a an algorithm uh that relies on a bunch of neurons. We I won't be getting into the details because like we'll, we'll, we'll do that like over the next few videos. But for the time being, what it, what it's important for you to understand is that a neural network is structured in a series of layer um neuronal layers and the different neurons act as computational units, right. So here we have in this neural network, a bunch of different layers. So there's like the first layer that's called input layer, which is the one that we use for actually providing the uh different data into the neural network, then we have the hidden layer",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1073s",
        "start_time": "1073.06"
    },
    {
        "id": "3cbd90ed",
        "text": "uh that relies on a bunch of neurons. We I won't be getting into the details because like we'll, we'll, we'll do that like over the next few videos. But for the time being, what it, what it's important for you to understand is that a neural network is structured in a series of layer um neuronal layers and the different neurons act as computational units, right. So here we have in this neural network, a bunch of different layers. So there's like the first layer that's called input layer, which is the one that we use for actually providing the uh different data into the neural network, then we have the hidden layer which process the information. And finally, we have the last layer that's called the output layer that we usually use to, to get like the inferences from a neural network, right. So it's important to understand that uh uh neural networks are machine learning algorithms because they are the algorithms of choice of deep learning.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1091s",
        "start_time": "1091.829"
    },
    {
        "id": "8fbdb8fe",
        "text": "and the different neurons act as computational units, right. So here we have in this neural network, a bunch of different layers. So there's like the first layer that's called input layer, which is the one that we use for actually providing the uh different data into the neural network, then we have the hidden layer which process the information. And finally, we have the last layer that's called the output layer that we usually use to, to get like the inferences from a neural network, right. So it's important to understand that uh uh neural networks are machine learning algorithms because they are the algorithms of choice of deep learning. So deep learning itself is a subset of machine learning as we've seen",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1114s",
        "start_time": "1114.65"
    },
    {
        "id": "ac4ba95b",
        "text": "which process the information. And finally, we have the last layer that's called the output layer that we usually use to, to get like the inferences from a neural network, right. So it's important to understand that uh uh neural networks are machine learning algorithms because they are the algorithms of choice of deep learning. So deep learning itself is a subset of machine learning as we've seen and it uses deep neural networks. So what's a deep neural network is a neural network that has more than one hidden layer? So that's it. So this is deep learning basically. So using deep neural nets for solving all sorts of like very complicated problems and basically, throughout this series of videos, we're gonna be uh just like coding",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1139s",
        "start_time": "1139.694"
    },
    {
        "id": "d8f1da31",
        "text": "So deep learning itself is a subset of machine learning as we've seen and it uses deep neural networks. So what's a deep neural network is a neural network that has more than one hidden layer? So that's it. So this is deep learning basically. So using deep neural nets for solving all sorts of like very complicated problems and basically, throughout this series of videos, we're gonna be uh just like coding um uh deep neural nets and we're gonna be seeing like different flavors of like neural nets. Cool. So now let's take a look at the actual differences between traditional machine learning and deep learning. So the the main one that comes to mind is that in traditional machine learning, you have, you do a lot of feature engineering.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1165s",
        "start_time": "1165.05"
    },
    {
        "id": "158eb1be",
        "text": "and it uses deep neural networks. So what's a deep neural network is a neural network that has more than one hidden layer? So that's it. So this is deep learning basically. So using deep neural nets for solving all sorts of like very complicated problems and basically, throughout this series of videos, we're gonna be uh just like coding um uh deep neural nets and we're gonna be seeing like different flavors of like neural nets. Cool. So now let's take a look at the actual differences between traditional machine learning and deep learning. So the the main one that comes to mind is that in traditional machine learning, you have, you do a lot of feature engineering. Whereas in deep learning, you have another type of approach which is more end to end. So what do you mean by that? So let me explain because it could be like a little bit like tough, like to understand. So",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1170s",
        "start_time": "1170.92"
    },
    {
        "id": "13acb91d",
        "text": "um uh deep neural nets and we're gonna be seeing like different flavors of like neural nets. Cool. So now let's take a look at the actual differences between traditional machine learning and deep learning. So the the main one that comes to mind is that in traditional machine learning, you have, you do a lot of feature engineering. Whereas in deep learning, you have another type of approach which is more end to end. So what do you mean by that? So let me explain because it could be like a little bit like tough, like to understand. So uh let's let's focus a little bit like on feature engineering. So as we said, like in supervised learning. What we usually do is like we pass a lot of like label data to uh a machine learning algorithm and the algorithm uh learns and extracts like the rules from the data. But uh when I, when I mentioned supervised learning, uh first I ignored",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1198s",
        "start_time": "1198.099"
    },
    {
        "id": "6c2ef266",
        "text": "Whereas in deep learning, you have another type of approach which is more end to end. So what do you mean by that? So let me explain because it could be like a little bit like tough, like to understand. So uh let's let's focus a little bit like on feature engineering. So as we said, like in supervised learning. What we usually do is like we pass a lot of like label data to uh a machine learning algorithm and the algorithm uh learns and extracts like the rules from the data. But uh when I, when I mentioned supervised learning, uh first I ignored on purpose, obviously a an important part of this that happens in traditional machine learning, which is like feature engineering. So let's go back to the idea uh to the example of like extracting onsets or identifying onsets from waveforms. So in that case, what we want to do if we are using, for example, super vector machines or like this more traditional",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1222s",
        "start_time": "1222.31"
    },
    {
        "id": "58c39ebb",
        "text": "uh let's let's focus a little bit like on feature engineering. So as we said, like in supervised learning. What we usually do is like we pass a lot of like label data to uh a machine learning algorithm and the algorithm uh learns and extracts like the rules from the data. But uh when I, when I mentioned supervised learning, uh first I ignored on purpose, obviously a an important part of this that happens in traditional machine learning, which is like feature engineering. So let's go back to the idea uh to the example of like extracting onsets or identifying onsets from waveforms. So in that case, what we want to do if we are using, for example, super vector machines or like this more traditional um ML algorithms is we extract features from the data itself. So in this case, for example, from a waveform, we can extract certain features like zero crossing rates or spectral flux or a bunch of other features. So we extract these features first and then we pass those features into",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1236s",
        "start_time": "1236.229"
    },
    {
        "id": "0f519ba4",
        "text": "on purpose, obviously a an important part of this that happens in traditional machine learning, which is like feature engineering. So let's go back to the idea uh to the example of like extracting onsets or identifying onsets from waveforms. So in that case, what we want to do if we are using, for example, super vector machines or like this more traditional um ML algorithms is we extract features from the data itself. So in this case, for example, from a waveform, we can extract certain features like zero crossing rates or spectral flux or a bunch of other features. So we extract these features first and then we pass those features into uh the machine learning algorithm and then the machine algorithm uh uh at the ML algorithm, hopefully we'll be able to",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1263s",
        "start_time": "1263.689"
    },
    {
        "id": "59893ef5",
        "text": "um ML algorithms is we extract features from the data itself. So in this case, for example, from a waveform, we can extract certain features like zero crossing rates or spectral flux or a bunch of other features. So we extract these features first and then we pass those features into uh the machine learning algorithm and then the machine algorithm uh uh at the ML algorithm, hopefully we'll be able to uh extracts the rules, figure out the rules for doing like this classification problem now. So like in feature engineering like and traditional machine learning, it's very important to understand which features are important for describing a certain problem. So we start from the road data, we extract certain features and we use those features for doing training and then inference obviously. So",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1290s",
        "start_time": "1290.719"
    },
    {
        "id": "b003171a",
        "text": "uh the machine learning algorithm and then the machine algorithm uh uh at the ML algorithm, hopefully we'll be able to uh extracts the rules, figure out the rules for doing like this classification problem now. So like in feature engineering like and traditional machine learning, it's very important to understand which features are important for describing a certain problem. So we start from the road data, we extract certain features and we use those features for doing training and then inference obviously. So in deep learning, it's a little bit different. We have an end to end approach. What do you mean by that? Well, it's uh on a an intuitive level. It's, it's kind of very simple to grasp because basically all we do with deep learning is we pass on raw data directly to the machine learning algorithm. And then without any feature extraction, the",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1316s",
        "start_time": "1316.849"
    },
    {
        "id": "f7fe25cc",
        "text": "uh extracts the rules, figure out the rules for doing like this classification problem now. So like in feature engineering like and traditional machine learning, it's very important to understand which features are important for describing a certain problem. So we start from the road data, we extract certain features and we use those features for doing training and then inference obviously. So in deep learning, it's a little bit different. We have an end to end approach. What do you mean by that? Well, it's uh on a an intuitive level. It's, it's kind of very simple to grasp because basically all we do with deep learning is we pass on raw data directly to the machine learning algorithm. And then without any feature extraction, the deep learning model hopefully will be able to figure out the different rules and change all the different weights uh so that it can do like its classification or regression uh tasks properly. So",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1325s",
        "start_time": "1325.56"
    },
    {
        "id": "8c95593b",
        "text": "in deep learning, it's a little bit different. We have an end to end approach. What do you mean by that? Well, it's uh on a an intuitive level. It's, it's kind of very simple to grasp because basically all we do with deep learning is we pass on raw data directly to the machine learning algorithm. And then without any feature extraction, the deep learning model hopefully will be able to figure out the different rules and change all the different weights uh so that it can do like its classification or regression uh tasks properly. So again, in with feature engineering and traditional machine learning, we just extract features and then we pass features to the uh machine learning algorithm. Whereas uh with uh deep learning, we just pass the road data itself, we don't extract any features. And in the case of onset detection as we, as we sow we just pass the whole waveform directly into the system.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1355s",
        "start_time": "1355.8"
    },
    {
        "id": "276708d6",
        "text": "deep learning model hopefully will be able to figure out the different rules and change all the different weights uh so that it can do like its classification or regression uh tasks properly. So again, in with feature engineering and traditional machine learning, we just extract features and then we pass features to the uh machine learning algorithm. Whereas uh with uh deep learning, we just pass the road data itself, we don't extract any features. And in the case of onset detection as we, as we sow we just pass the whole waveform directly into the system. And, and so this is why this is a an end to end uh approach because basically you just like provide raw data and hopefully you'll get the rules and uh the system, the model is gonna learn directly from that data, discovering the different patterns directly in the data,",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1383s",
        "start_time": "1383.989"
    },
    {
        "id": "6435d4f3",
        "text": "again, in with feature engineering and traditional machine learning, we just extract features and then we pass features to the uh machine learning algorithm. Whereas uh with uh deep learning, we just pass the road data itself, we don't extract any features. And in the case of onset detection as we, as we sow we just pass the whole waveform directly into the system. And, and so this is why this is a an end to end uh approach because basically you just like provide raw data and hopefully you'll get the rules and uh the system, the model is gonna learn directly from that data, discovering the different patterns directly in the data, right. So, so feature engineering uh versus end to end uh is just like one difference between traditional machine learning versus deep learning. There are a bunch of others. So usually with traditional machine learning, you, you can, you can you can make it with a relatively small data set. And here at least we're talking about like thousands of samples. But for deep learning, you really need",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1399s",
        "start_time": "1399.81"
    },
    {
        "id": "f2405848",
        "text": "And, and so this is why this is a an end to end uh approach because basically you just like provide raw data and hopefully you'll get the rules and uh the system, the model is gonna learn directly from that data, discovering the different patterns directly in the data, right. So, so feature engineering uh versus end to end uh is just like one difference between traditional machine learning versus deep learning. There are a bunch of others. So usually with traditional machine learning, you, you can, you can you can make it with a relatively small data set. And here at least we're talking about like thousands of samples. But for deep learning, you really need large data sets. And so here we're talking about hundreds of thousands, if not millions because uh traditional machine learning is way less computational intensive, whereas deep learning is very resource intensive. And so all of these like differences like mm make it like ideal for traditional machine learning to tackle relatively simple problems. Whereas for deep learning, um",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1428s",
        "start_time": "1428.219"
    },
    {
        "id": "ba3ce1f1",
        "text": "right. So, so feature engineering uh versus end to end uh is just like one difference between traditional machine learning versus deep learning. There are a bunch of others. So usually with traditional machine learning, you, you can, you can you can make it with a relatively small data set. And here at least we're talking about like thousands of samples. But for deep learning, you really need large data sets. And so here we're talking about hundreds of thousands, if not millions because uh traditional machine learning is way less computational intensive, whereas deep learning is very resource intensive. And so all of these like differences like mm make it like ideal for traditional machine learning to tackle relatively simple problems. Whereas for deep learning, um its ideal application is with complex problems, deep learning works really really well. For example, with problems that with traditional machine learning, like it's very difficult like to to to tackle like for example, like um image recognition or a bunch of like audio uh A I problems uh like speech recognition or like genre classification and all of this kind of uh of things, right?",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1448s",
        "start_time": "1448.579"
    },
    {
        "id": "7c145487",
        "text": "large data sets. And so here we're talking about hundreds of thousands, if not millions because uh traditional machine learning is way less computational intensive, whereas deep learning is very resource intensive. And so all of these like differences like mm make it like ideal for traditional machine learning to tackle relatively simple problems. Whereas for deep learning, um its ideal application is with complex problems, deep learning works really really well. For example, with problems that with traditional machine learning, like it's very difficult like to to to tackle like for example, like um image recognition or a bunch of like audio uh A I problems uh like speech recognition or like genre classification and all of this kind of uh of things, right? So and",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1478s",
        "start_time": "1478.505"
    },
    {
        "id": "3a4b8255",
        "text": "its ideal application is with complex problems, deep learning works really really well. For example, with problems that with traditional machine learning, like it's very difficult like to to to tackle like for example, like um image recognition or a bunch of like audio uh A I problems uh like speech recognition or like genre classification and all of this kind of uh of things, right? So and so we've seen the differences between machine traditional machine learning and deep learning. So a question that may arise is when should you use deep learning? Well, deep learning isn't always the solution. Well, I would say if you can, you should go with like traditional machine learning approaches first because like they are cheaper and they are quicker",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1508s",
        "start_time": "1508.689"
    },
    {
        "id": "301b027f",
        "text": "So and so we've seen the differences between machine traditional machine learning and deep learning. So a question that may arise is when should you use deep learning? Well, deep learning isn't always the solution. Well, I would say if you can, you should go with like traditional machine learning approaches first because like they are cheaper and they are quicker all around. So when should you use deep learning? So if you have a very large data set, then probably it makes sense like to have deep learning algorithms algorithms instead of machine traditional machine learning ones. And then if you are tackling a complex problem where traditional machine learning fails, then it, it's advisable to move on and try to use deep learning. And then",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1537s",
        "start_time": "1537.489"
    },
    {
        "id": "74ed9632",
        "text": "so we've seen the differences between machine traditional machine learning and deep learning. So a question that may arise is when should you use deep learning? Well, deep learning isn't always the solution. Well, I would say if you can, you should go with like traditional machine learning approaches first because like they are cheaper and they are quicker all around. So when should you use deep learning? So if you have a very large data set, then probably it makes sense like to have deep learning algorithms algorithms instead of machine traditional machine learning ones. And then if you are tackling a complex problem where traditional machine learning fails, then it, it's advisable to move on and try to use deep learning. And then you should have access to extensive computational resources, uh specifically uh GP us that you use deep learning. And one last thing that I would add here is that obviously you should have the expertise to tackle uh deep problems with deep learning because like they are",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1539s",
        "start_time": "1539.9"
    },
    {
        "id": "dde7de6a",
        "text": "all around. So when should you use deep learning? So if you have a very large data set, then probably it makes sense like to have deep learning algorithms algorithms instead of machine traditional machine learning ones. And then if you are tackling a complex problem where traditional machine learning fails, then it, it's advisable to move on and try to use deep learning. And then you should have access to extensive computational resources, uh specifically uh GP us that you use deep learning. And one last thing that I would add here is that obviously you should have the expertise to tackle uh deep problems with deep learning because like they are uh it's kind of like quite com it can get complicated and you want to have a very good understanding of how to like tweak parameters so that your deep learning like models are gonna work decently, right? Cool. So um given like, as I mentioned in my previous video, I'm gonna be focusing on audio like in music. So applications in deep learning",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1563s",
        "start_time": "1563.339"
    },
    {
        "id": "3af17f91",
        "text": "you should have access to extensive computational resources, uh specifically uh GP us that you use deep learning. And one last thing that I would add here is that obviously you should have the expertise to tackle uh deep problems with deep learning because like they are uh it's kind of like quite com it can get complicated and you want to have a very good understanding of how to like tweak parameters so that your deep learning like models are gonna work decently, right? Cool. So um given like, as I mentioned in my previous video, I'm gonna be focusing on audio like in music. So applications in deep learning uh for audio music. So here I just wanted to give you a little bit of an idea of what we can do with deep learning in audio. So we can use deep learning for uh tasks like speech recognition for uh classifying emotions just by looking at or analyzing voice or for doing things like no recognition.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1590s",
        "start_time": "1590.65"
    },
    {
        "id": "3ce3bf64",
        "text": "uh it's kind of like quite com it can get complicated and you want to have a very good understanding of how to like tweak parameters so that your deep learning like models are gonna work decently, right? Cool. So um given like, as I mentioned in my previous video, I'm gonna be focusing on audio like in music. So applications in deep learning uh for audio music. So here I just wanted to give you a little bit of an idea of what we can do with deep learning in audio. So we can use deep learning for uh tasks like speech recognition for uh classifying emotions just by looking at or analyzing voice or for doing things like no recognition. And in the music side of things that sometimes gets called music information retrieval. We can use deep learning to extract or to classify music genre to understand the different types of instruments that we have in a song and then also for doing mood classification. So we have a bunch of different songs and we should decide, is this a happy song or is this a sad song?",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1610s",
        "start_time": "1610.77"
    },
    {
        "id": "5a700b8b",
        "text": "uh for audio music. So here I just wanted to give you a little bit of an idea of what we can do with deep learning in audio. So we can use deep learning for uh tasks like speech recognition for uh classifying emotions just by looking at or analyzing voice or for doing things like no recognition. And in the music side of things that sometimes gets called music information retrieval. We can use deep learning to extract or to classify music genre to understand the different types of instruments that we have in a song and then also for doing mood classification. So we have a bunch of different songs and we should decide, is this a happy song or is this a sad song? And then we can use deep learning for doing music tagging. So basically to, to, to put tags onto different songs. And then the thing like, that's the most exciting for me and the work that I've also done like at Melo Drive.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1638s",
        "start_time": "1638.5"
    },
    {
        "id": "8479a243",
        "text": "And in the music side of things that sometimes gets called music information retrieval. We can use deep learning to extract or to classify music genre to understand the different types of instruments that we have in a song and then also for doing mood classification. So we have a bunch of different songs and we should decide, is this a happy song or is this a sad song? And then we can use deep learning for doing music tagging. So basically to, to, to put tags onto different songs. And then the thing like, that's the most exciting for me and the work that I've also done like at Melo Drive. So it's the application of deep learning for music generation. So you use system, you, you train a model that can generate music by itself and that's like super exciting stuff. And hopefully we'll be seeing like a little bit of this like down the line uh during uh the series, right? So we're almost at the end of this quite long video and I want just like to leave you with a few takeaway points. So",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1662s",
        "start_time": "1662.109"
    },
    {
        "id": "e1127bb9",
        "text": "And then we can use deep learning for doing music tagging. So basically to, to, to put tags onto different songs. And then the thing like, that's the most exciting for me and the work that I've also done like at Melo Drive. So it's the application of deep learning for music generation. So you use system, you, you train a model that can generate music by itself and that's like super exciting stuff. And hopefully we'll be seeing like a little bit of this like down the line uh during uh the series, right? So we're almost at the end of this quite long video and I want just like to leave you with a few takeaway points. So first of all, what's artificial intelligence? Well, artificial intelligence is the art and science of building a rational agents that acts to achieve their goals, given the environment and the beliefs they are in.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1687s",
        "start_time": "1687.819"
    },
    {
        "id": "ad87569f",
        "text": "So it's the application of deep learning for music generation. So you use system, you, you train a model that can generate music by itself and that's like super exciting stuff. And hopefully we'll be seeing like a little bit of this like down the line uh during uh the series, right? So we're almost at the end of this quite long video and I want just like to leave you with a few takeaway points. So first of all, what's artificial intelligence? Well, artificial intelligence is the art and science of building a rational agents that acts to achieve their goals, given the environment and the beliefs they are in. So machine learning is a subset of artificial intelligence, don't make the mistake that many people do. And it's so annoying when they say yeah A I and they use it interchangeably with ML. It's not true A I is way more than just machine learning. And machine learning is a subset of A I. Then there are different flavors of machine learning.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1703s",
        "start_time": "1703.81"
    },
    {
        "id": "e7e7ce2b",
        "text": "first of all, what's artificial intelligence? Well, artificial intelligence is the art and science of building a rational agents that acts to achieve their goals, given the environment and the beliefs they are in. So machine learning is a subset of artificial intelligence, don't make the mistake that many people do. And it's so annoying when they say yeah A I and they use it interchangeably with ML. It's not true A I is way more than just machine learning. And machine learning is a subset of A I. Then there are different flavors of machine learning. Uh And within three here. So one is supervised learning, which we'll be focusing on the most, then we have unsupervised learning and then we have reinforcement learning and then machine learning has a ton of different algorithms. So linear regression support vector machines, but mainly at least like for us, neural networks",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1733s",
        "start_time": "1733.15"
    },
    {
        "id": "39fc5a27",
        "text": "So machine learning is a subset of artificial intelligence, don't make the mistake that many people do. And it's so annoying when they say yeah A I and they use it interchangeably with ML. It's not true A I is way more than just machine learning. And machine learning is a subset of A I. Then there are different flavors of machine learning. Uh And within three here. So one is supervised learning, which we'll be focusing on the most, then we have unsupervised learning and then we have reinforcement learning and then machine learning has a ton of different algorithms. So linear regression support vector machines, but mainly at least like for us, neural networks and neural networks are used by deep learning. And in deep learning, we use deep neural networks which are neural networks that have more than one hidden layer. And remember, deep learning is a subset of machine learning.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1749s",
        "start_time": "1749.849"
    },
    {
        "id": "36fff0bd",
        "text": "Uh And within three here. So one is supervised learning, which we'll be focusing on the most, then we have unsupervised learning and then we have reinforcement learning and then machine learning has a ton of different algorithms. So linear regression support vector machines, but mainly at least like for us, neural networks and neural networks are used by deep learning. And in deep learning, we use deep neural networks which are neural networks that have more than one hidden layer. And remember, deep learning is a subset of machine learning. And finally, so even though deep learning is super fashionable nowadays, deep learning isn't always the way to go try the simple things first and then if they fail. So if super vector machines fail, if a simple logistic regression fails then move on to deep learning. But don't use it like as a as a go to solution, right? So",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1771s",
        "start_time": "1771.66"
    },
    {
        "id": "4d4bec7b",
        "text": "and neural networks are used by deep learning. And in deep learning, we use deep neural networks which are neural networks that have more than one hidden layer. And remember, deep learning is a subset of machine learning. And finally, so even though deep learning is super fashionable nowadays, deep learning isn't always the way to go try the simple things first and then if they fail. So if super vector machines fail, if a simple logistic regression fails then move on to deep learning. But don't use it like as a as a go to solution, right? So uh this was it like for this video. So in the next video, we're gonna talk about neurons which are the units which make up neural networks and we are gonna be",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1797s",
        "start_time": "1797.729"
    },
    {
        "id": "aef1caf3",
        "text": "And finally, so even though deep learning is super fashionable nowadays, deep learning isn't always the way to go try the simple things first and then if they fail. So if super vector machines fail, if a simple logistic regression fails then move on to deep learning. But don't use it like as a as a go to solution, right? So uh this was it like for this video. So in the next video, we're gonna talk about neurons which are the units which make up neural networks and we are gonna be coding a neuron from scratch ourselves. So thank you very much for watching this video. And if you liked it, you can subscribe and please like it and I'll see you next time. Cheers.",
        "video": "2- AI, machine learning and deep learning",
        "playlist": "Audio Deep Learning with Python",
        "youtube_video_id": "1LLxZ35ru_g",
        "youtube_link": "https://www.youtube.com/watch?v=1LLxZ35ru_g&t=1814s",
        "start_time": "1814.81"
    }
]